{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 11764513,
          "sourceType": "datasetVersion",
          "datasetId": 612177
        }
      ],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a0975823266c4c3787a73ff5814ac877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2dca892bcc34f47836d0467ea7123fc",
              "IPY_MODEL_393248a126574b02b73bf9299934b610",
              "IPY_MODEL_cdd54c527262459a9a59b4ee7fd3f657"
            ],
            "layout": "IPY_MODEL_c4e1fe83aadf4542abd118fb426b6e35"
          }
        },
        "a2dca892bcc34f47836d0467ea7123fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c02638d6a9b44518a096c90cbe7ee098",
            "placeholder": "​",
            "style": "IPY_MODEL_6d48fcfd6c134c8590f6df91991f6736",
            "value": "Batches: 100%"
          }
        },
        "393248a126574b02b73bf9299934b610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1433340d99b47bab41c396bf1360d6f",
            "max": 51,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbbf8a01037b4121a135433a3e75865e",
            "value": 51
          }
        },
        "cdd54c527262459a9a59b4ee7fd3f657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58af5b76ddc14d01b813a6805198ef73",
            "placeholder": "​",
            "style": "IPY_MODEL_40a94900fb164231bba8ab32de7ac8f4",
            "value": " 51/51 [00:02&lt;00:00, 26.64it/s]"
          }
        },
        "c4e1fe83aadf4542abd118fb426b6e35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c02638d6a9b44518a096c90cbe7ee098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d48fcfd6c134c8590f6df91991f6736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1433340d99b47bab41c396bf1360d6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbbf8a01037b4121a135433a3e75865e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58af5b76ddc14d01b813a6805198ef73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40a94900fb164231bba8ab32de7ac8f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "import kagglehub\n",
        "organizations_cornell_university_arxiv_path = kagglehub.dataset_download('cornell-university/arxiv')"
      ],
      "metadata": {
        "id": "Czhr9vUTcYLg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "301fdb99-5fe8-4d0e-a2c0-74e3536c2185"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/cornell-university/arxiv?dataset_version_number=232...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.44G/1.44G [00:14<00:00, 105MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling arXiv Abstracts with BERTopic\n",
        "\n",
        "This problem consists of grouping a large amount of unseen & unlabeled research papers from arXiv based on their keywords found in the abstract section. The tool being utilized is BERTopic which consists of modular layers that can be customized or simply use the default settings.\n",
        "<br>\n",
        "<br>\n",
        "Note: this python notebook is best utilized in Kaggle because of the access to 30GB RAM memory and 2x T4 GPUs\n",
        "\n",
        "# Quick Overview\n",
        "\n",
        "1) Loading data (with Category selection)\n",
        "2) Pre-process the data\n",
        "3) Default BERTopic (with examples)\n",
        "4) Pre-compute embeddings\n",
        "5) Fine-tune BERTopic Layers\n",
        "    - Embedding\n",
        "    - Clustering\n",
        "    - Tokenization of topics\n",
        "    - Weight tokens\n",
        "    - Representation of topics\n",
        "6) Custom BERTopic model and fitting\n",
        "7) Outputs\n",
        "8) Visualizations"
      ],
      "metadata": {
        "id": "qPw0WibecYLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bertopic\n",
        "\n",
        "# the following resolves warning with .fit_transform() method\n",
        "!pip install -q huggingface_hub[hf_xet]\n",
        "!pip install -q hf_xet\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T20:40:27.122744Z",
          "iopub.execute_input": "2025-05-13T20:40:27.123571Z",
          "iopub.status.idle": "2025-05-13T20:43:24.222001Z",
          "shell.execute_reply.started": "2025-05-13T20:40:27.12354Z",
          "shell.execute_reply": "2025-05-13T20:43:24.221115Z"
        },
        "id": "KbMVWtRIcYLk",
        "outputId": "8f933df2-f832-47c5-a1b7-06c4e5053df6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "\n",
        "import os\n",
        "file_path = ''\n",
        "for dirname, _, filenames in os.walk(organizations_cornell_university_arxiv_path):\n",
        "    for filename in filenames:\n",
        "        file_path = os.path.join(dirname, filename)\n",
        "        print('file_path:', file_path)\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\""
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T20:43:24.224068Z",
          "iopub.execute_input": "2025-05-13T20:43:24.224844Z",
          "iopub.status.idle": "2025-05-13T20:43:24.233184Z",
          "shell.execute_reply.started": "2025-05-13T20:43:24.224819Z",
          "shell.execute_reply": "2025-05-13T20:43:24.232496Z"
        },
        "id": "yZfS7x4ncYLm",
        "outputId": "28d41b29-0a4f-4460-f161-59934b283a7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file_path: /root/.cache/kagglehub/datasets/cornell-university/arxiv/versions/232/arxiv-metadata-oai-snapshot.json\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data\n",
        "#### (Specific category: cs.AI)\n",
        "\n",
        "The data is in a json file which contains different types of information including submitter, authors, title, comments, category, and the abstract. The total number of articles is 2,725,401 while specific categories contain a managable amount of articles ideal for limited memory (e.g. cs.AI has 12,180 articles)."
      ],
      "metadata": {
        "id": "ClMeCmMCcYLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Examples of Categories: cs.AI, stat.ML, cs.LG (aka Machine Learning)\n",
        "category_desired = 'stat.ML'\n",
        "papers = []\n",
        "with open(file_path, 'r') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        paper = json.loads(line)\n",
        "        if paper.get('categories') == category_desired:\n",
        "            papers.append(paper)\n",
        "print(\"Number of papers in\", category_desired, \":\", len(papers))\n",
        "\n",
        "# Convert list of papers to a DataFrame\n",
        "df = pd.DataFrame(papers)\n",
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T20:43:24.234142Z",
          "iopub.execute_input": "2025-05-13T20:43:24.2344Z",
          "iopub.status.idle": "2025-05-13T20:44:38.405823Z",
          "shell.execute_reply.started": "2025-05-13T20:43:24.23438Z",
          "shell.execute_reply": "2025-05-13T20:44:38.404991Z"
        },
        "id": "adTbChfPcYLm",
        "outputId": "3fd2ac4f-7277-4a02-b025-4e6cd4bf79f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of papers in stat.ML : 1601\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id              submitter  \\\n",
              "0  0705.2363         Marten Wegkamp   \n",
              "1  0706.3499  Bharath Sriperumbudur   \n",
              "2  0707.3536   Patrick Erik Bradley   \n",
              "3  0707.4072   Patrick Erik Bradley   \n",
              "4  0708.2377        Roberto Alamino   \n",
              "\n",
              "                                             authors  \\\n",
              "0                                     Marten Wegkamp   \n",
              "1  Bharath K. Sriperumbudur and Gert R. G. Lanckriet   \n",
              "2                               Patrick Erik Bradley   \n",
              "3                               Patrick Erik Bradley   \n",
              "4                 Roberto C. Alamino, Nestor Caticha   \n",
              "\n",
              "                                               title  \\\n",
              "0        Lasso type classifiers with a reject option   \n",
              "1  Metric Embedding for Nearest Neighbor Classifi...   \n",
              "2               Degenerating families of dendrograms   \n",
              "3                            Families of dendrograms   \n",
              "4   Online Learning in Discrete Hidden Markov Models   \n",
              "\n",
              "                                            comments  \\\n",
              "0  Published at http://dx.doi.org/10.1214/07-EJS0...   \n",
              "1                                   9 pages, 1 table   \n",
              "2                                13 pages, 8 figures   \n",
              "3  7 pages, 3 figures. To appear in: Proceedings ...   \n",
              "4                                 8 pages, 6 figures   \n",
              "\n",
              "                                         journal-ref  \\\n",
              "0  Electronic Journal of Statistics 2007, Vol. 1,...   \n",
              "1                                               None   \n",
              "2                       J. Classif. 25, 27-42 (2008)   \n",
              "3                                               None   \n",
              "4                                               None   \n",
              "\n",
              "                            doi            report-no categories license  \\\n",
              "0             10.1214/07-EJS058  IMS-EJS-EJS_2007_58    stat.ML    None   \n",
              "1                          None                 None    stat.ML    None   \n",
              "2     10.1007/s00357-008-9009-5                 None    stat.ML    None   \n",
              "3  10.1007/978-3-540-78246-9_12                 None    stat.ML    None   \n",
              "4             10.1063/1.2423274                 None    stat.ML    None   \n",
              "\n",
              "                                            abstract  \\\n",
              "0    We consider the problem of binary classifica...   \n",
              "1    The distance metric plays an important role ...   \n",
              "2    Dendrograms used in data analysis are ultram...   \n",
              "3    A conceptual framework for cluster analysis ...   \n",
              "4    We present and analyse three online algorith...   \n",
              "\n",
              "                                            versions update_date  \\\n",
              "0  [{'version': 'v1', 'created': 'Wed, 16 May 200...  2009-09-29   \n",
              "1  [{'version': 'v1', 'created': 'Sun, 24 Jun 200...  2007-06-26   \n",
              "2  [{'version': 'v1', 'created': 'Tue, 24 Jul 200...  2008-06-28   \n",
              "3  [{'version': 'v1', 'created': 'Fri, 27 Jul 200...  2009-12-01   \n",
              "4  [{'version': 'v1', 'created': 'Fri, 17 Aug 200...  2007-08-20   \n",
              "\n",
              "                                      authors_parsed  \n",
              "0                              [[Wegkamp, Marten, ]]  \n",
              "1  [[Sriperumbudur, Bharath K., ], [Lanckriet, Ge...  \n",
              "2                        [[Bradley, Patrick Erik, ]]  \n",
              "3                        [[Bradley, Patrick Erik, ]]  \n",
              "4     [[Alamino, Roberto C., ], [Caticha, Nestor, ]]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d43e7e2b-94e0-46d1-bc95-1fe13bb5a83d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>submitter</th>\n",
              "      <th>authors</th>\n",
              "      <th>title</th>\n",
              "      <th>comments</th>\n",
              "      <th>journal-ref</th>\n",
              "      <th>doi</th>\n",
              "      <th>report-no</th>\n",
              "      <th>categories</th>\n",
              "      <th>license</th>\n",
              "      <th>abstract</th>\n",
              "      <th>versions</th>\n",
              "      <th>update_date</th>\n",
              "      <th>authors_parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0705.2363</td>\n",
              "      <td>Marten Wegkamp</td>\n",
              "      <td>Marten Wegkamp</td>\n",
              "      <td>Lasso type classifiers with a reject option</td>\n",
              "      <td>Published at http://dx.doi.org/10.1214/07-EJS0...</td>\n",
              "      <td>Electronic Journal of Statistics 2007, Vol. 1,...</td>\n",
              "      <td>10.1214/07-EJS058</td>\n",
              "      <td>IMS-EJS-EJS_2007_58</td>\n",
              "      <td>stat.ML</td>\n",
              "      <td>None</td>\n",
              "      <td>We consider the problem of binary classifica...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Wed, 16 May 200...</td>\n",
              "      <td>2009-09-29</td>\n",
              "      <td>[[Wegkamp, Marten, ]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0706.3499</td>\n",
              "      <td>Bharath Sriperumbudur</td>\n",
              "      <td>Bharath K. Sriperumbudur and Gert R. G. Lanckriet</td>\n",
              "      <td>Metric Embedding for Nearest Neighbor Classifi...</td>\n",
              "      <td>9 pages, 1 table</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>stat.ML</td>\n",
              "      <td>None</td>\n",
              "      <td>The distance metric plays an important role ...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Sun, 24 Jun 200...</td>\n",
              "      <td>2007-06-26</td>\n",
              "      <td>[[Sriperumbudur, Bharath K., ], [Lanckriet, Ge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0707.3536</td>\n",
              "      <td>Patrick Erik Bradley</td>\n",
              "      <td>Patrick Erik Bradley</td>\n",
              "      <td>Degenerating families of dendrograms</td>\n",
              "      <td>13 pages, 8 figures</td>\n",
              "      <td>J. Classif. 25, 27-42 (2008)</td>\n",
              "      <td>10.1007/s00357-008-9009-5</td>\n",
              "      <td>None</td>\n",
              "      <td>stat.ML</td>\n",
              "      <td>None</td>\n",
              "      <td>Dendrograms used in data analysis are ultram...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Tue, 24 Jul 200...</td>\n",
              "      <td>2008-06-28</td>\n",
              "      <td>[[Bradley, Patrick Erik, ]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0707.4072</td>\n",
              "      <td>Patrick Erik Bradley</td>\n",
              "      <td>Patrick Erik Bradley</td>\n",
              "      <td>Families of dendrograms</td>\n",
              "      <td>7 pages, 3 figures. To appear in: Proceedings ...</td>\n",
              "      <td>None</td>\n",
              "      <td>10.1007/978-3-540-78246-9_12</td>\n",
              "      <td>None</td>\n",
              "      <td>stat.ML</td>\n",
              "      <td>None</td>\n",
              "      <td>A conceptual framework for cluster analysis ...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Fri, 27 Jul 200...</td>\n",
              "      <td>2009-12-01</td>\n",
              "      <td>[[Bradley, Patrick Erik, ]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0708.2377</td>\n",
              "      <td>Roberto Alamino</td>\n",
              "      <td>Roberto C. Alamino, Nestor Caticha</td>\n",
              "      <td>Online Learning in Discrete Hidden Markov Models</td>\n",
              "      <td>8 pages, 6 figures</td>\n",
              "      <td>None</td>\n",
              "      <td>10.1063/1.2423274</td>\n",
              "      <td>None</td>\n",
              "      <td>stat.ML</td>\n",
              "      <td>None</td>\n",
              "      <td>We present and analyse three online algorith...</td>\n",
              "      <td>[{'version': 'v1', 'created': 'Fri, 17 Aug 200...</td>\n",
              "      <td>2007-08-20</td>\n",
              "      <td>[[Alamino, Roberto C., ], [Caticha, Nestor, ]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d43e7e2b-94e0-46d1-bc95-1fe13bb5a83d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d43e7e2b-94e0-46d1-bc95-1fe13bb5a83d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d43e7e2b-94e0-46d1-bc95-1fe13bb5a83d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7baad160-1d8e-492a-8ee5-35461add25be\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7baad160-1d8e-492a-8ee5-35461add25be')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7baad160-1d8e-492a-8ee5-35461add25be button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1601,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1601,\n        \"samples\": [\n          \"1412.2295\",\n          \"1311.0219\",\n          \"1108.1483\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"submitter\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1104,\n        \"samples\": [\n          \"Ville Tolvanen\",\n          \"Andreas Doerr\",\n          \"Jason Jo\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1498,\n        \"samples\": [\n          \"Sigurd L{\\\\o}kse, Filippo Maria Bianchi, Arnt-B{\\\\o}rre Salberg, Robert\\n  Jenssen\",\n          \"Adri\\\\'an P\\\\'erez-Suay, Valero Laparra, Gonzalo Mateo-Garc\\\\'ia, Jordi\\n  Mu\\\\~noz-Mar\\\\'i, Luis G\\\\'omez-Chova, and Gustau Camps-Valls\",\n          \"Makoto Yamada, Denny Wu, Yao-Hung Hubert Tsai, Ichiro Takeuchi, Ruslan\\n  Salakhutdinov, Kenji Fukumizu\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1601,\n        \"samples\": [\n          \"A Likelihood Ratio Framework for High Dimensional Semiparametric\\n  Regression\",\n          \"Joint Estimation of Multiple Graphical Models from High Dimensional Time\\n  Series\",\n          \"Algebraic Geometric Comparison of Probability Distributions\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 764,\n        \"samples\": [\n          \"21 pages. arXiv admin note: substantial text overlap with\\n  arXiv:1502.07685\",\n          \"PhD thesis. Source code available at\\n  https://github.com/glouppe/phd-thesis\",\n          \"no comments\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"journal-ref\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 187,\n        \"samples\": [\n          \"Palar, Pramudita Satria, and Koji Shimoyama. \\\"On efficient global\\n  optimization via universal Kriging surrogate models.\\\" Structural and\\n  Multidisciplinary Optimization (2017): 1-21\",\n          \"JMLR W&CP, vol 37, 2015\",\n          \"Advances in Neural Information Processing Systems 26 [NIPS 2013],\\n  pp.1619--1627\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doi\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 212,\n        \"samples\": [\n          \"10.1109/JSTSP.2012.2233712\",\n          \"10.1002/sta4.162\",\n          \"10.1007/978-3-030-64221-1_16\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"report-no\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          \"IMS-EJS-EJS_2007_58\",\n          \"IMS-EJS-EJS_2007_39\",\n          \"IMS-AOS-AOS1238\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"stat.ML\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"license\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"http://creativecommons.org/licenses/by/3.0/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1601,\n        \"samples\": [\n          \"  We propose a likelihood ratio based inferential framework for high\\ndimensional semiparametric generalized linear models. This framework addresses\\na variety of challenging problems in high dimensional data analysis, including\\nincomplete data, selection bias, and heterogeneous multitask learning. Our work\\nhas three main contributions. (i) We develop a regularized statistical\\nchromatography approach to infer the parameter of interest under the proposed\\nsemiparametric generalized linear model without the need of estimating the\\nunknown base measure function. (ii) We propose a new framework to construct\\npost-regularization confidence regions and tests for the low dimensional\\ncomponents of high dimensional parameters. Unlike existing post-regularization\\ninferential methods, our approach is based on a novel directional likelihood.\\nIn particular, the framework naturally handles generic regularized estimators\\nwith nonconvex penalty functions and it can be used to infer least false\\nparameters under misspecified models. (iii) We develop new concentration\\ninequalities and normal approximation results for U-statistics with unbounded\\nkernels, which are of independent interest. We demonstrate the consequences of\\nthe general theory by using an example of missing data problem. Extensive\\nsimulation studies and real data analysis are provided to illustrate our\\nproposed approach.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"versions\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"update_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1051,\n        \"samples\": [\n          \"2018-03-13\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors_parsed\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-process the data\n",
        "\n",
        "(add small description)"
      ],
      "metadata": {
        "id": "Ch_fGeNOcYLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing or very short abstracts\n",
        "df = df.dropna(subset=['abstract'])\n",
        "df = df[df['abstract'].str.len() > 100]\n",
        "\n",
        "# Extract just the abstracts & lowercase\n",
        "abstracts = df['abstract'].tolist()\n",
        "abstracts = [doc.lower() for doc in abstracts]\n",
        "\n",
        "# Check how many abstracts are ready\n",
        "print(f\"Number of cleaned abstracts: {len(abstracts)}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T20:44:38.406884Z",
          "iopub.execute_input": "2025-05-13T20:44:38.407505Z",
          "iopub.status.idle": "2025-05-13T20:44:38.43083Z",
          "shell.execute_reply.started": "2025-05-13T20:44:38.407477Z",
          "shell.execute_reply": "2025-05-13T20:44:38.430131Z"
        },
        "id": "G3-k81bncYLn",
        "outputId": "8bd035c7-f3db-4ce3-e636-7bdf4ba9f4b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of cleaned abstracts: 1601\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# We will remove stopwords (e.g. the, at, are)\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    # convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # only keep letters and space, so we drop punctuation/symbols/numbers\n",
        "    text = ''.join(ch for ch in text if ch.isalpha() or ch.isspace())\n",
        "    words = text.split()\n",
        "\n",
        "    # parse out stopwords keeping keywords and words with small length\n",
        "    words = [w for w in words if w not in stop_words and len(w) > 2]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply to a list of abstracts\n",
        "abstracts = [preprocess(doc) for doc in abstracts]\n",
        "abstracts[:3]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T20:44:38.431735Z",
          "iopub.execute_input": "2025-05-13T20:44:38.432046Z",
          "iopub.status.idle": "2025-05-13T20:44:39.251132Z",
          "shell.execute_reply.started": "2025-05-13T20:44:38.432017Z",
          "shell.execute_reply": "2025-05-13T20:44:39.250386Z"
        },
        "id": "tsYYGkVjcYLn",
        "outputId": "7efc9c77-6fe7-4427-b19e-f8da07c22295",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['consider problem binary classification one particular cost choose classify observation present simple proof oracle inequality excess risk structural risk minimizers using lasso type penalty',\n",
              " 'distance metric plays important role nearest neighbor classification usually euclidean distance metric assumed mahalanobis distance metric optimized improve performance paper study problem embedding arbitrary metric spaces euclidean space goal improve accuracy classifier propose solution appealing framework regularization reproducing kernel hilbert space prove representerlike theorem classification embedding function determined solving semidefinite program interesting connection softmargin linear binary support vector machine classifier although main focus paper present general theoretical framework metric embedding setting demonstrate performance proposed method benchmark datasets show performs better mahalanobis metric learning algorithm terms leaveoneout generalization errors',\n",
              " 'dendrograms used data analysis ultrametric spaces hence objects nonarchimedean geometry known exist padic representation dendrograms completed point infinity viewed subtrees bruhattits tree associated padic projective line implications certain moduli spaces known algebraic geometry padic parameter spaces families dendrograms stochastic classification also handled within framework end calculate topology hidden part dendrogram']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of how a desired outcome should look:\n",
        "['note formally describe functionality calculate valid domain bdd represent solution space valid configuration formalization largely base clab configuration framework', <br>\n",
        " 'motivation profile hidden markov model phmms popular useful tool detection remote homologue protein family unfortunately performance satisfactory protein twilight zone ...']"
      ],
      "metadata": {
        "id": "nU-PyOUMcYLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Default BERTopic"
      ],
      "metadata": {
        "id": "yxJYw9BLcYLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = BERTopic()\n",
        "topics, probs = topic_model.fit_transform(abstracts[:1000])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T20:59:39.054671Z",
          "iopub.execute_input": "2025-05-13T20:59:39.055405Z",
          "iopub.status.idle": "2025-05-13T21:00:14.051179Z",
          "shell.execute_reply.started": "2025-05-13T20:59:39.055375Z",
          "shell.execute_reply": "2025-05-13T21:00:14.049768Z"
        },
        "id": "-vewy5nycYLo"
      },
      "outputs": [],
      "execution_count": 80
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:00:14.053188Z",
          "iopub.execute_input": "2025-05-13T21:00:14.053542Z",
          "iopub.status.idle": "2025-05-13T21:00:14.086452Z",
          "shell.execute_reply.started": "2025-05-13T21:00:14.053506Z",
          "shell.execute_reply": "2025-05-13T21:00:14.085534Z"
        },
        "id": "-NsiSEdOcYLo",
        "outputId": "ee33fff3-7f92-4510-d6f2-8ffbfc28ed8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Topic  Count                                         Name  \\\n",
              "0      -1    386               -1_data_model_algorithm_models   \n",
              "1       0     56            0_graph_graphs_network_clustering   \n",
              "2       1     52             1_matrix_subspace_algorithm_data   \n",
              "3       2     52              2_lasso_sparse_dictionary_group   \n",
              "4       3     51       3_gaussian_processes_process_inference   \n",
              "5       4     48            4_kernel_kernels_reproducing_test   \n",
              "6       5     44              5_dirichlet_topic_process_model   \n",
              "7       6     42         6_brain_data_neuroimaging_functional   \n",
              "8       7     38           7_clustering_cluster_clusters_data   \n",
              "9       8     34           8_graphical_models_structure_graph   \n",
              "10      9     27     9_causal_variables_acyclic_observational   \n",
              "11     10     26         10_optimization_policy_regret_demand   \n",
              "12     11     24                    11_svm_dwd_losses_machine   \n",
              "13     12     23                 12_carlo_monte_sampling_mcmc   \n",
              "14     13     22          13_trees_ensembles_ensemble_forests   \n",
              "15     14     18  14_screening_correlation_features_variables   \n",
              "16     15     17          15_manifold_embedding_manifolds_lle   \n",
              "17     16     16  16_variational_inference_gradient_gradients   \n",
              "18     17     14            17_deep_dropout_networks_bayesian   \n",
              "19     18     10      18_crossvalidation_risk_empirical_error   \n",
              "\n",
              "                                       Representation  \\\n",
              "0   [data, model, algorithm, models, learning, met...   \n",
              "1   [graph, graphs, network, clustering, nodes, sp...   \n",
              "2   [matrix, subspace, algorithm, data, completion...   \n",
              "3   [lasso, sparse, dictionary, group, sparsity, r...   \n",
              "4   [gaussian, processes, process, inference, mode...   \n",
              "5   [kernel, kernels, reproducing, test, density, ...   \n",
              "6   [dirichlet, topic, process, model, models, inf...   \n",
              "7   [brain, data, neuroimaging, functional, analys...   \n",
              "8   [clustering, cluster, clusters, data, method, ...   \n",
              "9   [graphical, models, structure, graph, model, v...   \n",
              "10  [causal, variables, acyclic, observational, ef...   \n",
              "11  [optimization, policy, regret, demand, constra...   \n",
              "12  [svm, dwd, losses, machine, classification, cl...   \n",
              "13  [carlo, monte, sampling, mcmc, hmc, algorithm,...   \n",
              "14  [trees, ensembles, ensemble, forests, tree, ra...   \n",
              "15  [screening, correlation, features, variables, ...   \n",
              "16  [manifold, embedding, manifolds, lle, data, me...   \n",
              "17  [variational, inference, gradient, gradients, ...   \n",
              "18  [deep, dropout, networks, bayesian, hidden, ne...   \n",
              "19  [crossvalidation, risk, empirical, error, pena...   \n",
              "\n",
              "                                  Representative_Docs  \n",
              "0   [paper study statistical properties semisuperv...  \n",
              "1   [partitioning graph groups vertices within gro...  \n",
              "2   [due challenging applications collaborative fi...  \n",
              "3   [study problem learning sparse linear regressi...  \n",
              "4   [study gaussian process regression model conte...  \n",
              "5   [paper propose family tractable kernels dense ...  \n",
              "6   [nonparametric mixture models based dirichlet ...  \n",
              "7   [brain decoding involves determination subject...  \n",
              "8   [mean shift clustering finds modes data probab...  \n",
              "9   [propose new class semiparametric exponential ...  \n",
              "10  [consider learning causal ordering variables l...  \n",
              "11  [work presents pesmoc predictive entropy searc...  \n",
              "12  [distance weighted discrimination dwd marginba...  \n",
              "13  [propose novel sampling framework inference pr...  \n",
              "14  [tree ensembles random forest boosted trees re...  \n",
              "15  [introduce new approach variable selection cal...  \n",
              "16  [paper presents new framework manifold learnin...  \n",
              "17  [stochastic variational inference relatively w...  \n",
              "18  [since learning typically slow boltzmann machi...  \n",
              "19  [article derive concentration inequalities cro...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f27fff5e-665c-4b64-b825-30d11fdc15bc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic</th>\n",
              "      <th>Count</th>\n",
              "      <th>Name</th>\n",
              "      <th>Representation</th>\n",
              "      <th>Representative_Docs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>386</td>\n",
              "      <td>-1_data_model_algorithm_models</td>\n",
              "      <td>[data, model, algorithm, models, learning, met...</td>\n",
              "      <td>[paper study statistical properties semisuperv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>56</td>\n",
              "      <td>0_graph_graphs_network_clustering</td>\n",
              "      <td>[graph, graphs, network, clustering, nodes, sp...</td>\n",
              "      <td>[partitioning graph groups vertices within gro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>52</td>\n",
              "      <td>1_matrix_subspace_algorithm_data</td>\n",
              "      <td>[matrix, subspace, algorithm, data, completion...</td>\n",
              "      <td>[due challenging applications collaborative fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>52</td>\n",
              "      <td>2_lasso_sparse_dictionary_group</td>\n",
              "      <td>[lasso, sparse, dictionary, group, sparsity, r...</td>\n",
              "      <td>[study problem learning sparse linear regressi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>51</td>\n",
              "      <td>3_gaussian_processes_process_inference</td>\n",
              "      <td>[gaussian, processes, process, inference, mode...</td>\n",
              "      <td>[study gaussian process regression model conte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4</td>\n",
              "      <td>48</td>\n",
              "      <td>4_kernel_kernels_reproducing_test</td>\n",
              "      <td>[kernel, kernels, reproducing, test, density, ...</td>\n",
              "      <td>[paper propose family tractable kernels dense ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>44</td>\n",
              "      <td>5_dirichlet_topic_process_model</td>\n",
              "      <td>[dirichlet, topic, process, model, models, inf...</td>\n",
              "      <td>[nonparametric mixture models based dirichlet ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6</td>\n",
              "      <td>42</td>\n",
              "      <td>6_brain_data_neuroimaging_functional</td>\n",
              "      <td>[brain, data, neuroimaging, functional, analys...</td>\n",
              "      <td>[brain decoding involves determination subject...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7</td>\n",
              "      <td>38</td>\n",
              "      <td>7_clustering_cluster_clusters_data</td>\n",
              "      <td>[clustering, cluster, clusters, data, method, ...</td>\n",
              "      <td>[mean shift clustering finds modes data probab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8</td>\n",
              "      <td>34</td>\n",
              "      <td>8_graphical_models_structure_graph</td>\n",
              "      <td>[graphical, models, structure, graph, model, v...</td>\n",
              "      <td>[propose new class semiparametric exponential ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>9</td>\n",
              "      <td>27</td>\n",
              "      <td>9_causal_variables_acyclic_observational</td>\n",
              "      <td>[causal, variables, acyclic, observational, ef...</td>\n",
              "      <td>[consider learning causal ordering variables l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>10</td>\n",
              "      <td>26</td>\n",
              "      <td>10_optimization_policy_regret_demand</td>\n",
              "      <td>[optimization, policy, regret, demand, constra...</td>\n",
              "      <td>[work presents pesmoc predictive entropy searc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>11</td>\n",
              "      <td>24</td>\n",
              "      <td>11_svm_dwd_losses_machine</td>\n",
              "      <td>[svm, dwd, losses, machine, classification, cl...</td>\n",
              "      <td>[distance weighted discrimination dwd marginba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>12</td>\n",
              "      <td>23</td>\n",
              "      <td>12_carlo_monte_sampling_mcmc</td>\n",
              "      <td>[carlo, monte, sampling, mcmc, hmc, algorithm,...</td>\n",
              "      <td>[propose novel sampling framework inference pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>13</td>\n",
              "      <td>22</td>\n",
              "      <td>13_trees_ensembles_ensemble_forests</td>\n",
              "      <td>[trees, ensembles, ensemble, forests, tree, ra...</td>\n",
              "      <td>[tree ensembles random forest boosted trees re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>14</td>\n",
              "      <td>18</td>\n",
              "      <td>14_screening_correlation_features_variables</td>\n",
              "      <td>[screening, correlation, features, variables, ...</td>\n",
              "      <td>[introduce new approach variable selection cal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>15</td>\n",
              "      <td>17</td>\n",
              "      <td>15_manifold_embedding_manifolds_lle</td>\n",
              "      <td>[manifold, embedding, manifolds, lle, data, me...</td>\n",
              "      <td>[paper presents new framework manifold learnin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>16_variational_inference_gradient_gradients</td>\n",
              "      <td>[variational, inference, gradient, gradients, ...</td>\n",
              "      <td>[stochastic variational inference relatively w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>17</td>\n",
              "      <td>14</td>\n",
              "      <td>17_deep_dropout_networks_bayesian</td>\n",
              "      <td>[deep, dropout, networks, bayesian, hidden, ne...</td>\n",
              "      <td>[since learning typically slow boltzmann machi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>18_crossvalidation_risk_empirical_error</td>\n",
              "      <td>[crossvalidation, risk, empirical, error, pena...</td>\n",
              "      <td>[article derive concentration inequalities cro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f27fff5e-665c-4b64-b825-30d11fdc15bc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f27fff5e-665c-4b64-b825-30d11fdc15bc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f27fff5e-665c-4b64-b825-30d11fdc15bc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a0d37993-c48e-4706-8fdf-0f5652184b77\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a0d37993-c48e-4706-8fdf-0f5652184b77')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a0d37993-c48e-4706-8fdf-0f5652184b77 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"topic_model\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"Topic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": -1,\n        \"max\": 18,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          -1,\n          16,\n          14\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 80,\n        \"min\": 10,\n        \"max\": 386,\n        \"num_unique_values\": 19,\n        \"samples\": [\n          386,\n          44,\n          24\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"-1_data_model_algorithm_models\",\n          \"16_variational_inference_gradient_gradients\",\n          \"14_screening_correlation_features_variables\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representation\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representative_Docs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "execution_count": 81
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic(0)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:00:14.087564Z",
          "iopub.execute_input": "2025-05-13T21:00:14.088081Z",
          "iopub.status.idle": "2025-05-13T21:00:14.094255Z",
          "shell.execute_reply.started": "2025-05-13T21:00:14.088051Z",
          "shell.execute_reply": "2025-05-13T21:00:14.093343Z"
        },
        "id": "XbfuzDVxcYLp",
        "outputId": "a7247d02-64c5-4aa3-eb7d-f4dd68d2af96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('graph', np.float64(0.05608021890451161)),\n",
              " ('graphs', np.float64(0.04391361855068794)),\n",
              " ('network', np.float64(0.0308825668121992)),\n",
              " ('clustering', np.float64(0.028191382569537415)),\n",
              " ('nodes', np.float64(0.02595808221364081)),\n",
              " ('spectral', np.float64(0.02435532602525256)),\n",
              " ('vertex', np.float64(0.024072513312475972)),\n",
              " ('networks', np.float64(0.023856096534178348)),\n",
              " ('model', np.float64(0.023688207143516368)),\n",
              " ('stochastic', np.float64(0.021385265929755804))]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "execution_count": 82
    },
    {
      "cell_type": "code",
      "source": [
        "# Show documents' topic, probability, Top_n_words\n",
        "topic_model.get_document_info(abstracts[:1000]).head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:00:14.096371Z",
          "iopub.execute_input": "2025-05-13T21:00:14.097337Z",
          "iopub.status.idle": "2025-05-13T21:00:14.132969Z",
          "shell.execute_reply.started": "2025-05-13T21:00:14.097306Z",
          "shell.execute_reply": "2025-05-13T21:00:14.132287Z"
        },
        "id": "fHXv27--cYLp",
        "outputId": "99d13419-49c3-4236-f311-7d98bd8365fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Document  Topic  \\\n",
              "0  consider problem binary classification one par...      2   \n",
              "1  distance metric plays important role nearest n...     15   \n",
              "2  dendrograms used data analysis ultrametric spa...      7   \n",
              "3  conceptual framework cluster analysis viewpoin...      7   \n",
              "4  present analyse three online algorithms learni...     -1   \n",
              "\n",
              "                                  Name  \\\n",
              "0      2_lasso_sparse_dictionary_group   \n",
              "1  15_manifold_embedding_manifolds_lle   \n",
              "2   7_clustering_cluster_clusters_data   \n",
              "3   7_clustering_cluster_clusters_data   \n",
              "4       -1_data_model_algorithm_models   \n",
              "\n",
              "                                      Representation  \\\n",
              "0  [lasso, sparse, dictionary, group, sparsity, r...   \n",
              "1  [manifold, embedding, manifolds, lle, data, me...   \n",
              "2  [clustering, cluster, clusters, data, method, ...   \n",
              "3  [clustering, cluster, clusters, data, method, ...   \n",
              "4  [data, model, algorithm, models, learning, met...   \n",
              "\n",
              "                                 Representative_Docs  \\\n",
              "0  [study problem learning sparse linear regressi...   \n",
              "1  [paper presents new framework manifold learnin...   \n",
              "2  [mean shift clustering finds modes data probab...   \n",
              "3  [mean shift clustering finds modes data probab...   \n",
              "4  [paper study statistical properties semisuperv...   \n",
              "\n",
              "                                         Top_n_words  Probability  \\\n",
              "0  lasso - sparse - dictionary - group - sparsity...     0.653100   \n",
              "1  manifold - embedding - manifolds - lle - data ...     0.459526   \n",
              "2  clustering - cluster - clusters - data - metho...     0.442203   \n",
              "3  clustering - cluster - clusters - data - metho...     0.454911   \n",
              "4  data - model - algorithm - models - learning -...     0.000000   \n",
              "\n",
              "   Representative_document  \n",
              "0                    False  \n",
              "1                    False  \n",
              "2                    False  \n",
              "3                    False  \n",
              "4                    False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bf2ecf14-e990-4072-8e41-94d10db34dd7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Name</th>\n",
              "      <th>Representation</th>\n",
              "      <th>Representative_Docs</th>\n",
              "      <th>Top_n_words</th>\n",
              "      <th>Probability</th>\n",
              "      <th>Representative_document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>consider problem binary classification one par...</td>\n",
              "      <td>2</td>\n",
              "      <td>2_lasso_sparse_dictionary_group</td>\n",
              "      <td>[lasso, sparse, dictionary, group, sparsity, r...</td>\n",
              "      <td>[study problem learning sparse linear regressi...</td>\n",
              "      <td>lasso - sparse - dictionary - group - sparsity...</td>\n",
              "      <td>0.653100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>distance metric plays important role nearest n...</td>\n",
              "      <td>15</td>\n",
              "      <td>15_manifold_embedding_manifolds_lle</td>\n",
              "      <td>[manifold, embedding, manifolds, lle, data, me...</td>\n",
              "      <td>[paper presents new framework manifold learnin...</td>\n",
              "      <td>manifold - embedding - manifolds - lle - data ...</td>\n",
              "      <td>0.459526</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dendrograms used data analysis ultrametric spa...</td>\n",
              "      <td>7</td>\n",
              "      <td>7_clustering_cluster_clusters_data</td>\n",
              "      <td>[clustering, cluster, clusters, data, method, ...</td>\n",
              "      <td>[mean shift clustering finds modes data probab...</td>\n",
              "      <td>clustering - cluster - clusters - data - metho...</td>\n",
              "      <td>0.442203</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>conceptual framework cluster analysis viewpoin...</td>\n",
              "      <td>7</td>\n",
              "      <td>7_clustering_cluster_clusters_data</td>\n",
              "      <td>[clustering, cluster, clusters, data, method, ...</td>\n",
              "      <td>[mean shift clustering finds modes data probab...</td>\n",
              "      <td>clustering - cluster - clusters - data - metho...</td>\n",
              "      <td>0.454911</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>present analyse three online algorithms learni...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1_data_model_algorithm_models</td>\n",
              "      <td>[data, model, algorithm, models, learning, met...</td>\n",
              "      <td>[paper study statistical properties semisuperv...</td>\n",
              "      <td>data - model - algorithm - models - learning -...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf2ecf14-e990-4072-8e41-94d10db34dd7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bf2ecf14-e990-4072-8e41-94d10db34dd7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bf2ecf14-e990-4072-8e41-94d10db34dd7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f8e8a9d3-bc46-456a-8434-33dd9c8a8a3a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f8e8a9d3-bc46-456a-8434-33dd9c8a8a3a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f8e8a9d3-bc46-456a-8434-33dd9c8a8a3a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"topic_model\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Document\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"distance metric plays important role nearest neighbor classification usually euclidean distance metric assumed mahalanobis distance metric optimized improve performance paper study problem embedding arbitrary metric spaces euclidean space goal improve accuracy classifier propose solution appealing framework regularization reproducing kernel hilbert space prove representerlike theorem classification embedding function determined solving semidefinite program interesting connection softmargin linear binary support vector machine classifier although main focus paper present general theoretical framework metric embedding setting demonstrate performance proposed method benchmark datasets show performs better mahalanobis metric learning algorithm terms leaveoneout generalization errors\",\n          \"present analyse three online algorithms learning discrete hidden markov models hmms compare baldichauvin algorithm using kullbackleibler divergence measure generalisation error draw learning curves simplified situations performance learning drifting concepts one presented algorithms analysed compared baldichauvin algorithm situations brief discussion learning symmetry breaking based results also presented\",\n          \"dendrograms used data analysis ultrametric spaces hence objects nonarchimedean geometry known exist padic representation dendrograms completed point infinity viewed subtrees bruhattits tree associated padic projective line implications certain moduli spaces known algebraic geometry padic parameter spaces families dendrograms stochastic classification also handled within framework end calculate topology hidden part dendrogram\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Topic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": -1,\n        \"max\": 15,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          15,\n          -1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"15_manifold_embedding_manifolds_lle\",\n          \"-1_data_model_algorithm_models\",\n          \"2_lasso_sparse_dictionary_group\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representation\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representative_Docs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Top_n_words\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"manifold - embedding - manifolds - lle - data - metric - learning - ppa - density - curves\",\n          \"data - model - algorithm - models - learning - method - problem - using - methods - based\",\n          \"lasso - sparse - dictionary - group - sparsity - regularization - linear - problem - coefficients - bounds\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Probability\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24102919686051008,\n        \"min\": 0.0,\n        \"max\": 0.6531000147268823,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.4595257511926933,\n          0.0,\n          0.44220339439500866\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representative_document\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "execution_count": 83
    },
    {
      "cell_type": "code",
      "source": [
        "# note: outputs may differ and not save\n",
        "try:\n",
        "  topic_model.visualize_topics()\n",
        "except Exception as error:\n",
        "  print(\"Unable to display visualization. The count for the first topic is likely most of the documents. Please try fitting the model again.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:00:14.133801Z",
          "iopub.execute_input": "2025-05-13T21:00:14.134531Z",
          "iopub.status.idle": "2025-05-13T21:00:14.232968Z",
          "shell.execute_reply.started": "2025-05-13T21:00:14.134506Z",
          "shell.execute_reply": "2025-05-13T21:00:14.232138Z"
        },
        "id": "8XcCiq9KcYLp"
      },
      "outputs": [],
      "execution_count": 84
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example output with cs.AI category:\n",
        "![](https://raw.githubusercontent.com/dnxv/BERTopic/refs/heads/main/outputs/visualize_topics-embedding_BAAI-min_50.jpg)"
      ],
      "metadata": {
        "id": "oTw80sVjcYLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_documents(abstracts)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:00:14.233891Z",
          "iopub.execute_input": "2025-05-13T21:00:14.234248Z",
          "iopub.status.idle": "2025-05-13T21:00:47.107251Z",
          "shell.execute_reply.started": "2025-05-13T21:00:14.234222Z",
          "shell.execute_reply": "2025-05-13T21:00:47.106348Z"
        },
        "id": "1p-lCWY3cYLq",
        "outputId": "47dcca7a-d000-431b-935f-ecd33843f04d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f5033b66-8dcc-4b9b-ac11-c2a290b02589\" class=\"plotly-graph-div\" style=\"height:750px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f5033b66-8dcc-4b9b-ac11-c2a290b02589\")) {                    Plotly.newPlot(                        \"f5033b66-8dcc-4b9b-ac11-c2a290b02589\",                        [{\"hoverinfo\":\"text\",\"hovertext\":[\"communitybased question answering cqa sites play important role addressing health information needs however significant number posted questions remain unanswered automatically answering posted questions provide useful source information online health communities study developed algorithm automatically answer healthrelated questions based past questions answers also aimed understand information embedded within online health content good features identifying valid answers proposed algorithm uses information retrieval techniques identify candidate answers resolved order rank candidates implemented semisupervised leaning algorithm extracts best answer question assessed approach curated corpus yahoo answers compared rulebased string similarity baseline dataset semisupervised learning algorithm accuracy umlsbased healthrelated features used model enhance algorithms performance proximately reasonably high rate accuracy obtained given data considerably noisy important features distinguishing valid answer invalid answer include text length number stop words contained test question distance test question questions corpus well number overlapping healthrelated terms questions overall automated system based historical pairs shown effective according data set case study developed general use health care domain also applied cqa sites\",\"population migration valuable information leads proper decision urbanplanning strategy massive investment many fields instance intercity migration posterior evidence see governments constrain population works intercommunity immigration might prior evidence real estate price hike timely data also impossible compare city favorable people suppose cities release different new regulations could also compare customers different real estate development groups come probably unfortunately data available paper leveraging data generated positioning team didi propose novel approach timely monitoring population migration community scale provincial scale migration detected soon week could faster setting week statistical purpose monitoring system developed applied nation wide china observations derived system presented paper new method migration perception origin insight nowadays people mostly moving personal access point also known wifi hotspot assume ratio moving migration population constant analysis comparative population migration would feasible exact quantitative research would also done sample research model regression procedures processing data includes many steps eliminating impact pseudomigration instance pocket wifi secondhand traded router distinguishing moving population moving companies identifying shifting finger print clusters etc\",\"linear regression models depend directly design matrix properties techniques efficiently estimate model coefficients partitioning rows design matrix increasingly popular largescale problems fit well modern parallel computing architectures propose simple measure concordance design matrix subset rows estimates well subset captures variancecovariance structure larger data set illustrate use measure heuristic method selecting row partition sizes balance statistical computational efficiency goals realworld problems\",\"infinite hidden markov models ihmms attractive nonparametric generalization classical hidden markov model automatically infer number hidden states system however due infinitedimensional nature transition dynamics performing inference ihmm difficult paper present infinitestate particle gibbs algorithm resample state trajectories ihmm proposed algorithm uses efficient proposal optimized ihmms leverages ancestor sampling suppress degeneracy standard algorithm algorithm demonstrates significant convergence improvements synthetic real world data sets additionally infinitestate algorithm lineartime complexity number states sampler competing methods scale quadratically\",\"linear dimensionality reduction methods cornerstone analyzing high dimensional data due simple geometric interpretations typically attractive computational properties methods capture many data features interest covariance dynamical structure correlation data sets inputoutput relationships margin data classes methods developed variety names motivations many fields perhaps result connections methods highlighted survey methods disparate literature optimization programs matrix manifolds discuss principal component analysis factor analysis linear multidimensional scaling fishers linear discriminant analysis canonical correlations analysis maximum autocorrelation factors slow feature analysis sufficient dimensionality reduction undercomplete independent component analysis linear regression distance metric learning optimization framework gives insight rarely discussed shortcomings wellknown methods suboptimality certain eigenvector solutions modern techniques optimization matrix manifolds enable generic linear dimensionality reduction solver accepts input data objective optimized returns output optimal lowdimensional projection data simple optimization framework allows straightforward generalizations novel variants classical methods demonstrate creating orthogonalprojection canonical correlations analysis broadly survey generic solver suggest linear dimensionality reduction move toward becoming blackbox objectiveagnostic numerical technology\",\"subdifferential convex functions singular spectrum real matrices widely studied matrix analysis optimization automatic control theory convex analysis optimization spaces tensors gaining much interest due potential applications signal processing statistics engineering goal paper present applications problem low rank tensor recovery based linear random measurement extending results tropp tensors setting\",\"incrementalonline state dynamic learning method proposed identification nonlinear gaussian state space models method embeds stochastic variational sparse gaussian process probabilistic state dynamic model inside particle filter framework model updating done measurement sample rate using stochastic gradient descent based optimization implemented state estimation filtering loop performance proposed method compared stateoftheart gaussian process based batch learning methods finally shown state estimation performance significantly improves due online learning state dynamics\",\"many complex ecosystems formed multiple microbial taxa involve intricate interactions amongst various subcommunities basic relationships frequently modeled cooccurrence networks nodes represent various players community weighted edges encode levels interaction setting composition community may viewed probability distribution nodes network paper develops methods modeling organization data well euclidean counterparts across spatial scales using notion diffusion distance introduce diffusion frechet functions diffusion frechet vectors associated probability distributions euclidean space vertex set weighted network respectively prove functional statistics stable respect wasserstein distance probability measures thus yielding robust descriptors shapes apply methodology investigate bacterial communities human gut seeking characterize divergence intestinal homeostasis patients clostridium difficile infection cdi effects fecal microbiota transplantation treatment used cdi patients proven significantly effective traditional treatment antibiotics proposed method proves useful deriving biomarker might help elucidate mechanisms drive processes\",\"propose novel approach nonlinear regression using twolayer neural network model structure sparsityfavoring hierarchical priors network weights present expectation propagation approach approximate integration posterior distribution weights hierarchical scale parameters priors residual scale using factorized posterior approximation derive computationally efficient algorithm whose complexity scales similarly ensemble independent sparse linear models approach enables flexible definition weight priors different sparseness properties independent laplace priors common scale parameter gaussian automatic relevance determination ard priors different relevance parameters inputs approach extended beyond standard activation functions model structures form flexible nonlinear predictors multiple sparse linear models effects hierarchical priors predictive performance algorithm assessed using simulated realworld data comparisons made two alternative models ard priors gaussian process covariance function marginal maximum posteriori estimates relevance parameters markov chain monte carlo integration unknown model parameters\",\"present probabilistic model natural images based gaussian scale mixtures simple multiscale representation contrast dominant approach modeling whole images focusing markov random fields formulate model terms directed graphical model show able generate images interesting higherorder correlations trained natural images samples occlusion based model importantly directed model enables perform principled evaluation easy generate visually appealing images demonstrate model also yields best performance reported date evaluated respect crossentropy rate measure tightly linked average loglikelihood\",\"briefly review recent progress techniques modeling analyzing hyperspectral images movies particular detecting plumes known unknown chemicals detecting chemicals known spectrum extend technique using single subspace modeling background mixture subspaces model tackle complicated background furthermore use partial least squares regression resampled training set boost performance detection unknown chemicals view problem anomaly detection problem use novel estimators lowsampled complexity intrinsically lowdimensional data highdimensions enable model normal spectra detect anomalies apply algorithms benchmark data sets made available automated target detection program cofunded nsf dtra nga compare applicable current stateoftheart algorithms favorable results\",\"present exploration rich theoretical connections several classes regularized models network flows recent results submodular function theory work unifies key aspects problems common theory leading novel methods working several important models interest statistics machine learning computer vision part review concepts network flows submodular function optimization theory foundational results examine connections network flows minimumnorm algorithm submodular optimization extending improving several current results leads concise representation structure large class pairwise regularized models important machine learning statistics computer vision part describe full regularization path class penalized regression problems dependent variables includes graphguided lasso total variation constrained models description also motivates practical algorithm allows efficiently find regularization path discretized version penalized models ultimately new algorithms scale highdimensional problems millions variables\",\"paper compares classical parametric methods recently developed bayesian methods system identification full bayes solution considered together one standard approximations based empirical bayes paradigm results regarding point estimators impulse response well confidence regions reported\",\"paper generalizes beta divergence beyond classical form associated power variance functions tweedie models generalized form represented compact definite integral function variance function exponential dispersion model compact integral form simplifies derivations many properties scaling translation expectation beta divergence show beta divergence half statistical deviance equivalent measures\",\"exact inference linear regression model spike slab priors often intractable expectation propagation used approximate inference however regular sequential form rep may fail converge model size training set small alternative propose provably convergent algorithm pcep pcep proved minimize energy function constraints bounded whose stationary points coincide solution rep experiments synthetic data indicate rep converge approximation generated pcep often better contrast rep converges methods perform similarly\",\"generalised degrees freedom gdf defined jasa represent sensitivity model fits perturbations data computed statistical model making possible principle derive number parameters machinelearning approaches defined originally normally distributed data investigate potential approach bernoullidata gdfvalues models simulated real data compared model complexityestimates crossvalidation similarly computed gdfbased aicc randomforest neural networks boosted regression trees demonstrated similarity crossvalidation gdfestimates binary data unstable inconsistently sensitive number data points perturbed simultaneously time extremely computerintensive calculation repeated fold crossvalidation robust based fewer assumptions faster compute findings suggest gdfapproach readily transfer bernoulli data wider range regression approaches\",\"propose robust inferential procedure assessing uncertainties parameter estimation highdimensional linear models dimension grow exponentially fast sample size method combines debiasing technique composite quantile function construct estimator asymptotically normal hence used construct valid confidence intervals conduct hypothesis tests estimator robust require existence first second moment noise distribution also preserves efficiency sense worst case efficiency loss less compared squarelossbased debiased lasso estimator many cases estimator close better latter especially noise heavytailed debiasing procedure require solving lpenalized composite quantile regression instead allows firststage estimator desired convergence rate empirical sparsity paper also provides new proof techniques developing theoretical guarantees inferential procedures nonsmooth loss functions establish main results exploit local curvature conditional expectation composite quantile loss apply empirical process theories control difference empirical quantities conditional expectations results established weaker assumptions compared existing work inference highdimensional quantile regression furthermore consider highdimensional simultaneous test regression parameters applying gaussian approximation multiplier bootstrap theories also study distributed learning exploit divideandconquer estimator reduce computation complexity sample size massive finally provide empirical results verify theory\",\"many areas machine learning becomes necessary find eigenvector decompositions large matrices discuss two methods reducing computational burden spectral decompositions venerable nystom extension newly introduced algorithm based random projections previous work centered ability reconstruct original matrix argue interesting relevant comparison relative performance clustering classification tasks using approximate eigenvectors features demonstrate performance task specific depends rank approximation\",\"consider class optimization problems arising computationally intensive lregularized mestimators function gradient values expensive compute particular instance interest lregularized mle learning conditional random fields crfs popular class statistical models varied structured prediction problems sequence labeling alignment classification label taxonomy lregularized mles crfs particularly expensive optimize since computing gradient values requires expensive inference step work propose use carefully constructed proximal quasinewton algorithm computationally intensive mestimation problems employ aggressive active set selection technique key contribution paper show proximal quasinewton method provably superlinearly convergent even absence strong convexity leveraging restricted variant strong convexity experiments proposed algorithm converges considerably faster current stateoftheart problems sequence labeling hierarchical classification\",\"separating short jobs long known technique improve scheduling performance paper describe method developed accurately predicting runtimes classes jobs enable separation method uses fact runtimes represented mixture overlapping gaussian distributions order train cart classifier provide prediction threshold separates short jobs long jobs determined evaluation classifier maximize prediction accuracy results indicate overall accuracy data set used study sensitivity specificity\",\"changepoints abrupt variations generative parameters data sequence online detection changepoints useful modelling prediction time series application areas finance biometrics robotics frequentist methods yielded online filtering prediction techniques bayesian papers focused retrospective segmentation problem examine case model parameters changepoint independent derive online algorithm exact inference recent changepoint compute probability distribution length current run time since last changepoint using simple messagepassing algorithm implementation highly modular algorithm may applied variety types data illustrate modularity demonstrating algorithm three different realworld data sets\",\"kalman filter used variety applications computing posterior distribution latent states state space model model requires linear relationship states observations extensions kalman filter proposed incorporate linear approximations nonlinear models extended kalman filter ekf unscented kalman filter ukf however argue cases dimensionality observed variables greatly exceeds dimensionality state variables model ptextstatetextobservation proves easier learn accurate latent space estimation derive validate call discriminative kalman filter dkf closedform discriminative version bayesian filtering readily incorporates offtheshelf discriminative learning techniques demonstrate given mild assumptions highly nonlinear models ptextstatetextobservation specified motivate validate synthetic datasets neural decoding nonhuman primates showing substantial increases decoding performance versus standard kalman filter\",\"present nonparametric prior reversible markov chains use completely random measures specifically gamma processes construct countably infinite graph weighted edges enforcing symmetry make edges undirected define prior random walks graphs results reversible markov chain resulting prior infinite transition matrices closely related hierarchical dirichlet process enforces reversibility reinforcement scheme recently proposed similar properties finetti measure well characterised take alternative approach explicitly constructing mixing measure allows straightforward efficient inference cost longer closed form predictive distribution use process construct reversible infinite hmm apply two real datasets one epigenomics one ion channel recording\",\"article addresses modeling reverberant recording environments context underdetermined convolutive blind source separation model contribution source mixture channels timefrequency domain zeromean gaussian random variable whose covariance encodes spatial characteristics source consider four specific covariance models including fullrank unconstrained model derive family iterative expectationmaximization algorithms estimate parameters model propose suitable procedures initialize parameters align order estimated sources across frequency bins based estimated directions arrival doa experimental results reverberant synthetic mixtures live recordings speech data show effectiveness proposed approach\",\"introduce sparse random projection important dimensionreduction tool machine learning estimation discretechoice models highdimensional choice sets initially highdimensional data compressed lowerdimensional euclidean space using random projections subsequently estimation proceeds using cyclic monotonicity moment inequalities implied multinomial choice model estimation procedure semiparametric require explicit distributional assumptions made regarding random utility errors random projection procedure justified via johnsonlindenstrauss lemma pairwise distances data points preserved data compression exploit show convergence estimator estimator works well simulations application supermarket scanner dataset\",\"study inference learning based sparse coding model spikeandslab prior standard sparse coding model used assumes independent latent sources linearly combine generate data points however instead using standard sparse prior laplace distribution study application flexible spikeandslab distribution models absence presence sources contribution independently strength contributes investigate two approaches optimize parameters spikeandslab sparse coding novel truncated approach comparison approach based standard factored variational distributions truncated approach regarded variational approach truncated posteriors variational distributions applications source separation find approaches improve stateoftheart number standard benchmarks argues use spikeandslab priors corresponding data domains furthermore find truncated approach improves standard factored approach source separation taskswhich hints biases introduced assuming posterior independence factored variational approach likewise standard benchmark image denoising find truncated approach improves factored variational approach performance factored approach saturates increasing numbers hidden dimensions performance truncated approach improves stateoftheart higher noise levels\",\"fastica algorithm one popular iterative algorithms domain linear independent component analysis despite success observed fastica occasionally yields outcomes correspond true solutions known demixing vectors ica problem outcomes commonly referred spurious solutions although fastica among extensively studied ica algorithms occurrence spurious solutions yet completely understood community contribution aim addressing issue first part work interested relationship demixing vectors local optimizers contrast function attractive unattractive fixed points fastica algorithm characterizations sets given inclusion relationship discovered second part investigate possible scenarios spurious solutions occur show certain bimodal gaussian mixtures distributions involved may exist spurious solutions attractive fixed points fastica case popular nonlinearities gauss tanh tend yield spurious solutions whereas kurtosis may give reliable results advices given practical choice nonlinearity function\",\"many nonparametric regressors recently shown converge rates depend intrinsic dimension data regressors thus escape curse dimension highdimensional data low intrinsic dimension manifold show knn regression also adaptive intrinsic dimension particular rates local query depend way masses balls centered vary radius furthermore show simple way choose locally nearly achieve minimax rate terms unknown intrinsic dimension vicinity also establish minimax rate depend particular choice metric space distribution rather minimax rate holds metric space doubling measure\",\"paper concerned obtaining distributionfree concentration inequalities mixture independent bernoulli variables incorporate notion variance missing mass total probability mass associated outcomes seen given sample important quantity connects density estimates obtained sample population discrete distributions therefore specifically motivated apply method study concentration missing mass expressed mixture bernoulli novel way derive first time bernsteinlike large deviation bounds missing mass whose exponents behave almost linearly respect deviation size also sharpen mcallester ortiz berend kontorovich large sample sizes case small deviations interesting case learning theory meantime approach shows heterogeneity issue introduced mcallester ortiz resolvable case missing mass sense one use standard inequalities may lead strong results thus postulate results general applied provide potentially sharp bernsteinlike bounds constraints\",\"existing methods sparse channel estimation typically provide estimate computed solution maximizing objective function defined sum loglikelihood function penalization term proportional lnorm parameter interest however penalization terms proven strong sparsityinducing properties work design pilotassisted channel estimators ofdm wireless receivers within framework sparse bayesian learning defining hierarchical bayesian prior models lead sparsityinducing penalization terms estimators result application variational messagepassing algorithm factor graph representing signal model extended hierarchical prior models numerical results demonstrate superior performance channel estimators compared traditional stateoftheart sparse methods\",\"kernel leastmeansquare klms algorithm appealing tool online identification nonlinear systems due simplicity robustness addition choosing reproducing kernel setting filter parameters designing klms adaptive filter requires select socalled dictionary order get finiteorder model dictionary significant impact performance requires careful consideration theoretical analysis klms function dictionary setting rarely ever addressed literature analysis previously published authors dictionary elements assumed governed probability density function input data paper modify study considering dictionary part filter parameters set theoretical analysis paves way future investigations klms dictionary design\",\"renewed interest formulating integration inference problem motivated obtaining full distribution numerical error propagated subsequent computation current methods bayesian quadrature demonstrate impressive empirical performance lack theoretical analysis important challenge reconcile probabilistic integrators rigorous convergence guarantees paper present first probabilistic integrator admits theoretical treatment called frankwolfe bayesian quadrature fwbq fwbq convergence true value integral shown exponential posterior contraction rates proven superexponential simulations fwbq competitive stateoftheart methods outperforms alternatives based frankwolfe optimisation approach applied successfully quantify numerical error solution challenging model choice problem cellular biology\",\"kernelbased kmeans clustering gained popularity due simplicity power implicit nonlinear representation data dominant concern memory requirement since memory scales square number data points provide new analysis class approximate kernel methods modest memory requirements propose specific onepass randomized kernel approximation followed standard kmeans transformed data analysis experiments suggest method accurate requiring drastically less memory standard kernel kmeans significantly less memory nystrom based approximations\",\"consider task fitting regression model involving interactions among potentially large set covariates wish enforce strong heredity propose family general framework task proposal generalization several existing methods vanish radchenko james hiernet bien allpairs lasso lasso using main effects formulated solution convex optimization problem solve using efficient alternating directions method multipliers admm algorithm algorithm guaranteed convergence global optimum easily specialized convex penalty function interest allows straightforward extension setting generalized linear models derive unbiased estimator degrees freedom family explore performance simulation study hiv sequence data set\",\"paper exact linear relation leading eigenvectors modularity matrix singular vectors uncentered data matrix developed based analysis concept modularity component defined properties developed shown modularity component analysis used cluster data similar traditional principal component analysis used except modularity component analysis require data centering\",\"reciprocating interactions represent central feature human exchanges target various recent experiments healthy participants psychiatric populations engaging dyads multiround exchanges repeated trust task behaviour exchanges involves complexities related agents preference equity partner beliefs partners appetite equity beliefs partners model partner agents may also plan different numbers steps future providing computationally precise account behaviour essential step towards understanding underlies choices natural framework interactive partially observable markov decision process ipomdp however various complexities make ipomdps inordinately computationally challenging show approximate solution multiround trust task using variant montecarlo tree search algorithm demonstrate algorithm efficient effective therefore used invert observations behavioural choices use generated behaviour elucidate richness sophistication interactive inference\",\"introduce supersparse linear integer models slim tool create scoring systems binary classification derive theoretical bounds true risk slim scoring systems present experimental results show slim scoring systems accurate sparse interpretable classification models\",\"structured sparsity recently emerged statistics machine learning signal processing promising paradigm learning highdimensional settings existing methods learning assumption structured sparsity rely prior knowledge weight penalize individual subsets variables subset selection process available general inferring group weights data key open research problem structured sparsityin paper propose bayesian approach problem group weight learning model group weights hyperparameters heavytailed priors groups variables derive approximate inference scheme infer hyperparameters empirically show able recover model hyperparameters data generated model demonstrate utility learning weights synthetic real denoising problems\",\"applications related airborne radars simulation always played important role mainly two fold reason unavailability desired data difficulty associated collection data controlled environment simple example regarding collection pure multipolar radar data even phenomenal development field radar hardware design signal processing till collection pure multipolar data challenge radar system designers till recently power computer simulation radar signal return available selected heavy cost associated main line electro magnetic simulators radar signal simulation secondly many simulators restricted marketting however fast progress made field simulation many current generic simulators used simulate radar returns realistic targets current article expounds steps towards generating synthetic aperture radar sar image database ground targets using eneric simulator also demonstrates help example images quality sar mage generated using general purpose simulator\",\"provide selfcontained proof theorem relating probabilistic coherence forecasts nondomination rival forecasts respect proper scoring rule theorem appears new closely related results achieved investigators\",\"theoretically investigate convergence rate support consistency correctly identifying subset nonzero coefficients large sample limit multiple kernel learning mkl focus mkl blockl regularization inducing sparse kernel combination blockl regularization inducing uniform kernel combination elasticnet regularization including blockl blockl regularization case true kernel combination sparse show sharper convergence rate blockl elasticnet mkl methods existing rate blockl mkl show elasticnet mkl requires milder condition consistent blockl mkl case optimal kernel combination exactly sparse prove elasticnet mkl achieve faster convergence rate blockl blockl mkl methods carefully controlling balance blockland blockl regularizers thus theoretical results overall suggest use elasticnet regularization mkl\",\"introduce general framework estimation inverse covariance precision matrices heterogeneous populations proposed framework uses laplacian shrinkage penalty encourage similarity among estimates disparate related subpopulations allowing differences among matrices propose efficient alternating direction method multipliers admm algorithm parameter estimation well extension faster computation high dimensions thresholding empirical covariance matrix identify joint block diagonal structure estimated precision matrices establish variable selection norm consistency proposed estimator distributions exponential polynomial tails extend applicability method settings unknown populations structure propose laplacian penalty based hierarchical clustering discuss conditions datadriven choice results consistent estimation precision matrices heterogenous populations extensive numerical studies applications gene expression data subtypes cancer distinct clinical outcomes indicate potential advantages proposed method existing approaches\",\"study statistical calibration adjusting features computational model observable controllable associated physical system focus functional calibration arises many manufacturing processes unobservable features called calibration variables function input variables major challenge many applications computational models expensive evaluated limited number times furthermore without making strong assumptions calibration variables identifiable propose bayesian nonisometric matching calibration bnmc allows calibration expensive computational models limited number samples taken computational model associated physical system bnmc replaces computational model dynamic gaussian process whose parameters trained calibration procedure resolve identifiability issue present calibration problem geometric perspective nonisometric curve surface matching enables take advantage combinatorial optimization techniques extract necessary information constructing prior distributions numerical experiments demonstrate terms prediction accuracy bnmc outperforms comparable existing calibration frameworks\",\"propose active set selection framework gaussian process classification cases dataset large enough render inference prohibitive scheme consists two step alternating procedure active set update rules hyperparameter optimization based upon marginal likelihood maximization active set update rules rely ability predictive distributions gaussian process classifier estimate relative contribution datapoint either included removed model means use include points potentially high impact classifier decision process removing less relevant introduce two active set rules based different criteria first one prefers model interpretable active set parameters whereas second puts computational complexity first thus model active set parameters directly control complexity also provide theoretical empirical support active set selection strategy good approximation full gaussian process classifier extensive experiments show approach compete stateoftheart classification techniques reasonable time complexity source code publicly available httpcogsysimmdtudkpassgp\",\"optimal transportation distances fundamental family parameterized distances histograms despite appealing theoretical properties excellent performance retrieval tasks intuitive formulation computation involves resolution linear program whose cost prohibitive whenever histograms dimension exceeds hundreds propose work new family optimal transportation distances look transportation problems maximumentropy perspective smooth classical optimal transportation problem entropic regularization term show resulting optimum also distance computed sinkhornknopps matrix scaling algorithm speed several orders magnitude faster transportation solvers also report improved performance classical optimal transportation distances mnist benchmark problem\",\"selecting important features nonlinear kernel spaces difficult challenge classification regression problems many features irrelevant kernel methods support vector machine kernel ridge regression sometimes perform poorly propose weighting features within kernel sparse set weights estimated conjunction original classification regression problem iterative algorithm knife alternates finding coefficients original problem finding feature weights kernel linearization addition slight modification knife yields efficient algorithm finding feature regularization paths paths features weight simulation results demonstrate utility knife kernel regression support vector machines variety kernels feature path realizations also reveal important nonlinear correlations among features prove useful determining subset significant variables results vowel recognition data parkinsons disease data microarray data also given\",\"archetypal analysis represents set observations convex combinations pure patterns archetypes original geometric formulation finding archetypes approximating convex hull observations assumes real valued unfortunately compatible many practical situations paper revisit archetypal analysis basic principles propose probabilistic framework accommodates observation types integers binary probability vectors corroborate proposed methodology convincing realworld applications finding archetypal winter tourists based binary survey data archetypal disasteraffected countries based disaster count data document archetypes based termfrequency data also present appropriate visualization tool summarize archetypal analysis solution better\",\"study statistical inference distributionally robust solution methods stochastic optimization problems focusing confidence intervals optimal values solutions achieve exact coverage asymptotically develop generalized empirical likelihood frameworkbased distributional uncertainty sets constructed nonparametric fdivergence ballsfor hadamard differentiable functionals particular stochastic optimization problems consequences theory provide principled method choosing size distributional uncertainty regions provide one twosided confidence intervals achieve exact coverage also give asymptotic expansion distributionally robust formulation showing robustification regularizes problems variance finally show optimizers distributionally robust formulations study enjoy essentially consistency properties classical sample average approximations general approach applies quickly mixing stationary sequences including geometrically ergodic harris recurrent markov chains\",\"modern scientific research massive datasets huge numbers observations frequently encountered facilitate computational process divideandconquer scheme often used analysis big data strategy full dataset first split several manageable segments final output averaged individual outputs segments despite popularity practice remains largely unknown whether distributive strategy provides valid theoretical inferences original data paper address fundamental issue distributed kernel regression dkr algorithmic feasibility measured generalization performance resulting estimator justify dkr uniform convergence rate needed bounding generalization error individual outputs brings new challenging issues big data setup mild conditions show proper number segments dkr leads estimator generalization consistent unknown regression function obtained results justify method dkr shed light feasibility using distributed algorithms processing big data promising preference method supported simulation real data examples\",\"consider problem multivariate regression setting relevant predictors could shared among different responses propose algorithm decomposes coefficient matrix product long matrix wide matrix elastic net penalty former ell penalty latter first matrix linearly transforms predictors set latent factors second one regresses responses factors algorithm simultaneously performs dimension reduction coefficient estimation automatically estimates number latent factors data formulation results nonconvex optimization problem despite flexibility impose effective lowdimensional structure difficult even impossible solve exactly reasonable time specify optimization algorithm based alternating minimization three different sets updates solve nonconvex problem provide theoretical results convergence optimality finally demonstrate effectiveness algorithm via experiments simulated real data\",\"stochastic variational inference collapsed models recently successfully applied large scale topic modelling paper propose stochastic collapsed variational inference algorithm hidden markov models sequential data setting given collapsed hidden markov model break long markov chain set short subchains propose novel sumproduct algorithm update posteriors subchains taking account boundary transitions due sequential dependencies experiments two discrete datasets show collapsed algorithm scalable large datasets memory efficient significantly accurate existing uncollapsed algorithm\",\"interested solving multiple measurement vector mmv problem instances underlying sparsity pattern exhibit spatiotemporal structure motivated electroencephalogram eeg source localization problem propose probabilistic model takes structure account generalizing structured spike slab prior associated expectation propagation inference scheme based numerical experiments demonstrate viability model approximate inference scheme\",\"consider problem jointly estimating parameters well structure binary valued markov random fields contrast earlier work focus one two problems formulate problem maximization ellregularized surrogate likelihood allows find sparse solution optimization technique efficiently incorporates cuttingplane algorithm order obtain tighter outer bound marginal polytope results improvement parameter estimates approximation marginals synthetic data compare algorithm two estimation tasks existing methods analyze method highdimensional setting number dimensions allowed grow number observations rate convergence estimate demonstrated depend explicitly sparsity underlying graph\",\"propose communicationefficient distributed estimation method sparse linear discriminant analysis lda high dimensional regime method distributes data size machines estimates local sparse lda estimator machine using data subset size distributed estimation method aggregates debiased local estimators machines sparsifies aggregated estimator show aggregated estimator attains statistical rate centralized estimation method long number machines chosen appropriately moreover prove method attain model selection consistency milder condition centralized method experiments synthetic real datasets corroborate theory\",\"introduce incremental variational inference apply latent dirichlet allocation lda incremental variational inference inspired incremental provides alternative stochastic variational inference incremental lda process massive document collections require set learning rate converges faster local optimum variational bound enjoys attractive property monotonically increasing study performance incremental lda large benchmark data sets introduce stochastic approximation incremental variational inference extends asynchronous distributed setting resulting distributed algorithm achieves comparable performance single host incremental variational inference significant speedup\",\"typical goal supervised dimension reduction find lowdimensional subspace input space projected input variables preserve maximal information output variables dependence maximization approach solves supervised dimension reduction problem maximizing statistical dependence projected input variables output variables wellknown statistical dependence measure mutual information based kullbackleibler divergence however known divergence sensitive outliers hand quadratic qmi variant based distance robust outliers divergence computationally efficient method estimate qmi data called leastsquares qmi lsqmi proposed recently reasons developing supervised dimension reduction method based lsqmi seems promising however qmi derivative qmi needed subspace search supervised dimension reduction derivative accurate qmi estimator necessarily good estimator derivative qmi paper propose directly estimate derivative qmi without estimating qmi show direct estimation derivative qmi accurate derivative estimated qmi finally develop supervised dimension reduction algorithm efficiently uses proposed derivative estimator demonstrate experiments proposed method robust outliers existing methods\",\"introduce new discriminant analysis method empirical discriminant analysis eda binary classification machine learning given dataset feature vectors method defines empirical feature map transforming training test data new data components gaussian empirical distributions map empirical version gaussian copula used probability mathematical finance purpose form feature mapped dataset close possible gaussian standard quadratic discriminants used classification discuss method general apply datasets computational biology\",\"probabilistic programming languages represent complex data intermingled models lines code efficient inference algorithms probabilistic programming languages make possible build unified frameworks compute interesting probabilities various large realworld problems structure model given constructing probabilistic program rather straightforward thus main focus learn best model parameters compute marginal probabilities paper provide new perspective build expressive probabilistic program continue time series data structure model given intuition behind method find descriptive covariance structure time series data nonparametric gaussian process regression report descriptive covariance structure efficiently derives probabilistic programming description accurately\",\"vast majority neural network literature focuses predicting point values given set response variables conditioned feature vector many cases need model full joint conditional distribution response variables rather simply making point predictions paper present two novel approaches conditional density estimation cde multiscale nets msns cde trend filtering multiscale nets transform cde regression task hierarchical classification task decomposing density series halfspaces learning boolean probabilities split cde trend filtering applies kth order graph trend filtering penalty unnormalized logits multinomial classifier network edge graph corresponding neighboring point discretized version density compare methods plain multinomial classifier networks mixture density networks mdns simulated dataset three realworld datasets results suggest two methods complementary msns work well highdataperfeature regime cdetf well suited fewsamplesperfeature scenarios overfitting primary concern\",\"big data bring new opportunities modern society challenges data scientists one hand big data hold great promises discovering subtle population patterns heterogeneities possible smallscale data hand massive sample size high dimensionality big data introduce unique computational statistical challenges including scalability storage bottleneck noise accumulation spurious correlation incidental endogeneity measurement errors challenges distinguished require new computational statistical paradigm article give overviews salient features big data features impact paradigm change statistical computational methods well computing architectures also provide various new perspectives big data analysis computation particular emphasis viability sparsest solution highconfidence set point exogeneous assumptions statistical methods big data validated due incidental endogeneity lead wrong statistical inferences consequently wrong scientific conclusions\",\"mixture models gamma inversegamma distributed mixture components useful medical image tissue segmentation posthoc models regression coefficients obtained linear regression within generalised linear modeling framework glm used case separate stochastic gaussian noise kind positive negative activation modeled gamma inversegamma distributed date common choice context gaussiangamma mixture models learned maximum likelihood approach recently extended algorithm mixture models inversegamma components introduce fully analytical variational bayes learning framework gamma andor inversegamma components use synthetic resting state fmri data compare performance algorithms terms area curve computational cost observed gaussiangamma model expensive specially considering high resolution images furthermore solutions highly variable occasionally overestimate activations severely bayesian gaussgamma general fastest algorithm provides dense solutions maximum likelihood gaussianinversegamma also fast provides general sparse solutions variational gaussianinversegamma mixture model robust cost acceptable even high resolution images presented methodology represents essential building block directly used complex inference tasks specially designed analyse mrifmri data models include example analytical variational mixture models adaptive spatial regularization better source models new spatial blind source separation approaches\",\"consider problem joint modelling metabolic signals gene expression systems biology applications propose approach based inputoutput factorial hidden markov models propose structured variational inference approach infer structure states model start classical free form structured variational mean field approach use expectation propagation approximate expectations needed variational loop show corresponds factored expectation constrained approximate inference validate model extensive simulations demonstrate applicability real world bacterial data set\",\"extend traditional worstcase minimax analysis stochastic convex optimization introducing localized form minimax complexity individual functions main result gives functionspecific lower upper bounds number stochastic subgradient evaluations needed optimize either function hardest local alternative given numerical precision bounds expressed terms localized computational analogue modulus continuity central statistical minimax analysis show computational modulus continuity explicitly calculated concrete cases relates curvature function optimum also prove superefficiency result demonstrates meaningful benchmark acting computational analogue fisher information statistical estimation nature practical implications results demonstrated simulations\",\"statistical dependencies independent component analysis ica cannot remove often provide rich information beyond linear independent components would thus useful estimate dependency structure data models proposed usually concentrated higherorder correlations energy square correlations yet linear correlations fundamental informative form dependency many real data sets linear correlations usually completely removed ica related methods analyzed developing new methods explicitly allow linearly correlated components paper propose probabilistic model linear nongaussian components allowed linear energy correlations precision matrix linear components assumed randomly generated higherorder process explicitly parametrized parameter matrix estimation parameter matrix shown particularly simple using score matching objective function quadratic form using simulations artificial data demonstrate proposed method improves identifiability nongaussian components simultaneously learning correlation structure applications simulated complex cells natural image input well spectrograms natural audio data show method finds new kinds dependencies components\",\"study problem estimating parameters regression model set observations consisting response predictor response assumed related predictor via regression model unknown parameters often models parameters estimated assumed constant consider general scenario parameters allowed evolve time natural assumption many applications model dynamics via linear update equation additive noise often used wide range engineering applications particularly wellknown widely used kalman filter system state seeks estimate maps parameter values derive approximate algorithm estimate mean variance parameter estimates online fashion generic regression model algorithm turns equivalent extended kalman filter specialize algorithm multivariate exponential family distribution obtain generalization generalized linear model glm common regression models encountered practice logistic exponential multinomial observations modeled exponential family distribution results used easily obtain algorithms online mean variance parameter estimation regression models context timedependent parameters lastly propose use algorithms contextual multiarmed bandit scenario far model parameters assumed static observations univariate gaussian bernoulli restrictions relaxed using algorithms described combine thompson sampling show resulting performance simulation\",\"study novel splinelike basis name falling factorial basis bearing many similarities classic truncated power basis advantage falling factorial basis enables rapid lineartime computations basis matrix multiplication basis matrix inversion falling factorial functions actually splines close enough splines provably retain favorable properties latter functions examine application two problems trend filtering arbitrary input points higherorder variant twosample kolmogorovsmirnov test\",\"introduce new approach amortizing inference directed graphical models learning heuristic approximations stochastic inverses designed specifically use proposal distributions sequential monte carlo methods describe procedure constructing learning structured neural network represents inverse factorization graphical model resulting conditional density estimator takes input particular values observed random variables returns approximation distribution latent variables recognition model learned offline independent particular dataset prior performing inference output networks used automaticallylearned highquality proposal distributions accelerate sequential monte carlo across diverse range problem settings\",\"important aspect classifier error rate quantifies predictive capacity thus accuracy error estimation critical error estimation problematic smallsample classifier design error must estimated using data classifier designed use prior knowledge form prior distribution uncertainty class featurelabel distributions true unknown featuredistribution belongs facilitate accurate error estimation meansquare sense circumstances accurate completely modelfree error estimation impossible paper provides analytic asymptotically exact finitesample approximations various performance metrics resulting bayesian minimum meansquareerror mmse error estimator case linear discriminant analysis lda multivariate gaussian model performance metrics include first second cross moments bayesian mmse error estimator true error lda therefore rootmeansquare rms error estimator lay theoretical groundwork kolmogorov doubleasymptotics bayesian setting enables derive asymptotic expressions desired performance metrics produce analytic finitesample approximations demonstrate accuracy via numerical examples various examples illustrate behavior approximations use determining necessary sample size achieve desired rms supplementary material contains derivations equations added figures\",\"purpose sufficient dimension reduction sdr find lowdimensional subspace input features sufficient predicting output values paper propose novel distributionfree sdr method called sufficient component analysis sca computationally efficient existing methods method solution computed iteratively performing dependence estimation maximization dependence estimation analytically carried recentlyproposed leastsquares mutual information lsmi dependence maximization also analytically carried utilizing epanechnikov kernel largescale experiments realworld image classification audio tagging problems proposed method shown compare favorably existing dimension reduction approaches\",\"symmetric binary matrices representing relations among entities commonly collected many areas focus dynamically evolving binary relational matrices interest inference relationship structure prediction propose nonparametric bayesian dynamic model reduces dimensionality characterizing binary matrix lowerdimensional latent space representation latent coordinates evolving continuous time via gaussian processes using logistic mapping function probability matrix space latent relational space obtain flexible computational tractable formulation employing polyagamma data augmentation efficient gibbs sampler developed posterior computation dimension latent space automatically inferred provide theoretical results flexibility model illustrate performance via simulation experiments also consider application comovements world financial markets\",\"consider statistical inverse learning problem observe image function linear operator iid random design points superposed additive noise distribution design points unknown general analyze simultaneously direct estimation inverse estimation learning problems general framework obtain strong weak minimax optimal rates convergence number observations grows large large class spectral regularization methods regularity classes defined appropriate source conditions improves completes previous results obtained related settings optimality obtained rates shown exponent also explicit dependency constant factor variance noise radius source condition set\",\"propose work new family kernels variablelength time series work builds upon vector autoregressive var model multivariate stochastic processes given multivariate time series consider likelihood function pthetax different parameters theta var model features describe compare two time series form product features pthetax pthetax integrated wrt theta using matrix normalinverse wishart prior among properties kernel easily computed dimension time series much larger lengths considered time series also generalized time series taking values arbitrary state spaces long state space endowed kernel kappa case kernel function gram matrices produced kappa observations subsequences observations enumerated describe computationally efficient implementation generalization uses lowrank matrix factorization techniques kernels compared known kernels using set benchmark classification tasks carried support vector machines\",\"focus interpolation method referred bayesian reconstruction paper whereas standard interpolation methods missing data interpolated deterministically bayesian reconstruction missing data interpolated probabilistically using bayesian treatment paper address framework bayesian reconstruction application traffic data reconstruction problem field traffic engineering latter part paper describe evaluation statistical performance bayesian traffic reconstruction model using statistical mechanical approach clarify statistical behavior\",\"paper examine problem approximating general linear dimensionality reduction ldr operator represented matrix mathbbrm times partial circulant matrix rows related circular shifts partial circulant matrices admit fast implementations via fourier transform methods subsampling operations investigation motivated desire leverage potential computational improvements largescale data processing tasks establish fundamental result large ldr matrices whose row spaces uniformly distributed fact cannot well approximated partial circulant matrices propose natural generalization partial circulant approximation framework entails approximating range space given ldr operator restricted domain inputs using matrix formed product partial circulant matrix rows times post processing matrix introduce novel algorithmic technique based sparse matrix factorization identifying factors comprising approximations provide preliminary evidence demonstrate potential approach\",\"monograph deals adaptive supervised classification using tools borrowed statistical mechanics information theory stemming pacbayesian approach pioneered david mcallester applied conception statistical learning theory forged vladimir vapnik using convex analysis set posterior probability measures show get local measures complexity classification model involving relative entropy posterior distributions respect gibbs posterior measures discuss relative bounds comparing generalization error two classification rules showing margin assumption mammen tsybakov replaced empirical measure covariance structure classification modelwe show associate posterior distribution effective temperature relating gibbs prior distribution level expected error rate estimate effective temperature data resulting estimator whose expected error rate converges according best possible power sample size adaptively margin parametric complexity assumptions describe study alternative selection scheme based relative bounds estimators present two step localization technique handle selection parametric model family show extend systematically results obtained inductive setting transductive learning use improve vapniks generalization bounds extending case sample made independent nonidentically distributed pairs patterns labels finally review briefly construction support vector machines show derive generalization bounds measuring complexity either number support vectors value transductive inductive margin\",\"effective accurate model selection important problem modern data analysis one major challenges computational burden required handle large data sets cannot stored processed one machine another challenge one may encounter presence outliers contaminations damage inference quality parallel divide conquer model selection strategy divides observations full data set roughly equal subsets perform inference model selection independently subset local subset inference method aggregates posterior model probabilities modelvariable selection criteria obtain final model using notion geometric median approach leads improved concentration finding correct model model parameters also provably robust outliers data contamination\",\"discovering statistically significant patterns databases important challenging problem main obstacle problem difficulty taking account selection bias bias arising fact patterns selected extremely large number candidates databases paper introduce new approach predictive pattern mining problems address selection bias issue approach built recently popularized statistical inference framework called selective inference selective inference statistical inferences statistical hypothesis testing conducted based sampling distributions conditional selection event selection event characterized tractable way statistical inferences made without minding selection bias issue however pattern mining problems difficult characterize entire selection process mining algorithms main contribution paper solve challenging problem class predictive pattern mining problems introducing novel algorithmic framework demonstrate approach useful finding statistically significant patterns databases\",\"unscented transformation efficient method solve state estimation problem nonlinear dynamic system utilizing derivativefree higherorder approximation approximating gaussian distribution rather approximating nonlinear function applying kalman filter type estimator leads wellknown unscented kalman filter ukf although ukf works well gaussian noises performance may deteriorate significantly noises nongaussian especially system disturbed heavytailed impulsive noises improve robustness ukf impulsive noises new filter nonlinear systems proposed work namely maximum correntropy unscented filter mcuf mcuf applied obtain prior estimates state covariance matrix robust statistical linearization regression based maximum correntropy criterion mcc used obtain posterior estimates state covariance satisfying performance new algorithm confirmed two illustrative examples\",\"quantitatively assessing relationships latent variables observed variables important understanding developing generative models representation learning paper propose latentobserved dissimilarity lod evaluate dissimilarity probabilistic characteristics latent observed variables also define four essential types generative models different independenceconditional independence configurations experiments using tractable realworld data show lod effectively capture differences models reflect capability higher layer learning also show conditional independence latent variables given observed variables contributes improving transmission information characteristics lower layers higher layers\",\"propose method performs anomaly detection localisation within heterogeneous data using pairwise undirected mixed graphical model data mixture categorical quantitative variables model learned dataset supposed contain anomaly use model temporal data potentially data stream using version twosided cusum algorithm proposed decision statistic based conditional likelihood ratio computed variable given others results show function allows detect anomalies variable variable thus localise variables involved anomalies precisely univariate methods based simple marginals\",\"study problem estimating temporally varying coefficient varying structure vcvs graphical model underlying nonstationary time series data social states interacting individuals microarray expression profiles gene networks opposed iid data invariant model widely considered current literature structural estimation particular consider scenario model evolves piecewise constant fashion propose procedure minimizes socalled tesla loss temporally smoothed regularized regression allows jointly estimating partition boundaries vcvs model coefficient sparse precision matrix block partition highly scalable proximal gradient method proposed solve resultant convex optimization problem conditions sparsistent estimation convergence rate partition boundaries network structure established first time estimators\",\"paper study predictive pattern mining problems goal construct predictive model based subset predictive patterns database main contribution introduce novel method called safe pattern pruning spp class predictive pattern mining problems spp method allows efficiently find superset predictive patterns database needed optimal predictive model advantage spp method existing boostingtype method former find superset single search database latter requires multiple searches spp method inspired recent development safe feature screening order extend idea safe feature screening predictive pattern mining derive novel pruning rule called safe pattern pruning spp rule used searching tree defined among patterns database spp rule property node corresponding pattern database pruned spp rule guaranteed patterns corresponding descendant nodes never needed optimal predictive model apply spp method graph mining itemset mining problems demonstrate computational advantage\",\"identifying homogeneous subgroups variables challenging high dimensional data analysis highly correlated predictors propose new method called hexagonal operator regression shrinkage equality selection horses short simultaneously selects positively correlated variables identifies predictive clusters achieved via constrained leastsquares problem regularization consists linear combination penalty coefficients another penalty pairwise differences coefficients specification penalty function encourages grouping positively correlated predictors combined sparsity solution construct efficient algorithm implement horses procedure show via simulation proposed method outperforms variable selection methods terms prediction error parsimony technique demonstrated two data sets small data set analysis soil appalachia high dimensional data set near infrared nir spectroscopy study showing flexibility methodology\",\"shown aictype criteria asymptotically efficient selectors tuning parameter nonconcave penalized regression methods assumption population variance known consistent estimator available relax assumption prove aic asymptotically efficient study performance finite samples classical regression known aic tends select overly complex models dimension maximum candidate model large relative sample size simulation studies suggest aic suffers shortcomings used penalized regression therefore propose use classical corrected aic aicc alternative prove maintains desired asymptotic properties broaden results prove efficiency aic penalized likelihood methods context generalized linear models dispersion parameter similar results exist literature restricted set candidate models employing results classical literature maximumlikelihood estimation misspecified models able establish result general set candidate models use simulations assess performance aic aicc well selectors finite samples scadpenalized lasso regressions real data example considered\",\"present new approach estimating interdependence industries economy applying data science solutions exploiting interfirm buyerseller network data show problem estimating interdependence industries similar problem uncovering latent block structure network science literature estimate underlying structure greater accuracy propose extension sparse block model incorporates node textual information unbounded number industries interactions among latter task accomplished extending wellknown chinese restaurant process two dimensions inference based collapsed gibbs sampling model evaluated synthetic realworld datasets show proposed model improves predictive accuracy successfully provides satisfactory solution motivated problem also discuss issues affect future performance approach\",\"ksupport norm regularizer successfully applied sparse vector prediction problems show belongs general class norms formulated parameterized infimum quadratics extend ksupport norm matrices observe special case matrix cluster norm using formulation derive efficient algorithm compute proximity operator norms improves upon standard algorithm ksupport norm allows apply proximal gradient methods cluster norm also describe solve regularization problems employ centered versions norms finally apply matrix regularizers different matrix completion multitask learning datasets results indicate spectral ksupport norm cluster norm give state art performance problems significantly outperforming trace norm elastic net penalties\",\"regularized discriminant analysis rda proposed friedman widely popular classifier lacks interpretability impractical highdimensional data sets present interpretable computationally efficient classifier called highdimensional rda hdrda designed smallsample highdimensional setting hdrda show training observation regardless class contributes class covariance matrix resulting interpretable estimator borrows pooled sample covariance matrix moreover show hdrda equivalent classifier reducedfeature space dimension approximately equal training sample size result matrix operations employed hdrda computationally linear number features making classifier wellsuited highdimensional classification practice demonstrate hdrda often superior several sparse regularized classifiers terms classification accuracy three artificial six real highdimensional data sets also timing comparisons hdrda implementation sparsediscrim package standard rda formulation klar package demonstrate number features increases computational runtime hdrda drastically smaller rda\",\"biological systems often modelled different levels abstraction depending particular aimsresources study different models often provide qualitatively concordant predictions specific parametrisations generally unclear whether model predictions quantitatively agreement whether agreement holds different parametrisations present generally applicable statistical machine learning methodology automatically reconcile predictions different models across abstraction levels approach based defining correction map random function modifies output model order match statistics output different model system use two biological examples give proofofprinciple demonstration methodology discuss advantages potential applications\",\"sampling replacement occurs many settings machine learning notably bagging ensemble technique validation scheme number unique original items bootstrap sample important role behaviour prediction models learned indeed uncontrived examples duplicate items effect purpose report present distribution number unique original items bootstrap sample clearly concisely view enabling machine learning researchers understand control quantity existing future resampling techniques describe key characteristics distribution along generalisation case items come distinct categories classification cases discuss normal limit conduct empirical investigation derive heuristic normal approximation permissible\",\"propose datadriven coarsegraining formulation context equilibrium statistical mechanics contrast existing techniques based finetocoarse map adopt opposite strategy prescribing probabilistic coarsetofine map corresponds directed probabilistic model coarse variables play role latent generators fine scale allatom data informationtheoretic perspective framework proposed provides improvement upon relative entropy method capable quantifying uncertainty due information loss unavoidably takes place process furthermore readily extended fully bayesian model various sources uncertainties reflected posterior model parameters latter used produce point estimates finescale reconstructions macroscopic observables importantly predictive posterior distributions quantities predictive posterior distributions reflect confidence model function amount data level coarsegraining issues model complexity model selection seamlessly addressed employing hierarchical prior favors discovery sparse solutions revealing prominent features coarsegrained model flexible parallelizable monte carlo expectationmaximization mcem scheme proposed carrying inference learning tasks comparative assessment proposed methodology presented lattice spin system spce water model\",\"present method variable selection sparse generalized additive model method doesnt assume specific functional form select large number candidates takes form incremental forward stagewise regression given functional form assumed devised approach termed roughening adjust residuals iterations simulations show new method competitive popular machine learning approaches also demonstrate performance using real datasets method available part nlnet package cran httpscranrprojectorgpackagenlnet\",\"paper propose study family sparsityinducing penalty functions since penalty functions related kinetic energy special relativity call emphkinetic energy plus kep functions construct kep function using concave conjugate chidistance function present several novel insights kep function particular derive thresholding operator based kep function prove mathematical properties asymptotic properties sparsity modeling moreover show coordinate descent algorithm especially appropriate kep function additionally discuss relationship kep penalty functions ell mcp theoretical empirical analysis validates kep function effective efficient highdimensional data modeling\",\"dictionary learning proven powerful tool many image processing tasks atoms typically defined small image patches drawback dictionary encodes basic structures addition approach treats patches different locations one single set means loss information features wellaligned across signals case instance multitrial magneto electroencephalography meeg learning dictionary entire signals could make use alignement reveal higherlevel features case however small missalignements phase variations features would compensated paper propose extension common dictionary learning framework overcome limitations allowing atoms adapt position across signals method validated simulated real neuroelectric data\",\"paper studies ordered weighted owl norm regularization sparse estimation problems strongly correlated variables prove sufficient conditions clustering based correlationcolinearity variables using owl norm socalled oscar particular case results extend previous ones oscar several ways squared error loss conditions hold general owl norm weaker assumptions also establish clustering conditions absolute error loss far know novel result furthermore characterize statistical performance owl norm regularization generative models certain clusters regression variables strongly even perfectly correlated variables different clusters uncorrelated show true pdimensional signal generating data involves clusters log samples suffice accurately estimate signal regardless number coefficients within clusters estimation ssparse signals completely independent variables requires many measurements words using owl pay price terms number measurements presence strongly correlated variables\",\"paper prove probabilistic continuous complexity conjecture continuous complexity theory states complexity solving continuous problem probability approaching converges limit complexity solving problem worst case prove conjecture holds space problem elements uniformly convex nonuniformly convex case striking counterexample problem identifying brownian path wiener space shown probabilistic complexity converges half worst case complexity limit\",\"softmax representation probabilities categorical variables plays prominent role modern machine learning numerous applications areas large scale classification neural language modeling recommendation systems however softmax estimation expensive large scale inference high cost associated computing normalizing constant introduce efficient approximation softmax probabilities takes form rigorous lower bound exact probability bound expressed product pairwise probabilities leads scalable estimation based stochastic optimization allows perform doubly stochastic estimation subsampling training instances class labels show new bound interesting theoretical properties demonstrate use classification problems\",\"principal components analysis widely used technique dimension reduction characterization variability multivariate populations interest lies studying rotation principal components used effectively within responsepredictor set relationship context mode hunting specifically focusing patient rule induction method prim first develop fast version algorithm fastprim normality facilitates theoretical studies follow using basic geometrical arguments demonstrate rotation predictor space alone fact generate improved mode estimators simulation results used illustrate findings\",\"propose segmented ihmm sihmm hierarchical infinite hidden markov model ihmm supports simple efficient inference scheme sihmm well suited segmentation problems goal identify points time series transitions one relatively stable regime new regime conventional ihmms often struggle problems since mechanism distinguishing high lowlevel dynamics hierarchical hmms hhmms better require much complex expensive inference algorithms sihmm retains simplicity efficiency ihmm outperforms variety segmentation problems achieving performance matches exceeds complicated hhmm\",\"measuring dependence two random variables important critical many applied areas variable selection brain network analysis however know kind functional relationship two covariates requires dependence measure equitable gives similar scores equally noisy relationship different types fact dependence score continuous random variable taking values thus theoretically impossible give similar scores paper introduce new definition equitability dependence measure powerequitable weakequitable show simulation hhg copula dependence coefficient cdc weakequitable\",\"compute expected value kullbackleibler divergence various fundamental statistical models respect canonical priors probability simplex obtain closed formulas expected model approximation errors depending dimension models cardinalities sample spaces uniform prior expected divergence model containing uniform distribution bounded constant gamma models consider bound approached state space large models dimension grow fast dirichlet priors expected divergence bounded similar way concentration parameters take reasonable values results serve reference values complicated statistical models\",\"sparse coding core building block many data analysis machine learning pipelines typically solved relying generic optimization techniques optimal class firstorder methods nonsmooth convex functions iterative soft thresholding algorithm accelerated version ista fista however methods dont exploit particular structure problem hand input data distribution acceleration using neural networks proposed citegregor coined lista showed empirically one could achieve high quality estimates iterations modifying parameters proximal splitting appropriately paper study reasons acceleration mathematical analysis reveals related specific matrix factorization gram kernel dictionary attempts nearly diagonalise kernel basis produces small perturbation ell ball factorization succeeds prove resulting splitting algorithm enjoys improved convergence bound respect nonadaptive version moreover analysis also shows conditions acceleration occur mostly beginning iterative process consistent numerical experiments validate analysis showing dictionaries factorization exist adaptive acceleration fails\",\"paper consider sparse identifiable linear latent variable factor linear bayesian network models parsimonious analysis multivariate data propose computationally efficient method joint parameter model inference model comparison consists fully bayesian hierarchy sparse models using slab spike priors twocomponent deltafunction continuous mixtures nongaussian latent factors stochastic search ordering variables framework call slim sparse linear identifiable multivariate modeling validated benchmarked artificial real biological data sets slim closest spirit lingam shimizu differs substantially inference bayesian network structure learning model comparison experimentally slim performs equally well better lingam comparable computational complexity attribute mainly stochastic search strategy used parsimony sparsity identifiability explicit part model propose two extensions basic iid linear framework nonlinear dependence observed variables called snim sparse nonlinear identifiable multivariate modeling allowing correlations latent variables called cslim correlated slim temporal andor spatial data source code scripts available httpcogsysimmdtudkslim\",\"systems biomedicine experimenter encounters different potential sources variation data individual samples multiple experimental conditions multivariable networklevel responses multiparametric cytometry often used analyzing patient samples issues critical computational methods identify cell populations individual samples without ability automatically match across samples difficult compare characterize populations typical experiments responding various stimulations distinctive particular patients timepoints especially many samples joint clustering matching jcm multilevel framework simultaneous modeling registration populations across cohort jcm models every population robust multivariate probability distribution simultaneously jcm fits randomeffects model construct overall batch template used registering populations across samples classifying new samples tackling systemslevel variation jcm supports practical biomedical applications involving large cohorts\",\"present novel algorithm westfallyoung light detecting patterns itemsets subgraphs statistically significantly enriched one two classes method corrects rigorously multiple hypothesis testing correlations patterns westfallyoung permutation procedure empirically estimates null distribution pattern frequencies class via permutations experiments westfallyoung light dramatically outperforms current stateoftheart approach terms runtime memory efficiency popular realworld benchmark datasets pattern mining key efficiency unlike existing methods algorithm neither needs solve underlying frequent itemset mining problem anew permutation needs store occurrence list frequent patterns westfallyoung light opens door significant pattern mining large datasets previously led prohibitive runtime memory costs\",\"work studies class algorithms learning sideinformation emerge extending generative models embedded contextrelated variables using finite mixture models fmm prototypical bayesian network show maximumlikelihood estimation mle parameters expectationmaximization improves regular unsupervised case approach performances supervised learning despite absence explicit ground truth data labeling direct application missing information principle mip algorithms performances proven range conventional supervised unsupervised mle extremities proportionally information content contextual assistance provided acquired benefits regard higher estimation precision smaller standard errors faster convergence rates improved classification accuracy regression fitness shown various scenarios also highlighting important properties differences among outlined situations applicability showcased three realworld unsupervised classification scenarios employing gaussian mixture models importantly exemplify natural extension methodology type generative model deriving equivalent contextaware algorithm variational autoencoders vas thus broadening spectrum applicability unsupervised deep learning artificial neural networks latter contrasted neuralsymbolic algorithm exploiting sideinformation\",\"paper addresses problem segmenting timeseries respect changes mean value variance first case time data modeled sequence independent normal distributed random variables unknown possibly changing mean value fixed variance main assumption mean value piecewise constant time task estimate change times mean values within segments second case mean value constant variance change assumption variance piecewise constant time want estimate change times variance values within segments find solutions problems study regularized maximum likelihood method related fused lasso method trend filtering parameters estimated free vary sample penalize variations estimated parameters lnorm time difference parameters used regularization term idea closely related total variation denoising main contribution convex formulation variance estimation problem parametrization based inverse variance formulated certain mean estimation problem implies results methods mean estimation applied challenging problem variance segmentationestimation\",\"propose general modeling inference framework composes probabilistic graphical models deep learning methods combines respective strengths model family augments graphical structure latent variables neural network observation models inference extend variational autoencoders use graphical model approximating distributions recognition networks output conjugate potentials components models learned simultaneously single objective giving scalable algorithm leverages stochastic variational inference natural gradients graphical model message passing reparameterization trick illustrate framework several example models application mouse behavioral phenotyping\",\"dictionary learning cuttingedge area imaging processing recently led stateoftheart results many signal processing tasks idea conduct linear decomposition signal using atoms learned usually overcompleted dictionary instead predefined basis determining proper size tobelearned dictionary crucial precision efficiency process existing dictionary learning algorithms choose size quite arbitrarily paper novel regularization method called grouped smoothly clipped absolute deviation gscad employed learning dictionary proposed method simultaneously learn sparse dictionary select appropriate dictionary size efficient algorithm designed based alternative direction method multipliers admm decomposes joint nonconvex problem nonconvex penalty two convex optimization problems several examples presented image denoising experimental results compared stateoftheart approaches\",\"develop automated variational inference method bayesian structured prediction problems gaussian process priors linearchain likelihoods approach need know details structured likelihood model scale large number observations furthermore show required expected likelihood term gradients variational objective elbo estimated efficiently using expectations lowdimensional gaussian distributions optimization elbo fully parallelizable sequences amenable stochastic optimization use along control variate techniques stateoftheart incremental optimization make framework useful practice results set natural language processing tasks show method good sometimes better hardcoded approaches including svmstruct crfs overcomes scalability limitations previous inference algorithms based sampling overall fundamental step developing automated inference methods bayesian structured prediction\",\"factorized information criterion fic recently developed information criterion based novel model selection methodology namely factorized asymptotic bayesian fab inference developed successfully applied various hierarchical bayesian models dirichlet process prior one well known representations chinese restaurant process crp derive another line model selection methods fic viewed prior distribution latent variable configurations view prove parameter dimensionality fic equivalent crp argue fic avoids inherent problem dpcrp data likelihood dominate impact prior thus model selection capability weaken increases however fic overestimates data likelihood result fic may overly biased towards models less components propose natural generalization fic finds middle ground crp fic may yield accurate model selection results fic\",\"boosting combines weak biased learners obtain effective learning algorithms classification prediction paper show connection boosting kernelbased methods highlighting theoretical practical applications context ell boosting start weak linear learner defined kernel show boosting learner equivalent estimation special boosting kernel depends well regression matrix noise variance hyperparameters number boosting iterations modeled continuous hyperparameter fit along parameters using standard techniques generalize boosting kernel broad new class boosting approaches general weak learners including based ell hinge vapnik losses approach allows fast hyperparameter tuning general class wide range applications including robust regression classification illustrate applications numerical examples synthetic real data\",\"first encountered pacbayesian concentration inequalities seemed rather disconnected good oldfashioned results like hoeffdings bernsteins inequalities least one flavour pacbayesian bounds actually close relation main innovation continuous version union bound along ingenious applications heres gist whats going presented machine learning perspective\",\"consider sequential decision making problems binary classification scenario learner takes active role repeatedly selecting samples action pool receives binary label selected alternatives problem motivated applications observations time consuming andor expensive resulting small samples goal identify best alternative highest response use bayesian logistic regression predict response alternative formulating problem markov decision process develop knowledgegradient type policy guide experiment maximizing expected value information labeling alternative provide finitetime analysis estimated error experiments benchmark uci datasets demonstrate effectiveness proposed method\",\"address structured covariance estimation elliptical distributions assuming covariance priori known belong given convex set set toeplitz banded matrices consider general method moments gmm optimization applied robust tylers scatter mestimator subject convex constraints unfortunately gmm turns nonconvex due objective instead propose new coca estimator convex relaxation efficiently solved prove relaxation tight unconstrained case finite number samples constrained case asymptotically illustrate advantages coca synthetic simulations structured compound gaussian distributions examples coca outperforms competing methods tylers estimator projection onto structure set\",\"stochastic variational inference svi stateoftheart algorithm scaling variational inference largedatasets inherently serial moreover requires parameters fit memory single processor problematic number parameters billions paper propose extreme stochastic variational inference esvi asynchronous lockfree algorithm perform variational inference mixture models massive real world datasets esvi overcomes limitations svi requiring processor access subset data subset parameters thus providing data model parallelism simultaneously demonstrate effectiveness esvi running latent dirichlet allocation lda umbcb dataset vocabulary million token size billion experiments found esvi outperforms svi wallclocktime also achieves better quality solution addition propose strategy speed computation save memory fitting large number topics\",\"probabilistic models often parameters translated scaled permuted otherwise transformed without changing model symmetries lead strong correlation multimodality posterior distribution models parameters pose challenges performing inference interpreting results work address automatic detection common problematic model symmetries introduce local symmetries cover many common cases amenable automatic detection show derive algorithms detect several broad classes local symmetries algorithms compatible probabilistic programming constructs arrays loops statements scale models many variables\",\"provide general theory expectationmaximization algorithm inferring high dimensional latent variable models particular make two contributions parameter estimation propose novel high dimensional algorithm naturally incorporates sparsity structure parameter estimation appropriate initialization algorithm converges geometric rate attains estimator nearoptimal statistical rate convergence based obtained estimator propose new inferential procedures testing hypotheses constructing confidence intervals low dimensional components high dimensional parameters broad family statistical models framework establishes first computationally feasible approach optimal estimation asymptotic inference high dimensions theory supported thorough numerical results\",\"multioutput gaussian processes mogp probability distributions vectorvalued functions previously used multioutput regression multiclass classification less explored facet multioutput gaussian process used generative model vectorvalued random fields context pattern recognition generative model multioutput able handle vectorvalued functions continuous inputs opposed example hidden markov models also offers ability model multivariate random functions high dimensional inputs report use discriminative training criteria known minimum classification error fit parameters multioutput gaussian process compare performance generative training discriminative training mogp emotion recognition activity recognition face recognition also compare proposed methodology hidden markov models trained generative discriminative way\",\"nongaussian component analysis ngca unsupervised linear dimension reduction method extracts lowdimensional nongaussian signals highdimensional data contaminated gaussian noise ngca regarded generalization projection pursuit independent component analysis ica multidimensional dependent nongaussian components indeed seminal approaches ngca based ica recently novel ngca approach called leastsquares ngca lsngca developed gives solution analytically leastsquares estimation logdensity gradients eigendecomposition however since prewhitening data involved lsngca performs unreliably data covariance matrix illconditioned often case highdimensional data analysis paper propose whiteningfree lsngca method experimentally demonstrate superiority\",\"sparse generalized eigenvalue problem gep plays pivotal role large family highdimensional statistical models including sparse fishers discriminant analysis canonical correlation analysis sufficient dimension reduction sparse gep involves solving nonconvex optimization problem existing methods theory context specific statistical models special cases sparse gep require restrictive structural assumptions input matrices paper propose twostage computational framework solve sparse gep first stage solve convex relaxation sparse gep taking solution initial value exploit nonconvex optimization perspective propose truncated rayleigh flow method rifle estimate leading generalized eigenvector show rifle converges linearly solution optimal statistical rate convergence many statistical models theoretically method significantly improves upon existing literature eliminating structural assumptions input matrices stages achieve analysis involves two key ingredients new analysis gradient based method nonconvex objective functions finegrained characterization evolution sparsity patterns along solution path thorough numerical studies provided validate theoretical results\",\"response variables nominal populations crossclassified respect multiple polytomies questions often arise degree association responses explanatory variables populations known introduce nominal association vector matrix evaluate dependence response variable explanatory variable measures provide detailed evaluations nominal associations local global levels also define general class global association measures embraces well known association measure goodmankruskal proposed association matrix also gives rise expected generalized confusion matrix classification hierarchy equivalence relations defined association vector matrix also shown\",\"compare risk ridge regression simple variant ordinary least squares one simply projects data onto finite dimensional subspace specified principal component analysis performs ordinary unregularized least squares regression subspace note shows risk ordinary least squares method within constant factor namely risk ridge regression\",\"practical bayesian optimization must often search structures differing numbers parameters instance may wish search neural network architectures unknown number layers relate performance data gathered different architectures define new kernel conditional parameter spaces explicitly includes information parameters relevant given structure show kernel improves model quality bayesian optimization results several simpler baseline kernels\",\"modeling structure complex networks using bayesian nonparametrics makes possible specify flexible model structures infer adequate model complexity observed data paper provides gentle introduction nonparametric bayesian modeling complex networks using infinite mixture model running example steps deriving model infinite limit finite parametric model inferring model parameters markov chain monte carlo checking models fit predictive performance explain advanced nonparametric models complex networks derived point relevant literature\",\"paper develops exact linear relationship leading eigenvector unnormalized modularity matrix eigenvectors adjacency matrix propose method approximating leading eigenvector modularity matrix derive error approximation also complete proof equivalence normalized adjacency clustering normalized modularity clustering numerical experiments show normalized adjacency clustering twice efficient normalized modularity clustering\",\"introduce general constructive setting density ratio estimation problem solution multidimensional integral equation equation right hand side known approximately also integral operator defined approximately show illposed problem rigorous solution obtain solution closed form key element solution novel vmatrix captures geometry observed samples compare method three wellknown previously proposed ones experimental results demonstrate good potential new approach\",\"deep learning methods attempt learn generic features unsupervised fashion large unlabelled data set generic features perform well best hand crafted features learning problem makes use data provide definition generic features characterize possible learn provide methods closely related autoencoder deep belief network deep learning order use notion deficiency illustrate value studying certain general learning problems\",\"propose new class metrics sets vectors functions used various stages data mining including exploratory data analysis learning result interpretation new distance functions unify generalize popular metrics jaccard bag distances sets manhattan distance vector spaces marczewskisteinhaus distance integrable functions prove new metrics complete show useful relationships fdivergences probability distributions extend approach structured objects concept hierarchies ontologies introduce informationtheoretic metrics directed acyclic graphs drawn according fixed probability distribution conduct empirical investigation demonstrate intuitive interpretation new metrics effectiveness realvalued highdimensional structured data extensive comparative evaluation demonstrates new metrics outperformed multiple similarity dissimilarity functions traditionally used data mining including minkowski family fractional family two fdivergences cosine distance two correlation coefficients finally argue new class metrics particularly appropriate rapid processing highdimensional structured data distancebased learning\",\"goal crossdomain object matching cdom find correspondence two sets objects different domains unsupervised way photo album summarization typical application cdom photos automatically aligned designed frame expressed cartesian coordinate system cdom usually formulated finding mapping objects one domain photos objects domain frame pairwise dependency maximized stateoftheart cdom method employs kernelbased dependency measure drawback kernel parameter needs determined manually paper propose alternative cdom methods naturally address model selection problem experiments image matching unpaired voice conversion photo album summarization tasks effectiveness proposed methods demonstrated\",\"using knearest neighbors method one often ignores uncertainty choice account uncertainty holmes adams proposed bayesian framework knearest neighbors knn bayesian knn bknn approach uses pseudolikelihood function standard markov chain monte carlo mcmc techniques draw posterior samples holmes adams focused performance bknn terms misclassification error assess ability quantify uncertainty present evidence show bknn still significantly underestimates model uncertainty\",\"use statistical learning methods construct adaptive state estimator nonlinear stochastic systems optimal state estimation form kalman filter requires knowledge systems process measurement uncertainty propose uncertainties estimated conditioned past observed data without making assumptions systems prior distribution systems prior distribution time step constructed ensemble leastsquares estimates subsampled sets data via jackknife sampling new data acquired state estimates process uncertainty measurement uncertainty updated accordingly described manuscript\",\"article present elitist particle filter based evolutionary strategies epfes efficient approach nonlinear system identification epfes derived frequentlyemployed statespace model relevant information nonlinear system captured unknown state vector similar classical particle filtering epfes consists set particles respective weights represent different realizations latent state vector likelihood solution optimization problem main innovation epfes includes evolutionary elitistparticle selection combines longterm information instantaneous sampling approximated continuous posterior distribution article propose two advancements previouslypublished elitistparticle selection process epfes shown generalization widelyused gaussian particle filter thus evaluated respect latter two completely different scenarios first consider socalled univariate nonstationary growth model timevariant latent state variable evolutionary selection elitist particles evaluated nonrecursively calculated particle weights second problem nonlinear acoustic echo cancellation addressed simulated scenario speech input signal using longterm fitness measures highlight efficacy wellgeneralizing epfes estimating nonlinear system even large search spaces finally illustrate similarities epfes evolutionary algorithms outline future improvements fusing achievements fields research\",\"scaled complex wishart distribution widely used model multilook full polarimetric sar data whose adequacy attested literature classification segmentation image analysis techniques depend model devised many employ type dissimilarity measure paper derive analytic expressions four stochastic distances relaxed scaled complex wishart distributions general form important particular cases using distances inequalities obtained lead new ways deriving bartlett revised wishart distances expressiveness four analytic distances assessed respect variation parameters distances used deriving new tests statistics proved asymptotic chisquare distribution adopting test size comparison criterion sensitivity study performed means monte carlo experiments suggesting bhattacharyya statistic outperforms others power tests also assessed applications actual data illustrate discrimination homogeneity identification capabilities distances\",\"problem learning sparse model conceptually interpreted process identifying active featuressamples optimizing model recently introduced safe screening allows identify part nonactive featuressamples far safe screening individually studied either feature screening sample screening paper introduce new approach safely screening features samples simultaneously alternatively iterating feature sample screening steps significant advantage considering simultaneously rather individually synergy effect sense results previous safe feature screening exploited improving next safe sample screening performances viceversa first theoretically investigate synergy effect illustrate practical advantage intensive numerical experiments problems large numbers features samples\",\"multivariate normal density monotonic function distance mean ellipsoidal shape due underlying euclidean metric suggest replace metric locally adaptive smoothly changing riemannian metric favors regions high local density resulting locally adaptive normal distribution land generalization normal distribution manifold setting data assumed lie near potentially lowdimensional manifold embedded mathbbrd land parametric depending mean covariance maximum entropy distribution given metric underlying metric however nonparametric develop maximum likelihood algorithm infer distribution parameters relies combination gradient descent monte carlo integration extend land mixture models provide corresponding algorithm demonstrate efficiency land fit nontrivial probability distributions synthetic data eeg measurements human sleep\",\"additive nonparametric regression models provide attractive tool variable selection high dimensions relationship response predictors complex offer greater flexibility compared parametric nonlinear regression models better interpretability scalability nonparametric regression models however achieving sparsity simultaneously number nonparametric components well variables within nonparametric component poses stiff computational challenge article develop novel bayesian additive regression model using combination hard soft shrinkages separately control number additive components variables within component efficient algorithm developed select importance variables estimate interaction network excellent performance obtained simulated real data examples\",\"present vectorspace markov random fields vsmrfs novel class undirected graphical models variable belong arbitrary vector space vsmrfs generalize recent line work scalarvalued uniparameter exponential family mixed graphical models thereby greatly broadening class exponential families available allowing multinomial dirichlet distributions specifically vsmrfs joint graphical model distributions nodeconditional distributions belong generic exponential families general vector space domains also present sparsistent mestimator learning class mrfs recovers correct set edges high probability validate approach via set synthetic data experiments well realworld case study four million foods popular diet tracking app myfitnesspal results demonstrate algorithm performs well empirically vsmrfs capable capturing highlighting interesting structure complex realworld data code algorithm open source publicly available\",\"paper addresses problem scalable optimization lregularized conditional gaussian graphical models conditional gaussian graphical models generalize wellknown gaussian graphical models conditional distributions model output network influenced conditioning input variables highly scalable optimization methods exist sparse gaussian graphical model estimation stateoftheart methods conditional gaussian graphical models efficient enough importantly fail due memory constraints large problems paper propose new optimization procedure based newton method efficiently iterates two subproblems leading drastic improvement computation time compared previous methods extend method scale large problems memory constraints using block coordinate descent limit memory usage achieving fast convergence using synthetic genomic data show methods solve one million dimensional problems high accuracy little day single machine\",\"paper consider clustering data assumed come one finitely many pointed convex polyhedral cones model referred union polyhedral cones uopc model similar union subspaces uos model data subspace generated unknown basis uopc model data cone assumed generated finite number unknown emphextreme raysto cluster data model consider several algorithms sparse subspace clustering nonnegative constraints lasso ncl least squares approximation lsa knearest neighbor knn algorithm arrive affinity data points spectral clustering applied resulting affinity matrix cluster data different polyhedral cones show average knn outperforms ncl lsa algorithm provide deterministic conditions correct clustering affinity measure cones shown long cones coherent long density data within cone exceeds threshold knn leads accurate clustering finally simulation results real datasets mnist yaleface datasets depict proposed algorithm works well real data indicating utility uopc model proposed algorithm\",\"propose new yet natural algorithm learning graph structure general discrete graphical models aka markov random fields samples algorithm finds neighborhood node sequentially adding nodes produce largest reduction empirical conditional entropy greedy sense choice addition based reduction achieved iteration sequential nature gives lower computational complexity compared existing comparisonbased techniques involve exhaustive searches every node set certain size main result characterizes sample complexity procedure function node degrees graph size girth factorgraph representation subsequently specialize result case ising models provide simple transparent characterization sample complexity function model graph parameters tree graphs algorithm classical chowliu algorithm sense considered extension graphs cycles\",\"paper introduces linear statespace model timevarying dynamics time dependency obtained forming state dynamics matrix timevarying linear combination set matrices time dependency weights linear combination modelled another linear gaussian dynamical model allowing model learn dynamics process changes previous approaches used switching models small set possible state dynamics matrices model selects one matrices time thus jumping model forms dynamics linear combination changes smooth continuous model motivated physical processes described linear partial differential equations whose parameters vary time example process could temperature field whose evolution driven varying wind direction posterior inference performed using variational bayesian approximation experiments stochastic advectiondiffusion processes realworld weather processes show model timevarying dynamics outperform previously introduced approaches\",\"johnsonlindenstrauss lemma allows projection points pdimensional euclidean space onto kdimensional euclidean space fracln emphnepsilonepsilon pairwise distances preserved within factor pmepsilon working directly distributions random distances rather resorting moment generating function technique improvement lower bound obtained additional reduction dimension compared bounds found literature least cases additional reduction achieved using moment generating function technique provide lower bound using pairwise distances space points projected pairwise distances space projected points comparison results obtained literature shows bound presented provides additional reduction\",\"multitask sparse feature learning aims improve generalization performance exploiting shared features among tasks successfully applied many applications including computer vision biomedical informatics existing multitask sparse feature learning algorithms formulated convex sparse regularization problem usually suboptimal due looseness approximating elltype regularizer paper propose nonconvex formulation multitask sparse feature learning based novel nonconvex regularizer solve nonconvex optimization problem propose multistage multitask feature learning msmtfl algorithm also provide intuitive interpretations detailed convergence reproducibility analysis proposed algorithm moreover present detailed theoretical analysis showing msmtfl achieves better parameter estimation error bound convex formulation empirical studies synthetic realworld data sets demonstrate effectiveness msmtfl comparison state art multitask sparse feature learning algorithms\",\"introduce means automating machine learning big data tasks performing scalable stochastic bayesian optimisation algorithm parameters hyperparameters often critical tuning algorithm parameters relied domain expertise experts along laborious handtuning brute search lengthy sampling runs background bayesian optimisation finding increasing use automating parameter tuning making algorithms accessible even nonexperts however state art bayesian optimisation incapable scaling large number evaluations algorithm performance required fit realistic models complex big data describe stochastic sparse bayesian optimisation strategy solve problem using many thousands noisy evaluations algorithm performance subsets data order effectively train algorithms big data provide comprehensive benchmarking possible sparsification strategies bayesian optimisation concluding nystrom approximation offers best scaling performance real tasks proposed algorithm demonstrates substantial improvement state art tuning parameters gaussian process time series prediction task real big data\",\"constrained adaptive filtering algorithms inculding constrained least mean square clms constrained affine projection cap constrained recursive least squares crls extensively studied many applications existing constrained adaptive filtering algorithms developed mean square error mse criterion ideal optimality criterion gaussian noises assumption however fails model behavior nongaussian noises found practice motivated robustness simplicity maximum correntropy criterion mcc nongaussian impulsive noises paper proposes new adaptive filtering algorithm called constrained maximum correntropy criterion cmcc specifically cmcc incorporates linear constraint mcc filter solve constrained optimization problem explicitly proposed adaptive filtering algorithm easy implement low computational complexity terms convergence accuracy say lower mean square deviation stability significantly outperform mse based constrained adaptive algorithms presence heavytailed impulsive noises additionally mean square convergence behaviors studied energy conservation relation sufficient condition ensure mean square convergence steadystate mean square deviation msd proposed algorithm obtained simulation results confirm theoretical predictions gaussian non gaussian noises demonstrate excellent performance novel algorithm comparing conventional methods\",\"latent force models lfm principled approaches incorporating solutions differential equations within nonparametric inference methods unfortunately development application lfms inhibited computational cost especially closedform solutions lfm unavailable case many real world problems latent forces exhibit periodic behaviour given develop new sparse representation lfms considerably improves computational efficiency well broadening applicability principled way domains periodic near periodic latent forces approach uses linear basis model approximate one generative model periodic force assume latent forces generated gaussian process priors develop linear basis model fully expresses priors apply approach model thermal dynamics domestic buildings show effective predicting dayahead temperatures within homes also apply approach within queueing theory quasiperiodic arrival rates modelled latent forces cases demonstrate approach implemented efficiently using statespace methods encode linear dynamic systems via lfms show state estimates obtained using periodic latent force models reduce root mean squared error nonperiodic models nearest rival approach resonator model\",\"sparse bayesian learning sbl gaussian scale mixtures gsms used model sparsityinducing priors realize class concave penalty functions regression task realvalued signal models motivated relative scarcity formal tools sbl complexvalued models paper proposes gsm model bessel model induces concave penalty functions estimation complex sparse signals properties bessel model analyzed applied type type estimation analysis reveals tuning parameters mixing pdf different penalty functions invoked depending estimation type used value noise variance whether real complex signals estimated using bessel model derive sparse estimator based modification expectationmaximization algorithm formulated type estimation estimator includes special instance algorithms proposed tipping faul babacan numerical results show superiority proposed estimator stateoftheart estimators terms convergence speed sparseness reconstruction error robustness low medium signaltonoise ratio regimes\",\"recent popularity graphical clustering methods increased focus information samples show learning cluster structure using edge features naturally simultaneously determines likely number clusters addresses data scale issues results particularly useful instances large number clusters labeled edges applications domain include image segmentation community discovery entity resolution model extension planted partition model solution uses results correlation clustering achieves partition olognclose loglikelihood true clustering\",\"project investigate idea reducing dimensionality datasets using borel isomorphism purpose subsequently applying supervised learning algorithms originally suggested supervisor pestov dagstuhl preprint consistent learning algorithm example knn retains universal consistency borel isomorphism applied series concrete examples borel isomorphisms reduce number dimensions dataset provided based multiplying data orthogonal matrices dimensionality reducing borel isomorphism applied test accuracy resulting classifier lower dimensional space various data sets working phoneme voice recognition dataset dimension classes phonemes show borel isomorphic reduction dimension leads minimal drop accuracy conclusion discuss prospects method\",\"paper study statistical properties semisupervised learning considered important problem community machine learning standard supervised learning labeled data observed classification regression problems formalized supervised learning semisupervised learning unlabeled data also obtained addition labeled data hence exploiting unlabeled data important improve prediction accuracy semisupervised learning problems regarded semiparametric estimation problem missing data discriminative probabilistic models considered unlabeled data useless improve estimation accuracy recently revealed weighted estimator using unlabeled data achieves better prediction accuracy comparison learning method using labeled data especially discriminative probabilistic model misspecified improvement semiparametric model missing data possible semiparametric model misspecified paper apply densityratio estimator obtain weight function semisupervised learning benefit approach proposed estimator require wellspecified probabilistic models probability unlabeled data based statistical asymptotic theory prove estimation accuracy method outperforms supervised learning using labeled data numerical experiments present usefulness methods\",\"presence weak overall correlation may useful investigate correlation significantly substantially pronounced subpopulation two different testing procedures compared based rankings values two variables data set large number observations first maintains level gaussian copulas second adapts general alternatives sense number parameters used test grows analysis wine quality illustrates methods detect heterogeneity association chemical properties wine attributable mix different cultivars\",\"recent decades seen interest prediction problems bayesian methodology used ubiquitously sampling approximating posterior predictive distribution bayesian model allows one make inferential statements potentially observable random quantities given observed data purpose note use statistical decision theory basis justify use posterior predictive distribution making point prediction\",\"popular cubic smoothing spline estimate regression function arises minimizer penalized sum squares sumjyj mutj lambdaintab mut data tjyj minimization taken infinitedimensional function space space functions square integrable second derivatives calculations carried finitedimensional space reduction minimizing infinite dimensional space minimizing finite dimensional space occurs general objective functions data may related function another way sum squares may replaced suitable expression penalty intab mut might take different form paper reviews reproducing kernel hilbert space structure provides finitedimensional solution general minimization problem particular attention paid penalties based linear differential operators case one sometimes easily calculate minimizer explicitly using greens functions\",\"paper focus developing efficient sensitivity analysis methods computationally expensive objective function case minimization performed computationally expensive means evaluation takes significant amount time therefore main goal use small number function evaluations infer sensitivity information different parameters correspondingly consider optimization procedure adaptive experimental design reuse available function evaluations initial design points establish surrogate model called response surface sensitivity analysis performed lieu furthermore propose new local multivariate sensitivity measure example around optimal solution high dimensional problems corresponding objectiveoriented experimental design proposed order make generated surrogate better suitable accurate calculation proposed specific local sensitivity quantities addition demonstrate better performance gaussian radial basis function interpolator kriging cases relatively high dimensionality experimental design points numerical experiments demonstrate optimization procedure objectiveoriented experimental design behavior much better classical latin hypercube design addition performance kriging good gaussian rbf especially case high dimensional problems\",\"scale data growing every day reducing dimensionality aka sketching highdimensional data emerged task paramount importance relevant issues address context include sheer volume data may consist categorical samples typically streaming format acquisition possibly missing entries cope challenges present paper develops novel categorical subspace learning approach unravel latent structure three prominent categorical bilinear models namely probit tobit logit deterministic probit tobit models treat data quantized values analogvalued process lying lowdimensional subspace probabilistic logit model relies low dimensionality data loglikelihood ratios leveraging low intrinsic dimensionality sought models rank regularized maximumlikelihood estimator devised solved recursively via alternating majorizationminimization sketch highdimensional categorical data fly resultant procedure alternates sketching new incomplete datum refining latent subspace leading lightweight firstorder algorithms highly parallelizable tasks per iteration extra degree freedom quantization thresholds also learned jointly along subspace enhance predictive power sought models performance subspace iterates analyzed infinite finite data streams former asymptotic convergence stationary point set batch estimator established latter sublinear regret bounds derived empirical cost simulated tests synthetic realworld datasets corroborate merits novel schemes realtime movie recommendation chessgame classification\",\"multivariate analysis mva comprises family wellknown methods feature extraction exploit correlations among input variables data representation one important property enjoyed methods uncorrelation among extracted features recently regularized versions mva methods appeared literature mainly goal gain interpretability solution cases solutions longer obtained closed manner frequent recur iteration two steps one orthogonal procrustes problem letter shows procrustes solution optimal perspective overall mva method proposes alternative approach based solution eigenvalue problem method ensures preservation several properties original methods notably uncorrelation extracted features demonstrated theoretically collection selected experiments\",\"proposed new statistical dependency measure called copula dependency coefficientcdc two sets variables based copula robust outliers easy implement powerful appropriate highdimensional variables properties important many applications experimental results show cdc detect dependence variables additive nonadditive models\",\"propose spectral clustering method based local principal components analysis pca performing local pca selected neighborhoods algorithm builds nearest neighbor graph weighted according discrepancy principal subspaces neighborhoods applies spectral clustering opposed standard spectral methods based solely pairwise distances points algorithm able resolve intersections establish theoretical guarantees simpler variants within prototypical mathematical framework multimanifold clustering evaluate algorithm various simulated data sets\",\"rapid growth crowdsourcing platforms become easy relatively inexpensive collect dataset labeled multiple annotators short time however due lack control quality annotators abnormal annotators may affected position bias potentially degrade quality final consensus labels paper introduce statistical framework model detect annotators position bias order control false discovery rate fdr without prior knowledge amount biased annotators expected fraction false discoveries among discoveries high order assure discoveries indeed true replicable key technical development relies new knockoff filters adapted problem new algorithms based inverse scale space dynamics whose discretization potentially suitable large scale crowdsourcing data analysis studies supported experiments simulated examples realworld data proposed framework provides useful tool quantitatively studying annotators abnormal behavior crowdsourcing data arising machine learning sociology computer vision multimedia etc\",\"propose novel algorithm solve expectation propagation relaxation bayesian inference continuousvariable graphical models contrast previous algorithms method provably convergent marrying convergent ideas opperwinther covariance decoupling techniques wipfnagarajan nickischseeger runs least order magnitude faster commonly used solver\",\"propose method called ideal regression approximating arbitrary system polynomial equations system particular type using techniques approximate computational algebraic geometry show solve ideal regression directly without resorting numerical optimization ideal regression useful whenever solution learning problem described system polynomial equations example demonstrate formulate stationary subspace analysis ssa source separation problem terms ideal regression also yields consistent estimator ssa compare estimator simulations previous optimizationbased approaches ssa\",\"multivariate categorical data occur many applications machine learning one main difficulties vectors categorical variables sparsity number possible observations grows exponentially vector length dataset diversity might poor comparison recent models gained significant improvement supervised tasks data models embed observations continuous space capture similarities building ideas propose bayesian model unsupervised task distribution estimation multivariate categorical data model vectors categorical variables generated nonlinear transformation continuous latent space nonlinearity captures multimodality distribution continuous representation addresses sparsity model ties together many existing models linking linear categorical latent gaussian model gaussian process latent variable model gaussian process classification derive inference model based recent developments sampling based variational inference show empirically model outperforms linear discrete counterparts imputation tasks sparse data\",\"paper considers problem estimating structure multiple related directed acyclic graph dag models building recent developments exact estimation dags using integer linear programming ilp present ilp approach joint estimation multiple dags require vertices dag share common ordering furthermore allow also potentially unknown dependency structure dags results presented simulated data fmri data obtained multiple subjects\",\"consider inverse problem reconstructing posterior measure trajec tories diffusion process discrete time observations continuous time constraints cast problem bayesian framework derive approximations posterior distributions single time marginals using variational approximate inference show approximation extended wide class discretestate markov jump pro cesses making use chemical langevin equation empirical results show proposed method computationally efficient provides good approximations classes inverse problems\",\"network models popular modeling representing complex relationships dependencies observed variables data comes dynamic stochastic process single static network model cannot adequately capture transient dependencies gene regulatory dependencies throughout developmental cycle organism kolar proposed method based kernelsmoothing lpenalized logistic regression estimating timevarying networks nodal observations collected timeseries observational data paper establish conditions proposed method consistently recovers structure timevarying network work complements previous empirical findings providing sound theoretical guarantees proposed estimation procedure completeness include numerical simulations paper\",\"problems machine learning involve noisy input data classification methods reached limiting accuracies based standard data sets consisting feature vectors classes greater accuracy require incorporation prior structural information data learning study methods regularize feature vectors unsupervised regularization methods analogous supervised regularization estimating functions study regularization denoising feature vectors using tikhonov regularization methods functions feature vector xxldotsxnxqqn viewed function index smoothed using prior information structure involve penalty functional feature vectors analogous statistical learning use proximity graph structure set indices feature vector regularization inherits property function denoising accuracy nonmonotonic denoising regularization parameter alpha assumptions noise level data structure show best reconstruction accuracy also occurs finite positive alpha index spaces graph structures adapt two standard function denoising methods used local averaging kernel regression general index space discrete set notion proximity metric space subset graphnetwork feature vectors functions notion continuity show improves feature vector recovery thus subsequent classification regression done give example gene expression analysis cancer classification genome index space network structure based proteinprotein interactions\",\"address problem detecting changes multivariate datastreams investigate intrinsic difficulty changedetection methods face data dimension scales particular consider general approach changes detected comparing distribution loglikelihood datastream different time windows despite fact approach constitutes frame several changedetection methods effectiveness data dimension scales never investigated indeed goal paper show magnitude change naturally measured symmetric kullbackleibler divergence pre postchange distributions detectability change given magnitude worsens data dimension increases problem refer emphdetectability loss due linear relationship variance loglikelihood data dimension analytically derive detectability loss gaussiandistributed datastreams empirically demonstrate problem holds also realworld datasets harmful even low datadimensions say\",\"important topic systems biology developing statistical methods automatically find causal relations gene regulatory networks prior knowledge causal connectivity many methods developed time series data however discovery methods based steadystate data often necessary preferable since obtaining time series data expensive andor infeasible many biological systems conventional approach causal bayesian networks however estimation bayesian networks illposed many cases cannot uniquely identify underlying causal network gives large class equivalent causal networks cannot distinguished based data distribution propose new discovery algorithm uniquely identifying underlying causal network genes best knowledge proposed method first algorithm learning gene networks based fully identifiable causal model called lingam compare algorithm competing algorithms using artificiallygenerated data although definitely better test based real microarray gene expression data\",\"propose likelihood ratio based inferential framework high dimensional semiparametric generalized linear models framework addresses variety challenging problems high dimensional data analysis including incomplete data selection bias heterogeneous multitask learning work three main contributions develop regularized statistical chromatography approach infer parameter interest proposed semiparametric generalized linear model without need estimating unknown base measure function propose new framework construct postregularization confidence regions tests low dimensional components high dimensional parameters unlike existing postregularization inferential methods approach based novel directional likelihood particular framework naturally handles generic regularized estimators nonconvex penalty functions used infer least false parameters misspecified models iii develop new concentration inequalities normal approximation results ustatistics unbounded kernels independent interest demonstrate consequences general theory using example missing data problem extensive simulation studies real data analysis provided illustrate proposed approach\",\"study twolevel multiview learning two views pacbayesian framework approach sometimes referred late fusion consists learning sequentially multiple viewspecific classifiers first level combining viewspecific classifiers second level main theoretical result generalization bound risk majority vote exhibits term diversity predictions viewspecific classifiers result comes controlling tradeoff diversity accuracy key element multiview learning complements results multiview learning finally experiment principle multiview datasets extracted reuters rcvrcv collection\",\"consider learning highdimensional multiresponse linear models structured parameters exploiting noise correlations among responses propose alternating estimation altest procedure estimate model parameters based generalized dantzig selector suitable sample size resampling assumptions show error estimates generated altest high probability converges linearly certain minimum achievable level tersely expressed geometric measures gaussian width sets related parameter structure best knowledge first nonasymptotic statistical guarantee altesttype algorithm applied estimation problem general structures\",\"article derive new stepsize adaptation normalized least mean square algorithm nlms describing task linear acoustic echo cancellation bayesian network perspective similar wellknown kalman filter equations model acoustic wave propagation loudspeaker microphone latent state vector define linear observation equation model relation state vector observation well linear process equation model temporal progress state vector based additional assumptions statistics random variables observation process equation apply expectationmaximization algorithm derive nlmslike filter adaptation exploiting conditional independence rules bayesian networks reveal resulting emnlms algorithm stepsize update equivalent optimalstepsize calculation proposed yamamoto kitayama adopted many textbooks main difference instantaneous stepsize value estimated step algorithm instead approximated artificially extending acoustic echo path emnlms algorithm experimentally verified synthesized scenarios white noise male speech input signal\",\"hearing aid algorithms need tuned fitted match impairment specific patient lack fundamental fitting theory strong contributing factor unsatisfying sound experience hearing aid patients paper proposes probabilistic modeling approach design algorithms proposed method relies generative probabilistic model hearing loss problem provides automated inference corresponding signal processing algorithm fitting solution well principled performance evaluation metric three tasks realized message passing algorithms factor graph representation generative model principle allows fast implementation hearing aid mobile device hardware methods theoretically worked simulated custombuilt factor graph toolbox specific hearing loss model\",\"early stopping well known approach reduce time complexity performing training model selection large scale learning machines hand memoryspace rather time complexity main constraint many applications randomized subsampling techniques proposed tackle issue paper ask whether early stopping subsampling ideas combined fruitful way consider question least squares regression setting propose form randomized iterative regularization based early stopping subsampling context analyze statistical computational properties proposed method theoretical results complemented validated thorough experimental analysis\",\"distributions permutations arise applications ranging multiobject tracking ranking instances difficulty dealing distributions caused size domain factorial number considered entities makes direct definition multinomial distribution permutation space impractical small work propose embedding permutations given surface hypersphere defined mathbbmrn result embedding acquire ability define continuous distributions hypersphere benefits directional statistics provide polynomial time projections continuous hypersphere representation nelement permutation space framework provides way use continuous directional probability densities methods developed thereof establishing densities permutations demonstration benefits framework derive inference procedure statespace model permutations demonstrate approach applications\",\"canonical correlation analysis cca widely used statistical tool well established theory favorable performance wide range machine learning problems however computing cca huge datasets slow since involves implementing decomposition singular value decomposition huge matrices paper introduce lcca iterative algorithm compute cca fast huge sparse datasets theory asymptotic convergence finite time accuracy lcca established experiments also show lcca outperform fast cca approximation schemes two real datasets\",\"matching datasets multiple modalities become important task data analysis existing methods often rely embedding transformation single modality without utilizing correspondence information often results suboptimal matching performance paper propose nonlinear manifold matching algorithm using shortestpath distance joint neighborhood selection specifically joint nearestneighbor graph built modalities shortestpath distance within modality calculated joint neighborhood graph followed embedding matching common lowdimensional euclidean space compared existing algorithms approach exhibits superior performance matching disparate datasets multiple modalities\",\"novel concentration inequalities obtained missing mass total probability mass outcomes observed sample derive distributionfree deviation bounds sublinear exponents deviation size missing mass improve results berend kontorovich yari saeed khanloo haffari small deviations important case learning theory\",\"increasing body evidence suggesting exact nearest neighbour search highdimensional spaces affected curse dimensionality fundamental level necessarily mean true nearest neighbours based learning algorithms knn classifier analyse question number levels show answer different first main observation show consistency approximate nearest neighbour classifier however performance classifier high dimensions provably unstable second main observation point existing model statistical learning oblivious dimension domain every learning problem admits universally consistent deterministic reduction onedimensional case means borel isomorphism\",\"range fields including geosciences molecular biology robotics computer vision one encounters problems involve random variables manifolds currently lack flexible probabilistic models manifolds fast easy train define extremely flexible class exponential family distributions manifolds torus sphere rotation groups show distributions gradient loglikelihood computed efficiently using noncommutative generalization fast fourier transform fft discuss applications bayesian camera motion estimation harmonic exponential families serve conjugate priors modelling spatial distribution earthquakes surface earth experimental results show harmonic densities yield significantly higher likelihood best competing method orders magnitude faster train\",\"support vector machines svms special kernel based methods belong successful learning methods since decade svms informally described kind regularized mestimators functions demonstrated usefulness many complicated reallife problems last years great part statistical research svms concentrated question design svms universally consistent statistically robust nonparametric classification nonparametric regression purposes many applications qualitative prior knowledge distribution unknown function estimated present prediction function good interpretability desired semiparametric model additive model interest paper mainly address question design svms choosing reproducing kernel hilbert space rkhs corresponding kernel obtain consistent statistically robust estimators additive models give explicit construction kernels thus rkhss leads combination lipschitz continuous loss function consistent statistically robust smvs additive models examples quantile regression based pinball loss function regression based epsiloninsensitive loss function classification based hinge loss function\",\"recently general method analyzing statistical accuracy algorithm developed applied simple latent variable models balakrishnan method basin attraction valid initialization required ball around truth using steins lemma extend results case estimating centers twocomponent gaussian mixture dimensions particular significantly expand basin attraction intersection half space ball around origin signaltonoise ratio least constant multiple sqrtdlog show random initialization strategy feasible\",\"consider problem maximizing unknown function compact convex set using observations possible observe optimization function essentially relies learning induced bipartite ranking rule based idea relate global optimization bipartite ranking allows address problems high dimensional input space well cases functions weak regularity properties paper introduces novel metaalgorithms global optimization rely choice bipartite ranking method theoretical properties provided well convergence guarantees equivalences various optimization methods obtained byproduct eventually numerical evidence given show main algorithm paper adapts empirically underlying ranking structure essentially outperforms existing stateoftheart global optimization algorithms typical benchmarks\",\"patent lawsuits costly timeconsuming ability forecast patent litigation time litigation allows companies better allocate budget time managing patent portfolios develop predictive models estimating likelihood litigation patents expected time litigation based textual nontextual features work focuses improving stateoftheart relying different set features employing sophisticated algorithms realistic data rate patent litigations low consequently makes problem difficult initial model predicting likelihood modified capture timetolitigation perspective\",\"unsupervised discovery latent representations addition useful density modeling visualisation exploratory data analysis also increasingly important learning features relevant discriminative tasks autoencoders particular proven effective way learn latent codes reflect meaningful variations data continuing challenge however guiding autoencoder toward representations useful particular tasks complementary challenge find codes invariant irrelevant transformations data common way introducing problemspecific guidance autoencoders incorporation parametric component ties latent representation label information work argue preferable approach relies instead nonparametric guidance mechanism conceptually ensures exists function predict label information without explicitly instantiating function superiority guidance mechanism confirmed two datasets particular approach able incorporate invariance information lighting elevation etc small norb object recognition dataset yields stateoftheart performance single layer nonconvolutional network\",\"present new boosting algorithm motivated large margins theory boosting give experimental evidence new algorithm significantly robust label noise existing boosting algorithm\",\"factorial hidden markov models fhmms powerful tools modeling sequential data learning fhmms yields challenging simultaneous model selection issue selecting number multiple markov chains dimensionality chain main contribution address model selection issue extending factorized asymptotic bayesian fab inference fhmms first offer better approximation marginal loglikelihood previous fab inference key idea integrate transition probabilities yet still apply laplace approximation emission probabilities second prove two similar hidden states fhmm one redundant fab almost surely shrink eliminate one making model parsimonious experimental results show fab fhmms significantly outperforms stateoftheart nonparametric bayesian ifhmm variational fhmm model selection accuracy competitive heldout perplexity\",\"signal processing problems involve challenging task multidimensional probability density function pdf estimation work propose solution problem using family rotationbased iterative gaussianization rbig transforms general framework consists sequential application univariate marginal gaussianization transform followed orthonormal transform proposed procedure looks differentiable transforms known pdf unknown pdf estimated point original domain particular aim zero mean unit covariance gaussian convenience rbig formally similar classical iterative projection pursuit algorithms however show unlike methods particular class rotations used special qualitative relevance context since looking interestingness critical issue pdf estimation key difference approach focuses univariate part marginal gaussianization problem rather multivariate part rotation difference implies one may select convenient rotation suited practical application differentiability invertibility convergence rbig theoretically experimentally analyzed relation methods radial gaussianization oneclass support vector domain description svdd deep neural networks dnn also pointed practical performance rbig successfully illustrated number multidimensional problems image synthesis classification denoising multiinformation estimation\",\"paper develop statistical theory implementation deep learning models show elegant variable splitting scheme alternating direction method multipliers optimises deep learning objective allow nonsmooth nonconvex regularisation penalties induce sparsity parameter weights provide link traditional shallow layer statistical models principal component sliced inverse regression deep layer models also define degrees freedom deep learning predictor predictive mse criteria perform model selection comparing architecture designs focus deep multiclass logistic learning although methods apply generally results suggest interesting previously underexploited relationship deep learning proximal splitting techniques illustrate methodology provide multiclass logit classification analysis fishers iris data illustrate convergence algorithm finally conclude directions future research\",\"factorial hidden markov models fhmms powerful models sequential data scale well long sequences propose scalable inference learning algorithm fhmms draws ideas stochastic variational inference neural network copula literatures unlike existing approaches proposed algorithm requires message passing procedure among latent variables distributed network computers speed learning experiments corroborate proposed algorithm introduce approximation bias compared proven structured meanfield algorithm achieves better performance long sequences large fhmms\",\"present new online boosting algorithm adapting weights boosted classifier yields closer approximation freund schapires adaboost algorithm previous online boosting algorithms also contribute new way deriving online algorithm ties together previous online boosting work assume weak hypotheses selected beforehand weights updated online boosting update rule derived minimizing adaboosts loss viewed incremental form equations show optimization computationally expensive however fast online approximation possible compare approximation error batch adaboost synthetic datasets generalization error face datasets mnist dataset\",\"kernel methods obtain superb performance terms accuracy various machine learning tasks since effectively extract nonlinear relations however time complexity rather large especially clustering tasks paper define general class kernels easily approximated randomization kernels appear various applications particular traditional spectral clustering landmarkbased spectral clustering landmarkbased subspace clustering show data points clusters landmarks randomization procedure results algorithm complexity oknd furthermore bound error original clustering scheme randomization illustrate power framework propose new fast landmark subspace fls clustering algorithm experiments synthetic real datasets demonstrate superior performance fls accelerating subspace clustering marginal sacrifice accuracy\",\"gaussian process latent variable model gplvm nonlinear probabilistic method embedding high dimensional dataset terms low dimensional latent variables paper illustrate maximum posteriori map estimation latent variables hyperparameters used model selection hence determine optimal number latent variables appropriate model alternative variational approaches developed recently may useful want use nongaussian prior kernel functions dont automatic relevance determination ard parameters using second order expansion latent variable posterior marginalise latent variables obtain estimate hyperparameter posterior secondly use gplvm integrate multiple data sources simultaneously embedding terms common latent variables present results synthetic data illustrate successful detection retrieval low dimensional structure high dimensional data demonstrate integration multiple data sources leads robust performance finally show data used binary classification tasks attain significant gain prediction accuracy low dimensional representation used\",\"hierarchical probabilistic models mixture models used cluster analysis models two types variables observable latent cluster analysis latent variable estimated expected additional information improve accuracy estimation latent variable many proposed learning methods able use additional data include semisupervised learning transfer learning however statistical point view complex probabilistic model encompasses initial additional data might less accurate due higherdimensional parameter present paper presents theoretical analysis accuracy model clarifies factor greatest effect accuracy advantages obtaining additional data disadvantages increasing complexity\",\"annealed importance sampling ais common algorithm estimate partition functions useful stochastic models one important problem obtaining accurate ais estimates selection annealing schedule conventionally annealing schedule often determined heuristically simply set linearly increasing sequence paper propose algorithm optimal schedule deriving functional dominates ais estimation error numerically minimizing functional experimentally demonstrate proposed algorithm mostly outperforms conventional scheduling schemes large quantization numbers\",\"approximate bayesian computation abc using sequential monte carlo method provides comprehensive platform parameter estimation model selection sensitivity analysis differential equations however method like monte carlo methods incurs significant computational cost requires explicit numerical integration differential equations carry inference paper propose novel method circumventing requirement explicit integration using derivatives gaussian processes smooth observations parameters estimated evaluate methods using synthetic data generated model biological systems described ordinary delay differential equations upon comparing performance method existing abc techniques demonstrate produces comparably reliable parameter estimates significantly reduced execution time\",\"modelling real world complexity music challenge machine learning address task modeling melodic sequences music genre perform comparative analysis two probabilistic models dirichlet variable length markov model dirichletvmm time convolutional restricted boltzmann machine tcrbm show tcrbm learns descriptive music features underlying chords typical melody transitions dynamics assess models future prediction compare performance vmm current state art melody generation show models perform significantly better vmm dirichletvmm marginally outperforming tcrbm finally evaluate short order statistics models using kullbackleibler divergence test sequences model samples show proposed methods match statistics music genre significantly better vmm\",\"extend adaptive regression spline model incorporating saturation natural requirement function extend constant outside certain range fit saturating splines data using convex optimization problem space measures solve using efficient algorithm based conditional gradient method unlike many existing approaches algorithm solves original infinitedimensional splines degree least two optimization problem without prespecified knot locations adapt algorithm fit generalized additive models saturating splines coordinate functions show saturation requirement allows model simultaneously perform feature selection nonlinear function fitting finally briefly sketch method extended higher order splines different requirements extension outside data range\",\"many real world network problems often concern multivariate nodal attributes image textual multiview feature vectors nodes rather simple univariate nodal attributes existing graph estimation methods built gaussian graphical models covariance selection algorithms handle data neither theories developed around methods directly applied paper propose new principled framework estimating graphs multiattribute data instead estimating partial correlation current literature method estimates partial canonical correlations naturally accommodate complex nodal features computationally provide efficient algorithm utilizes multiattribute structure theoretically provide sufficient conditions guarantee consistent graph recovery extensive simulation studies demonstrate performance method various conditions furthermore provide illustrative applications uncovering gene regulatory networks gene protein profiles uncovering brain connectivity graph functional magnetic resonance imaging data\",\"classical approach inverse problems based optimization misfit function despite computational appeal approach suffers many shortcomings nonuniqueness solutions modeling prior knowledge etc bayesian formalism inverse problems avoids difficulties encountered optimization approach albeit increased computational cost work use information theoretic arguments cast bayesian inference problem terms optimization problem resulting scheme combines theoretical soundness fully bayesian inference computational efficiency simple optimization\",\"stochastic variational inference collapsed models recently successfully applied large scale topic modelling paper propose stochastic collapsed variational inference algorithm sequential data setting algorithm applicable finite hidden markov models hierarchical dirichlet process hidden markov models datasets generated emission distributions exponential family experiment results two discrete datasets show inference efficient accurate uncollapsed version stochastic variational inference\",\"present general framework classifying partially observed dynamical systems based idea learning model space contrast existing approaches using model point estimates represent individual data items employ posterior distributions models thus taking account principled manner uncertainty due generative observational andor dynamic noise observation sampling time processes evaluate framework two testbeds biological pathway model stochastic doublewell system crucially show classifier performance impaired model class used inferring posterior distributions much simple observationgenerating model class provided reduced complexity inferential model class captures essential characteristics needed given classification task\",\"many inference problems involving questions optimality ask maximum minimum finite set unknown quantities technical report derives first two posterior moments maximum two correlated gaussian variables first two posterior moments two generating variables corresponding gaussian approximations minimizing relative entropy shown used build heuristic approximation maximum relationship finite set gaussian variables allowing approximate inference expectation propagation quantities\",\"many high dimensional sparse learning problems formulated nonconvex optimization popular approach solve nonconvex optimization problems convex relaxations linear semidefinite programming paper study statistical limits convex relaxations particularly consider two problems mean estimation sparse principal submatrix edge probability estimation stochastic block model exploit sumofsquares relaxation hierarchy sharply characterize limits broad class convex relaxations result shows statistical optimality needs compromised achieving computational tractability using convex relaxations compared existing results computational lower bounds statistical problems consider general polynomialtime algorithms rely computational hardness hypotheses problems like planted clique detection theory focuses broad class convex relaxations rely unproven hypotheses\",\"best knowledge general wellfounded robust methods statistical unsupervised learning unsupervised methods explicitly implicitly depend kernel covariance operator kernel kernel crosscovariance operator kernel cco sensitive contaminated data even using bounded positive definite kernels first propose robust kernel covariance operator robust kernel robust kernel crosscovariance operator robust kernel cco based generalized loss function instead quadratic loss function second propose influence function classical kernel canonical correlation analysis classical kernel cca third using influence function propose visualization method detect influential observations two sets data finally propose method based robust kernel robust kernel cco called robust kernel cca designed contaminated data less sensitive noise classical kernel cca principles describe also apply many kernel methods must deal issue kernel kernel cco experiments synthesized imaging genetics analysis demonstrate proposed visualization robust kernel cca applied effectively ideal data contaminated data robust methods show superior performance stateoftheart methods\",\"large sample size brings computation bottleneck modern data analysis subsampling one efficient strategies handle problem previous studies researchers make cus subsampling replacement ssr subsampling without replacement sswr paper investigate kind sswr poisson subsampling pss fast algorithm ordinary leastsquare problem establish nonasymptotic property error bound correspond ing subsample estimator provide tradeoff computation cost approximation efficiency besides nonasymptotic result provide asymptotic consistency normality subsample estimator methodologically propose twostep subsampling algorithm efficient respect statistical objective independent linear model assumption synthetic real data used empirically study proposed subsampling strategies argue empirical studies proposed twostep algorithm obvious advantage assumed linear model accurate pss strategy performs obviously better ssr subsampling ratio increases\",\"concentration inequalities indispensable tools studying generalization capacity learning models hoeffdings mcdiarmids inequalities commonly used giving bounds independent data distribution although makes widely applicable drawback bounds loose specific cases although efforts devoted improving bounds find bounds tightened distributiondependent scenarios conditions inequalities relaxed particular propose four types conditions probabilistic boundedness bounded differences derive several distributiondependent extensions hoeffdings mcdiarmids inequalities extensions provide bounds functions satisfying conditions existing inequalities special cases tighter bounds furthermore obtain generalization bounds unbounded hierarchybounded loss functions finally discuss potential applications extensions learning theory\",\"using proper model characterize time series crucial making accurate predictions work use timevarying autoregressive process tvar describe nonstationary time series model mixture multiple stable autoregressive processes introduce new model selection technique based gap statistics learn appropriate number filters needed model time series define new distance measure stable filters draw reference curve used measure much adding new filter improves performance model choose number filters maximum gap reference curve end propose new method order generate uniform random stable filters root domain numerical results provided demonstrating performance proposed approach\",\"contribution deals generalized symmetric fastica algorithm domain independent component analysis ica generalized symmetric version fastica shown potential achieve cramerrao bound crb allowing usage different nonlinearity functions parallel implementations oneunit fastica spite appealing property rigorous study asymptotic error generalized symmetric fastica algorithm still missing community fact existing results exhibit certain limitations ignoring impact data standardization asymptotic statistics based heuristic approach work aim filling blank first result contribution characterization limits generalized symmetric fastica shown algorithm optimizes function sum contrast functions used traditional oneunit fastica correction sign based characterization derive closedform analytic expression asymptotic covariance matrix generalized symmetric fastica estimator using method estimating equation mestimator\",\"classical stochastic gradient methods well suited minimizing expectedvalue objective functions however apply minimization nonlinear function involving expected values composition two expectedvalue functions problems form minx mathbfev fvbigmathbfew gwxbig order solve stochastic composition problem propose class stochastic compositional gradient descent scgd algorithms viewed stochastic versions quasigradient method scgd update solutions based noisy sample gradients fvgw use auxiliary variable track unknown quantity mathbfewgwx prove scgd converge almost surely optimal solution convex optimization problems long solution exists convergence involves interplay two iterations different time scales nonsmooth convex problems scgd achieve convergence rate general case strongly convex case taking samples smooth convex problems scgd accelerated converge rate general case strongly convex case nonconvex problems prove limit point generated scgd stationary point also provide convergence rate analysis indeed stochastic setting one wants optimize compositions expectedvalue functions common practice proposed scgd methods find wide applications learning estimation dynamic programming etc\",\"concerned approximation problem symmetric positive semidefinite matrix due motivation class nonlinear machine learning methods discuss approximation approach call matrix ridge approximation particular define matrix ridge approximation incomplete matrix factorization plus ridge term moreover present probabilistic interpretations using normal latent variable model wishart model approximation approach idea behind latent variable model turn leads efficient iterative method handling matrix ridge approximation problem finally illustrate applications approximation approach multivariate data analysis empirical studies spectral clustering gaussian process regression show matrix ridge approximation iteration potentially useful\",\"independent component analysis ica aims decomposing observed random vector statistically independent variables deflationbased implementations popular oneunit fastica algorithm variants extract independent components one another novel method deflationary ica referred robustica put forward paper simple technique consists performing exact line search optimization kurtosis contrast function step size leading global maximum contrast along search direction found among roots fourthdegree polynomial polynomial rooting performed algebraically thus low cost iteration among practical benefits robustica avoid prewhitening deals real complexvalued mixtures possibly noncircular sources alike absence prewhitening improves asymptotic performance algorithm robust local extrema shows high convergence speed terms computational cost required reach given source extraction quality particularly short data records features demonstrated comparative numerical analysis synthetic data robusticas capabilities processing realworld data involving noncircular complex strongly supergaussian sources illustrated biomedical problem atrial activity extraction atrial fibrillation electrocardiograms ecgs outperforms alternative icabased technique\",\"slow feature analysis sfa method extracting slowly varying driving forces quickly varying nonstationary time series show possible sfa detect component even slower driving force envelope modulated sine wave shown depends circumstances like embedding dimension time series predictability base frequency whether driving force slower subcomponent detected observe phase transition one regime purpose work quantify influence various parameters phase transition conclude percieved slow sfa varies less fast switching one regime occurs perhaps showing similarity human perception\",\"generalized canonical correlation analysis gcca aims finding latent lowdimensional common structure multiple views feature vectors different domains entities unlike principal component analysis pca handles single view gcca able integrate information different feature spaces focus maxvar gcca popular formulation recently gained renewed interest multilingual processing speech modeling classic maxvar gcca problem solved optimally via eigendecomposition matrix compounds whitened correlation matrices views solution serious scalability issues directly amenable incorporating pertinent structural constraints nonnegativity sparsity canonical components posit regularized maxvar gcca nonconvex optimization problem propose alternating optimization aobased algorithm handle algorithm alternates inexact solutions regularized least squares subproblem manifoldconstrained nonconvex subproblem thereby achieving substantial memory computational savings important benefit design easily handle structurepromoting regularization show algorithm globally converges critical point sublinear rate approaches global optimal solution linear rate regularization considered judiciously designed simulations largescale word embedding tasks employed showcase effectiveness proposed algorithm\",\"asymptotic pseudotrajectory approach stochastic approximation benaim hofbauer sorin extended asynchronous stochastic approximations setvalued mean field asynchronicity process incorporated mean field produce convergence results remain similar equivalent synchronous process addition allows many restrictive assumptions previously associated asynchronous stochastic approximation removed framework extended coupled asynchronous stochastic approximation process setvalued mean fields twotimescales arguments used similar manner original work area borkar applicability approach demonstrated learning markov decision process\",\"paper proposes novel scheme reducedrank gaussian process regression method based approximate series expansion covariance function terms eigenfunction expansion laplace operator compact subset mathbbrd approximate eigenbasis eigenvalues covariance function expressed simple functions spectral density gaussian process allows inference solved computational cost scaling mathcalonm initial mathcalom hyperparameter learning basis functions data points furthermore basis functions independent parameters covariance function allows fast hyperparameter learning approach also allows rigorous error analysis hilbert space theory show approximation becomes exact size compact subset number eigenfunctions infinity also show convergence rate truncation error independent input dimensionality provided differentiability order covariance function increases appropriately squared exponential covariance function always bounded simm regardless input dimensionality expansion generalizes hilbert spaces inner product defined integral specified input density method compared previously proposed methods theoretically empirical tests simulated real data\",\"consider problem learning binary classifier training set positive unlabeled examples inductive transductive setting problem often referred emphpu learning differs standard supervised classification problem lack negative examples training set corresponds ubiquitous situation many applications information retrieval gene ranking identified set data interest sharing particular property wish automatically retrieve additional data sharing property among large easily available pool unlabeled data propose conceptually simple method akin bagging approach inductive transductive learning problems converting series supervised binary classification problems discriminating known positive examples random subsamples unlabeled set empirically demonstrate relevance method simulated real data performs least well existing methods faster\",\"consider problem uncertainty assessment low dimensional components high dimensional models specifically propose decorrelated score function handle impact high dimensional nuisance parameters consider hypothesis tests confidence regions generic penalized mestimators unlike existing inferential methods tailored individual models approach provides general framework high dimensional inference applicable wide range applications testing perspective develop general theorems characterize limiting distributions decorrelated score test statistic null hypothesis local alternatives results provide asymptotic guarantees type errors local powers proposed test furthermore show decorrelated score function used construct point confidence region estimators semiparametrically efficient also generalize framework broaden applications first extend handle high dimensional null hypothesis number parameters interest increase exponentially fast sample size second establish theory model misspecification third beyond likelihood framework introducing generalized score test based general loss functions thorough numerical studies conducted back developed theoretical results\",\"dasgupta shulman showed tworound variant algorithm learn mixture gaussian distributions near optimal precision high probability gaussian distributions well separated dimension sufficiently high paper generalize theory learning mixture highdimensional bernoulli templates template binary vector template generates examples randomly switching binary components independently certain probability computer vision applications binary vector feature map image binary component indicates whether local feature structure present absent within certain cell image domain bernoulli template considered statistical model images objects parts objects category show tworound algorithm learn mixture bernoulli templates near optimal precision high probability bernoulli templates sufficiently different number features sufficiently high illustrate theoretical results synthetic real examples\",\"abtesting popular technique web companies since makes possible accurately predict impact modification simplicity random split across users one critical aspects abtest duration important reliably compute confidence intervals associated metric interest know stop test paper define clean mathematical framework model abtest process propose three algorithms based bootstrapping central limit theorem compute reliable confidence intervals extend metrics common probabilities success apply absolute relative increments used comparison metrics including number occurrences particular event clickthrough rate implying ratio\",\"mixed linear regression involves recovery two unknown vectors unlabeled linear measurements sample comes exactly one vectors know one classic problem natural empirically popular approach solution algorithm settings prone bad local minima however iteration fast alternating guessing labels solving labels paper provide new initialization procedure based finding leading two eigenvectors appropriate matrix show resampled version algorithm provably converges correct vectors natural assumptions sampling distribution nearly optimal unimprovable sample complexity provides first characterization ems performance also much lower sample complexity compared standard randomly initialized methods problem\",\"propose novel algebraic framework treating probability distributions represented cumulants mean covariance matrix example consider unsupervised learning problem finding subspace several probability distributions agree instead minimizing objective function involving estimated cumulants show treating cumulants elements polynomial ring directly solve problem lower computational cost higher accuracy moreover algebraic viewpoint probability distributions allows invoke theory algebraic geometry demonstrate compact proof identifiability criterion\",\"problem supervised classification discrimination functional data considered special interest popular knearest neighbors knn classifier first relying recent result cerou guyader prove consistency knn classifier functional data whose distribution belongs broad family gaussian processes triangular covariance functions second practical side check behavior knn method compared functional classifiers carried small simulation study analysis several real functional data sets global uniform winner emerges comparisons overall performance knn method together sound intuitive motivation relative simplicity suggests could represent reasonable benchmark classification problem functional data\",\"extend stochastic gradient variational bayes perform posterior inference weights stickbreaking processes development allows define stickbreaking variational autoencoder sbvae bayesian nonparametric version variational autoencoder latent representation stochastic dimensionality experimentally demonstrate sbvae semisupervised variant learn highly discriminative latent representations often outperform gaussian vaes\",\"last years due growing ubiquity unlabeled data much effort spent machine learning community develop better understanding improve quality classifiers exploiting unlabeled data following manifold regularization approach laplacian support vector machines lapsvms shown state art performance semisupervised classification paper present two strategies solve primal lapsvm problem order overcome issues original dual formulation whereas training lapsvm dual requires two steps using primal form allows collapse training single step moreover computational complexity training algorithm reduced using preconditioned conjugate gradient combined number labeled unlabeled examples speed training using early stopping strategy based prediction unlabeled data available labeled validation examples allows algorithm quickly compute approximate solutions roughly classification accuracy optimal ones considerably reducing training time due simplicity training lapsvm primal starting point additional enhancements original lapsvm formulation dealing large datasets present extensive experimental evaluation real world data showing benefits proposed approach\",\"propose new high dimensional semiparametric principal component analysis pca method named copula component analysis coca semiparametric model assumes unspecified marginally monotone transformations distributions multivariate gaussian coca improves upon pca sparse pca three aspects robust modeling assumptions robust outliers data contamination iii scaleinvariant yields interpretable results prove coca estimators obtain fast estimation rates feature selection consistent dimension nearly exponentially large relative sample size careful experiments confirm coca outperforms sparse pca synthetic realworld datasets\",\"work consider problem detecting anomalous spatiotemporal behavior videos approach learn normative multiframe pixel joint distribution detect deviations using likelihood based approach due extreme lack available training samples relative dimension distribution use mean covariance approach consider methods learning spatiotemporal covariance lowsample regime approach estimate covariance using parameter reduction sparse models first method considered representation covariance sum kronecker products greenewald found accurate approximation setting propose learning algorithms relevant problem consider sparse multiresolution model choi apply kronecker product methods parameter reduction well introducing modifications enhanced efficiency greater applicability spatiotemporal covariance matrices apply methods detection crowd behavior anomalies university minnesota crowd anomaly dataset achieve competitive results\",\"paper consider statistical problem learning linear model noisy samples existing work focused approximating least squares solution using leveragebased scores importance sampling distribution however finite sample statistical guarantees computationally efficient optimal sampling strategies proposed evaluate statistical properties different sampling strategies propose simple yet effective estimator easy theoretical analysis useful multitask linear regression derive exact mean square error proposed estimator given sampling scores based minimizing mean square error propose optimal sampling scores estimator predictor show influenced noisetosignal ratio numerical simulations match theoretical analysis well\",\"study adaptive estimation copula correlation matrix sigma semiparametric elliptical copula model context correlations connected kendalls tau sine function transformation hence natural estimate sigma plugin estimator hatsigma kendalls tau statistic first obtain sharp bound operator norm hatsigmasigma study factor model sigma propose refined estimator widetildesigma fitting lowrank matrix plus diagonal matrix hatsigma using least squares nuclear norm penalty lowrank matrix bound operator norm hatsigmasigma serves scale penalty term obtain finite sample oracle inequalities widetildesigma also consider elementary factor copula model sigma propose closedform estimators estimation procedures entirely datadriven\",\"expectation propagation provides framework approximate inference model consideration latent gaussian field approximation gaussian show approximations systematically corrected perturbative expansion made exact intractable correction applied models partition function moments interest correction expressed higherorder cumulants neglected eps local matching moments expansion see correct first order considering higher orders corrections increasing polynomial complexity applied approximation second order provides correction quadratic time apply array gaussian process ising models corrections generalize arbitrarily complex approximating families illustrate treestructured ising model approximations furthermore provide polynomialtime assessment approximation error also provide theoretical practical insights exactness solution\",\"two recently introduced criteria estimation generative models based reduction binary classification noisecontrastive estimation nce estimation procedure generative model trained able distinguish data samples noise samples generative adversarial networks gans pairs generator discriminator networks generator network learning generate samples attempting fool discriminator network believing samples real data estimation procedures use function drive learning naturally raises questions related well whether function related maximum likelihood estimation mle nce corresponds training internal data model belonging discriminator network using fixed generator network show variant nce dynamic generator network equivalent maximum likelihood estimation since pairing learned discriminator appropriate dynamically selected generator recovers mle one might expect reverse hold pairing learned generator certain discriminator however show recovering mle learned generator requires departing distinguishability game specifically expected gradient nce discriminator made match expected gradient mle one allowed use nonstationary noise distribution nce choice discriminator network make expected gradient gan generator match mle iii existing theory guarantee gans converge nonconvex case suggests key next step gan research determine whether gans converge modify training algorithm force convergence\",\"informationmaximization clustering learns probabilistic classifier unsupervised manner mutual information feature vectors cluster assignments maximized notable advantage approach involves continuous optimization model parameters substantially easier solve discrete optimization cluster assignments however existing methods still involve nonconvex optimization problems therefore finding good local optimal solution straightforward practice paper propose alternative informationmaximization clustering method based squaredloss variant mutual information novel approach gives clustering solution analytically computationally efficient way via kernel eigenvalue decomposition furthermore provide practical model selection procedure allows objectively optimize tuning parameters included kernel function experiments demonstrate usefulness proposed approach\",\"sharply characterize performance different penalization schemes problem selecting relevant variables multitask setting previous work focuses regression problem conditions design matrix complicate analysis clearer simpler picture emerges studying normal means model model often used field statistics simplified model provides laboratory studying complex procedures\",\"bayesian networks bns graphical models useful representing highdimensional probability distributions great deal interest recent years nphard problem learning structure observed data typically one assigns score various structures search becomes optimization problem approached either deterministic stochastic methods paper walk space graphs modeling appearance disappearance edges birth death process compare novel approach popular metropolishastings search strategy give empirical evidence birth death process superior mixing properties\",\"approximate bayesian computation abc likelihoodfree monte carlo methods abc methods use comparison simulated data using different parameters drew prior distribution observed data comparison process based computing distance summary statistics simulated data observed data complex models usually difficult define methodology choosing constructing summary statistics recently nonparametric abc proposed uses dissimilarity measure discrete distributions based empirical kernel embeddings alternative summary statistics nonparametric abc outperforms methods including abc kernel abc synthetic likelihood abc however assumes probability distributions discrete robust dealing observations paper propose apply kernel embeddings using smoother density estimator parzen estimator comparing empirical data distributions computing abc posterior synthetic data real data used test bayesian inference method compare method respect stateoftheart methods demonstrate method robust estimator posterior distribution terms number observations\",\"finding statistically significant highorder interaction features predictive modeling important challenging task difficulty lies fact recent applications highdimensional covariates number possible highorder interaction features would extremely large identifying statistically significant features huge pool candidates would highly challenging computational statistical senses work problem consider two stage algorithm first select set highorder interaction features marginal screening make statistical inferences regression model fitted selected features statistical inferences called postselection inference psi receiving increasing attention literature one seminal recent advancements psi literature works lee authors presented algorithmic framework computing exact sampling distributions psi main challenge applying approach highorder interaction models cope fact psi general depends selected features also unselected features making hard apply extremely highdimensional highorder interaction models goal paper overcome difficulty introducing novel efficient method psi key idea exploit underlying tree structure among highorder interaction features develop pruning method tree enables quickly identify group unselected features guaranteed influence psi experimental results indicate proposed method allows reliably identify statistically significant highorder interaction features reasonable computational cost\",\"define discuss first sparse coding algorithm based closedform updates continuous latent variables underlying generative model consists standard spikeandslab prior gaussian noise model closedform solutions mstep equations derived generalizing probabilistic pca resulting algorithm take modes potentially multimodal posterior account computational cost algorithm scales exponentially number hidden dimensions however current computational resources still possible efficiently learn model parameters mediumscale problems thus model applied typical range source separation tasks numerical experiments artificial data verify likelihood maximization show derived algorithm recovers sparse directions standard sparse coding distributions source separation benchmarks comprised realistic data show algorithm competitive recent methods\",\"bayesian optimization effective methodology global optimization functions expensive evaluations relies querying distribution functions defined relatively cheap surrogate model accurate model distribution functions critical effectiveness approach typically fit using gaussian processes gps however since gps scale cubically number observations challenging handle objectives whose optimization requires many evaluations massively parallelizing optimization work explore use neural networks alternative gps model distributions functions show performing adaptive basis function regression neural network parametric form performs competitively stateoftheart gpbased approaches scales linearly number data rather cubically allows achieve previously intractable degree parallelism apply large scale hyperparameter optimization rapidly finding competitive models benchmark object recognition tasks using convolutional networks image caption generation using neural language models\",\"models complex systems often formalized sequential software simulators computationally intensive programs iteratively build probable system configurations given parameters initial conditions simulators enable modelers capture effects difficult characterize analytically summarize statistically however many realworld applications simulations need inverted match observed data typically requires custom design derivation implementation sophisticated inversion algorithms give framework inverting broad class complex software simulators via probabilistic programming automatic inference using lines probabilistic code approach based formulation inversion approximate inference simple sequential probabilistic model implement four inference strategies including metropolishastings sequentialized metropolishastings scheme particle markov chain monte carlo scheme requiring fewer lines probabilistic code demonstrate framework applying invert real geological software simulator oil gas industry\",\"recently theoretical guarantees obtained matrix completion nonuniform sampling regime particular sampling distribution aligns underlying matrixs leverage scores high probability nuclear norm minimization exactly recover low rank matrix article analyze scenario nonuniform sampling distribution may may align underlying matrixs leverage scores explore learning parameters weighted nuclear norm minimization terms empirical sampling distribution provide sufficiency condition learned weights provide exact recovery guarantee weighted nuclear norm minimization established specific choice weights terms true sampling distribution allows weighted nuclear norm minimization exactly recover low rank matrix also allows quantifiable relaxation exact recovery conditions article extend quantifiable relaxation exact recovery conditions specific choice weights defined analogously terms empirical distribution opposed true sampling distribution accomplish employ concentration measure bound large deviation bound also present numerical evidence healthy robustness weighted nuclear norm minimization algorithm choice empirically learned weights numerical experiments show variety easily computable empirical weights weighted nuclear norm minimization outperforms unweighted nuclear norm minimization nonuniform sampling regime\",\"machine learning science discovering statistical dependencies data use dependencies perform predictions last decade machine learning made spectacular progress surpassing human performance complex tasks object recognition car driving computer gaming however central role prediction machine learning avoids progress towards generalpurpose artificial intelligence one way forward argue causal inference fundamental component human intelligence yet ignored learning algorithms causal inference problem uncovering causeeffect relationships variables data generating system causal structures provide understanding systems behave changing unseen environments turn knowledge causal dynamics allows answer questions describing potential responses system hypothetical manipulations interventions thus understanding cause effect one step machine learning towards machine reasoning machine intelligence currently available causal inference algorithms operate specific regimes rely assumptions difficult verify practice thesis advances art causal inference three different ways first develop framework study statistical dependence based copulas random features second build framework interpret problem causal inference task distribution classification yielding family novel causal inference algorithms third discover causal structures convolutional neural network features using algorithms algorithms presented thesis scalable exhibit strong theoretical guarantees achieve stateoftheart performance variety realworld benchmarks\",\"supervised linear feature extraction achieved fitting reduced rank multivariate model paper studies rank penalized rank constrained vector generalized linear models perspective thresholding rules build framework fitting singular value penalized models use feature extraction solving rank constraint form problem propose progressive feature space reduction fast computation high dimensions little performance loss novel projective crossvalidation proposed parameter tuning nonconvex setups real data applications given show power methodology supervised dimension reduction feature extraction\",\"paper novel approach coding nominal data proposed given nominal data rank form complex number assigned proposed method lose information attribute brings properties previously unknown approach based knew properties used classification analyzed example shows classification use coded nominal data numerical well coded nominal data effective classification uses numerical data\",\"study paper consequences using mean absolute percentage error mape measure quality regression models prove existence optimal mape model show universal consistency empirical risk minimization based mape also show finding best model mape equivalent weighted mean absolute error mae regression apply weighting strategy kernel regression behavior mape kernel regression illustrated simulated data\",\"present derivation kullback leibler kldivergence also known relative entropy von mises fisher vmf distribution ddimensions\",\"dimensionality reduction methods common field high dimensional data analysis typically algorithms dimensionality reduction computationally expensive therefore applications analysis massive amounts data impractical example repeated computations due accumulated data computationally prohibitive paper outofsample extension scheme used complementary method dimensionality reduction presented describe algorithm performs outofsample extension newlyarrived data points unlike extension algorithms nystrom algorithm proposed algorithm uses intrinsic geometry data properties dimensionality reduction map prove error proposed algorithm bounded additionally outofsample extension algorithm provides degree abnormality newlyarrived data point\",\"note compares two recently published machine learning methods constructing flexible tractable families variational hiddenvariable posteriors first method called hierarchical variational models enriches inference model extra variable called auxiliary deep generative models enriches generative model instead conclude two methods mathematically equivalent\",\"exploration hydrocarbon resources highly complicated expensive process various geological geochemical geophysical factors developed combined together highly significant design seismic data acquisition survey locate exploratory wells since incorrect imprecise locations lead waste time money operation objective study locate highpotential oil gas field sheet ahwaz including oil fields reduce time costs exploration production processes regard maps developed using gis functions factors including minimum maximum total organic carbon toc yield potential hydrocarbons production tmax peak production index oxygen index hydrogen index well presence proximity high residual bouguer gravity anomalies proximity anticline axis faults topography curvature maps obtained asmari formation subsurface contours model integrate maps study employed artificial neural network adaptive neurofuzzy inference system anfis methods results obtained model validation demonstrated neural network rms kappa trained better models anfis predicts potential areas accurately however method failed predict oil fields wrongly predict areas potential zones\",\"propose novel algorithm greedy forward feature selection regularized leastsquares rls regression classification also known leastsquares support vector machine ridge regression algorithm call greedy rls starts empty feature set iteration adds feature whose addition provides best leaveoneout crossvalidation performance method considerably faster previously proposed ones since time complexity linear number training examples number features original data set desired size set selected features therefore side effect obtain new training algorithm learning sparse linear rls predictors used large scale learning speed possible due matrix calculus based shortcuts leaveoneout feature addition experimentally demonstrate scalability algorithm ability find good quality feature sets\",\"propose novel nonparametric adaptive anomaly detection algorithm high dimensional data based ranksvm data points first ranked based scores derived nearest neighbor graphs npoint nominal data train ranksvm using ranked data testpoint declared anomaly alphafalse alarm level predicted score alphapercentile resulting anomaly detector shown asymptotically optimal adaptive false alarm rate alpha decision region converges alphapercentile level set unknown underlying density addition illustrate number synthetic realdata experiments statistical performance computational efficiency anomaly detector\",\"crowdsourcing popular paradigm effectively collecting labels low cost dawidskene estimator widely used inferring true labels noisy labels provided nonexpert crowdsourcing workers however since estimator maximizes nonconvex loglikelihood function hard theoretically justify performance paper propose twostage efficient algorithm multiclass crowd labeling problems first stage uses spectral method obtain initial estimate parameters second stage refines estimation optimizing objective function dawidskene estimator via algorithm show algorithm achieves optimal convergence rate logarithmic factor conduct extensive experiments synthetic real datasets experimental results demonstrate proposed algorithm comparable accurate empirical approach outperforming several recently proposed methods\",\"given two graphs graph matching problem align two vertex sets minimize number adjacency disagreements two graphs seeded graph matching problem graph matching problem first given partial alignment tasked completing paper modify stateoftheart approximate graph matching algorithm faq vogelstein make fast approximate seeded graph matching algorithm adapt applicability include graphs differently sized vertex sets extend algorithm provide individual vertex nomination list likely matches demonstrate effectiveness algorithm via simulation real data experiments indeed knowledge even seeds extremely effective seeded graph matching algorithm used recover naturally existing alignment partially observed\",\"consider problem estimating undirected trianglefree graphs high dimensional distributions trianglefree graphs form rich graph family allows arbitrary loopy structures cliques inferential tractability propose graphical fermats principle regularize distribution family principle enforces existence distributiondependent pseudometric two nodes smaller distance two nodes geodesic path include two nodes guided principle show greedy strategy able recover true graph resulting algorithm requires pairwise distance matrix input computationally even efficient calculating minimum spanning tree consider graph estimation problems different settings including discrete nonparametric distribution families thorough numerical results provided illustrate usefulness proposed method\",\"clustering one important unsupervised problems machine learning statistics among many existing algorithms kernel kmeans drawn much research attention due ability find nonlinear cluster boundaries inherent simplicity two main approaches kernel kmeans svd kernel matrix convex relaxations despite attention kernel clustering received theoretical applied quarters much known robustness methods paper first introduce semidefinite programming relaxation kernel clustering problem prove suitable model specification ksvd sdp approaches consistent limit albeit sdp strongly consistent achieves exact recovery whereas ksvd weakly consistent fraction misclassified nodes vanish\",\"autoencoder neural network implemented estimate missing data genetic algorithm implemented network optimization estimating missing data missing data treated missing random mechanism implementing maximum likelihood algorithm network performance determined calculating mean square error network prediction network optimized implementing decision forest impact missing data investigated decision forrests found improve results\",\"structural equation models sems widely adopted inference causal interactions complex networks recent examples include unveiling topologies hidden causal networks processes spreading diseases rumors propagate appeal sems settings stems simplicity tractability since typically assume linear dependencies among observable variables acknowledging limitations inherent adopting linear models present paper advocates nonlinear sems account possible nonlinear dependencies among network nodes advocated approach leverages kernels powerful encompassing framework nonlinear modeling efficient estimator affordable tradeoffs put forth interestingly pursuit novel kernelbased approach yields convex regularized estimator promotes edge sparsity amenable proximalsplitting optimization methods end solvers complementary merits developed leveraging alternating direction method multipliers proximal gradient iterations experiments conducted simulated data demonstrate novel approach outperforms linear sems respect edge detection errors furthermore tests real gene expression dataset unveil interesting new edges revealed linear sems could shed light regulatory behavior human genes\",\"new shrinkagebased construction developed compressible vector boldsymbolxinmathbbrn cases components naturally associated tree structure important examples corresponds coefficients wavelet blockdct representation data method consider detail numerical results presented based increments gamma process however demonstrate general framework appropriate many types shrinkage priors within levy process family gamma process special case bayesian inference carried approximating posterior samples mcmc algorithm well constructing heuristic variational approximation posterior also consider expectationmaximization map point solution stateoftheart results manifested compressive sensing denoising applications latter spiky nongaussian noise\",\"propose method inferring conditional independence graph cig highdimensional gaussian vector time series discretetime process finitelength observation contrast existing approaches rely parametric process model autoregressive model observed random process instead require certain smoothness properties fourier domain process proposed inference scheme works even sample sizes much smaller number scalar process components true underlying cig sufficiently sparse theoretical performance analysis provides conditions guarantee probability proposed inference method deliver wrong cig prescribed value conditions imply lower bounds sample size new method consistent asymptotically numerical experiments validate theoretical performance analysis demonstrate superior performance scheme compared existing parametric approach case model mismatch\",\"inference learning graphical models wellstudied problems statistics machine learning found many applications science engineering however exact inference intractable general graphical models suggests problem seeking best approximation collection random variables within tractable family graphical models paper focus class planar ising models exact inference tractable using techniques statistical physics based techniques recent methods planarity testing planar embedding propose simple greedy algorithm learning best planar ising model approximate arbitrary collection binary random variables possibly sample data given set pairwise correlations among variables select planar graph optimal planar ising model defined graph best approximate set correlations demonstrate method simulations application modeling senate voting records\",\"runtime kernel partial least squares kpls compute fit quadratic number examples however necessity obtaining sensitivity measures degrees freedom model selection confidence intervals detailed analysis requires cubic runtime thus constitutes computational bottleneck realworld data analysis propose novel algorithm kpls computes fit also approximate degrees freedom error bars quadratic runtime algorithm exploits close connection kernel pls lanczos algorithm approximating eigenvalues symmetric matrices uses approximation compute trace powers kernel matrix quadratic runtime\",\"measurements made satellite remote sensing moderate resolution imaging spectroradiometer modis globally distributed aerosol robotic network aeronet compared comparison two datasets measurements aerosol optical depth values show biases two data products paper present general framework towards identifying relevant set variables responsible observed bias present general framework identify possible factors influencing bias might associated measurement conditions solar sensor zenith angles solar sensor azimuth scattering angles surface reflectivity various measured wavelengths etc specifically performed analysis remote sensing aqualand data set used machine learning technique neural network case perform multivariate regression groundtruth training data sets finally used mutual information observed predicted values measure similarity identify relevant set variables search brute force method consider possible combinations computations involves huge number crunching exercise implemented writing jobparallel program\",\"consider inference structure undirected graphical model exact bayesian framework specifically aim achieving inference closeform posteriors avoiding sampling step task would intractable without restriction considered graphs limit exploration mixtures spanning trees consider inference structure undirected graphical model bayesian framework avoid convergence issues highly demanding monte carlo sampling focus exact inference specifically aim achieving inference closeform posteriors avoiding sampling step aim restrict set considered graphs mixtures spanning trees investigate conditions priors tree structures parameters exact bayesian inference achieved conditions derive fast exact algorithm compute posterior probability edge belong tree model using algebraic result called matrixtree theorem show assumption made prevent approach perform well synthetic flow cytometry data\",\"canonical correlation analysis cca one popular methods frequency recognition steadystate visual evoked potential ssvepbased braincomputer interfaces bcis despite efficiency potential problem using preconstructed sinecosine waves required reference signals cca method often result optimal recognition accuracy due lack features real eeg data address problem study proposes novel method based multiset canonical correlation analysis msetcca optimize reference signals used cca method ssvep frequency recognition msetcca method learns multiple linear transforms implement joint spatial filtering maximize overall correlation among canonical variates hence extracts ssvep common features multiple sets eeg data recorded stimulus frequency optimized reference signals formed combination common features completely based training data experimental study eeg data ten healthy subjects demonstrates msetcca method improves recognition accuracy ssvep frequency comparison cca method two competing methods multiway cca mwaycca phase constrained cca pcca especially small number channels short time window length superiority indicates proposed msetcca method new promising candidate frequency recognition ssvepbased bcis\",\"given iid samples unknown continuous density hyperrectangle attempt learn piecewise constant function approximates underlying density nonparametrically density estimate defined binary split built sequentially according discrepancy criteria key ingredient control discrepancy adaptively subrectangle achieve overall bound prove estimate even though simple appears preserves estimation power exploiting structure directly applied important pattern recognition tasks mode seeking density landscape exploration demonstrate applicability simulations examples\",\"paper describes novel method approximate polynomial coefficients regression functions particular interest multidimensional classification derivation simple offers fast robust classification technique resistant overfitting\",\"propose novel graphical model selection gms scheme highdimensional stationary time series discrete time process method based natural generalization graphical lasso glasso introduced originally gms based iid samples estimates conditional independence graph cig time series finite length observation glasso time series defined solution lregularized maximum approximate likelihood problem solve optimization problem using alternating direction method multipliers admm approach nonparametric assume finite dimensional autoregressive parametric model observed process instead require process sufficiently smooth spectral domain gaussian processes characterize performance method theoretically deriving upper bound probability algorithm fails correctly identify cig numerical experiments demonstrate ability method recover correct cig limited amount samples\",\"introduce randomized dependence coefficient rdc measure nonlinear dependence random variables arbitrary dimension based hirschfeldgebeleinrenyi maximum correlation coefficient rdc defined terms correlation random nonlinear copula projections invariant respect marginal distribution transformations low computational cost easy implement five lines code included end paper\",\"study sparse nonnegative least squares snnls problem snnls occurs naturally wide variety applications unknown nonnegative quantity must recovered linear measurements present unified framework snnls based rectified power exponential scale mixture prior sparse codes show proposed framework encompasses large class snnls algorithms provide computationally efficient inference procedure based multiplicative update rules update rules convenient solving large sets snnls problems simultaneously required contexts like sparse nonnegative matrix factorization snmf provide theoretical justification proposed approach showing local minima objective function optimized sparse snnls algorithms presented guaranteed converge set stationary points objective function extend framework snmf showing framework leads many well known snmf algorithms specific choices prior providing guarantee popular subclass proposed algorithms converges set stationary points objective function finally study performance proposed approaches synthetic realworld data\",\"capturing dependence structure multivariate extreme events major concern many fields involving management risks stemming multiple sources portfolio monitoring insurance environmental risk management anomaly detection one convenient nonparametric characterization extremal dependence framework multivariate extreme value theory evt angular measure provides direct information probable directions extremes relative contribution featurecoordinate largest observations modeling angular measure high dimensional problems major challenge multivariate analysis rare events present paper proposes novel methodology aiming exhibiting sparsity pattern within dependence structure extremes done estimating amount mass spread angular measure representative sets directions corresponding specific subcones dimension reduction technique paves way towards scaling existing multivariate evt methods beyond nonasymptotic study providing theoretical validity framework method propose direct application first anomaly detection algorithm based multivariate evt algorithm builds sparse normal profile extreme behaviours confronted new possibly abnormal extreme observations illustrative experimental results provide strong empirical evidence relevance approach\",\"hierarchical parametric models consisting observable latent variables widely used unsupervised learning tasks example mixture model representative hierarchical model clustering statistical point view models regular singular due distribution data regular case models identifiability onetoone relation probability density function model expression parameter fisher information matrix positive definite estimation accuracy observable latent variables studied singular case hand models identifiable fisher matrix positive definite conventional statistical analysis based inverse fisher matrix applicable recently algebraic geometrical analysis developed used elucidate bayes estimation observable variables present paper applies analysis latentvariable estimation determines theoretical performance results clarify behavior convergence posterior distribution found posterior observablevariable estimation different one latentvariable estimation difference markov chain monte carlo method based parameter latent variable cannot construct desired posterior distribution\",\"propose new two stage algorithm ling large scale regression problems ling risk well known ridge regression fixed design setting computed much faster experiments shown ling performs well terms prediction accuracy computational efficiency compared large scale regression algorithms like gradient descent stochastic gradient descent principal component regression simulated real datasets\",\"present analyse three online algorithms learning discrete hidden markov models hmms compare baldichauvin algorithm using kullbackleibler divergence measure generalisation error draw learning curves simplified situations performance learning drifting concepts one presented algorithms analysed compared baldichauvin algorithm situations brief discussion learning symmetry breaking based results also presented\",\"paper develop method learning nonlinear systems multiple outputs inputs begin modelling errors nominal predictor system using latent variable framework using maximum likelihood principle derive criterion learning model resulting optimization problem tackled using majorizationminimization approach finally develop convex majorization technique show enables recursive identification method method learns parsimonious predictive models tested synthetic real nonlinear systems\",\"many realworld problems encountered several disciplines deal modeling timeseries containing different underlying dynamical regimes probabilistic approaches often employed paper describe several approaches common framework graphical models give unified overview models previously introduced literature simpler comprehensive previous descriptions enables highlight commonalities differences among models observed past addition present several new models inference routines naturally derived within unified viewpoint\",\"propose original model inferring team strengths using markov random field used generate historical estimates offensive defensive strengths team time model designed applied sports soccer hockey contest outcomes take value limited discrete space perform inference using combination expectation maximization loopy belief propagation challenges working nonconvex optimization problem highdimensional parameter space discussed performance model demonstrated professional soccer data english premier league\",\"article introduces new algorithm reconstructing epsilonmachines data well decisional states defined internal states system lead decision based userprovided utility payoff function utility function encodes priori knowledge external system quantifies bad make mistakes intrinsic underlying structure system modeled epsilonmachine causal states decisional states form partition lowerlevel causal states defined according higherlevel users knowledge complex systems perspective decisional states thus emerging patterns corresponding utility function transitions decisional states correspond events lead change decision new remapf algorithm estimates epsilonmachine decisional states data application examples given hidden model reconstruction cellular automata filtering edge detection images\",\"bayesian optimization powerful tool finetuning hyperparameters wide variety machine learning models success machine learning led practitioners diverse realworld settings learn classifiers practical problems machine learning becomes commonplace bayesian optimization becomes attractive method practitioners automate process classifier hyperparameter tuning key observation data used tuning models settings often sensitive certain data genetic predisposition personal email statistics car accident history properly private may risk inferred bayesian optimization outputs address introduce methods releasing best hyperparameters classifier accuracy privately leveraging strong theoretical guarantees differential privacy known bayesian optimization convergence bounds prove assumption private quantities also nearoptimal finally even assumption satisfied use different smoothness guarantees protect privacy\",\"subsampling methods recently proposed speed least squares estimation large scale settings however algorithms typically robust outliers corruptions observed covariates concept influence developed regression diagnostics used detect corrupted observations shown paper property influence also develop randomized approximation motivates proposed subsampling algorithm large scale corrupted linear regression limits influence data points since highly influential points contribute residual error general model corrupted observations show theoretically empirically variety simulated real datasets algorithm improves current stateoftheart approximation schemes ordinary least squares\",\"matrix generalized inverse gaussian mathcalmgig distribution arises naturally settings distribution symmetric positive semidefinite matrices certain key properties distribution effective ways sampling distribution carefully studied paper show mathcalmgig unimodal mode obtained solving algebraic riccati equation equation based property propose importance sampling method mathcalmgig mode proposal distribution matches target proposed sampling method efficient existing approaches use proposal distributions may mode far mathcalmgigs mode illustrate posterior distribution latent factor models probabilistic matrix factorization pmf marginalized one latent factor mathcalmgig distribution characterization leads novel collapsed monte carlo cmc inference algorithm latent factor models illustrate cmc lower log loss perplexity mcmc needs fewer samples\",\"consider problem minimizing sum functions convex parameter set mathcalc subset mathbbrp ngg pgg regime algorithms utilize subsampling techniques known effective paper use subsampling techniques together lowrank approximation design new randomized batch algorithm possesses comparable convergence rate newtons method yet much smaller periteration cost proposed algorithm robust terms starting point step size enjoys composite convergence rate namely quadratic convergence start linear convergence iterate close minimizer develop theoretical analysis also allows select nearoptimal algorithm parameters theoretical results used obtain convergence rates previously proposed subsampling based algorithms well demonstrate results apply wellknown machine learning problems lastly evaluate performance algorithm several datasets various scenarios\",\"consider robust covariance estimation group symmetry constraints nongaussian covariance estimation tyler scatter estimator multivariate generalized gaussian distribution methods usually involve nonconvex minimization problems recently shown underlying principle behind success extended form convexity geodesics manifold positive definite matrices modern approach improve estimation accuracy exploit prior knowledge via additional constraints restricting attention specific classes covariances adhere prior symmetry structures paper prove group symmetry constraints also geodesically convex therefore incorporated various nongaussian covariance estimators practical examples sets include circulant persymmetric complexquaternion proper structures provide simple numerical technique finding maximum likelihood estimates constraints demonstrate performance advantage using synthetic experiments\",\"practical model building processes often timeconsuming many different models must trained validated paper introduce novel algorithm used computing lower upper bounds model validation errors without actually training model key idea behind algorithm using side information available suboptimal model reasonably good suboptimal model available algorithm compute lower upper bounds many useful quantities making inferences unknown target model demonstrate advantage algorithm context model selection regularized learning problems\",\"present revival interest bistatic radar systems research area gained momentum given strategic advantages bistatic configuration tech nological advances past years largescale implementation bistatic systems scope near future bistatic systems replace monostatic systems least par tially existing usages monostatic system manageable bistatic system detailed investigation possibilities automatic target recognition atr facil ity bistatic radar system presented lack data experiments carried simulated data still results positive make positive case introduction bistatic configuration first found contrary popular expectation bistatic atr performance might substantially worse monostatic atr performance bistatic atr performed fairly well though better monostatic atr second atr per formance deteriorate substantially increasing bistatic angle last polarimetric data bistatic scattering found distinct information contrary expert opinions along results suggestions also made stabilise bistaticatr per formance changing bistatic angle finally new fast robust atr algorithm developed present work presented\",\"large number algorithms machine learning principal component analysis pca nonlinear kernel extensions recent spectral embedding support estimation methods rely estimating linear subspace samples paper introduce general formulation problem derive novel learning error estimates results rely natural assumptions spectral properties covariance operator associated data distribu tion hold wide class metrics subspaces special cases discuss sharp error estimates reconstruction properties pca spectral support estimation key analysis operator theoretic approach broad applicability spectral learning methods\",\"survey present compare different approaches estimate mutual information data analyse general dependencies variables interest system demonstrate performance difference versus correlation analysis optimal case linear dependencies first use piecewise constant bayesian methodology using general dirichlet prior estimation method use twostage approach approximate probability distribution first calculate marginal joint entropies demonstrate performance bayesian approach versus others computing dependency different variables also compare linear correlation analysis finally apply correlation analysis identification bias determination aerosol optical depth aod satellite based moderate resolution imaging spectroradiometer modis ground based aerosol robotic network aeronet observe aod measurements two instruments might different location reason bias explored quantifying dependencies bias variables including cloud cover surface reflectivity others\",\"propose framework perform streaming covariance selection approach employs regularization constraints timevarying sparsity parameter iteratively estimated via stochastic gradient descent allows regularization parameter efficiently learnt online manner proposed framework developed linear regression models extended graphical models via neighbourhood selection mild assumptions able obtain convergence results nonstochastic setting capabilities approach demonstrated using synthetic data well neuroimaging data\",\"investigate learning rate multiple kernel leaning mkl elasticnet regularization consists ellregularizer inducing sparsity ellregularizer controlling smoothness focus sparse setting total number kernels large number nonzero components ground truth relatively small prove elasticnet mkl achieves minimax learning rate ellmixednorm ball bound sharper convergence rates ever shown property smoother truth faster convergence rate\",\"item response theory irt models categorical response data widely used analysis educational data computerized adaptive testing psychological surveys however irt models rely assumption categories strictly ordered assumption ordering known priori assumptions impractical many realworld scenarios multiplechoice exams levels incorrectness distractor categories often unknown number results exist irt models unordered categorical data tend restrictive modeling assumptions lead poor data fitting performance practice furthermore existing unordered categorical models parameters difficult interpret work propose novel methodology unordered categorical irt call sprite short stochastic polytomous response item model analyzes ordered unordered categories offers interpretable outputs iii provides improved data fitting compared existing models compare sprite existing item response models demonstrate efficacy synthetic realworld educational datasets\",\"technical note considers problems blind sparse learning inference electrogram egm signals atrial fibrillation conditions first introduce mathematical model observed signals takes account multiple foci typically appearing inside heart propose reconstruction model based fixed dictionary discuss several alternatives choosing dictionary order obtain sparse solution takes account biological restrictions problem first alternative using lasso regularization followed postprocessing stage removes low amplitude coefficients violating refractory period characteristic cardiac cells alternative propose novel regularization term called cross products lasso cplasso able incorporate biological constraints directly optimization problem unfortunately resulting problem nonconvex show solved efficiently approximated way making use successive convex approximations sca finally spectral analysis performed clean activation sequence obtained sparse learning stage order estimate number latent foci frequencies simulations synthetic real data provided validate proposed approach\",\"recent advances bayesian learning largescale data witnessed emergence stochastic gradient mcmc algorithms sgmcmc stochastic gradient langevin dynamics sgld stochastic gradient hamiltonian mcmc sghmc stochastic gradient thermostat finitetime convergence properties sgld storder euler integrator recently studied corresponding theory general sgmcmcs explored paper consider general sgmcmcs highorder integrators develop theory analyze finitetime convergence properties asymptotic invariant measures theoretical results show faster convergence rates accurate invariant measures sgmcmcs higherorder integrators example proposed efficient ndorder symmetric splitting integrator mean square error mse posterior average sghmc achieves optimal convergence rate iterations compared sghmc sgld storder euler integrators furthermore convergence results decreasingstepsize sgmcmcs also developed convergence rates fixedstepsize counterparts specific decreasing sequence experiments synthetic real datasets verify theory show advantages proposed method two largescale real applications\",\"nonparametric methods widely applicable statistical inference problems since rely modeling assumptions context fresh look advocated permeates benefits variable selection compressive sampling robustify nonparametric regression outliers data markedly deviating postulated models variational counterpart leasttrimmed squares regression shown closely related lpseudonormregularized estimator encourages sparsity vector explicitly modeling outliers connection suggests efficient solvers based convex relaxation lead naturally variational mtype estimator equivalent leastabsolute shrinkage selection operator lasso outliers identified judiciously tuning regularization parameters amounts controlling sparsity outlier vector along whole robustification path lasso solutions reduced bias enhanced generalization capability attractive features improved estimator obtained replacing lpseudonorm nonconvex surrogate novel robust splinebased smoother adopted cleanse load curve data key task aiding operational decisions envisioned smart grid system computer simulations tests real load curve data corroborate effectiveness novel sparsitycontrolling robust estimators\",\"given family probability measures space probability measures hilbert space goal paper highlight one ore curves summarize efficiently family propose study problem optimal transport wasserstein geometry using curves restricted geodesic segments metric show concepts play key role euclidean pca data centering orthogonality principal directions find natural equivalent optimal transport geometry using wasserstein means differential geometry implementation ideas however computationally challenging achieve scalable algorithms handle thousands measures propose use relaxed definition geodesics regularized optimal transport distances interest approach demonstrated images seen either shapes color histograms\",\"generative adversarial networks gans successful deep generative models gans based twoplayer minimax game however objective function derived original motivation changed obtain stronger gradients learning generator propose novel algorithm repeats density ratio estimation fdivergence minimization algorithm offers new perspective toward understanding gans able make use multiple viewpoints obtained research density ratio estimation divergence stable relative density ratio useful\",\"independent component analysis ica powerful method blind source separation based assumption sources statistically independent though ica proven useful employed many applications complete statistical independence restrictive assumption practice additionally important prior information data sparsity usually available sparsity natural property data form diversity incorporated ica model relax independence assumption resulting improvement overall separation performance work propose new variant ica entropy bound minimization icaebma flexible yet parameterfree algorithmthrough direct exploitation sparsity using new sparseicaebm algorithm study synergy independence sparsity simulations synthetic well functional magnetic resonance imaging fmrilike data\",\"methods transfer learning try combine knowledge several related tasks domains improve performance test task inspired causal methodology relax usual covariate shift assumption assume holds true subset predictor variables conditional distribution target variable given subset predictors invariant tasks show assumption motivated ideas field causality focus problem domain generalization examples test task observed prove adversarial setting using subset prediction optimal domain generalization provide examples tasks sufficiently diverse estimator therefore outperforms pooling data even average examples test task available also provide method transfer knowledge training tasks exploit available features prediction however provide guarantees method introduce practical method allows automatic inference subset provide corresponding code present results synthetic data sets gene deletion data set\",\"using bayesian approach consider problem recovering sparse signals additive sparse dense noise typically sparse noise models outliers impulse bursts data loss handle sparse noise existing methods simultaneously estimate sparse signal interest sparse noise interest estimating sparse signal without need estimating sparse noise construct robust relevance vector machine rvm rvm sparse noise ever present dense noise treated combined noise model precision combined noise modeled diagonal matrix show new rvm update equations correspond nonsymmetric sparsity inducing cost function combined modeling found computationally efficient also extend method blocksparse signals noise known unknown block structures simulations show performance computation efficiency new rvm several applications recovery sparse block sparse signals housing price prediction image denoising\",\"present first treebased regressor whose convergence rate depends intrinsic dimension data namely assouad dimension regressor uses rptree partitioning procedure simple randomized variant trees\",\"simple computationally efficient scheme treestructured vector quantization presented unlike previous methods quantization error depends intrinsic dimension data distribution rather apparent dimension space data happen lie\",\"many problems lowlevel computer vision image processing denoising deconvolution tomographic reconstruction superresolution addressed maximizing posterior distribution sparse linear model slm show higherorder bayesian decisionmaking problems optimizing image acquisition magnetic resonance scanners addressed querying slm posterior covariance unrelated densitys mode propose scalable algorithmic framework slm posteriors full highresolution images approximated first time solving variational optimization problem convex iff posterior mode finding convex methods successfully drive optimization sampling trajectories realworld magnetic resonance imaging bayesian experimental design attempted methodology provides new insight similarities differences sparse reconstruction approximate bayesian inference important implications compressive sensing realworld images\",\"response problem vidyasagar state criterion pac learnability concept class mathscr family nonatomic diffuse measures domain omega uniform glivenkocantelli property respect nonatomic measures longer necessary condition consistent learnability cannot general expected criterion stated terms combinatorial parameter vcmathscr cmathrmmodomega call dimension mathscr modulo countable sets new parameter obtained thickening single points definition dimension uncountable clusters equivalently vcmathscr cmoddomegaleq every countable subclass mathscr dimension leq outside countable subset omega new parameter also expressed classical dimension mathscr calculated suitable subset compactification omega make measurability assumptions mathscr assuming instead validity martins axiom similar results obtained function learning terms fatshattering dimension modulo countable sets like classical distributionfree case finiteness parameter sufficient necessary pac learnability nonatomic measures\",\"introduce multivariate stochastic volatility model asset returns imposes restrictions structure volatility matrix treats elements functions latent stochastic processes number assets prohibitively large propose factor multivariate stochastic volatility model variances correlations factors evolve stochastically time inference achieved via carefully designed feasible scalable markov chain monte carlo algorithm combines two computationally important ingredients utilizes invariant prior metropolis proposal densities simultaneously updating latent paths quadratic rather cubic computational complexity evaluating multivariate normal densities required apply modelling computational methodology stock daily returns euro stoxx index data period years matlab software paper available httpwwwauebgrusersmtitsiascodemsvzip\",\"given iid observations unknown absolute continuous distribution defined domain omega propose nonparametric method learn piecewise constant function approximate underlying probability density function density estimate piecewise constant function defined binary partition omega key ingredient algorithm use discrepancy concept originates quasi monte carlo analysis control partition process resulting algorithm simple efficient provable convergence rate empirically demonstrate efficiency density estimation method present applications wide range tasks including finding good initializations kmeans\",\"aim paper provide new method learning relationships data obtained independently unlike existing methods like matching proposed technique require contextual information provided dependency variables interest monotone therefore easily combined matching order exploit advantages methods technique described mix quantile matching deconvolution provide theoretical empirical validation\",\"bayesian optimization recently emerged popular efficient tool global optimization hyperparameter tuning currently established bayesian optimization practice requires userdefined bounding box assumed contain optimizer however little known probed objective function difficult prescribe bounds work modify standard bayesian optimization framework principled way allow automatic resizing search space introduce two alternative methods compare two common synthetic benchmarking test functions well tasks tuning stochastic gradient descent optimizer multilayered perceptron convolutional neural network mnist\",\"gaussian graphical models ggms probabilistic tools choice analyzing conditional dependencies variables complex systems finding changepoints structural evolution ggm therefore essential detecting anomalies underlying system modeled ggm order detect structural anomalies ggm consider problem estimating changes precision matrix corresponding gaussian distribution take twostep approach solving problem estimating background precision matrix using system observations past without anomalies estimating foreground precision matrix using sliding temporal window anomaly monitoring primary contribution estimating foreground precision using novel contrastive inverse covariance estimation procedure order accurately learn structural changes ggm maximize penalized loglikelihood penalty norm difference foreground precision estimated already learned background precision modify alternating direction method multipliers admm algorithm sparse inverse covariance estimation perform contrastive estimation foreground precision matrix results simulated ggm data show significant improvement precision recall detecting structural changes ggm compared noncontrastive sliding window baseline\",\"multitask learning shown significantly enhance performance multiple related learning tasks variety situations present fused logistic regression sparse multitask learning approach binary classification specifically introduce sparsity inducing penalties parameter differences related logistic regression models encode similarity across related tasks resulting joint learning task cast form lends efficiently optimized recursive variant alternating direction method multipliers show results synthetic data describe regime settings multitask approach achieves significant improvements single task learning approach discuss implications applying fused logistic regression different real world settings\",\"preterm births occur alarming rate preemies higher risk infant mortality developmental retardation longterm disabilities predicting preterm birth difficult even experienced clinicians welldesigned clinical study thus far reaches modest sensitivity specificity take different approach exploiting databases normal hospital operations aims twofold derive easytouse interpretable prediction rule quantified uncertainties construct accurate classifiers preterm birth prediction approach automatically generate select hundreds thousands possible predictors using stabilityaware techniques derived large database women simplified prediction rule items sensitivity specificity\",\"study fundamental tradeoffs computational tractability statistical accuracy general family hypothesis testing problems combinatorial structures based upon oracle model computation captures interactions algorithms data establish general lower bound explicitly connects minimum testing risk computational budget constraints intrinsic probabilistic combinatorial structures statistical problems lower bound mirrors classical statistical lower bound cam allows quantify optimal statistical performance achievable given limited computational budgets systematic fashion unified framework sharply characterize statisticalcomputational phase transition two testing problems namely normal mean detection sparse principal component detection normal mean detection consider two combinatorial structures namely sparse set perfect matching problems identify significant gaps optimal statistical accuracy achievable computational tractability constraints classical statistical lower bounds compared existing works computational lower bounds statistical problems consider general polynomialtime algorithms turing machines rely computational hardness hypotheses problems like planted clique detection focus oracle computational model covers broad range popular algorithms rely unproven hypotheses moreover result provides intuitive concrete interpretation intrinsic computational intractability highdimensional statistical problems one byproduct result lower bound strict generalization matrix permanent problem independent interest\",\"study propose automatic learning method variables selection based lasso epidemiology context one aim approach overcome pretreatment experts medicine epidemiology collected data pretreatment consist recoding variables choose interactions based expertise approach proposed uses available explanatory variables without treatment generate automatically interactions lead high dimension use lasso one robust methods variable selection high dimension avoid fitting two levels crossvalidation used target variable account variable lasso estimators biased variables selected lasso debiased glm used predict distribution main vector malaria anopheles results show climatic environmental variables mains factors associated malaria risk exposure\",\"nonlinear similarity measures defined kernel space correntropy extract higherorder statistics data offer potentially significant performance improvement linear counterparts especially nongaussian signal processing machine learning work propose new similarity measure kernel space called kernel risksensitive loss krsl provide important properties apply krsl adaptive filtering investigate robustness develop mkrsl algorithm analyze mean square convergence performance compared correntropy krsl offer efficient performance surface thereby enabling gradient based method achieve faster convergence speed higher accuracy still maintaining robustness outliers theoretical analysis results superior performance new algorithm confirmed simulation\",\"inductive probabilistic classification rule must generally obey principles bayesian predictive inference observed unobserved stochastic quantities jointly modeled parameter uncertainty fully acknowledged posterior predictive distribution several rules recently considered asymptotic behavior characterized assumption observed features variables used building classifier conditionally independent given simultaneous labeling training samples unknown origin extend theoretical results predictive classifiers acknowledging feature dependencies either graphical models sparser alternatives defined stratified graphical models also show experimentation synthetic real data predictive classifiers based stratified graphical models consistently best accuracy compared predictive classifiers based either conditionally independent features ordinary graphical models\",\"extremes play special role anomaly detection beyond inference simulation purposes probabilistic tools borrowed extreme value theory evt angular measure also used design novel statistical learning methods anomaly detectionranking paper proposes new algorithm based multivariate evt learn rank observations high dimensional space respect degree abnormality procedure relies original dimensionreduction technique extreme domain possibly produces sparse representation multivariate extremes allows gain insight dependence structure thereof escaping curse dimensionality representation output unsupervised methodology propose combined anomaly detection technique tailored nonextreme data performs linearly dimension almost linearly data odn log fits large scale problems approach paper novel evt never used multivariate version field anomaly detection illustrative experimental results provide strong empirical evidence relevance approach\",\"propose simple kernel based nearest neighbor approach handwritten digit classification distance actually kernel defining similarity two images carefully study effects different number neighbors weight schemes report results nearest neighbors similar images vote test set error rate mnist database could reach close many advanced models\",\"recurring problem building probabilistic latent variable models regularization model selection instance choice dimensionality latent space context belief networks latent variables problem adressed automatic relevance determination ard employing monte carlo inference present variational inference approach ard deep generative models using doubly stochastic variational inference provide fast scalable learning show empirical results standard dataset illustrating effects contracting latent space automatically show resulting latent representations significantly compact without loss expressive power learned models\",\"stateoftheart speaker recognition relays models need large amount training data models successful tasks like nist sre sufficient data available however real applications usually much data many cases speaker labels unknown present method adapt plda model domain large amount labeled data another unlabeled data describe generative model produces sets data unknown labels modeled like latent variables used variational bayes estimate hidden variables derive equations model model used papers unsupervised adaptation plda using variational bayes methods publised icassp unsupervised training plda variational bayes published iberspeech variational bayesian plda speaker diarization mgb challenge published asru\",\"deep learning methods multitask neural networks recently applied ligandbased virtual screening drug discovery applications using set industrial admet datasets compare neural networks standard baseline models analyze multitask learning effects random crossvalidation relevant temporal validation scheme confirm multitask learning provide modest benefits singletask models show smaller datasets tend benefit larger datasets multitask learning additionally find adding massive amounts side information guaranteed improve performance relative simpler multitask learning results emphasize multitask effects highly datasetdependent suggesting use datasetspecific models maximize overall performance\",\"variational inference algorithms proven successful bayesian analysis large data settings recent advances using stochastic variational inference svi however methods largely studied independent exchangeable data settings develop svi algorithm learn parameters hidden markov models hmms timedependent data setting challenge applying stochastic optimization setting arises dependencies chain must broken consider minibatches observations propose algorithm harnesses memory decay chain adaptively bound errors arising edge effects demonstrate effectiveness algorithm synthetic experiments large genomics dataset batch algorithm computationally infeasible\",\"introduce gamsel generalized additive model selection penalized likelihood approach fitting sparse generalized additive models high dimension method interpolates null linear additive models allowing effect variable estimated either zero linear lowcomplexity curve determined data present blockwise coordinate descent procedure efficiently optimizing penalized likelihood objective dense grid tuning parameter producing regularization path additive models demonstrate performance method real simulated data examples compare existing techniques additive model selection\",\"propose novel class timevarying nonparanormal graphical models allows model high dimensional heavytailed systems evolution latent network structures model develop statistical tests presence edges locally fixed index value globally range values tests developed highdimensional regime robust model selection mistakes require commonly assumed minimum signal strength testing procedures based high dimensional debiasingfree moment estimator uses novel kernel smoothed kendalls tau correlation matrix input statistic estimator consistently estimates latent inverse pearson correlation matrix uniformly index variable kernel bandwidth rate convergence shown minimax optimal method supported thorough numerical simulations application neural imaging data set\",\"consider following signal recovery problem given measurement matrix phiin mathbbrntimes noisy observation vector cin mathbbrn constructed phitheta epsilon epsilonin mathbbrn noise vector whose entries follow iid centered subgaussian distribution recover signal theta dtheta sparse rca linear transformation dinmathbbrmtimes one natural method using convex optimization solve following problem mintheta phitheta lambdadtheta paper provides upper bound estimate error shows consistency property method assuming design matrix phi gaussian random matrix specifically show noiseless case condition number bounded measurement number ngeq omegaslogp sparsity number true solution recovered high probability noisy case condition number bounded measurement increases faster slogp slogpon estimate error converges zero probability infinity results consistent special case dboldiptimes equivalently lasso improve existing analysis condition number plays critical role analysis consider condition numbers two cases including fused lasso random graph condition number fused lasso case bounded constant condition number random graph case bounded high probability mover textedgeover textvertex larger certain constant numerical simulations consistent theoretical results\",\"distributed learning probabilistic models multiple data repositories minimum communication increasingly important study simple communicationefficient learning framework first calculates local maximum likelihood estimates mle based data subsets combines local mles achieve best possible approximation global mle given whole dataset study frameworks statistical properties showing efficiency loss compared global setting relates much underlying distribution families deviate full exponential families drawing connection theory information loss fisher rao efron show fullexponentialfamilyness represents lower bound error rate arbitrary combinations local mles achieved kldivergencebased combination method common linear combination method also study empirical properties methods showing method significantly outperforms linear combination practical settings issues model misspecification nonconvexity heterogeneous data partitions\",\"speaker recognition scenarios find conversations recorded simultaneously multiple channels case interviews nist sre dataset take advantage propose modification plda model considers two different intersession variability terms first term tied recordings belonging conversation whereas second thus former mainly intends capture variability due phonetic content conversation latter tries capture channel variability document derive equations model model applied paper handling recordings acquired simultaneously multiple channels plda published interspeech\",\"discuss meanfield theory cellular automata model metalearning metalearning process combining outcomes individual learning procedures order determine final decision higher accuracy single learning method method constructed ensemble interacting learning agents acquire process incoming information using various types different versions machine learning algorithms abstract learning space agents located constructed using fully connected model couples agents random strength values cellular automata network simulates higher level integration information acquired independent learning trials final classification incoming input data therefore defined stationary state metalearning system using simple majority rule yet minority clusters share opposite classification outcome observed system therefore probability selecting proper class given input data estimated even without prior knowledge affiliation fuzzy logic easily introduced system even learning agents build simple binary classification machine learning algorithms calculating percentage agreeing agents\",\"incorporating spatial information hyperspectral unmixing procedures shown positive effects due inherent spatialspectral duality hyperspectral scenes current research works consider spatial information mainly focused linear mixing model paper investigate variational approach incorporating spatial correlation nonlinear unmixing procedure nonlinear algorithm operating reproducing kernel hilbert spaces associated ell local variation norm spatial regularizer derived experimental results synthetic real data illustrate effectiveness proposed scheme\",\"many data mining applications collection sufficiently large datasets time consuming expensive hand industrial methods data collection create huge databases make difficult direct applications advanced machine learning algorithms address problems consider active learning may efficient either experimental design data filtering paper demonstrate using online evaluation opportunity provided challenge quite competitive results may produced using small percentage available data also present several alternative criteria may useful evaluation active learning processes author paper attended special presentation barcelona results wcci challenge discussed\",\"propose approach multivariate nonparametric regression generalizes reduced rank regression linear models additive model estimated dimension qdimensional response shared pdimensional predictor variable control complexity model employ functional form kyfan nuclear norm resulting set function estimates low rank backfitting algorithms derived justified using nonparametric form nuclear norm subdifferential oracle inequalities excess risk derived exhibit scaling behavior procedure high dimensional setting methods illustrated gene expression data\",\"consider problem transforming samples one continuous source distribution samples another target distribution demonstrate optimal transport theory source distribution easily sampled target distribution logconcave tractably solved convex optimization show special case source prior target posterior bayesian inference tractably calculate normalization constant draw posterior iid samples remarkably bayesian tractability criterion simply log concavity prior likelihood criterion tractable calculation maximum posteriori point estimate simulated data demonstrate attain bayes risk simulations physiologic data demonstrate improvements point estimation intensive care unit outcome prediction electroencephalographybased sleep staging\",\"dimensionality reduction topic recent interest paper present classification constrained dimensionality reduction ccdr algorithm account label information algorithm account multiple classes well semisupervised setting present outofsample expressions labeled unlabeled data unlabeled data introduce method embedding new point preprocessing classifier labeled data introduce method improves embedding training phase using outofsample extension investigate classification performance using ccdr algorithm hyperspectral satellite imagery data demonstrate performance gain local global classifiers demonstrate improvement knearest neighbors algorithm performance present connection intrinsic dimension estimation optimal embedding dimension obtained using ccdr algorithm\",\"developed efficient algorithm maximum likelihood joint tracking association problem strong clutter gmti data using iterative procedure dynamic logic process vaguetocrisp new tracker overcomes combinatorial complexity tracking highlycluttered scenarios results significant improvement signaltoclutter ratio\",\"taking account highorder interactions among covariates valuable many practical regression problems however computationally challenging task number highorder interaction features considered would extremely large unless number covariates sufficiently small paper propose novel efficient algorithm lassobased sparse learning highorder interaction models basic strategy reducing number features employ idea recently proposed safe feature screening sfs rule sfs rule property feature satisfies rule feature guaranteed nonactive lasso solution meaning safely screenedout prior lasso training process large number features screenedout training lasso computational cost memory requirment dramatically reduced however applying sfs rule extremely large number highorder interaction features would computationally infeasible key idea solving computational issue exploit underlying tree structure among highorder interaction features specifically introduce pruning condition called safe feature pruning sfp rule property rule satisfied certain node tree highorder interaction features corresponding descendant nodes guaranteed nonactive optimal solution algorithm extremely efficient making possible work order interactions original covariates number possible highorder interaction features greater\",\"introduce computationally effective algorithm linear model selection consisting three steps screeningorderingselection sos screening predictors based thresholded lasso penalized least squares screened predictors fitted using least squares ordered respect statistics finally model selected using greedy generalized information criterion gic penalized nested family induced ordering give nonasymptotic upper bounds error probability step sos algorithm terms penalties obtain selection consistency different scenarios conditions needed screening consistency lasso traditional setting give sanovtype bounds error probabilities orderingselection algorithm surprising consequence selection error greedy gic asymptotically larger exhaustive gic also obtain new bounds prediction estimation errors lasso proved parallel algorithm used practice formal version\",\"properties data frequently seen vary depending sampled situations usually changes along time evolution owing environmental effects one way analyze data find invariances representative features kept constant changes aim paper identify one feature namely interactions dependencies among variables common across multiple datasets collected different conditions end propose common substructure learning cssl framework based graphical gaussian model present simple learning algorithm based dual augmented lagrangian alternating direction method multipliers confirm performance cssl existing techniques finding unchanging dependency structures multiple datasets numerical simulations synthetic data real world application anomaly detection automobile sensors\",\"order identify important variables involved making optimal treatment decision proposed penalized least squared regression framework fixed number predictors robust misspecification conditional mean model two problems arise world explosively big data effective methods needed handle ultrahigh dimensional data set example dimension predictors nonpolynomial order sample size propensity score conditional mean models need estimated data dimensionality paper propose twostep estimation procedure deriving optimal treatment regime dimensionality steps penalized regressions employed nonconcave penalty function conditional mean model response given predictors may misspecified asymptotic properties weak oracle properties selection consistency oracle distributions proposed estimators investigated addition study limiting distribution estimated value function obtained optimal treatment regime empirical performance proposed estimation method evaluated simulations application depression dataset stard study\",\"learning hidden markov model hmm sequen tial observations often complemented realvalued summary response variables generated path hid den states settings arise numerous domains includ ing many applications biology like motif discovery genome annotation paper present flexible frame work jointly modeling latent sequence features functional mapping relates summary response variables hidden state sequence algorithm com patible rich set mapping functions results show availability additional continuous response vari ables simultaneously improve annotation quential observations yield good prediction performance synthetic data realworld datasets\",\"networks capture intuition relationships world describe friendships facebook users interactions financial markets synapses connecting neurons brain networks richly structured cliques friends sectors stocks smorgasbord cell types govern neurons connect networks like social network friendships directly observed many cases indirect view network actions constituents understanding network mediates activity work focus problem latent network discovery case observable activity takes form mutuallyexcitatory point process known hawkes process build previous work taken bayesian approach problem specifying prior distributions latent network structure likelihood observed activity given network extend work proposing discretetime formulation developing computationally efficient stochastic variational inference svi algorithm allows scale approach long sequences observations demonstrate algorithm calcium imaging data used chalearn neural connectomics challenge\",\"obtain index complexity random sequence allowing role measure classical probability theory played function call generating mechanism typically generating mechanism finite automata generate set biased sequences applying finite state automata specified number states set binary sequences thus index complexity random sequence number states automata detail optimal algorithms predict sequences generated way\",\"estimation response functions important task dynamic medical imaging task arises example dynamic renal scintigraphy impulse response retention functions estimated functional magnetic resonance imaging hemodynamic response functions required functions observed directly estimation complicated recorded images subject superposition underlying signals therefore response functions estimated via blind source separation deconvolution performance algorithm heavily depends used models response functions response functions real image sequences rather complicated finding suitable parametric form problematic paper study estimation response functions using nonparametric bayesian priors priors designed favor desirable properties functions sparsity smoothness assumptions used within hierarchical priors blind source separation deconvolution algorithm comparison resulting algorithms priors performed synthetic dataset well real datasets dynamic renal scintigraphy shown flexible nonparametric priors improve estimation response functions cases matlab implementation resulting algorithms freely available download\",\"recently number mostly ellnorm regularized least squares type deterministic algorithms proposed address problem emphsparse adaptive signal estimation system identification bayesian perspective task equivalent maximum posteriori probability estimation sparsity promoting heavytailed prior parameters interest following different approach paper develops unifying framework sparse emphvariational bayes algorithms employ heavytailed priors conjugate hierarchical form facilitate posterior inference resulting fully automated variational schemes first presented batch iterative form shown properly exploiting structure batch estimation task new sparse adaptive variational bayes algorithms derived ability impose track sparsity realtime processing timevarying environment important feature proposed algorithms completely eliminate need computationally costly parameter finetuning necessary ingredient sparse adaptive deterministic algorithms extensive simulation results provided demonstrate effectiveness new sparse variational bayes algorithms stateoftheart deterministic techniques adaptive channel estimation results show proposed algorithms numerically robust exhibit general superior estimation performance compared deterministic counterparts\",\"hierarchical learning models mixture models bayesian networks widely employed unsupervised learning tasks clustering analysis consist observable hidden variables represent given data hidden generation process respectively pointed conventional statistical analysis applicable models redundancy latent variable produces singularities parameter space recent years method based algebraic geometry allowed analyze accuracy predicting observable variables using bayesian estimation however analyze latent variables sufficiently studied even though one main issues unsupervised learning determine accurately latent variable estimated previous study proposed method used range latent variable redundant compared model generating data present paper extends method situation latent variables redundant dimensions formulate new error functions derive asymptotic forms calculation error functions demonstrated twolayered bayesian networks\",\"multiplayer online battle arena moba games among played digital games world games teams players fight arena environments gameplay focused tactical combat mastering mobas requires extensive practice exemplified popular moba defence ancients dota paper present three datadriven measures spatiotemporal behavior dota zone changes distribution team members time series clustering via fuzzy approach present method obtaining accurate positional data dota investigate behavior varies across measures function skill level teams using four tiers novice professional players results indicate spatiotemporal behavior moba teams related team skill professional teams smaller withinteam distances conducting zone changes amateur teams temporal distribution withinteam distances professional highskilled teams also generally follows patterns distinct lower skill ranks\",\"paper propose novel framework construction sparsityinducing priors particular define priors mixture exponential power distributions generalized inverse gaussian density epgig epgig variant generalized hyperbolic distributions special cases include gaussian scale mixtures laplace scale mixtures furthermore laplace scale mixtures subserve bayesian framework sparse learning nonconvex penalization densities epgig explicitly expressed moreover corresponding posterior distribution also follows generalized inverse gaussian distribution properties lead algorithms bayesian sparse learning show algorithms bear interesting resemblance iteratively reweighted ell ell methods addition present two extensions grouped variable selection logistic regression\",\"paper demonstrate tempering markov chain monte carlo samplers bayesian models recursively subsampling observations without replacement improve performance baseline samplers terms effective sample size per computation present two tempering subsampling algorithms subsampled parallel tempering subsampled tempered transitions provide asymptotic analysis computational cost tempering subsampling verify tempering subsampling costs less traditional tempering demonstrate algorithms bayesian approaches learning mean high dimensional multivariate normal estimating gaussian process hyperparameters\",\"correlation matrices play key role many multivariate methods graphical model estimation factor analysis current stateoftheart estimating large correlation matrices focuses use pearsons sample correlation matrix although pearsons sample correlation matrix enjoys various good properties gaussian models effective estimator facing heavytailed distributions robust alternative han liu stat assoc advocated use transformed version kendalls tau sample correlation matrix estimating high dimensional latent generalized correlation matrix transelliptical distribution family elliptical copula transelliptical family assumes unspecified marginal monotone transformations data follow elliptical distribution paper study theoretical properties kendalls tau sample correlation matrix transformed version proposed han liu stat assoc estimating population kendalls tau correlation matrix latent pearsons correlation matrix spectral restricted spectral norms regard spectral norm highlight role effective rank quantifying rate convergence regard restricted spectral norm first time present sign subgaussian condition sufficient guarantee rankbased correlation matrix estimator attains fast rate convergence cases need moment condition\",\"present convex approach probabilistic segmentation modeling time series data approach builds upon recent advances multivariate total variation regularization seeks learn separate set parameters distribution observations time point additional penalty encourages parameters remain constant time propose efficient optimization methods solving resulting large optimization problems twostage procedure estimating recurring clusters models based upon kernel density estimation finally show number realworld segmentation tasks resulting methods often perform well better existing latent variable models substantially easier train\",\"bayesian mixture models widely applied unsupervised learning exploratory data analysis markov chain monte carlo based gibbs sampling splitmerge moves widely used inference models however methods restricted limited types transitions suffer torpid mixing low accept rates even problems modest size propose method considers broader range transitions close equilibrium exploiting multiple chains parallel using past states adaptively inform proposal distribution method significantly improves gibbs splitmerge sampling quantified using convergence diagnostics acceptance rates adaptive mcmc methods use past states inform proposal distribution given rise many ingenious sampling schemes continuous problems present work seen important first step bringing benefits partitionbased problems\",\"present supervisedlearning algorithm graph data set graphs arbitrary twicedifferentiable loss functions sparse linear models possible subgraph features date shown possible subgraph features several types sparse learning adaboost lpboost larslasso sparse pls regression performed particularly emphasis placed simultaneous learning relevant features infinite set candidates first generalize techniques used preceding studies derive unifying bounding technique arbitrary separable functions carefully use bounding make block coordinate gradient descent feasible infinite subgraph features resulting fast converging algorithm solve wider class sparse learning problems graph data also empirically study differences existing approaches convergence property selected subgraph features searchspace sizes discuss several unnoticed issues sparse learning possible subgraph features\",\"general approach anomaly detection novelty detection consists estimating high density regions minimum volume sets oneclass support vector machine ocsvm stateoftheart algorithm estimating regions high dimensional data yet suffers practical limitations applied limited number samples lead poor performance even picking best hyperparameters moreover solution ocsvm sensitive selection hyperparameters makes hard optimize unsupervised setting present new approach estimate sets using ocsvm different choice parameter controlling proportion outliers solution function ocsvm learnt training set desired probability mass obtained adjusting offset test set prevent overfitting models learnt different traintest splits aggregated reduce variance induced random splits approach makes possible tune hyperparameters automatically obtain nested set estimates experimental results show approach outperforms standard ocsvm formulation suffering less curse dimensionality kernel density estimates results actual data sets also presented\",\"hierarchical probabilistic models gaussian mixture models widely used unsupervised learning tasks models consist observable latent variables represent observable data underlying datageneration process respectively unsupervised learning tasks cluster analysis regarded estimations latent variables based observable ones estimation latent variables semisupervised learning labels observed precise unsupervised one concerns clarify effect labeled data however sufficient theoretical analysis accuracy estimation latent variables previous study distributionbased error function formulated asymptotic form calculated unsupervised learning generative models shown estimation latent variables bayes method accurate maximumlikelihood method present paper reveals asymptotic forms error function bayesian semisupervised learning discriminative generative models results show generative model uses given data performs better model well specified\",\"consider statistical well algorithmic aspects solving largescale leastsquares problems using randomized sketching algorithms problem input data mathbbrn times times mathbbrn sketching algorithms use sketching matrix sinmathbbrr times rather solving problem using full data sketching algorithms solve problem using sketched data prior work typically adopted algorithmic perspective made statistical assumptions input instead assumed data fixed worstcase prior results show using sketching matrices random projections leveragescore sampling algorithms error solving original problem small constant statistical perspective typically consider meansquared error performance randomized sketching algorithms data generated according statistical model beta epsilon epsilon noise process provide rigorous comparison perspectives leading insights differ first develop framework assessing algorithmic statistical aspects randomized sketching methods consider statistical prediction efficiency statistical residual efficiency sketched estimator use framework provide upper bounds several types random projection random sampling sketching algorithms among results show upper bounded typically requires sample size substantially larger lower bounds developed subsequent results show upper bounds improved\",\"goal twosample tests assess whether two samples sim sim drawn distribution perhaps intriguingly one relatively unexplored method build twosample tests use binary classifiers particular construct dataset pairing examples positive label pairing examples negative label null hypothesis true classification accuracy binary classifier heldout subset dataset remain near chancelevel show classifier twosample tests cst learn suitable representation data fly return test statistics interpretable units simple null distribution predictive uncertainty allow interpret differ goal paper establish properties performance uses cst first analyze main theoretical properties second compare performance variety stateoftheart alternatives third propose use evaluate sample quality generative models intractable likelihoods generative adversarial networks gans fourth showcase novel application gans together cst causal discovery\",\"paper addresses problem filtering statespace model standard approaches filtering assume probabilistic model observations observation model given explicitly least parametrically consider setting assumption satisfied assume knowledge observation model provided examples stateobservation pairs setting important appears state variables defined quantities different observations propose kernel monte carlo filter novel filtering method focused setting approach based framework kernel mean embeddings enables nonparametric posterior inference using stateobservation examples proposed method represents state distributions weighted samples propagates samples sampling estimates state posteriors kernel bayes rule resamples kernel herding particular sampling resampling procedures novel expressed using kernel mean embeddings theoretically analyze behaviors reveal following properties similar corresponding procedures particle methods performance sampling degrade effective sample size weighted sample small resampling improves sampling performance increasing effective sample size first demonstrate theoretical findings synthetic experiments show effectiveness proposed filter artificial real data experiments include visionbased mobile robot localization\",\"nonparametric regression massive numbers samples features increasingly important problem big settings common strategy partition feature space separately apply simple models partition set propose alternative approach avoids partitioning associated sensitivity neighborhood choice distance metrics using random compression combined gaussian process regression proposed approach particularly motivated setting response conditionally independent features given projection low dimensional manifold conditionally random compression matrix smoothness parameter posterior distribution regression surface posterior predictive distributions available analytically running analysis parallel many random compression matrices smoothness parameters model averaging used combine results algorithm implemented rapidly even big problems strong theoretical justification found yield state art predictive performance\",\"vector autoregressive var model powerful tool modeling complex time series exploited many fields however fitting high dimensional var model poses unique challenges one hand dimensionality caused modeling large number time series higher order autoregressive processes usually much higher time series length hand temporal dependence structure var model gives rise extra theoretical challenges high dimensions one popular approach assume transition matrix sparse fit var model using least squares method lassotype penalty manuscript propose alternative way estimating var model main idea via exploiting temporal dependence structure formulate estimating problem linear program instant advantage proposed approach lassotype estimators estimation equation decomposed multiple subequations accordingly efficiently solved parallel fashion addition method brings new theoretical insights var model analysis far theoretical results developed high dimensions song bickel kock callot mainly pose assumptions design matrix formulated regression problems conditions indirect transition matrices transparent contrast results show operator norm transition matrices plays important role estimation accuracy provide explicit rates convergence estimation prediction addition provide thorough experiments synthetic realworld equity data show empirical advantages method lassotype estimators parameter estimation forecasting\",\"concerned obtaining novel concentration inequalities missing mass total probability mass outcomes observed sample derive first time distributionfree bernsteinlike deviation bounds sublinear exponents deviation size missing mass also improve results mcallester ortiz andberend kontorovich small deviations interesting case learning theory known majority standard inequalities cannot directly used analyze heterogeneous sums sums whose terms large difference magnitude generic intuitive approach shows heterogeneity issue introduced mcallester ortiz resolvable least case missing mass via regulating terms using novel thresholding technique\",\"many applications desirable extract relevant aspects data principled way information bottleneck method one seeks code maximizes information relevance variable constraining information encoded original data unfortunately however method computationally demanding data highdimensional andor nongaussian propose approximate variational scheme maximizing lower bound objective analogous variational using method derive algorithm recover features relevant sparse finally demonstrate kernelized versions algorithm used address broad range problems nonlinear relation\",\"multiple multivariate data sets derive conditions generalized canonical correlation analysis gcca improves classification performance projected datasets compared standard canonical correlation analysis cca using two data sets illustrate theoretical results simulations real data experiment\",\"machine learning methods clustering classification rely distance function describe relationships datapoints complex datasets hard avoid making arbitrary choices defining distance function compare images one must choose spatial scale signals temporal scale right scale hard pin preferable results depend tightly exact value one picked topological data analysis seeks address issue focusing notion neighbourhood instead distance shown cases simpler solution available checked strongly distance relationships depend hyperparameter using dimensionality reduction variant dynamical multidimensional scaling mds formulated embeds datapoints curves resulting algorithm based concaveconvex procedure cccp provides simple efficient way visualizing changes invariances distance patterns hyperparameter varied variant analyze dependence multiple hyperparameters also presented cmds algorithm straightforward implement use extend provided illustrate possibilities cmds cmds applied several realworld data sets\",\"work compute lower lipschitz bounds ellp pooling operators infty well ellp pooling operators preceded halfrectification layers give sufficient conditions design invertible neural network layers numerical experiments mnist image patches confirm pooling layers inverted phase recovery algorithms moreover regularity inverse pooling controlled lower lipschitz constant empirically verified nearest neighbor regression\",\"variational methods recently considered scaling training process gaussian process classifiers large datasets alternative describe train classifiers efficiently using expectation propagation proposed method allows handling datasets millions data instances precisely used training distributed fashion data instances sent different nodes required computations carried maximizing estimate marginal likelihood using stochastic approximation gradient several experiments indicate method described competitive variational approach\",\"present robust alternative principal component analysis pca called elliptical component analysis eca analyzing high dimensional elliptically distributed data eca estimates eigenspace covariance matrix elliptical data cope heavytailed elliptical distributions multivariate rank statistic exploited modellevel consider two settings either leading eigenvectors covariance matrix nonsparse sparse methodologically propose eca procedures nonsparse sparse settings theoretically provide nonasymptotic asymptotic analyses quantifying theoretical performances eca nonsparse setting show ecas performance highly related effective rank covariance matrix sparse setting results twofold show sparse eca estimator based combinatoric program attains optimal rate convergence based recent developments estimating sparse leading eigenvectors show computationally efficient sparse eca estimator attains optimal rate convergence suboptimal scaling\",\"present new algorithms compute mean set empirical probability measures optimal transport metric mean known wasserstein barycenter measure minimizes sum wasserstein distances element set propose two original algorithms compute wasserstein barycenters build upon subgradient method direct implementation algorithms however costly would require repeated resolution large primal dual optimal transport problems compute subgradients extending work cuturi propose smooth wasserstein distance used definition wasserstein barycenters entropic regularizer recover strictly convex objective whose gradients computed considerably cheaper computational cost using matrix scaling algorithms use algorithms visualize large family images solve constrained clustering problem\",\"manuscript consider problem jointly estimating multiple graphical models high dimensions assume data collected subjects consists possibly dependent observations graphical models subjects vary assumed change smoothly corresponding measure closeness subjects propose kernel based method jointly estimating graphical models theoretically double asymptotic framework dimension increase provide explicit rate convergence parameter estimation characterizes strength one borrow across different individuals impact data dependence parameter estimation empirically experiments synthetic real resting state functional magnetic resonance imaging rsfmri data illustrate effectiveness proposed method\",\"consider forwardbackward greedy algorithms solving sparse feature selection problems general convex smooth functions stateoftheart greedy method forwardbackward greedy algorithm fobaobj requires solve large number optimization problems thus scalable largesize problems fobagdt algorithm uses gradient information feature selection forward iteration significantly improves efficiency fobaobj paper systematically analyze theoretical properties forwardbackward greedy algorithms main contributions derive better theoretical bounds existing analyses regarding fobaobj general smooth convex functions show fobagdt achieves theoretical performance fobaobj condition restricted strong convexity condition new bounds consistent bounds special case least squares fills previously existing theoretical gap general convex smooth functions show restricted strong convexity condition satisfied number independent samples barklog bark sparsity number dimension variable apply fobagdt conditional random field objective sensor selection problem human indoor activity recognition results show fobagdt outperforms methods including ones based forward greedy selection lregularization\",\"provide theoretical analysis statistical computational properties penalized mestimators formulated solution possibly nonconvex optimization problem many important estimators fall category including least squares regression nonconvex regularization generalized linear models nonconvex regularization sparse elliptical random design regression problems intractable calculate global solution due nonconvex formulation paper propose approximate regularization pathfollowing method solving variety learning problems nonconvex objective functions unified analytic framework simultaneously provide explicit statistical computational rates convergence local solution attained algorithm computationally algorithm attains global geometric rate convergence calculating full regularization path optimal among firstorder algorithms unlike existing methods attain geometric rates convergence one single regularization parameter algorithm calculates full regularization path iteration complexity particular provide refined iteration complexity bound sharply characterize performance stage along regularization path statistically provide sharp sample complexity analysis approximate local solutions along regularization path particular analysis improves upon existing results providing refined sample complexity bound well exact support recovery result final estimator results show final estimator attains oracle statistical property due usage nonconvex penalty\",\"study problem prediction evolving graph data formulate problem minimization convex objective encouraging sparsity lowrank solution reflect natural graph properties convex formulation allows obtain oracle inequalities efficient solvers provide empirical results algorithm comparison competing methods point two open questions related compressed sensing algebra lowrank sparse matrices\",\"one objectively measure performance individual offensive lineman nfl existing literature proposes various measures rely subjective assessments game film yet develop objective methodology evaluate performance using variety statistics related offensive linemans performance develop framework objectively analyze overall performance individual offensive lineman determine specific linemen overvalued undervalued relative salary identify eight players across nfl seasons considered overvalued undervalued corroborate results existing metrics based subjective evaluation best knowledge techniques set forth work utilized previous works evaluate performance nfl players position including offensive linemen\",\"study problem estimating data sparse approximation inverse covariance matrix estimating sparsity constrained inverse covariance matrix key component gaussian graphical model learning one numerically challenging address challenge developing new adaptive gradientbased method carefully combines gradient information adaptive stepscaling strategy results scalable highly competitive method algorithm like predecessors maximizes ellnorm penalized loglikelihood per iteration arithmetic complexity best methods class experiments reveal approach outperforms stateoftheart competitors often significantly large problems\",\"consider setting linear regression high dimension focus problem constructing adaptive honest confidence sets sparse parameter theta want construct confidence set theta contains theta high probability small possible diameter confidence set depend sparsity theta larger wider confidence set however practice unknown paper focuses constructing confidence set theta contains theta high probability whose diameter adaptive unknown sparsity implementable practice\",\"consider statistical algorithmic aspects solving largescale leastsquares problems using randomized sketching algorithms prior results show emphalgorithmic perspective using sketching matrices constructed random projections leveragescore sampling number samples much smaller original sample size worstcase error solving original problem small relative error emphstatistical perspective one typically considers meansquared error performance randomized sketching algorithms data generated according statistical linear model paper provide rigorous comparison perspectives leading insights differ first develop framework assessing unified manner algorithmic statistical aspects randomized sketching methods consider statistical prediction efficiency statistical residual efficiency sketched estimator use framework provide upper bounds several types random projection random sampling algorithms among results show upper bounded much smaller typically requires number samples substantially larger lower bounds developed subsequent work show upper bounds improved\",\"propose method inferring conditional indepen dence graph cig highdimensional discretetime gaus sian vector random process finitelength observations approach rely parametric model autoregressive model vector random process rather assumes certain spectral smoothness proper ties proposed inference scheme compressive works sample sizes much smaller number scalar process components provide analytical conditions method correctly identify cig high probability\",\"propose loco algorithm largescale ridge regression distributes features across workers cluster important dependencies variables preserved using structured random projections cheap compute must communicated show loco obtains solution close exact ridge regression solution fixed design setting verify experimentally simulation study well application climate prediction furthermore show loco achieves significant speedups compared stateoftheart distributed algorithm largescale regression problem\",\"empirically evaluate stochastic annealing strategy bayesian posterior optimization variational inference variational inference deterministic approach approximate posterior inference bayesian models typically nonconvex objective function locally optimized parameters approximating distribution investigate annealing method optimizing objective aim finding better local optimal solution compare deterministic annealing methods annealing show stochastic annealing provide clear improvement gmm hmm performance lda tends favor deterministic annealing methods\",\"gaussian probability densities omnipresent applied mathematics gaussian cumulative probabilities hard calculate univariate case study utility expectation propagation approximate integration method problem rectangular integration regions approximation highly accurate also extend derivations general case polyhedral integration regions however find polyhedral case eps answer though often accurate almost arbitrarily wrong consider unexpected results empirically theoretically problem gaussian probabilities generally results elucidate interesting nonobvious feature yet studied detail\",\"consider problem extracting lowdimensional linear latent variable structure highdimensional random variables specifically show mild conditions structure manifests linear space spans conditional means possible consistently recover structure using information second moments random variables finding specialized oneparameter exponential families whose variance function quadratic means allows derivation explicit estimator latent structure approach serves latent variable model estimator tool dimension reduction highdimensional matrix data composed many related variables theoretical results verified simulation studies application genomic data\",\"importance sampling widely used machine learning statistics power limited restriction using simple proposals importance weights tractably calculated address problem studying blackbox importance sampling methods calculate importance weights samples generated unknown proposal blackbox mechanism method allows use better richer proposals solve difficult problems somewhat counterintuitively also additional benefit improving estimation accuracy beyond typical importance sampling theoretical empirical analyses provided\",\"estimation dependencies multiple variables central problem analysis financial time series common approach express dependencies terms copula function typically copula function assumed constant may inaccurate covariates could large influence dependence structure data account bayesian framework estimation conditional copulas proposed framework parameters copula nonlinearly related arbitrary conditioning variables evaluate ability method predict timevarying dependencies several equities currencies observe consistent performance gains compared static copula models timevarying copula methods\",\"adaptive filtering algorithms operating reproducing kernel hilbert spaces demonstrated superiority linear counterpart nonlinear system identification unfortunately undesirable characteristic methods order filters grows linearly number input data dramatically increases computational burden memory requirement variety strategies based dictionary learning proposed overcome severe drawback works analyze problem updating dictionary timevarying environment paper present analytical study convergence behavior gaussian leastmeansquare algorithm case statistics dictionary elements partially match statistics input data allows emphasize need updating dictionary online way discarding obsolete elements adding appropriate ones introduce kernel leastmeansquare algorithm lnorm regularization automatically perform task stability mean method analyzed performance tested experiments\",\"certain situations shall undoubtedly common big data era datasets available massive computing statistics full sample hardly feasible unfeasible natural approach context consists using survey schemes substituting full data statistics counterparts based resulting random samples manageable size main purpose paper investigate impact survey sampling unequal inclusion probabilities stochastic gradient descentbased mestimation methods largescale statistical machinelearning problems precisely prove presence priori information one may significantly increase asymptotic accuracy choosing appropriate first order inclusion probabilities without affecting complexity striking results described limit theorems also illustrated numerical experiments\",\"strategy early stopping regularization technique based choosing stopping time iterative algorithm focusing nonparametric regression reproducing kernel hilbert space analyze early stopping strategy form gradientdescent applied leastsquares loss function propose datadependent stopping rule involve holdout crossvalidation data prove upper bounds squared error resulting function estimate measured either lpn norm upper bounds lead minimaxoptimal rates various kernel classes including sobolev smoothness classes forms reproducing kernel hilbert spaces show simulation stopping rule compares favorably two stopping rules one based holdout data based steins unbiased risk estimate also establish tight connection early stopping strategy solution path kernel ridge regression estimator\",\"linear autoregressive models serve basic representations discrete time stochastic processes different attempts made provide nonlinear versions basic autoregressive process including different versions based kernel methods motivated powerful framework hilbert space embeddings distributions paper apply methodology kernel embedding autoregressive process order provide nonlinear version autoregressive process shows increased performance linear model highly complex time series use method proposed onestep ahead forecasting different timeseries compare performance nonlinear methods\",\"mixture experts moe model popular neural network architecture nonlinear regression classification class moe mean functions known uniformly convergent unknown target function assuming target function sobolev space sufficiently differentiable domain estimation compact unit hypercube provide alternative result shows class moe mean functions dense class continuous functions arbitrary compact domains estimation result viewed universal approximation theorem moe models\",\"report derive nonnegative series expansion jensenshannon divergence jsd two probability distributions series expansion shown useful numerical calculations jsd probability distributions nearly equal consequently small numerical errors dominate evaluation\",\"study sparse principal component analysis high dimensional vector autoregressive time series doubly asymptotic framework allows dimension scale series length treat transition matrix time series nuisance parameter directly apply sparse principal component analysis multivariate time series data independent provide explicit nonasymptotic rates convergence leading eigenvector estimation extend result principal subspace estimation analysis illustrates spectral norm transition matrix plays essential role determining final rates also characterize sufficient conditions sparse principal component analysis attains optimal parametric rate theoretical results backed thorough numerical studies\",\"paper study nonconvex penalization using bernstein functions whose firstorder derivatives completely monotone bernstein function induce class nonconvex penalty functions highdimensional sparse estimation problems derive thresholding function based bernstein penalty discuss important mathematical properties sparsity modeling show coordinate descent algorithm especially appropriate regression problems penalized bernstein function also consider application bernstein penalty classification problems devise proximal alternating linearized minimization method based theory kurdykalojasiewicz inequality conduct convergence analysis alternating iteration procedures particularly exemplify family bernstein nonconvex penalties based generalized gamma measure conduct empirical analysis family\",\"graphical models provide powerful tools uncover complicated patterns multivariate data commonly used bayesian statistics machine learning paper introduce package bdgraph performs bayesian structure learning general undirected graphical models decomposable nondecomposable continuous discrete mixed variables package efficiently implements recent improvements bayesian literature including mohammadi wit dobra mohammadi speed computations computationally intensive tasks implemented interfaced package parallel computing capabilities addition package contains several functions simulation visualization well several multivariate datasets taken literature used describe package capabilities paper includes brief overview statistical methods implemented package main part paper explains use package furthermore illustrate packages functionality real artificial examples\",\"offer novel view adaboost statistical setting propose bayesian model binary classification label noise modeled hierarchically using variational inference optimize dynamic evidence lower bound derive new boostinglike algorithm called viboost show close connections adaboost give experimental results four datasets\",null],\"marker\":{\"color\":\"#CFD8DC\",\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"other\",\"showlegend\":false,\"x\":[-0.010108528,3.7231026,1.0338918,4.543571,1.4387732,0.08753926,2.5989063,4.230394,2.6948092,3.005133,0.9640317,0.221403,2.5575972,4.20952,3.652719,-0.10782257,0.18023607,1.7646773,0.2792733,1.6828698,2.1418955,3.2607143,4.5475607,2.6060686,0.9913055,3.001399,1.5834488,1.4508681,-0.081126064,-0.3820022,2.1886055,3.6821218,1.8541859,-0.7210696,3.1431456,0.21765727,0.24708062,-0.38534713,1.9644942,1.9837899,0.7985092,3.181149,2.618552,1.7339946,2.6072676,-0.5173686,2.5518951,0.41137025,0.99443287,1.0677261,4.6863074,3.7247717,0.7336967,0.38232136,4.616396,1.0325005,0.70907044,3.8643243,1.5515559,0.9844909,3.0550988,4.5810375,3.2720883,1.7001565,2.167821,0.88746434,5.0388236,0.45182317,1.3510026,4.295625,0.1498234,1.8702418,3.8561645,0.8855884,0.9568539,1.7708627,0.43518892,2.516918,3.342206,1.0500859,4.429518,0.38940462,1.6355126,0.037070453,3.8149037,0.33480373,0.7762391,3.2772973,3.401673,3.6594963,-0.29404882,-0.18816833,0.2699527,1.611448,3.097433,0.2922005,1.7083718,4.5852966,3.9960434,4.2140803,0.49664006,4.2183137,3.2852511,0.43392137,1.7246954,2.19225,3.3183813,0.28253323,3.6822066,4.7473693,-0.087071024,-0.08035755,1.0085077,0.7598222,4.5300694,3.8661985,3.7615154,2.4096675,1.4311987,1.379267,2.4877186,1.103271,2.6381228,4.588051,3.1472485,2.8725429,2.075846,2.3851576,1.8502195,3.8288445,2.6962948,2.564964,2.1943614,-0.57604533,2.98198,-0.23001315,2.5094173,4.91069,1.9133128,5.054367,3.2745223,1.1416315,0.31008026,1.1724123,2.3843248,3.173651,-0.3884484,3.1421623,1.4346882,1.2495126,4.102396,3.5252688,0.16037454,2.636602,2.1074774,1.4145806,4.1094728,3.458832,0.6634485,3.8930905,0.963641,2.6271217,6.086219,3.5457509,4.471567,3.4325018,2.1986103,4.554857,0.42544848,1.0908221,0.81286025,2.60308,2.603315,1.7490563,2.3502285,1.6452421,1.8158623,-0.089489594,1.4365126,2.3716254,0.93800604,3.6513207,0.2734904,1.8298882,2.0894518,-0.04799828,4.6331844,0.76534384,2.42828,4.604264,-0.12486915,1.8223467,2.5213163,4.101347,3.4607806,3.5713174,1.881739,-0.11810965,4.2304006,3.433566,4.6524706,3.411848,3.7769358,0.15945046,3.349998,1.9627419,-0.08583504,1.8966186,1.5497406,3.2804515,1.7653339,1.6418355,1.9758756,1.896122,3.8215363,1.338207,1.0970103,0.25439876,2.7617762,1.8283219,0.75441456,1.8279532,1.2299066,3.3755827,1.0409999,4.2672358,1.0148094,3.3802795,4.1246195,3.7077236,1.3922402,1.9407523,-0.007804412,5.0064692,3.8624187,-0.6719569,0.6689885,2.7543523,3.935056,3.4172978,1.1131828,1.100709,1.2902738,0.3132149,4.259074,1.3909239,3.4074156,2.145601,-0.1873047,1.041559,0.31758806,3.9962556,5.0398154,1.8925164,1.6458693,4.4786077,2.9876194,4.544939,5.0708246,1.1292487,1.4542454,5.0425663,1.8380525,2.4335432,-0.056321125,4.5611844,4.080387,0.36769712,1.0364176,4.0439286,0.97848654,4.4895287,2.2749765,3.3819075,0.61879534,1.838668,2.591637,1.9221009,3.9359167,0.33306652,0.77162725,1.801971,1.9240696,1.718389,1.5464641,0.26100352,0.7382515,2.4622738,-0.06583641,3.7618291,-0.1037604,2.6343215,1.5073729,1.4720905,1.1319121,-0.35026997,1.4061966,1.367744,3.0254693,0.5294234,3.94347,2.4378374,1.8116252,2.6491635,2.2758439,0.5606653,-0.35123554,3.0340936,-0.7921353,1.9677889,5.097695,1.0549587,1.4342,3.361979,2.6571245,0.5296049,4.530904,-0.19483297,4.1918845,-0.004962346,0.37778744,2.613595,1.6024493,1.9887084,0.9713631,-0.19692922,3.7673988,1.3934937,0.28962615,-0.7052594,-0.7012184,2.1799183,0.121338785,4.6499734,4.4967275,2.972373,3.0823488,2.7116997,4.066188,3.2926383,-0.39119992,3.7692752,4.0827084,3.094013,4.4786363,0.11928576,1.0307003,4.0102715,3.1983075,1.22781,3.1590762,0.934389,1.8346992,-0.112008184,0.46012157,1.8427267,2.389216,0.20957881,3.3253021,1.8643386,2.6712055,4.0937395,0.30267286,0.37644777,0.15029036,0.5792884,0.2484289,0.24765602,3.2483318,4.5340014,0.99912983,3.5835836,2.8909805,4.0351534,3.353835,4.1022873,2.2001069,1.5449706,1.3312037,1.9152082,1.8169683,4.222254,1.9749167,-0.14742924,5.0016117,0.0091926595,2.099609],\"y\":[0.78814477,5.6062293,3.950543,0.62058693,4.211274,5.1816077,2.021134,5.461005,1.3137172,3.2039745,4.9207172,3.3970525,2.1240344,1.3896042,1.6425356,2.3248262,2.5888612,5.480777,3.2496307,1.235324,1.2751975,1.4402089,0.59128255,3.2686634,3.3298504,3.4116323,4.653027,2.985562,1.8634644,4.2885075,2.1976917,0.5725763,5.7628536,3.2154307,6.395471,5.9725513,1.5609616,4.29451,5.0612273,0.061533615,2.8423896,5.065623,1.1187711,1.2578312,3.8957083,2.5816526,5.533639,2.0952382,0.6630758,4.064536,1.0492018,1.6151932,3.538496,2.432814,1.0180163,5.098,2.0985367,0.6140322,1.4099917,0.6642479,3.448265,0.85727537,0.12294272,4.3654995,0.8405996,4.5720706,3.9291031,2.1623263,4.7584825,3.440632,3.5961547,1.9210578,1.7880361,4.6576104,1.3622266,0.6469931,0.82021564,2.0748768,1.045136,4.9780536,5.0999627,0.79536474,5.5041876,2.688951,2.5413542,3.9395895,2.0216885,1.9726632,-0.30567372,1.3553513,3.2236614,3.7937508,4.4837303,5.5740085,-0.15201344,1.5394485,4.337019,0.70235026,4.116048,1.3538054,4.4542756,4.5001493,4.8622117,0.8140215,1.2433305,1.3947114,0.8709544,4.495127,1.2836668,1.5106848,1.1338797,1.8725783,1.0408292,3.7094846,0.9345824,0.6762723,3.0806303,1.4344928,4.8863673,4.3854036,4.271498,4.0849524,0.21626274,5.3311434,6.401806,2.1193607,1.0788491,5.483487,3.6730866,1.2041768,1.9580435,2.1336038,5.2980475,2.9806325,3.3816018,3.1276348,2.254038,4.0049524,5.825915,4.869345,1.4780592,3.3866541,3.7593129,0.58534986,2.1634507,1.473857,4.30384,6.149878,3.2644637,1.2771109,4.0281982,0.7724328,3.0010612,0.13974461,1.1922674,4.2802157,4.0384183,6.425471,1.0176255,1.5132679,3.5266724,1.315549,5.9733043,0.51954824,5.1167746,4.5647507,1.3666433,5.1093416,2.5873554,2.506947,3.3270009,3.2388988,3.2682097,0.70046836,3.6548476,4.6114187,3.6928723,1.876218,3.1462376,3.7448237,1.9148666,2.9829144,3.3211474,0.8554416,1.1244609,1.1424437,0.7402125,4.320731,0.89116806,0.8774925,1.064707,5.6878204,1.2959541,2.0099857,-0.20038188,0.68500316,1.2624333,3.1091137,4.550363,0.3889168,1.0721977,1.8608205,1.5115387,4.1423445,4.574933,0.498525,1.8915848,1.8578157,4.67768,0.12149853,5.4762516,4.6172605,4.5874987,4.0760074,1.0523913,3.42286,1.0501554,2.589669,2.89613,0.89972246,4.0533214,2.4940846,2.1047103,0.849995,1.4945219,4.137897,4.994656,-0.3044299,4.021058,1.6387326,1.0068871,5.819802,2.7122395,4.6058793,1.8212355,2.8485973,4.5481434,0.4704133,0.3481977,-0.3379174,1.0498325,4.253298,3.0281754,2.1907494,1.3766841,4.2202206,0.90221107,1.2029399,3.3973587,4.979602,1.2459326,6.104092,4.9050474,5.8053207,1.1238117,5.1292467,3.3580623,3.8390398,4.3405533,3.7954078,1.548126,4.4506736,4.4458346,2.6338906,1.2802132,3.8096447,4.027753,4.723356,4.9713173,1.969909,3.9890485,0.5321326,2.154453,1.5531166,1.2740134,0.91138756,0.2850982,0.53237134,0.8115203,3.2782252,3.7268798,0.6627813,5.050209,5.337096,1.5955312,3.6298015,2.885434,4.234381,3.9639018,0.32578608,3.0698388,3.8807347,0.94565487,4.7632275,1.0243121,4.3321743,2.9748287,4.2309127,3.3975444,1.6607907,0.66432476,2.602402,3.6691911,0.22256656,1.2922152,3.5595217,2.254191,-0.37842333,3.0865571,2.2159424,4.0137796,4.9950247,3.27209,0.95647645,3.1526697,3.5580804,0.9546223,3.190681,4.3344154,4.728937,2.3841236,3.246988,1.0410187,2.2377923,1.095545,3.0787663,1.6421129,4.244007,4.6963754,3.0738642,3.2660708,1.3730788,2.6559978,0.81133634,4.8292966,-0.34597483,3.5342047,3.1338012,2.0440674,5.8719544,4.3045473,0.72401047,4.009869,2.5186415,1.6734507,4.2133584,4.991586,2.0278518,-0.39122048,1.0124289,1.042972,3.0568047,1.8453662,1.8747355,4.478298,4.3559356,5.485279,3.5213068,1.4960905,4.593522,3.8802009,4.32272,3.3249094,3.306313,4.261683,1.3286254,3.6303575,2.5959094,-0.38030717,3.8752174,3.9512792,0.40780142,2.068018,3.277108,-0.3020199,4.0416236,2.177911,0.5262417,2.1433516,1.9128397,1.6692995,1.3751619,4.647903,3.8016958,4.4431314,1.1689676,2.7508445],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"paper propose bayesian nonparametric approach modelling sparse timevarying networks positive parameter associated node network models sociability node sociabilities assumed evolve time modelled via dynamic point process model model able capture long term evolution sociabilities moreover yields sparse graphs number edges grows subquadratically number nodes evolution sociabilities described tractable timevarying generalised gamma process provide theoretical insights model apply three datasets simulated network network hyperlinks communities reddit network cooccurences words reuters news articles september attacks\",\"random graphs distributed according stochastic blockmodels special case latent position graphs adjacency spectral embedding followed appropriate vertex classification asymptotically bayes optimal approach requires knowledge critically depends model dimension paper propose sparse representation vertex classifier require information model dimension classifier represents test vertex sparse combination vertices training set uses recovered coefficients classify test vertex prove consistency proposed classifier stochastic blockmodels demonstrate sparse representation classifier predict vertex labels higher accuracy adjacency spectral embedding approaches via simulation studies real data experiments results demonstrate robustness effectiveness proposed vertex classifier model dimension unknown\",\"vertex clustering stochastic blockmodel graph wide applicability subject extensive research thispaper provide short proof adjacency spectral embedding used obtain perfect clustering stochastic blockmodel degreecorrected stochastic blockmodel also show analogous result general random dot product graph model\",\"interaction transitivity sparsity two common features empirical networks implies local regions large sparse networks dense call blessing transitivity consequences modeling inference extant research suggests statistical inference stochastic blockmodel difficult edges sparse however conclusion confounded fact asymptotic limit previous studies merely sparse also nontransitive retain transitivity blocks cannot grow faster expected degree thus sparse models blocks must remain asymptotically small previous algorithmic research demonstrates small local clusters amenable computation visualization interpretation compared global graph partitions paper provides first statistical results demonstrate small transitive clusters also amenable statistical estimation theorem shows local clustering algorithm high probability detect transitive stochastic block fixed size nodes embedded large graph constraint ambient graph large sparseit could generated random adversarysuggesting theoretical explanation robust empirical performance local clustering algorithms\",\"present method based orthogonal symmetric nonnegative matrix trifactorization normalized laplacian matrix community detection complex networks exact factorization given order may exist hard compute obtain approximate factorization solving optimization problem establish connection factors obtained factorization nonnegative basis invariant subspace estimated matrix drawing parallel spectral clustering using factorization clustering networks motivated analyzing blockdiagonal laplacian matrix blocks representing connected components graph method shown consistent community detection graphs generated stochastic block model degree corrected stochastic block model simulation results real data analysis show effectiveness methods wide variety situations including sparse highly heterogeneous graphs usual spectral clustering known fail method also performs better state art popular benchmark network datasets political web blogs karate club data\",\"consider problem unveiling implicit network structure node interactions user interactions social network based highfrequency timestamps inference based minimization leastsquares loss associated multivariate hawkes model penalized ell trace norm interaction tensor provide first theoretical analysis problem includes sparsity lowrank inducing penalizations result involves new datadriven concentration inequality matrix martingales continuous time observable variance result independent interest broad range possible applications since extends matrix martingales former results restricted scalar case consequence analysis construction sharply tuned ell tracenorm penalizations leads datadriven scaling variability information available users numerical experiments illustrate significant improvements achieved use datadriven penalizations\",\"stochastic block model sbm widely used random graph model networks communities despite recent burst interest recovering communities sbm statistical computational points view still gaps understanding fundamental information theoretic computational limits recovery paper consider sbm full generality restriction number sizes communities grow number nodes well connection probabilities inside across communities generality allows move past artifacts homogenous sbm understand right parameters relative densities communities define various recovery thresholds outline implications generalizations via set illustrative examples instance log considered standard lower bound cluster size exact recovery via convex methods homogenous sbm show possible right circumstances sizes spread smaller cluster denser recover small clusters sqrtlog size polylogarithmic\",\"paper consider problem link prediction timeevolving graphs assume certain graph features node degree follow vector autoregressive var model propose use information improve accuracy prediction strategy involves joint optimization procedure space adjacency matrices var matrices takes account sparsity low rank properties matrices oracle inequalities derived illustrate tradeoffs choice smoothing parameters modeling joint effect sparsity low rank property estimate computed efficiently using proximal methods generalized forwardbackward agorithm\",\"partitioning graph groups vertices within group densely connected vertices assigned different groups known graph clustering often used gain insight organisation large scale networks visualisation purposes whereas large number dedicated techniques recently proposed static graphs design online graph clustering methods tailored evolving networks challenging problem much less documented literature motivated broad variety applications concerned ranging study biological networks analysis networks scientific references exploration communications networks world wide web main purpose paper introduce novel computationally efficient approach graph clustering evolutionary context namely method promoted article viewed incremental eigenvalue solution spectral clustering method described incremental eigenvalue solution general technique finding approximate eigenvectors symmetric matrix given change well outlining approach detail present theoretical bound quality approximate eigenvectors using perturbation theory derive novel spectral clustering algorithm called incremental approximate spectral clustering iasc iasc algorithm simple implement efficacy demonstrated synthetic real datasets modelling evolution hiv epidemic citation network purchase history graph ecommerce website\",\"consider principal component analysis pca decomposable gaussian graphical models exploit prior information models order distribute computation purpose reformulate problem sparse inverse covariance concentration domain solve global eigenvalue problem using sequence local eigenvalue problems cliques decomposable graph demonstrate application methodology context decentralized anomaly detection abilene backbone network based topology network propose approximate statistical graphical model distribute computation pca\",\"consider problem grouping multiple graphs several clusters using singular value thesholding nonnegative factorization derive model selection information criterion estimate number clusters demonstrate approach using swimmer data set well simulated data set compare performance two standard clustering algorithms\",\"many statistical methods network data parameterize edgeprobability attributing latent traits vertices block structure assume exchangeability sense aldoushoover representation theorem empirical studies networks indicate many realworld networks powerlaw distribution vertices turn implies number edges scale slower quadratically number vertices assumptions fundamentally irreconcilable aldoushoover theorem implies quadratic scaling number edges recently caron fox proposed use different notion exchangeability due kallenberg obtained network model admits powerlaw behaviour retaining desirable statistical properties however model capture latent vertex traits blockstructure work reintroduce use blockstructure network models obeying kallenbergs notion exchangeability thereby obtain model admits inference blockstructure edge inhomogeneity derive simple expression likelihood efficient sampling method obtained model significantly difficult implement existing approaches blockmodelling performs well real network datasets\",\"present two graphbased algorithms multiclass segmentation highdimensional data algorithms use diffuse interface model based ginzburglandau functional related total variation compressed sensing image processing multiclass extension introduced using gibbs simplex functionals doublewell potential modified handle multiclass case first algorithm minimizes functional using convex splitting numerical scheme second algorithm uses graph adaptation classical numerical merrimanbenceosher mbo scheme alternates diffusion thresholding demonstrate performance algorithms experimentally synthetic data grayscale color images several benchmark data sets mnist coil webkb also make use fast numerical solvers finding eigenvectors eigenvalues graph laplacian take advantage sparsity matrix experiments indicate results competitive better current stateoftheart multiclass segmentation algorithms\",\"existing approaches analyzing asymptotics graph laplacians typically assume wellbehaved kernel function smoothness assumptions remove smoothness assumption generalize analysis graph laplacians include previously unstudied graphs including knn graphs also introduce kernelfree framework analyze graph constructions shrinking neighborhoods general apply analyze locally linear embedding lle also describe given limiting laplacian operator desirable properties convergent spectrum sparseness achieved choosing appropriate graph construction\",\"several problems network intrusion community detection disease outbreak described observations attributed nodes edges graph applications presence intrusion community disease outbreak characterized novel observations unknown connected subgraph problems formulated terms optimization suitable objectives connected subgraphs problem generally computationally difficult overcome combinatorics connectivity embedding connected subgraphs linear matrix inequalities lmi computationally efficient tests realized optimizing convex objective functions subject lmi constraints prove means novel euclidean embedding argument tests minimax optimal exponential family distributions lattices show internal conductance connected subgraph family plays fundamental role characterizing detectability\",\"hypergraph partitioning lies heart number problems machine learning network sciences many algorithms hypergraph partitioning proposed extend standard approaches graph partitioning case hypergraphs however theoretical aspects methods seldom received attention literature compared extensive studies guarantees graph partitioning instance consistency results spectral graph partitioning stochastic block model well known paper present planted partition model sparse random nonuniform hypergraphs generalizes stochastic block model derive error bound spectral hypergraph partitioning algorithm model using matrix concentration inequalities best knowledge first consistency result related partitioning nonuniform hypergraphs\",\"stochastic blockmodels among prominent statistical models cluster analysis complex networks clusters defined groups nodes statistically similar link probabilities within groups recent extension karrer newman incorporates node degree correction model degree heterogeneity within group although demonstrably leads better performance several networks obvious whether modelling node degree always appropriate necessary formulate degree corrected stochastic blockmodel nonparametric bayesian model incorporating parameter control amount degree correction inferred data additionally formulation yields principled ways inferring number groups well predicting missing links network used quantify models predictive performance synthetic data demonstrate including degree correction yields better performance recovering true group structure predicting missing links degree heterogeneity present whereas performance par data degree heterogeneity within clusters seven real networks ground truth group structure available show predictive performance equal whether degree correction included however networks significantly fewer clusters discovered correcting degree indicating data compactly explained clusters heterogenous degree nodes\",\"define class euclidean distances weighted graphs enabling perform thermodynamic soft graph clustering class constructed form raw coordinates encountered spectral clustering extended means higherdimensional embeddings schoenberg transformations geographical flow data properly conditioned illustrate procedure well visualization aspects\",\"community detection graphs subject many algorithms recent methods want optimize modularity function shows maximum relationships within communities found minimum intercommunity relations algorithms applied unipartite multipartite directed graphs however given npcompleteness problem algorithms heuristics guarantee optimum paper introduce algorithm based approximate solution obtained efficient detection algorithm modifie achieve local optimum based function reassignment function potential function therefore computed optimum nash equilibrium supplement method overlap function allows simultaneously two detection modes several experiments show interest approach\",\"statistical inference graphs burgeoning field applied theoretical statistics communities well throughout wider world science engineering business etc many applications faced reality errorfully observed graphs existence edge two vertices based imperfect assessment paper consider graph wish perform inference task inference task considered vertex classification however observe rather potential edge vchoose observe edgefeature use classify edgenotedge thus errorfully observe observe graph widetildeg vwidetildee edges widetildee arise classifications edgefeatures expected errorful moreover face quantityquality tradeoff regarding edgefeatures observe informative edgefeatures expensive hence number potential edges assessed decreases quality edgefeatures studied problem formulating quantityquality tradeoff simple class random graphs model namely stochastic blockmodel consider simple optimal vertex classifier classifying derive optimal quantityquality operating point subsequent graph inference face tradeoff optimal operating points quantityquality tradeoff surprising illustrate issue methods intermediate tasks chosen maximize performance ultimate inference task finally investigate quantityquality tradeoff errorful obesrvations elegans connectome graph\",\"consider problem vertex classification graphs constructed latent position model shown previously approach embedding graphs euclidean space followed classification space yields universally consistent vertex classifier however major technical difficulty approach arises classifying unlabeled outofsample vertices without including embedding stage paper studied outofsample extension graph embedding step impact subsequent inference tasks show latent position graph model sufficiently large mapping outofsample vertices close true latent position demonstrate successful inference outofsample vertices possible\",\"laplacian eigenvectors graph constructed data set used many spectral manifold learning algorithms diffusion maps spectral clustering given graph constructed random sample ddimensional compact submanifold mathbbrd establish spectral convergence rate graph laplacian implies consistency spectral clustering algorithm via standard perturbation argument simple numerical study indicates necessity denoising step applying spectral algorithms\",\"latent block model lbm flexible probabilistic tool describe interactions node sets bipartite networks account interactions time varying intensity nodes unknown classes paper propose non stationary temporal extension lbm clusters simultaneously two node sets bipartite network constructs classes time intervals interactions stationary number clusters well membership classes obtained maximizing exact completedata integrated likelihood relying greedy search approach experiments simulated real data carried order assess proposed methodology\",\"graph clustering involves task dividing nodes clusters edge density higher within clusters opposed across clusters natural classic popular statistical setting evaluating solutions problem stochastic block model also referred planted partition model paper present new algorithma convexified version maximum likelihoodfor graph clustering show classic stochastic block model setting outperforms existing methods polynomial factors cluster size allowed general scalings fact within logarithmic factors known lower bounds spectral methods evidence suggesting polynomial time algorithm would significantly better show guarantee carries general extension stochastic block model method handle settings semirandom graphs heterogeneous degree distributions unequal cluster sizes unaffiliated nodes partially observed graphs planted cliquecoloring etc particular results provide best exact recovery guarantees date planted partition planted kdisjointcliques planted noisy coloring models general cluster sizes settings match best existing results logarithmic factors\",\"consider problem embedding unweighted directed knearest neighbor graphs lowdimensional euclidean space knearest neighbors vertex provides ordinal information distances points distances use ordinal information along lowdimensionality recover coordinates points arbitrary similarity transformations rigid transformations scaling furthermore also illustrate possibility robustly recovering underlying density via total variation maximum penalized likelihood estimation tvmple method make existing approaches scalable using instance localtoglobal algorithm based group synchronization recently proposed literature context sensor network localization structural biology augment scaling synchronization step demonstrate scalability approach large graphs show compares local ordinal embedding loe algorithm recently proposed recovering configuration cloud points pairwise ordinal comparisons sparse set distances\",\"community detection fundamental problem network analysis made challenging overlaps communities often occur practice propose general flexible interpretable generative model overlapping communities thought generalization degreecorrected stochastic block model develop efficient spectral algorithm estimating community memberships deals overlaps employing kmedians algorithm rather usual kmeans clustering spectral domain show algorithm asymptotically consistent networks sparse overlaps communities large numerical experiments simulated networks many real social networks demonstrate method performs well compared number benchmark methods overlapping community detection\",\"spectral clustering sensitive graphs constructed data particularly proximal imbalanced clusters present show ratiocut rcut normalized cut ncut objectives tailored imbalanced data since tend emphasize cut sizes cut values propose graph partitioning problem seeks minimum cut partitions minimum size constraints partitions deal imbalanced data approach parameterizes family graphs adaptively modulating node degrees fixed node set yield set parameter dependent cuts reflecting varying levels imbalance solution problem obtained optimizing parameters present rigorous limit cut analysis results justify approach demonstrate superiority method unsupervised semisupervised experiments synthetic real data sets\",\"explosion interest statistical models analyzing network data considerable interest class exponential random graph erg models especially connection difficulties computing maximum likelihood estimates issues associated difficulties relate broader structure discrete exponential families paper reexamines issues two parts first consider closure kdimensional exponential families distribution discrete base measure polyhedral convex support mathrmp show normal fan mathrmp geometric object plays fundamental role deriving statistical geometric properties corresponding extended exponential families discuss relevance maximum likelihood estimation theoretical computational standpoint second apply results analysis erg models particular means detailed example provide characterization properties erg models particular certain behaviors erg models known degeneracy\",\"prove central limit theorem components eigenvectors corresponding largest eigenvalues normalized laplacian matrix finite dimensional random dot product graph corollary show stochastic blockmodel graphs rows spectral embedding normalized laplacian converge multivariate normals furthermore mean covariance matrix row functions associated vertexs block membership together prior results eigenvectors adjacency matrix compare via chernoff information multivariate normal distributions choice embedding method impacts subsequent inference demonstrate neither embedding method dominates respect inference task recovering latent block assignments\",\"labeled stochastic block model random graph model representing networks community structure interactions multiple types simplest form consists two communities approximately equal size edges drawn labeled random probability depending whether two endpoints belong community conjectured citeheimlicher correlated reconstruction identification partition correlated true partition underlying communities would feasible model parameter exceeds threshold prove one half conjecture reconstruction impossible threshold positive direction introduce weighted graph exploit label information suitable choice weight function show threshold specific constant reconstruction achieved minimum bisection semidefinite relaxation minimum bisection spectral method combined removal edges incident vertices high degree furthermore show hypothesis testing labeled stochastic block model labeled erdhosrenyi random graph model exhibits phase transition conjectured reconstruction threshold\",\"prove criterion markov equivalence provided zhao may involve set features graph exponential number vertices\",\"spectral clustering graphbased semisupervised learning ssl algorithms sensitive graphs constructed data particular data proximal unbalanced clusters algorithms lead poor performance wellknown graphs knn fullrbf epsilongraphs objectives ratiocut rcut normalized cut ncut attempt tradeoff cut values cluster sizes tailored unbalanced data propose novel graph partitioning framework parameterizes family graphs adaptively modulating node degrees knn graph propose model selection scheme choose sizable clusters separated smallest cut values framework able adapt varying levels unbalancedness data naturally used small cluster detection theoretically justify ideas limit cut analysis unsupervised semisupervised experiments synthetic real data sets demonstrate superiority method\",\"present method estimate block membership nodes random graph generated stochastic blockmodel use embedding procedure motivated random dot product graph model particular example latent position model embedding associates node vector vectors clustered via minimization square error criterion prove method consistent assigning nodes blocks negligible number nodes misassigned prove consistency method directed undirected graphs consistent block assignment makes possible consistent parameter estimation stochastic blockmodel extend result setting number blocks grows slowly number nodes method also computationally feasible even large graphs compare method laplacian spectral clustering analysis simulated data graph derived wikipedia documents\",\"work develops generic framework called bagofpaths bop link network data analysis central idea assign probability distribution set paths network precisely gibbsboltzmann distribution defined bag paths network representation considers paths independently show distribution probability drawing path connecting two nodes easily computed closed form simple matrix inversion probability captures notion relatedness nodes graph two nodes considered highly related connected many preferably lowcost paths application two families distances nodes derived bop probabilities interestingly second distance family interpolates shortest path distance resistance distance addition extends bellmanford formula computing shortest path distance order integrate suboptimal paths simply replacing minimum operator soft minimum operator experimental results semisupervised classification show new distance families competitive stateoftheart approaches addition distance measures studied paper bagofpaths framework enables straightforward computation many relevant network measures\",\"nonparametric detection existence anomalous structure network investigated nodes corresponding anomalous structure one exists receive samples generated distribution different distribution generating samples nodes anomalous structure exist nodes receive samples generated assumed distributions arbitrary unknown goal design statistically consistent tests probability errors converging zero network size becomes asymptotically large kernelbased tests proposed based maximum mean discrepancy measures distance mean embeddings distributions reproducing kernel hilbert space detection anomalous interval line network first studied sufficient conditions minimum maximum sizes candidate anomalous intervals characterized order guarantee proposed test consistent also shown certain necessary conditions must hold guarantee test universally consistent comparison sufficient necessary conditions yields proposed test orderlevel optimal nearly optimal respectively terms minimum maximum sizes candidate anomalous intervals generalization results networks developed numerical results provided demonstrate performance proposed tests\",\"paper focus stochastic block model sbma probabilistic tool describing interactions nodes network using latent clusters sbm assumes networkhas stationary structure connections time varying intensity taken account words interactions two groups forced features whole observation time overcome limitationwe propose partition whole time horizon interactions observed develop non stationary extension sbmallowing simultaneously cluster nodes network along fixed time intervals interactions take place number clusters nodes time intervals well class memberships finallyobtained maximizing completedata integrated likelihood means greedy search approach showing model works properly simulated data focus real data set thus consider three days acm hypertext conference held turinjune july proximity interactions attendees first day modelled interestingclustering daily hours finally obtained times social gathering coffee breaks recovered approach applications large networks limited due computational complexity greedy search dominated bythe number kmax dmax clusters used initialization thereforeadvanced clustering tools considered reduce number clusters expected data making greedy search applicable large networks\",\"unbalanced data arises many learning tasks clustering multiclass data hierarchical divisive clustering semisupervised learning graphbased approaches popular tools problems graph construction important aspect graphbased learning show graphbased algorithms fail unbalanced data many popular graphs knn epsilonneighborhood fullrbf graphs propose novel graph construction technique encodes global statistical information node degrees ranking scheme rank data sample estimate pvalue proportional total number data samples smaller density ranking scheme serves surrogate density reliably estimated indicates whether data sample close valleysmodes rankmodulated degreermd scheme able significantly sparsify graph near valleys provides adaptive way cope unbalanced data theoretically justify method limit cut analysis unsupervised semisupervised experiments synthetic real data sets demonstrate superiority method\",\"many real world graphs graphs molecules exhibit structure multiple different scales existing kernels graphs either purely local purely global character contrast building hierarchy nested subgraphs multiscale laplacian graph kernels mlg kernels define paper account structure range different scales heart mlg construction another new graph kernel called feature space laplacian graph kernel flg kernel property lift base kernel defined vertices two graphs kernel graphs mlg kernel applies flg kernels subgraphs recursively make mlg kernel computationally feasible also introduce randomized projection procedure similar nystrom method rkhs operators\",\"laplacian mixture models identify overlapping regions influence unlabeled graph network data scalable computationally efficient way yielding useful lowdimensional representations combining laplacian eigenspace finite mixture modeling methods provide probabilistic fuzzy dimensionality reductions domain decompositions variety input data types including mixture distributions feature vectors graphs networks provable optimal recovery using algorithm analytically shown nontrivial class cluster graphs heuristic approximations scalable highperformance implementations described empirically tested connections pagerank community detection network analysis demonstrate wide applicability approach origins fuzzy spectral methods beginning generalized heat diffusion equations physics reviewed summarized comparisons dimensionality reduction clustering methods challenging unsupervised machine learning problems also discussed\",\"given time series graphs fixed vertex set represents actors edge vertex vertex time represents existence communications event actors tth time period wish detect anomalies andor change points consider collection graph features invariants demonstrate adaptive fusion provides superior inferential efficacy compared naive equal weighting certain class anomaly detection problems simulation results using latent process model time series graphs well illustrative experimental results time series graphs derived enron email data show fusion statistic provide superior inference compared individual invariants alone results also demonstrate adaptive weighting scheme fusion invariants performs better naive equal weighting\",\"contagions spread popular news stories infectious diseases propagate cascades dynamic networks unobservable topologies however social signals product purchase time blog entry timestamps measurable implicitly depend underlying topology making possible track time interestingly network topologies often jump discrete states may account sudden changes observed signals present paper advocates switched dynamic structural equation model capture topologydependent cascade evolution well discrete states driving underlying topologies conditions proposed switched model identifiable established leveraging edge sparsity inherent social networks recursive ellnorm regularized leastsquares estimator put forth jointly track states network topologies efficient firstorder proximalgradient algorithm developed solve resulting optimization problem numerical experiments synthetic data real cascades measured span one year conducted test results corroborate efficacy advocated approach\",\"lately several suggestions parametrized distances graph generalize shortest path distance commute time resistance distance need developing distances risen observation abovementioned common distances many situations fail take account global structure graph article develop theory one family graph node distances known randomized shortest path dissimilarity foundation statistical physics show randomized shortest path dissimilarity easily computed closed form pairs nodes graph moreover come new definition distance measure call free energy distance free energy distance seen upgrade randomized shortest path dissimilarity defines metric addition satisfies graphgeodetic property derivation computation free energy distance also straightforward make comparison set generalized distances interpolate shortest path distance commute time resistance distance comparison focuses applicability distances graph node clustering classification comparison general shows parametrized distances perform well tasks particular see results obtained free energy distance among best experiments\",\"problem finding overlapping communities networks gained much attention recently optimizationbased approaches use nonnegative matrix factorization nmf variants global optimum cannot provably attained general modelbased approaches popular mixedmembership stochastic blockmodel mmsb airoldi use parameters node specify overlapping communities standard inference techniques cannot guarantee consistency link two approaches establishing sufficient conditions symmetric nmf optimization unique solution mmsb proposing computationally efficient algorithm called geonmf provably optimal hence consistent broad parameter regime demonstrate accuracy simulated realworld datasets\",\"performance spectral clustering considerably improved via regularization demonstrated empirically amini provide attempt quantifying improvement theoretical analysis stochastic block model sbm extensions previous results spectral clustering relied minimum degree graph sufficiently large good performance examining scenario regularization parameter tau large show minimum degree assumption potentially removed special case sbm two blocks results require maximum degree large grow faster log opposed minimum degree importantly show usefulness regularization situations nodes belong welldefined clusters results rely biasvariancelike tradeoff arises understanding concentration sample laplacian eigen gap function regularization parameter byproduct bounds propose datadriven technique textitdkest standing estimated daviskahan bounds choosing regularization parameter technique shown work well simulations real data set\",\"estimation probabilities network edges observed adjacency matrix important applications predicting missing links network denoising usually addressed estimating graphon function determines matrix edge probabilities illdefined without strong assumptions network structure propose novel computationally efficient method based neighborhood smoothing estimate expectation adjacency matrix directly without making structural assumptions graphon estimation requires neighborhood smoothing method requires little tuning competitive meansquared error rate outperforms many benchmark methods link prediction simulated real networks\",\"visual rendering graphs key task mapping complex network data although graph drawing algorithms emphasize aesthetic appeal certain applications traveltime maps place importance visualization structural network properties present paper advocates two graph embedding approaches centrality considerations comply node hierarchy problem formulated first one constrained multidimensional scaling mds solved via block coordinate descent iterations successive approximations guaranteed convergence kkt point addition regularization term enforcing graph smoothness incorporated goal reducing edge crossings second approach leverages locallylinear embedding lle algorithm assumes graph encodes data sampled lowdimensional manifold closedform solutions resulting centralityconstrained optimization problems determined yielding meaningful embeddings experimental results demonstrate efficacy approaches especially visualizing large networks order thousands nodes\",\"spectral embedding uses eigenfunctions discrete laplacian weighted graph obtain coordinates embedding abstract data set euclidean space propose new preprocessing step first using eigenfunctions simulate lowfrequency wave moving data using position well change time wave obtain refined metric classical methods dimensionality reduction applied motivated behavior waves symmetries wave equation hunting technique bats shown effective practice also works partial differential equations method yields improved results even classical heat equation\",\"paper presents novel spectral algorithm additive clustering designed identify overlapping communities networks algorithm based geometric properties spectrum expected adjacency matrix random graph model call stochastic blockmodel overlap sbmo adaptive version algorithm require knowledge number hidden communities proved consistent sbmo degrees graph slightly logarithmic algorithm shown perform well simulated data realworld graphs known overlapping communities\",\"detection anomalous activity graphs statistical problem arises many applications network surveillance disease outbreak detection activity monitoring social networks beyond wide applicability graph structured anomaly detection serves case study difficulty balancing computational complexity statistical power work develop first principles generalized likelihood ratio test determining well connected region activation vertices graph gaussian noise test computationally infeasible provide relaxation called lovasz extended scan statistic less uses submodularity approximate intractable generalized likelihood ratio demonstrate connection less maximum aposteriori inference markov random fields provides polytime algorithm less using electrical network theory able control type error less prove conditions less risk consistent finally consider specific graph models torus knearest neighbor graphs epsilonrandom graphs show graphs results provide nearoptimal performance matching results known lower bounds\",\"given graph vertices deemed interesting priori vertex nomination task order remaining vertices nomination list concentration interesting vertices top list previous work yielded several approaches problem theoretical results setting graph drawn stochastic block model sbm including vertex nomination analogue bayes optimal classifier paper prove maximum likelihood mlbased vertex nomination consistent sense performance mlbased scheme asymptotically matches bayes optimal scheme prove theorems form model parameters known unknown additionally introduce prove consistency related scalable restrictedfocus vertex nomination scheme finally incorporate vertex edge features mlbased vertex nomination briefly explore empirical effectiveness approach\",\"many graph clustering quality functions suffer resolution limit inability find small clusters large graphs called resolutionlimitfree quality functions limit property previously introduced hard clustering graph partitioning investigate resolutionlimitfree property context nonnegative matrix factorization nmf hard soft graph clustering use nmf hard clustering setting common approach assign node highest membership cluster show case symmetric nmf resolutionlimitfree becomes hardness constraints used part optimization resulting function strongly linked constant potts model soft clustering nodes belong one cluster varying degrees membership setting resolutionlimitfree turns strong property therefore introduce locality roughly states changing one part graph affect clustering parts graph argue desirable property provide conditions nmf quality functions local propose novel class local probabilistic nmf quality functions soft graph clustering\",\"recent years increased interest statistical analysis data multiple types relations among set entities multirelational data represented multilayer graphs set vertices represents entities multiple types edges represent different relations among community detection multilayer graphs consider two random graph models multilayer stochastic blockmodel mlsbm model restricted parameter space restricted multilayer stochastic blockmodel rmlsbm derive consistency results community assignments maximum likelihood estimators mles models mlsbm assumed true model either number nodes number types edges grow compare mles two models baseline approaches separate modeling layers aggregating layers majority voting rmlsbm shown advantage mlsbm either growth rate number communities high growth rate average degree component graphs multigraph low also derive minimax rates error sharp thresholds achieving consistency community detection models used compare multilayer models baseline model aggregate stochastic block model simulation studies real data applications confirm superior performance multilayer approaches comparison baseline procedures\",\"natural approach analyze interaction data form whatconnectstowhatwhen create timeseries rather sequence graphs temporal discretization bandwidth selection spatial discretization vertex contraction discretization together nonnegative factorization techniques useful obtaining clustering graphs motivating application performing clustering graphs opposed vertex clustering found neuroscience social network analysis also used enhance community detection vertex clustering way conditioning cluster labels paper formulate problem clustering graphs model selection problem approach involves information criteria nonnegative matrix factorization singular value thresholding illustrate techniques using real simulated data\",\"mincut clustering based minimizing one two heuristic costfunctions proposed shi malik spawned tremendous research analytic algorithmic graph partitioning image segmentation communities last decade however unclear heuristics derived general principle facilitating generalization new problem settings motivated existing graph partitioning framework derive relationships optimizing relevance information defined information bottleneck method regularized cut kpartitioned graph fast mixing graphs show cost functions introduced shi malik well approximated rate loss predictive information location random walkers graph graphs generated stochastic algorithm designed model community structure optimal information theoretic partition optimal mincut partition shown high probability\",\"latent space model family random graphs assigns realvalued vectors nodes graph edge probabilities determined latent positions latent space models provide natural statistical framework graph visualizing clustering latent space model particular interest random dot product graph rdpg fit using efficient spectral method however method based heuristic fail even simple cases consider closely related latent space model logistic rdpg uses logistic link function map latent positions edge likelihoods model show asymptotically exact maximum likelihood inference latent position vectors achieved using efficient spectral method method involves computing top eigenvectors normalized adjacency matrix scaling eigenvectors using regression step novel regression scaling step essential part proposed method simulations show proposed method accurate robust common practices also show effectiveness approach standard real networks karate club political blogs\",\"network metrics form fundamental part network analysis toolbox used quantitatively measure different aspects network metrics give insights underlying network structure function work connect network metrics modern probabilistic machine learning focus centrality metric used wide variety applications web search geneanalysis first formulate eigenvectorbased bayesian centrality model determining node importance compared existing methods probabilistic model allows assimilation multiple edge weight observations inclusion priors extraction uncertainties enable tractable inference develop variational lower bound vbc demonstrated effective variety networks two synthetic five realworld graphs bridge model sparse gaussian processes sparse variational bayesian centrality gaussian process vbcgp learns mapping node attributes latent centrality hence capable predicting centralities node features potentially represent large number nodes using limited number inducing inputs experiments show vbcgp learns highquality mappings compares favorably twostep baseline full trained node attributes precomputed centralities finally present two casestudies using vbcgp first ascertain relevant features taxi transport network second distribute limited number vaccines mitigate severity viral outbreak\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"0_graph_graphs_network\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"0_graph_graphs_network\"],\"textfont\":{\"size\":12},\"x\":[4.3544536,4.10671,3.98736,4.0557656,3.6198967,4.3451138,4.175763,4.418073,3.5651066,3.809271,3.4197123,4.3446646,3.4824083,3.7517986,4.3377976,3.712312,4.1911416,3.6891005,3.9136071,4.275119,4.106504,3.6375527,4.0325255,3.8863578,3.9237516,3.9305854,3.6597097,4.353791,3.9950495,4.2776237,4.303075,3.6344905,4.002814,4.2163563,4.4220676,3.9713895,3.6047542,3.803883,3.6062376,4.254187,4.319638,4.102828,3.930399,3.7905555,4.428915,4.037177,3.608483,3.9223835,4.3491797,4.2557435,3.5237482,4.3115587,3.4519753,3.5119846,3.9884758,4.284802,3.9820666],\"y\":[5.8518615,6.3884664,6.442881,6.2835617,6.3614936,5.838968,6.1929154,5.9288435,6.3670006,6.201371,6.317106,5.97016,6.288978,6.5309134,6.063469,6.402739,6.090464,6.4411817,6.472351,6.0936255,6.437003,6.5049224,6.0812387,6.3290277,6.113701,6.4727926,6.4597793,6.033717,6.4277935,6.097092,6.226224,6.4441104,6.418812,5.59987,5.9825425,5.9248176,6.434825,6.4919114,6.420054,6.167731,5.6370015,5.8255653,6.463948,6.5327063,5.9631495,6.0773277,6.535606,6.4995284,6.047884,6.2366347,6.3198576,6.023396,6.237802,6.303977,6.4000325,5.667126,6.2208548],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"sparse subspace clustering ssc elegant approach unsupervised segmentation data points cluster located linear subspaces model applies instance motion segmentation restrictions camera model hold ssc requires problems based lnorm solved infer points belong subspace unknown subspaces wellseparated algorithm guaranteed succeed algorithm rests upon assumption points subspace well spread question happens condition violated yet investigated work effect particular distributions subspace analyzed shown ssc fails infer correct labels points subspace fall one cluster\",\"nongaussian component analysis ngca aimed identifying linear subspace projected data follows nongaussian distribution paper propose novel ngca algorithm based logdensity gradient estimation unlike existing methods proposed ngca algorithm identifies linear subspace using eigenvalue decomposition without iterative procedures thus computationally reasonable furthermore theoretical analysis prove identified subspace converges true subspace optimal parametric rate finally practical performance proposed algorithm demonstrated artificial benchmark datasets\",\"paper present unified analysis matrix completion general lowdimensional structural constraints induced norm regularization consider two estimators general problem structured matrix completion provide unified upper bounds sample complexity estimation error analysis relies results generic chaining establish two intermediate results independent interest characterizing size complexity low dimensional subsets high dimensional ambient space certain partial complexity measure encountered analysis matrix completion problems characterized terms well understood complexity measure gaussian widths shown form restricted strong convexity holds matrix completion problems general norm regularization provide several nontrivial examples structures included framework notably recently proposed spectral ksupport norm\",\"consider problem noisy bit matrix completion exact rank constraint true underlying matrix instead observing subset noisy continuousvalued entries matrix observe subset noisy bit binary measurements generated according probabilistic model consider constrained maximum likelihood estimation constraint entrywise infinitynorm exact rank constraint contrast previous work used convex relaxations rank provide upper bound matrix estimation error model compared existing results bound faster convergence rate matrix dimensions fraction revealed bit observations fixed independent matrix dimensions also propose iterative algorithm solving nonconvex optimization certificate global optimality limiting point algorithm based low rank factorization validate method synthetic real data improved performance existing methods\",\"section incorrect removed submissions rewritten version posted future\",\"present unified framework lowrank matrix estimation nonconvex penalties first prove proposed estimator attains faster statistical rate traditional lowrank matrix estimator nuclear norm penalty moreover rigorously show certain condition magnitude nonzero singular values proposed estimator enjoys oracle property exactly recovers true rank matrix besides attaining faster rate far know first work establishes theory lowrank matrix estimation nonconvex penalties confirming advantages nonconvex penalties matrix completion numerical experiments synthetic real world datasets corroborate theory\",\"nonnegative matrix factorization nmf shown identifiable separability assumption columnsor rows input data matrix belong convex cone generated columnsor rows real applications however separability assumption hard satisfy following paper look linear programming based reformulation locate extreme rays convex cone noisy setting furthermore order deal large scale data employ firstorder methods fom mitigate computational complexity primarily results large number constraints show performance algorithm real synthetic data sets\",\"introduce develop novel approach outlier detection based adaptation random subspace learning proposed method handles highdimension lowsample size traditional lowdimensional highsample size datasets essentially avoid computational bottleneck techniques like minimum covariance determinant mcd computing needed determinants associated measures much lower dimensional subspaces theoretical computational development approach reveal computationally efficient regularized methods highdimensional lowsample size often competes favorably existing methods far percentage correct outlier detection concerned\",\"paper propose online algorithm compute matrix factorizations proposed algorithm updates dictionary matrix associated coefficients using single observation time algorithm performs lowrank updates dictionary matrix derive algorithm defining simple objective function minimize whenever observation arrived extend algorithm handling missing data also provide minibatch extension enables compute matrix factorization big datasets demonstrate efficiency algorithm real dataset give comparisons wellknown algorithms stochastic gradient matrix factorization nonnegative matrix factorization nmf\",\"describe ways define calculate lnorm signal subspaces less sensitive outlying data lcalculated subspaces focus computation maximumprojection principal component data matrix containing signal samples dimension conclude general problem formally nphard asymptotically large prove however case engineering interest fixed dimension asymptotically large sample support present optimal algorithm complexity ond generalize multiple lmaxprojection components present explicit optimal subspace calculation algorithm form matrix nuclearnorm evaluations conclude illustrations lsubspace signal processing fields data dimensionality reduction directionofarrival estimation\",\"paper consider low rank matrix estimation using either matrixversion dantzig selector hatalambdad matrixversion lasso estimator hatalambdal consider subgaussian measurements measurements xldotsxninmathbbrmtimes iid subgaussian entries suppose textrmrankar proved ngeq cmrvee rlogmlogn hatalambdad hatalambdal obtain optimal upper boundsexcept logarithmic terms estimation accuracy spectral norm applying metric entropy grassmann manifolds construct near matching minimax lower bound estimation accuracy spectral norm also give upper bounds matching minimax lower boundexcept logarithmic terms estimation accuracy schattenq norm every leq qleqinfty direct corollary show upper bounds minimax lower bounds estimation accuracy kyfank norms every leq kleq\",\"paper considers emphvolume minimization volminbased structured matrix factorization smf volmin factorization criterion decomposes given data matrix basis matrix times structured coefficient matrix via finding minimumvolume simplex encloses columns data matrix recent work showed volmin guarantees identifiability factor matrices mild conditions realistic wide variety applications paper focuses theoretical practical aspects volmin theory side exact equivalence two independently developed sufficient conditions volmin identifiability proven thereby providing comprehensive understanding aspect volmin algorithm side computational complexity sensitivity outliers two key challenges associated realworld applications volmin addressed via new volmin algorithm handles volume regularization computationally simple way automatically detects iteratively downweights outliers simultaneously simulations realdata experiments using remotely sensed hyperspectral image reuters document corpus employed showcase effectiveness proposed algorithm\",\"paper considers problem robust subspace recovery given set points mathbbrd many lie ddimensional subspace recover underlying subspace show tylers mestimator used recover underlying subspace percentage inliers larger data points lie general position empirically tylers mestimator compares favorably convex subspace recovery algorithms simulations experiments real data sets\",\"many applications data analysis rely decomposition data matrix lowrank sparse component existing methods tackle task use nuclear norm lcost functions convex relaxations rank constraint sparsity measure respectively employ thresholding techniques propose method allows reconstructing tracking subspace upperbounded dimension incomplete corrupted observations require priori information number outliers core algorithm intrinsic conjugate gradient method set orthogonal projection matrices socalled grassmannian nonconvex sparsity measures used outlier detection leads improved performance terms robustly recovering tracking lowrank matrix particular approach cope outliers underlying matrix higher rank stateoftheart methods\",\"present numerical algorithm nonnegative matrix factorization nmf problems noisy separability nmf problem separability stated one finding vertices convex hull data points research interest paper find vectors close vertices possible situation noise added data points algorithm designed capture shape convex hull data points using enclosing ellipsoid show algorithm correctness robustness properties theoretical practical perspectives correctness means data points contain noise algorithm find vertices convex hull robustness means data points contain noise algorithm find nearvertices finally apply algorithm document clustering report experimental results\",\"matrix factorization become common approach collaborative filtering due ease implementation scalability large data sets two existing drawbacks basic model incorporate side information either users items assumes common variance users extend work constrained probabilistic matrix factorization deriving gibbs updates side feature vectors items salakhutdinov minh show bayesian treatment constrained pmf model outperforms simple map estimation also consider extensions heteroskedastic precision introduced literature lakshminarayanan bouchard archambeau show tends result overfitting deterministic approximation algorithms variational inference observed entries user item matrix distributed nonuniform manner light propose truncated precision model experimental results suggest model tends delay overfitting\",\"paper presents novel algorithms exploit intrinsic algebraic combinatorial structure matrix completion task estimating missing tries general low rank setting positive data achieve results performing state art nuclear norm accuracy computational efficiency simulations task predicting athletic performance partially observed data\",\"nonnegative matrix factorization nmf aims factorize matrix two optimized nonnegative matrices appropriate intended applications method widely used unsupervised learning tasks including recommender systems rating matrix users items document clustering weighting matrix papers keywords however traditional nmf methods typically assume number latent factors dimensionality loading matrices fixed assumption makes inflexible many applications paper propose nonparametric nmf framework mitigate issue using dependent indian buffet processes dibp nutshell apply correlation function generation two stick weights associated pair columns loading matrices still maintaining respective marginal distribution specified ibp consequence generation two loading matrices columnwise indirectly correlated framework two classes correlation function proposed using bivariate beta distribution using copula function methods allow adopt work various applications flexibly choosing appropriate parameter settings compared stateofthe art approaches area using gaussian process gpbased dibp work seen much flexible terms allowing two corresponding binary matrix columns greater variations nonzero entries experiments realworld synthetic datasets show three proposed models perform well document clustering task comparing standard nmf without predefining dimension factor matrices bivariate beta distributionbased copulabased models better flexibility gpbased model\",\"study low rank matrix tensor completion propose novel algorithms employ adaptive sampling schemes obtain strong performance guarantees algorithms exploit adaptivity identify entries highly informative learning column space matrix tensor consequently results hold even row space highly coherent contrast previous analyses absence noise show one exactly recover times matrix rank merely omegan rlogr matrix entries also show one recover order tensor using omegan rtt logr entries noisy recovery algorithm consistently estimates low rank matrix corrupted noise using omegan textrmpolylogn entries complement study simulations verify theory demonstrate scalability algorithms\",\"main goal paper propose novel method perform matrix completion online motivated wide variety applications ranging design recommender systems sensor network localization seismic data reconstruction consider matrix completion problem entries matrix interest observed gradually precisely place situation predictive rule refined incrementally rather recomputed scratch time sample observed entries increases extension existing matrix completion methods sequential prediction context indeed major issue big data era yet little addressed literature algorithm promoted article builds upon soft impute approach introduced mazumder major novelty essentially arises use randomised technique computing updating singular value decomposition svd involved algorithm though disarming simplicity method proposed turns efficient requiring reduced computations several numerical experiments based real datasets illustrating performance displayed together preliminary results giving theoretical basis\",\"text investigates relations two wellknown family algorithms matrix factorisations recursive linear filters describing probabilistic model approximate inference corresponds matrix factorisation algorithm using probabilistic model derive matrix factorisation algorithm recursive linear filter precisely derive matrixvariate recursive linear filter order perform efficient inference high dimensions also show possible interpret algorithm nontrivial stochastic gradient algorithm demonstrations comparisons image restoration task given\",\"highdimensional data often lie lowdimensional subspaces corresponding different classes belong finding sparse representations data points dictionary built using collection data helps uncover lowdimensional subspaces address problems clustering classification subset selection paper address problem recovering sparse representations noisy data points dictionary whose columns correspond corrupted data lying close union subspaces consider constrained ellminimization study conditions solution proposed optimization satisfies approximate subspacesparse recovery condition specifically show noisy data point perturbed subspace noise magnitude varepsilon reconstructed using data points subspace small error order ovarepsilon coefficients corresponding data points subspaces sufficiently small order ovarepsilon impose randomness assumption arrangement subspaces distribution data points subspace framework based novel generalization nullspace property setting data lie multiple subspaces number data points subspace exceeds dimension subspace data points corrupted noise moreover assuming random distribution data points show coefficients desired support reconstruct given point high accuracy also sufficiently large values order\",\"paper considers problem matrix completion observed entries noisy contain outliers begins introducing new optimization criterion recovered matrix defined solution criterion uses celebrated huber function robust statistics literature downweigh effects outliers practical algorithm developed solve optimization involved algorithm fast straightforward implement monotonic convergent furthermore proposed methodology theoretically shown stable well defined sense promising empirical performance demonstrated via sequence simulation experiments including image inpainting\",\"motivated largescale collaborativefiltering applications present noncommuting latent factor nclf tensorcompletion approach modeling threeway arrays diagonal like standard parafac wherein different terms distinguish different kinds threeway relations coclusters determined permutations latent factors first key component algebraic representation usage two noncommutative real trilinear operations building blocks approximation operations standard three dimensional tripleproduct trilinear product twodimensional real vector space representation real clifford algebra certain majorana spinor operations purely ternary cannot decomposed two groupoperations relevant spaces second key component method combining operations using permutationsymmetry preserving linear combinations apply model movielens fannie mae datasets find outperforms parafac model propose future directions unsupervisedlearning\",\"consider factoring lowrank tensors presence outlying slabs problem important practice data collected many realworld applications speech fluorescence social network data fit paradigm prior work tackles problem iteratively selecting fixed number slabs fitting procedure may converge formulate problem groupsparsity promoting point view propose alternating optimization framework handle corresponding ellp pleq minimizationbased lowrank tensor factorization problem proposed algorithm features similar periteration complexity plain trilinear alternating least squares tals algorithm convergence proposed algorithm also easy analyze framework alternating optimization variants addition regularization constraints easily incorporated make use empha priori information latent loading factors simulations real data experiments blind speech separation fluorescence data analysis social network mining used showcase effectiveness proposed algorithm\",\"robust tensor recovery plays instrumental role robustifying tensor decompositions multilinear data analysis outliers gross corruptions missing values diverse array applications paper study problem robust lowrank tensor recovery convex optimization framework drawing upon recent advances robust principal component analysis tensor completion propose tailored optimization algorithms global convergence guarantees solving constrained lagrangian formulations problem algorithms based highly efficient alternating direction augmented lagrangian accelerated proximal gradient methods also propose nonconvex model often improve recovery results convex models investigate empirical recoverability properties convex nonconvex formulations compare computational performance algorithms simulated data demonstrate number real applications practical effectiveness convex optimization framework robust lowrank tensor recovery\",\"present technique significantly speeding alternating least squares als gradient descent two widely used algorithms tensor factorization exploiting properties khatrirao product show efficiently address computationally challenging substep algorithms algorithm dfacto requires two sparse matrixvector products easy parallelize dfacto scalable also average times faster competing algorithms variety datasets instance dfacto takes seconds machines perform one iteration als algorithm seconds perform one iteration algorithm million million million dimensional tensor billion nonzero entries\",\"nonnegative matrix factorization nmf common tool audio source separation learning nmf large audio databases one major drawback complexity time ofkn updating dictionary dimension input power spectrograms number basis spectra thus forbidding application signals longer hour provide online algorithm complexity ofk time memory updates dictionary show audio simulations online approach faster short audio signals allows analyze audio signals several hours\",\"due challenging applications collaborative filtering matrix completion problem widely studied past years different approaches rely different structure assumptions matrix hand focus completion possibly lowrank matrix binary entries socalled bit matrix completion problem approach relies tools machine learning theory empirical risk minimization convex relaxations propose algorithm compute variational approximation pseudoposterior thanks convex relaxation corresponding minimization problem biconvex thus method behaves well practice also study performance variational approximation pacbayesian learning bounds contrary previous works focused upper bounds estimation error various matrix norms able derive analysis pac bound prediction error algorithm focus essentially convex relaxation hinge loss present complete analysis complete simulation study test movielens data set however also discuss variational approximation deal logistic loss\",\"propose unified framework estimating lowrank matrices nonconvex optimization based gradient descent algorithm framework quite general applied noisy noiseless observations general case noisy observations show algorithm guaranteed linearly converge unknown lowrank matrix minimax optimal statistical error provided appropriate initial estimator generic noiseless setting algorithm converges unknown lowrank matrix linear rate enables exact recovery optimal sample complexity addition develop new initialization algorithm provide desired initial estimator outperforms existing initialization algorithms nonconvex lowrank matrix estimation illustrate superiority framework three examples matrix regression matrix completion onebit matrix completion also corroborate theory extensive experiments synthetic data\",\"propose general framework reducedrank modeling matrixvalued data applying generalized nuclear norm penalty directly model lowdimensional latent variables associated rows columns framework flexibly incorporates row column features smoothing kernels sources side information penalizing deviations row column models moreover large class models estimated scalably using convex optimization computational bottleneck case one singular value decomposition per iteration large easytoapply matrix framework generalizes traditional convex matrix completion multitask learning methods well maximum posteriori estimation large class popular hierarchical bayesian models\",\"simple interpretation matrix completion problem introduced based statistical models combined wellknown results missing data analysis interpretation indicates matrix completion still valid principled estimation procedure even without missing completely random mcar assumption almost current theoretical studies matrix completion assume\",\"study problem lowrank tensor factorization presence missing data ask following question many sampled entries need efficiently exactly reconstruct tensor lowrank orthogonal decomposition propose novel alternating minimization based method iteratively refines estimates singular vectors show certain standard assumptions method recover threemode ntimes ntimes dimensional rankr tensor exactly log randomly sampled entries process proving result solve two challenging subproblems tensors missing data first process analyzing initialization step prove generalization celebrated result szemeredie spectrum random graphs next prove global convergence alternating minimization good initialization simulations suggest dependence sample size dimensionality indeed tight\",\"suppose two large multidimensional data sets noisy measurements underlying random process principle components analysis performed separately data sets reduce dimensionality circumstances may happen two lowerdimensional data sets inordinately large procrustean fittingerror purpose manuscript quantify incommensurability phenomenon particular specified conditions square procrustean fittingerror two normalized lowerdimensional data sets asymptotically convex combination via correlation parameter hausdorff distance projection subspaces maximum possible value square procrustean fittingerror normalized data show gives rise incommensurability phenomenon employ illustrative simulations well real data experiment explore incommensurability phenomenon may appreciable impact\",\"collaborative convex framework factoring data matrix nonnegative product sparse coefficient matrix proposed restrict columns dictionary matrix coincide certain columns data matrix thereby guaranteeing physically meaningful dictionary dimensionality reduction use linfty regularization select dictionary data show leads exact convex relaxation case distinct noise free data also show relax restrictiontox constraint initializing alternating minimization approach solution convex model obtaining dictionary close necessarily focus applications proposed framework hyperspectral endmember abundances identification also show application blind source separation nmr data\",\"recent years structured matrix recovery problems gained considerable attention real world applications recommender systems computer vision much existing work focused matrices lowrank structure limited progress made matrices types structure paper present nonasymptotic analysis estimation generally structured matrices via generalized dantzig selector generic subgaussian measurements show estimation error always succinctly expressed terms geometric measures suitable sets depend structure underlying true matrix addition derive general bounds geometric measures structures characterized unitarily invariant norms large family covering matrix norms practical interest examples provided illustrate utility theoretical development\",\"problem lowrank matrix estimation recently received lot attention due challenging applications lot work done rankpenalized methods convex relaxation theoretical applied sides however papers considered bayesian estimation paper review different type priors considered matrices favour lowrank also prove obtained bayesian estimators suitable assumptions enjoys optimality properties ones based penalization\",\"sparse representation classifier src utilized various classification problems makes use minimization works well image recognition satisfying subspace assumption paper propose new implementation src via screening establish equivalence original src regularity conditions prove classification consistency latent subspace model contamination results demonstrated via simulations real data experiments new algorithm achieves comparable numerical performance significantly faster\",\"given set pairwise comparisons classical ranking problem computes single ranking best represents preferences users paper study problem inferring individual preferences arising context making personalized recommendations particular assume users types users type provide similar pairwise comparisons items according bradleyterry model propose efficient algorithm accurately estimates individual preferences almost users max nlog log pairwise comparisons per type near optimal sample complexity grows logarithmically algorithm three steps first user compute emphnetwin vector projection binommdimensional vector pairwise comparisons onto mdimensional linear subspace second cluster users based netwin vectors third estimate single preference cluster separately netwin vectors much less noisy high dimensional vectors pairwise comparisons clustering accurate projection confirmed numerical experiments moreover show cluster approximately correct maximum likelihood estimation bradleyterry model still close true preference\",\"assume data independently sampled mixture distribution unit ball ddimensional euclidean space components first component uniform distribution ball representing outliers components uniform distributions along ddimensional linear subspaces restricted ball study simultaneous recovery underlying subspaces recovery best subspace largest number points minimizing lpaveraged distances data points ddimensional subspaces ddimensional space unlike minimization problems minimization nonconvex thus requires different methods analysis show underlying subspaces best subspace precisely recovered minimization overwhelming probability result extends additive homoscedastic uniform noise around subspaces uniform distribution strip around near recovery error proportional noise level hand show underlying subspaces best subspace cannot recovered even nearly recovered relaxations also discussed use results paper partially justifying recent effective algorithms modeling data mixtures multiple subspaces well discussing effect using variants minimizations ransactype strategies single subspace recovery\",\"due space limitations submission source separation clustering phaselocked subspaces accepted publication ieee transactions neural networks presented results without proof proofs provided paper\",\"motivated generating personalized recommendations using ordinal preference data study question learning mixture multinomial logit mnl model parameterized class distributions permutations partial ordinal preference data pairwise comparisons despite long standing importance across disciplines including social choice operations research revenue management little known question case single mnl models mixture computationally statistically tractable learning pairwise comparisons feasible however even learning mixture two mnl components infeasible general given state affairs seek conditions feasible learn mixture model computationally statistically efficient manner present sufficient condition well efficient algorithm learning mixed mnl models partial preferencescomparisons data particular mixture mnl components objects learnt using samples whose size scales polynomially concretely rnlog rll model parameters sufficiently incoherent algorithm two phases first learn pairwise marginals component using tensor decomposition second learn model parameters component using rank centrality introduced negahban process proving results obtain generalization existing analysis tensor decomposition realistic regime partial information sample available\",\"extracting underlying lowdimensional space highdimensional signals often reside long center numerous algorithms signal processing machine learning literature past decades time working incomplete partly observed large scale datasets recently commonplace diverse reasons called big data era currently living calls devising online subspace learning algorithms suitably handle incomplete data envisaged objective recursively estimate unknown subspace processing streaming data sequentially thus reducing computational complexity obviating need storing whole dataset memory paper online variational bayes subspace learning algorithm partial observations presented account unawareness true rank subspace commonly met practice lowrankness explicitly imposed sought subspace data matrix exploiting sparse bayesian learning principles moreover sparsity simultaneously lowrankness favored subspace matrix sophisticated hierarchical bayesian scheme adopted proposed algorithm becomes adept dealing applications whereby underlying subspace may also sparse sparse dictionary learning problems shown new subspace tracking scheme outperforms stateoftheart counterparts terms estimation accuracy variety experiments conducted simulated real data\",\"past years robust pca established standard tool reliable lowrank approximation matrices presence outliers recently robust pca approach via nuclear norm minimization extended matrices linear structures appear applications system identification data series analysis time shown control rank structured approximation via matrix factorization approaches drawbacks methods either lie lack robustness outliers static nature repeated batchprocessing present robust structured lowrank approximation method grassmannian one hand allows fast reinitialization online setting due subspace identification manifolds robust outliers due smooth approximation ellpnorm cost function hand method evaluated online time series forecasting tasks simulated realworld data\",\"motivated electricity consumption metering extend existing nonnegative matrix factorization nmf algorithms use linear measurements observations instead matrix entries objective estimate multiple time series fine temporal scale temporal aggregates measured individual series furthermore algorithm extended take account individual autocorrelation provide better estimation using recent convex relaxation quadratically constrained quadratic program extensive experiments synthetic realworld electricity consumption datasets illustrate effectiveness matrix recovery algorithms\",\"density matrices positively semidefinite hermitian matrices unit trace describe state quantum system goal paper develop minimax lower bounds error rates estimation low rank density matrices trace regression models used quantum state tomography particular case pauli measurements explicit dependence bounds rank complexity parameters bounds established several statistically relevant distances including quantum versions kullbackleibler divergence relative entropy distance hellinger distance called bures distance schatten pnorm distances sharp upper bounds oracle inequalities least squares estimator von neumann entropy penalization obtained showing minimax lower bounds attained logarithmic factors distances\",\"nonorthogonal joint diagonalization njd free prewhitening widely studied context blind source separation bss array signal processing etc however njd used retrieve jointly diagonalizable structure single set target matrices mostly formulized single dataset thus insufficient handle multiple datasets interset dependences scenario often encountered joint bss jbss applications present generalized njd gnjd algorithm simultaneously perform asymmetric njd upon multiple sets target matrices mutually linked loading matrices using decomposition successive rotations enable jbss multiple datasets indicationexploitation mutual dependences experiments synthetic realworld datasets provided illustrate performance proposed algorithm\",\"lowrank representationlrr significant method segmenting data generated union subspaces however known solving lrr program challenging terms time complexity memory footprint size nuclear norm regularized matrix nbyn number samples paper thereby develop fast online implementation lrr reduces memory cost opd ambient dimension estimated rankd crux end nonconvex reformulation lrr program pursues basis dictionary generates uncorrupted observations build theoretical guarantee sequence solutions produced algorithm converges stationary point empirical expected loss function asymptotically extensive experiments synthetic realistic datasets substantiate algorithm fast robust memory efficient\",\"density matrices positively semidefinite hermitian matrices unit trace describe states quantum systems many quantum systems physical interest represented highdimensional low rank density matrices popular problem quantum state tomography qst estimate unknown low rank density matrix quantum system conducting pauli measurements main contribution twofold first establish minimax lower bounds schatten pnorms leq pleq infty low rank density matrices estimation pauli measurements previous paper minimax lower bounds proved trace regression model gaussian noise noise assumed common variance paper prove bounds binomial observation model meets actual model qst second study dantzig estimator estimating unknown low rank density matrix binomial observation model using pauli measurements previous papers studied least squares estimator projection estimator proved optimal convergence rates least squares estimator schatten pnorms leq pleq stronger condition optimal convergence rates projection estimator schatten pnorms leq pleq infty paper show results two distinct estimators simultaneously obtained dantzig estimator moreover better convergence rates schatten norm distances proved dantzig estimator conditions weaker needed previous papers objective function replaced negative von neumann entropy obtain sharp convergence rate kullbackleibler divergence\",\"new geometricallymotivated algorithm nonnegative matrix factorization developed applied discovery latent topics text image document corpora algorithm based robustly finding clustering extreme points empirical crossdocument wordfrequencies correspond novel words unique topic contrast related approaches based solving nonconvex optimization problems using suboptimal approximations locallyoptimal methods heuristics new algorithm convex polynomial complexity competitive qualitative quantitative performance compared current stateoftheart approaches synthetic realworld datasets\",\"paper considers problem subspace clustering noise specifically study behavior sparse subspace clustering ssc either adversarial random noise added unlabelled input data points assumed union lowdimensional subspaces show modified version ssc emphprovably effective correctly identifying underlying subspaces even noisy data extends theoretical guarantee algorithm practical settings provides justification success ssc class real applications\",\"recent years rank aggregation received significant attention machine learning community goal problem combine partially revealed preferences objects large population single relatively consistent ordering objects however many cases might want single ranking instead opt individual rankings study version problem known collaborative ranking problem assume individual users provide pairwise preferences example purchasing one item another preferences wish obtain rankings items users opportunity explore results interesting connection standard matrix completion problem provide theoretical justification nuclear norm regularized optimization procedure provide highdimensional scaling results show error estimating user preferences behaves number observations increase\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"1_matrix_subspace_algorithm\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"1_matrix_subspace_algorithm\"],\"textfont\":{\"size\":12},\"x\":[1.1990924,1.3537556,0.3017358,0.3441437,1.1613292,0.47719362,0.75188076,0.9484063,0.6558581,1.2677245,0.2135,0.84813285,1.144072,0.70441836,0.78453225,0.48210913,0.32622826,0.78263175,0.3776941,0.36930513,0.45500985,1.2151716,0.37512258,0.6329347,0.7283071,0.3867253,0.5155967,0.7601985,0.28950801,0.46320632,0.5383296,0.3314594,0.49020433,1.327925,0.6708433,0.26815432,0.46974474,1.129469,0.24528047,1.1579092,1.1948866,0.25453216,0.91711116,0.57280695,0.7056152,0.17809053,0.9017248,0.83912146,0.19638377,0.905181,1.1674929,0.27663314,0.67412364],\"y\":[5.6196084,5.2143507,5.5329123,5.6003666,5.6294813,5.6437297,5.1358104,5.437464,5.362072,5.288683,5.4086943,5.4198093,5.5868363,5.6261435,5.407988,5.563298,5.690628,5.321069,5.6407733,5.614438,5.4680605,5.537818,5.6240845,5.3924847,5.359116,5.5276136,5.2637157,5.256405,5.6887627,5.5897098,5.674037,5.6662383,5.528997,5.472042,5.145502,5.533936,5.656035,5.5682187,5.9106994,5.541431,5.6473536,5.9295354,5.538536,5.7075157,5.329144,5.401152,5.060196,5.641083,5.4118204,5.5817018,5.619608,5.893885,5.5175114],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"paper aims achieving good estimator gradient function highdimensional space often functions sensitive coordinates gradient function almost sparse propose method gradient estimation combines ideas spalls simultaneous perturbation stochastic approximation compressive sensing aim obtain good estimator without many function evaluations application estimating gradient outer product matrix well standard optimization problems illustrated via simulations\",\"recently focus penalized loglikelihood covariance estimation sparse inverse covariance precision matrices penalty responsible inducing sparsity common choice convex norm however best estimator performance always achieved penalty natural sparsity promoting norm nonconvex penalty lack convexity deterred use sparse maximum likelihood estimation paper consider nonconvex penalized loglikelihood inverse covariance estimation present novel cyclic descent algorithm optimization convergence local minimizer proved highly nontrivial demonstrate via simulations reduced bias superior quality penalty compared penalty\",\"paper study problem recovering group sparse vector small number linear measurements past common approach use various group sparsityinducing norms group lasso norm purpose using theory convex relaxations show also possible use ellnorm minimization group sparse recovery introduce new concept called group robust null space property grnsp show suitable conditions group version restricted isometry property grip implies grnsp thus leads group sparse recovery groups equal size bounds less conservative known bounds moreover results apply even situations groups different sizes specialized conventional sparsity bounds reduce one wellknown best possible conditions sparse recovery relationship grnsp grip new even conventional sparsity substantially streamlines proofs known results using relationship derive bounds ellpnorm residual error vector measurement matrix consists random samples subgaussian random variable present bounds number measurements less conservative currently known bounds\",\"given iid observations random vector highdimensional vector lowdimensional index variable study problem estimating conditional inverse covariance matrix omegaz exex mid zxex mid mid assumption set nonzero elements small depend index variable develop novel procedure combines ideas local constant smoothing group lasso estimating conditional inverse covariance matrix proximal iterative smoothing algorithm used solve corresponding convex optimization problems prove procedure recovers conditional independence assumptions distribution mid high probability result established developing uniform deviation bound highdimensional conditional covariance matrix population counterpart may independent interest furthermore develop pointwise confidence intervals individual elements conditional inverse covariance matrix perform extensive simulation studies demonstrate proposed procedure outperforms sensible competitors illustrate proposal stock price data set\",\"consider convexconcave saddle point problems separable structure nonstrongly convex functions propose efficient stochastic block coordinate descent method using adaptive primaldual updates enables flexible parallel optimization largescale problems method shares efficiency flexibility block coordinate descent methods simplicity primaldual methods utilizing structure separable convexconcave saddle point problem capable solving wide range machine learning applications including robust principal component analysis lasso feature selection group lasso etc theoretically empirically demonstrate significantly better performance stateoftheart methods applications\",\"recent studies literature paid much attention sparsity linear classification tasks one motivation imposing sparsity assumption linear discriminant direction rule noninformative features making hardly contribution classification problem work focused scenarios binary classification presence multiclass data preceding researches recommended individually pairwise sparse linear discriminant analysislda however sparsity explored paper estimator grouped lasso type proposed take advantage sparsity multiclass data enjoys appealing nonasymptotic properties allows insignificant correlations among features estimator exhibits superior capability simulated real data\",\"careful tuning regularization parameter indispensable many machine learning tasks significant impact generalization performances nevertheless current practice regularization parameter tuning art science hard tell many gridpoints would needed crossvalidation obtaining solution sufficiently small error paper propose novel framework computing lower bound errors function regularization parameter call regularization path error lower bounds proposed framework used providing theoretical approximation guarantee set solutions sense far error current best solution could away best possible error entire range regularization parameters demonstrate numerical experiments theoretically guaranteed choice regularization parameter sense possible reasonable computational costs\",\"article considers problem multigroup classification setting number variables larger number observations several methods proposed literature address problem however variable selection performance either unknown suboptimal results known twogroup case work provide sharp conditions consistent recovery relevant variables multigroup case using discriminant analysis proposal gaynanova achieve rates convergence attain optimal scaling sample size number variables sparsity level rates significantly faster best known results multigroup case moreover coincide optimal minimax rates twogroup case validate theoretical results numerical analysis\",\"introduce application group lasso design experiments note trying explain experimental design group lasso conversely explain use idea group lasso experimental design showing problem constructing optimal design matrix transformed problem group lasso numerical examples show obtain orthogonal arrays solutions group lasso problems\",\"compute approximate solutions regularized linear regression using regularization also known lasso initialization step algorithm lass lasszero uses computationally efficient stepwise search determine locally optimal solution given regularization solution present theoretical results consistency orthogonality appropriate handling redundant features empirically use synthetic data demonstrate lass solutions closer true sparse support regularization models additionally realworld data lass finds parsimonious solutions regularization maintaining similar predictive accuracy\",\"recent paper shown lasso algorithm exhibits nearideal behavior following sense suppose eta satisfies restricted isometry property rip sufficiently small constant vert eta vert leq epsilon minimizing vert vert subject vert vert leq epsilon leads estimate hatx whose error vert hatx vert bounded universal constant times error achieved oracle knows location nonzero components world optimization lasso algorithm generalized several directions group lasso sparse group lasso either without treestructured overlapping groups recently sorted lasso paper shown algorithm exhibits nearideal behavior sense provided norm used define sparsity index decomposable penalty norm minimized effort enforce sparsity gammadecomposable iii compressibility condition terms group restricted isometry property satisfied specifically group lasso sparse group lasso permissible overlap groups well sorted ellnorm minimization exhibit nearideal behavior explicit bounds residual error derived contain previously known results special cases\",\"propose nonconvex estimator joint multivariate regression precision matrix estimation high dimensional regime sparsity constraints gradient descent algorithm hard thresholding developed solve nonconvex estimator attains linear rate convergence true regression coefficients precision matrix simultaneously statistical error compared existing methods along line research little theoretical guarantee proposed algorithm computationally much efficient provable convergence guarantee also attains optimal finite sample statistical rate logarithmic factor thorough experiments synthetic real datasets back theory\",\"number recent work studied effectiveness feature selection using lasso known restricted isometry properties rip lasso generally lead exact recovery set nonzero coefficients due looseness convex relaxation paper considers feature selection property nonconvex regularization solution given multistage convex relaxation scheme appropriate conditions show local solution obtained procedure recovers set nonzero coefficients without suffering bias lasso relaxation complements parameter estimation results procedure\",\"recently sparsitybased algorithms proposed superresolution spectrum estimation however achieve adequately high resolution realworld signal analysis dictionary atoms close frequency thereby resulting coherent design popular convex compressed sensing methods break presence high coherence large noise propose new regularization approach handle model collinearity obtain parsimonious frequency selection simultaneously takes advantage pairing structure sine cosine atoms frequency dictionary probabilistic spectrum screening also developed fast computation high dimensions dataresampling version highdimensional bayesian information criterion used determine regularization parameters experiments show efficacy efficiency proposed algorithms challenging situations small sample size high frequency resolution low signaltonoise ratio\",\"dictionaryaided sparse regression approach recently emerged promising alternative hyperspectral unmixing remote sensing using available spectral library dictionary approach identifies underlying materials given hyperspectral image selecting small subset spectral samples dictionary represent whole image drawback current developments actual spectral signature scene often assumed zero mismatch corresponding dictionary sample assumption considered ideal practice paper tackle spectral signature mismatch problem proposing dictionaryadjusted nonconvex sparsityencouraging regression danser framework main idea incorporate dictionary correcting variables formulation simple low periteration complexity algorithm tailordesigned practical realization danser using dictionary correcting idea also propose robust subspace solution dictionary pruning extensive simulations realdata experiments show proposed method effective mitigating undesirable spectral signature mismatch effects\",\"propose new stochastic dual coordinate ascent technique applied wide range regularized learning problems method based alternating direction multiplier method admm deal complex regularization functions structured regularizations although original admm batch method proposed method offers stochastic update rule iteration requires one sample observations moreover method naturally afford minibatch update gives speed convergence show mild assumptions method converges exponentially numerical experiments show method actually performs efficiently\",\"study problem learning sparse linear regression vector additional conditions structure sparsity pattern problem relevant machine learning statistics signal processing well known linear regression benefit knowledge underlying regression vector sparse combinatorial problem selecting nonzero components vector relaxed regularizing squared error convex penalty function like ell norm however many applications additional conditions structure regression vector sparsity pattern available incorporating information learning method may lead significant decrease estimation error paper present family convex penalty functions encode prior knowledge structure vector formed absolute values regression coefficients family subsumes ell norm flexible enough include different models sparsity patterns practical theoretical importance establish basic properties penalty functions discuss examples computed explicitly moreover present convergent optimization algorithm solving regularized least squares penalty functions numerical simulations highlight benefit structured sparsity advantage offered approach lasso method related methods\",\"modeling data linear combinations elements learned dictionary focus much recent research machine learning neuroscience signal processing signals natural images admit sparse representations well established models well suited restoration tasks context learning dictionary amounts solving largescale matrix factorization problem done efficiently classical optimization tools approach also used learning features data purposes image classification tuning dictionary supervised way tasks proven difficult paper present general formulation supervised dictionary learning adapted wide variety tasks present efficient algorithm solving corresponding optimization problem experiments handwritten digit classification digital art identification nonlinear inverse image problems compressed sensing demonstrate approach effective largescale settings well suited supervised semisupervised classification well regression tasks data admit sparse representations\",\"propose scalable efficient statistically motivated computational framework graphical lasso friedman covariance regularization framework received significant attention statistics community past years existing algorithms trouble scaling dimensions larger thousand proposal significantly enhances stateoftheart moderate sized problems gracefully scales larger problems algorithms become practically infeasible requires key new ideas operate primal problem use subtle variation blockcoordinatemethods drastically reduces computational complexity orders magnitude provide rigorous theoretical guarantees convergence complexity algorithm demonstrate effectiveness proposal via experiments believe framework extends applicability graphical lasso largescale modern applications like bioinformatics collaborative filtering social networks among others\",\"consider problem robustifying highdimensional structured estimation robust techniques key realworld applications often involve outliers data corruption focus trimmed versions structurally regularized mestimators highdimensional setting including popular least trimmed squares estimator well analogous estimators generalized linear models graphical models using possibly nonconvex loss functions present general analysis statistical convergence rates consistency take closer look trimmed versions lasso graphical lasso estimators special cases optimization side show extend algorithms mestimators fit trimmed variants provide guarantees numerical convergence generality competitive performance highdimensional trimmed estimators illustrated numerically simulated realworld genomics data\",\"lasso computationally efficient regression regularization procedure produce sparse estimators number predictors large oracle inequalities provide probability loss bounds lasso estimator deterministic choice regularization parameter bounds tend zero appropriately controlled thus commonly cited theoretical justification lasso ability handle highdimensional settings unfortunately practice regularization parameter selected deterministic quantity instead chosen using random datadependent procedure address shortcoming previous theoretical work study loss lasso estimator tuned optimally prediction assuming orthonormal predictors sparse true model prove probability best possible predictive performance lasso deteriorates increases positive arbitrarily close one given sufficiently high signal noise ratio sufficiently large demonstrate empirically amount deterioration performance far worse oracle inequalities suggest provide real data example deterioration observed\",\"study problem demixing pair sparse signals noisy nonlinear observations superposition mathematically consider nonlinear signal observation model gaitx ildotsm phi wpsi denotes superposition signal phi psi orthonormal bases mathbbrn zinmathbbrn sparse coefficient vectors constituent signals represents noise moreover represents nonlinear link function aiinmathbbrn ith row measurement matrix ainmathbbrmtimes problems nature arise several applications ranging astronomy computer vision machine learning paper make concrete algorithmic progress demixing problem specifically consider two scenarios case demixing procedure knowledge link function case demixing algorithm perfect knowledge link function cases provide fast algorithms recovery constituents observations moreover support algorithms rigorous theoretical analysis derive nearly tight upper bounds sample complexity proposed algorithms achieving stable recovery component signals also provide range numerical simulations illustrate performance proposed algorithms real synthetic signals images\",\"paper develops general theoretical framework analyze structured sparse recovery problems using notation dual certificate although certain aspects dual certificate idea already used previous work due lack general coherent theory analysis far carried limited scopes specific problems context current paper makes two contributions first introduce general definition dual certificate use develop unified theory sparse recovery analysis convex programming second present class structured sparsity regularization called structured lasso calculations readily performed theoretical framework new theory includes many seemingly loosely related previous work special cases also implies new results improve existing ones even standard formulations regularization\",\"present alternating augmented lagrangian method convex optimization problems cost function sum two terms one separable variable blocks second separable difference consecutive variable blocks examples problems include fused lasso estimation total variation denoising multiperiod portfolio optimization transaction costs iteration method first step involves separately optimizing variable block carried parallel second step separable variables carried efficiently apply algorithm segmentation data based changes inmean mean filtering changes variance variance filtering numerical example show implementation around times faster compared generic optimization solver sdpt\",\"describe simple efficient permutation based procedure selecting penalty parameter lasso procedure intended applications variable selection primary focus applied variety structural settings including generalized linear models briefly discuss connections permutation selection existing theory lasso addition present simulation study analysis three real data sets permutation selection compared crossvalidation bayesian information criterion bic selection method based recently developed testing procedures lasso\",\"present sparse estimation dictionary learning framework compressed fiber sensing based probabilistic hierarchical sparse model handle severe dictionary coherence selective shrinkage achieved using weibull prior related nonconvex optimization pnorm constraints addition leverage specific dictionary structure promote collective shrinkage based local similarity model incorporated form kernel function joint prior density sparse coefficients thereby establishing markov random fieldrelation approximate inference accomplished using hybrid technique combines hamilton monte carlo gibbs sampling estimate dictionary parameter pursue two strategies relying either deterministic probabilistic model dictionary parameter first strategy parameter estimated based alternating estimation second strategy jointly estimated along sparse coefficients performance evaluated comparison existing method various scenarios using simulations experimental data\",\"sparse coding consists representing signals sparse linear combinations atoms selected dictionary consider extension framework atoms assumed embedded tree achieved using recently introduced treestructured sparse regularization norm proven useful several applications norm leads regularized problems difficult optimize propose paper efficient algorithms solving precisely show proximal operator associated norm computable exactly via dual approach viewed composition elementary proximal operators procedure complexity linear close linear number atoms allows use accelerated gradient techniques solve treestructured sparse approximation problem computational cost traditional ones using lnorm method efficient scales gracefully millions variables illustrate two types applications first consider fixed hierarchical dictionaries wavelets denoise natural images apply optimization tools context dictionary learning learned dictionary elements naturally organize prespecified arborescent structure leading better performance reconstruction natural image patches applied text documents method learns hierarchies topics thus providing competitive alternative probabilistic topic models\",\"motivated problems arise number applications online marketing explosives detection observations usually modeled using poisson statistics model observation poisson random variable whose mean sparse linear superposition known patterns unlike many conventional problems observations identically distributed since associated different sensing modalities analyze performance maximum likelihood decoder poisson setting involves nonlinear optimization yet computationally tractable derive fundamental sample complexity bounds sparse recovery measurements contaminated poisson noise contrast leastsquares linear regression setting gaussian noise observe addition sparsity scale parameters also fundamentally impacts ell error poisson setting show tightness upper bounds theoretically experimentally particular derive minimax matching lower bound meansquared error show constrained decoder minimax optimal regime\",\"minimum description length mdl principle states optimal model given data set compresses best due practial limitations model restricted class linear regression models address study formulations lasso forward stepwise regression interested sparsifying feature set preserving generalization ability derive wellprincipled set codes parameters error residuals along smooth approximations lengths codes allow gradient descent optimization description length show sparsification feature selection using approach faster lasso several datasets uci statlib repositories favorable generalization accuracy fully automatic requiring neither crossvalidation tuning regularization hyperparameters allowing even nonlinear expansion feature set followed sparsification\",\"paper introduce new optimization formulation sparse regression compressed sensing called clot combined lone two wherein regularizer convex combination ell ellnorms formulation differs elastic net formulation regularizer convex combination ell ellnorm squared shown context compressed sensing formulation achieve robust recovery sparse vectors whereas new clot formulation achieves robust recovery also like unlike lasso clot formulation achieves grouping effect wherein coefficients highly correlated columns measurement design matrix assigned roughly comparable values already known lasso grouping effect therefore clot formulation combines best features lasso robust sparse recovery grouping effect clot formulation special case another one called sgl sparse group lasso introduced literature previously without analysis either grouping effect robust sparse recovery shown sgl achieves robust sparse recovery also achieves version grouping effect coefficients highly correlated columns belonging group measurement design matrix assigned roughly comparable values\",\"present two sets theoretical results grouped lasso overlap jacob obozinski vert linear regression setting method allows joint selection predictors sparse regression allowing complex structured sparsity predictors encoded set groups flexible framework suggests arbitrarily complex structures encoded intricate set groups results show strategy results unexpected theoretical consequences procedure particular give two sets results finite sample bounds prediction estimation asymptotic distribution selection sets results give insight consequences choosing increasingly complex set groups procedure well happens set groups cannot recover true sparsity pattern additionally results demonstrate differences similarities grouped lasso procedure without overlapping groups analysis shows set groups must chosen caution overly complex set groups damage analysis\",\"paper consider problem hypersparse aggregation namely given dictionary functions look optimal aggregation algorithm writes tilde sumjm thetaj many zero coefficients thetaj possible problem particular interest contains many irrelevant functions appear tildef provide exact oracle inequality tilde two coefficients nonzero entails tilde optimal aggregation algorithm since selectors suboptimal aggregation procedures proves minimal number elements required construction optimal aggregation procedures every situations simulated example algorithm proposed dictionary obtained using lars problem selection regularization parameter lasso also give example use aggregation achieve minimax adaptation anisotropic besov spaces previously known minimax theory regression random design\",\"significant attention given minimizing penalized least squares criterion estimating sparse solutions large linear systems equations penalty responsible inducing sparsity natural choice socalled norm paper develop momentumized iterative shrinkage thresholding mist algorithm minimizing resulting nonconvex criterion prove convergence local minimizer simulations large data sets show superior performance proposed method methods\",\"life sciences experts generally use empirical knowledge recode variables choose interactions perform selection classical approach aim work perform automatic learning algorithm variables selection lead know experts help decision simply replaced machine improve knowledge results lasso method detect optimal subset variables estimation prediction conditions paper propose novel approach uses automatically variables available interactions double crossvalidation combine lasso select best subset variables glm simple crossvalidation perform predictions algorithm assures stability consistency estimators\",\"group lasso penalized regression method used regression problems covariates partitioned groups promote sparsity group level existing methods finding group lasso estimator either use gradient projection methods update entire coefficient vector simultaneously step update one group coefficients time using inexact line search approximate optimal value group coefficients groups coefficients fixed present new method computation group lasso linear regression case single line search sls algorithm operates computing exact optimal value group coefficients fixed one univariate line search perform simulations demonstrating sls algorithm often efficient existing computational methods also extend sls algorithm sparse group lasso problem via signed single line search ssls algorithm give theoretical results support algorithms\",\"consider empirical risk minimization problem linear supervised learning regularization structured sparsityinducing norms defined sums euclidean norms certain subsets variables extending usual ellnorm group ellnorm allowing subsets overlap leads specific set allowed nonzero patterns solutions problems first explore relationship groups defining norm resulting nonzero patterns providing forward backward algorithms back forth groups patterns allows design norms adapted specific prior knowledge expressed terms nonzero patterns also present efficient active set algorithm analyze consistency variable selection leastsquares linear regression low highdimensional settings\",\"consider problems detection localization contiguous block weak activation large matrix small number noisy possibly adaptive compressive linear measurements closely related problem compressed sensing task estimate sparse vector using small number linear measurements contrary results compressed sensing shown neither adaptivity contiguous structure help much show reliable localization magnitude weakest signals strongly influenced structure ability choose measurements adaptively detection neither adaptivity structure reduce requirement magnitude signal characterize precise tradeoffs various problem parameters signal strength number measurements required reliably detect localize block activation sufficient conditions complemented information theoretic lower bounds\",\"consider problem binary classification one particular cost choose classify observation present simple proof oracle inequality excess risk structural risk minimizers using lasso type penalty\",\"consider problem dictionary learning assumption observed signals represented sparse linear combinations columns single large dictionary matrix particular analyze minimax risk dictionary learning problem governs mean squared error mse performance learning scheme regardless computational complexity following established informationtheoretic method based fanos inequality derive lower bound minimax risk given dictionary learning problem lower bound yields characterization samplecomplexity lower bound required number observations consistent dictionary learning schemes exist bounds may compared performance given learning scheme allowing characterize far method optimal performance\",\"study theoretical properties learning dictionary signals mathbf xiin mathbb via lminimization assume mathbf xis iid random linear combinations columns complete square invertible reference dictionary mathbf mathbb rktimes random linear coefficients generated either ssparse gaussian model bernoulligaussian model first population case establish sufficient almost necessary condition reference dictionary mathbf locally identifiable local minimum expected lnorm objective function condition covers sparse dense cases random linear coefficients significantly improves sufficient condition gribonval schnass addition show complete mucoherent reference dictionary dictionary absolute pairwise column innerproduct muin local identifiability holds even random linear coefficient vector omu nonzeros average moreover local identifiability results also translate finite sample case high probability provided number signals scales oklog\",\"synthesis model signals represented sparse combinations atoms dictionary dictionary learning describes acquisition process underlying dictionary given set training samples ideally would achieved optimizing expectation factors underlying distribution training data practice necessary information distribution available therefore real world applications achieved minimizing empirical average available samples main goal paper provide sample complexity estimate controls extent empirical average deviates cost function estimate provides suitable estimate accuracy representation learned dictionary presented approach exemplifies general results proposed authors sample complexity dictionary learning matrix factorizations gribonval gives concrete bounds sample complexity dictionary learning cover variety sparsity measures employed learning procedure\",\"propose new sparsitysmoothness penalty highdimensional generalized additive models combination sparsity smoothness crucial mathematical theory well performance finitesample data present computationally efficient algorithm provable numerical convergence properties optimizing penalized likelihood furthermore provide oracle results yield asymptotic optimality estimator high dimensional sparse additive models finally adaptive version sparsitysmoothness penalized approach yields large additional performance gains\",\"finding sparse solutions underdetermined systems linear equations fundamental problem signal processing statistics become subject interest recent years general systems infinitely many solutions however may shown sufficiently sparse solutions may identified uniquely words corresponding linear transformation invertible restrict domain sufficiently sparse vectors property may used example solve underdetermined blind source separation bss problem find sparse representation signal overcomplete dictionary primitive elements socalled atomic decomposition main drawback current methods finding sparse solutions computational complexity paper show detecting active components potential solution components considerable value framework fast solution problem may devised idea leads family algorithms called iterative detectionestimation ide converge solution successive detection estimation active part comparing performance ides one successful method date based linear programming improvement speed two three orders magnitude observed\",\"concave regularization methods provide natural procedures sparse recovery however difficult analyze high dimensional setting recently sparse recovery results established specific local solutions obtained via specialized numerical procedures still fundamental relationship solutions whether identical relationship global minimizer underlying nonconvex formulation unknown current paper fills conceptual gap presenting general theoretical framework showing appropriate conditions global solution nonconvex regularization leads desirable recovery performance moreover suitable conditions global solution corresponds unique sparse local solution obtained via different numerical procedures unified framework present overview existing results discuss connections unified view work leads satisfactory treatment concave high dimensional sparse estimation procedures serves guideline developing numerical procedures concave regularization\",\"performance lasso well understood assumptions standard linear model homoscedastic noise however several applications standard model describe important features data paper examines lasso performs nonstandard model motivated medical imaging applications applications variance noise scales linearly expectation observation like heteroscedastic models noise terms poissonlike model textitnot independent design matrix specifically paper studies sign consistency lasso sparse poissonlike model addition studying sufficient conditions sign consistency lasso estimate paper also gives necessary conditions sign consistency sets conditions comparable results homoscedastic model showing measure signal noise ratio large lasso performs well poissonlike data homoscedastic data simulations reveal lasso performs equally well terms model selection performance poissonlike data homoscedastic data properly scaled noise variance across range parameterizations taken whole results suggest lasso robust poissonlike heteroscedastic noise\",\"investigate properties estimators obtained minimization uprocesses lasso penalty highdimensional settings attention focused ranking problem popular machine learning related guessing ordering objects basis observed predictors prove oracle inequality excess risk considered estimator well bound distance estimator oracle\",\"performance orthogonal matching pursuit omp variable selection analyzed random designs contrasted deterministic case since performance measured averaging distribution design matrix one far less stringent sparsity constraints coefficient vector demonstrate exact sparse vectors performance omp similar known results lasso algorithm textitieee trans inform theory textbf moreover variable selection relaxed sparsity assumption coefficient vector whereby one control ell norm smaller coefficients also analyzed consequence results also show coefficient estimate satisfies strong oracle type inequalities\",\"compressed sensing order recover sparse nearly sparse vector possibly noisy measurements popular approach ellnorm minimization upper bounds ell norm error true estimated vectors given reviewed bounds ellnorm given unknown vector conventionally sparse group sparse instead variety alternatives ellnorm proposed literature including group lasso sparse group lasso group lasso tree structured overlapping groups however error bounds available modified objective functions present paper unified approach presented deriving upper bounds error true vector approximation based notion decomposable gammadecomposable norms bounds presented cover norms mentioned also provide guideline choosing norms future accommodate alternate forms sparsity\",\"paper study nonconvex penalization using bernstein functions since bernstein function concave nonsmooth origin induce class nonconvex functions highdimensional sparse estimation problems derive threshold function based bernstein penalty give mathematical properties sparsity modeling show coordinate descent algorithm especially appropriate penalized regression problems bernstein penalty additionally prove bernstein function defined concave conjugate varphidivergence develop conjugate maximization algorithm finding sparse solution finally particularly exemplify family bernstein nonconvex penalties based generalized gamma measure conduct empirical analysis family\",\"present extension sparse pca sparse dictionary learning sparsity patterns dictionary elements structured constrained belong prespecified set shapes emphstructured sparse pca based structured regularization recently introduced classical sparse priors deal textitcardinality regularization use encodes higherorder information data propose efficient simple optimization procedure solve problem experiments two practical tasks face recognition study dynamics protein complex demonstrate benefits proposed structured approach unstructured approaches\",\"popular sparse estimation methods based ellrelaxation lasso dantzig selector require knowledge variance noise order properly tune regularization parameter constitutes major obstacle applying methods several frameworkssuch time series random fields inverse problemsfor noise rarely homoscedastic level hard know advance paper propose new approach joint estimation conditional mean conditional variance highdimensional auto regression setting attractive feature proposed estimator efficiently computable even large scale problems solving secondorder cone program socp present theoretical analysis numerical results assessing performance proposed procedure\",\"paper investigates phase retrieval problem aims recover signal magnitudes linear measurements develop statistically computationally efficient algorithms situation measurements corrupted sparse outliers take arbitrary values propose novel approach robustify gradient descent algorithm using sample median guide pruning spurious samples initialization local search adopting poisson loss reshaped quadratic loss respectively obtain two algorithms termed mediantwf medianrwf provably recover signal nearoptimal number measurements measurement vectors composed iid gaussian entries logarithmic factor even constant fraction measurements adversarially corrupted show algorithms stable presence additional dense bounded noise analysis accomplished developing nontrivial concentration results medianrelated quantities may independent interest provide numerical experiments demonstrate effectiveness approach\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"2_lasso_sparse_dictionary\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"2_lasso_sparse_dictionary\"],\"textfont\":{\"size\":12},\"x\":[-0.10318665,0.18069372,-0.54434717,0.17493312,0.22017233,-0.58840984,0.29729414,-0.68315095,-0.7071972,-0.4736185,-0.6341265,0.10494672,-0.56335336,-0.26701057,0.56800926,0.2428243,-0.28858325,0.31052858,-0.6862325,-0.6708619,-0.56787276,0.24328896,-0.3696318,0.20478658,-0.74230886,-0.34249073,0.29478633,-0.13749333,-0.4222829,-0.55984455,-0.697903,-0.5117552,-0.17150587,-0.794397,-0.69056165,-0.4490264,-0.10578133,-0.5652464,-0.0017126607,-0.009672556,0.09701745,-0.21470848,0.011719898,-0.27445695,-0.45303693,-0.62766856,-0.56250215,-0.5699683,-0.14783545,0.23520891,-0.25054315,-0.16592805,-0.2582693],\"y\":[3.8762476,3.6749365,4.0107265,3.4690628,3.461212,3.7366061,3.3024852,3.7578754,3.860311,3.6957076,3.8223536,3.736371,3.7333615,4.3830004,4.856925,3.422631,3.867657,4.4591856,3.2983754,3.3357084,3.5634983,4.6290126,4.088821,3.4366426,3.3204033,4.2586155,4.478933,4.495188,3.5700123,3.994408,3.8086095,3.9647872,4.0218134,3.0540116,3.828398,3.7643003,4.4550247,3.5385997,4.3436904,4.453196,4.446337,3.6006277,4.5148697,3.9276688,3.7048855,3.5329187,3.862323,3.984917,3.8440554,4.442184,3.8286192,4.5011387,3.9042165],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"quantitative modeling posttranscriptional regulation process challenging problem systems biology mechanical model regulatory process needs able describe available spatiotemporal protein concentration mrna expression data recover continuous spatiotemporal fields rigorous methods required identify model parameters promising approach deal difficulties proposed using gaussian process prior distribution latent function protein concentration mrna expression study consider partial differential equation mechanical model differential operators latent function since operators stake linear information physical model encoded kernel function hybrid monte carlo methods employed carry bayesian inference partial differential equation parameters gaussian process kernel parameters spatiotemporal field protein concentration mrna expression reconstructed without explicitly solving partial differential equation\",\"propose family multivariate gaussian process models correlated outputs based assuming likelihood function takes generic form multivariate exponential family distribution efd denote model multivariate generalized gaussian process model derive taylor laplace algorithms approximate inference generic model instantiating efd specific parameter functions obtain two novel models corresponding inference algorithms correlated outputs vonmises angle regression dirichlet regressing multinomial simplex\",\"standard sparse pseudoinput approximations gaussian process cannot handle complex functions well sparse spectrum alternatives attempt answer known overfit suggest use variational inference sparse spectrum approximation avoid issues model covariance function finite fourier series approximation treat random variable random covariance function posterior variational distribution placed variational distribution transforms random covariance function fit data study properties approximate inference compare alternative ones extend distributed stochastic domains approximation captures complex functions better standard approaches avoids overfitting\",\"introduce new class nonstationary kernels derive covariance functions novel family stochastic processes refer string gaussian processes string gps construct string gps allow multiple types local patterns data ensuring mild global regularity condition paper illustrate efficacy approach using synthetic data demonstrate model outperforms competing approaches well studied reallife datasets exhibit nonstationary features\",\"circular variables arise multitude datamodelling contexts ranging robotics social sciences largely overlooked machine learning community paper partially redresses imbalance extending standard probabilistic modelling tools circular domain first introduce new multivariate distribution circular variables called multivariate generalised von mises mgvm distribution distribution constructed restricting renormalising general multivariate gaussian distribution unit hypertorus previously proposed multivariate circular distributions shown special cases construction second introduce new probabilistic model circular regression inspired gaussian processes method probabilistic principal component analysis circular hidden variables models leverage standard modelling tools covariance functions methods automatic relevance determination third show posterior distribution models mgvm distribution enables development efficient variational freeenergy scheme performing approximate inference approximate maximumlikelihood learning\",\"new bayesian approach linear system identification proposed series recent papers main idea frame linear system identification predictor estimation infinite dimensional space aid regularizationbayesian techniques approach guarantees identification stable predictors based prediction error minimization unluckily stability predictors guarantee stability impulse response system paper propose compare various techniques address issue simulations results comparing techniques provided\",\"paper introduce novel online time series forecasting model refer pmgp filter show model equivalent gaussian process regression advantage online forecasting online learning hyperparameters constant rather cubic time complexity constant rather squared memory requirement number observations without resorting approximations moreover proposed model expressive family covariance functions implied latent process namely spectral matern kernels recently proven capable approximating arbitrarily well translationinvariant covariance function benefit approach compared competing models demonstrated using experiments several reallife datasets\",\"online passiveaggressive learning class online marginbased algorithms suitable wide range realtime prediction tasks including classification regression algorithms formulated terms deterministic pointestimation problems governed set userdefined hyperparameters approach fails capture modelprediction uncertainty makes performance highly sensitive hyperparameter configurations paper introduce novel learning framework regression overcomes limitations contribute bayesian statespace interpretation regression along novel online variational inference scheme produces probabilistic predictions also offers benefit automatic hyperparameter tuning experiments various realworld data sets show approach performs significantly better standard linear gaussian statespace model\",\"present scalable gaussian process model identifying characterizing smooth multidimensional changepoints automatically learning changes expressive covariance structure use random kitchen sink features flexibly define change surface combination expressive spectral mixture kernels capture complex statistical structure finally use novel methods additive nonseparable kernels scale model large datasets demonstrate model numerical real world data including large spatiotemporal disease dataset identify previously unknown heterogeneous changes space time\",\"study gaussian process regression model context training data noise input output presence two sources noise makes task learning accurate predictive models extremely challenging however instances additional constraints may available reduce uncertainty resulting predictive models particular consider case monotonically ordered latent input occurs many application domains deal temporal data present novel inference learning approach based nonparametric gaussian variational approximation learn model taking account new constraints resulting strategy allows one gain access posterior estimates input output results improved predictive performance compare proposed models stateoftheart noisy input gaussian process nigp competing approaches synthetic real sealevel rise data experimental results suggest proposed approach consistently outperforms selected methods time reducing computational costs learning inference\",\"gaussian process models also called kriging models often used mathematical approximations expensive experiments however number observation required building emulator becomes unrealistic using classical covariance kernels dimension input increases oder get round curse dimensionality popular approach consider simplified models additive models ambition present work give insight covariance kernels well suited building additive kriging models describe properties resulting models\",\"study largescale spatial systems contain exogenous variables environmental factors significant predictors spatial processes building predictive models processes challenging large numbers observations present makes inefficient apply full kriging order reduce computational complexity paper proposes sparse pseudoinput local kriging splk utilizes hyperplanes partition domain smaller subdomains applies sparse approximation full kriging subdomain also develop optimization procedure find desired hyperplanes alleviate problem discontinuity global predictor impose continuity constraints boundaries neighboring subdomains furthermore partitioning domain smaller subdomains makes possible use different parameter values covariance function region therefore heterogeneity data structure effectively captured numerical experiments demonstrate splk outperforms comparable algorithms commonly applied spatial datasets\",\"document travel may expect short snippets document also travel introduce general framework incorporating types invariances discriminative classifier framework imagines data drawn slice levy process slice levy process earlier point time obtain additional pseudoexamples used train classifier show scheme two desirable properties preserves bayes decision boundary equivalent fitting generative model limit rewind time back construction captures popular schemes gaussian feature noising dropout training well admitting new generalizations\",\"paper introduces kernelbased information criterion kic model selection regression analysis novel kernelbased complexity measure kic efficiently computes interdependency parameters model using variablewise variance yields selection better robust regressors experimental results show superior performance simulated real data sets compared leaveoneout crossvalidation loocv kernelbased information complexity icomp maximum log marginal likelihood gaussian process regression gpr\",\"consider gaussian process formulation multiple kernel learning problem goal select convex combination kernel matrices best explains data improve generalisation unseen data sparsity kernel weights obtained adopting hierarchical bayesian approach gaussian process priors imposed latent functions generalised inverse gaussians associated weights construction equivalent imposing product heavytailed process priors function space variational inference algorithm derived regression binary classification\",\"latent variable timeseries models among heavily used tools machine learning applied statistics models advantage learning latent structure noisy observations temporal ordering data assumed meaningful correlation structure exists across time highlystructured models linear dynamical system lineargaussian observations closedform inference procedures kalman filter case exception general rule exact posterior inference complex generative models intractable consequently much work timeseries modeling focuses approximate inference procedures one particular class models extend recent developments stochastic variational inference develop blackbox approximate inference technique latent variable models latent dynamical structure propose structured gaussian variational approximate posterior carries intuition standard kalman filtersmoother importantly permits use inference approach approximate posterior much general nonlinear latent variable generative models show approach recovers accurate estimates case basic models closedform posteriors interestingly performs well comparison variational approaches designed bespoke fashion specific nonconjugate models\",\"paper present framework fitting multivariate hawkes processes largescale problems number events observed history number event types dimensions proposed lowrank hawkes process lrhp framework introduces lowrank approximation kernel matrix allows perform nonparametric learning triggering kernels using ondr operations rank approximation comes major improvement existing stateoftheart inference algorithms ond furthermore lowrank approximation allows lrhp learn representative patterns interaction event types may valuable analysis complex processes real world datasets efficiency scalability approach illustrated numerical experiments simulated well real datasets\",\"paper introduce novel framework making exact nonparametric bayesian inference latent functions particularly suitable big data tasks firstly introduce class stochastic processes refer string gaussian processes string gps mistaken gaussian processes operating text construct string gps finitedimensional marginals exhibit suitable local conditional independence structures allow scalable distributed flexible nonparametric bayesian inference without resorting approximations ensuring mild global regularity constraints furthermore string priors naturally cope heterogeneous input data gradient learned latent function readily available explanatory analysis secondly provide theoretical results relating approach standard paradigm particular prove string gps gaussian processes provides complementary global perspective framework finally derive scalable distributed mcmc scheme supervised learning tasks string priors proposed mcmc scheme computational time complexity mathcalon memory requirement mathcalodn data size dimension input space illustrate efficacy proposed approach several synthetic realworld datasets including dataset millions input points attributes\",\"informally call stochastic process learnable admits generalization error approaching zero probability concept class finite vcdimension iid processes simplest example mixture learnable processes need learnable certainly generalization error need decay rate paper argue natural predictive pac condition past observations mixture component sample path definition matches realistic learner might demand also allows sidestep several otherwise grave problems learning dependent data particular give novel pac generalization bound mixtures learnable processes generalization error worse mixture component also provide characterization mixtures absolutely regular betamixing processes independent probabilitytheoretic interest\",\"present novel approach fully nonstationary gaussian process regression gpr three key parameters noise variance signal variance lengthscale simultaneously inputdependent develop gradientbased inference methods learn unknown function nonstationary model parameters without requiring model approximations propose infer full parameter posterior hamiltonian monte carlo hmc conveniently extends analytical gradientbased gpr learning guiding sampling model gradients also learn map solution posterior gradient ascent experiments several synthetic datasets modelling temporal gene expression nonstationary gpr shown necessary modeling realistic inputdependent dynamics performs comparably conventional stationary previous nonstationary gpr models otherwise\",\"interest multioutput kernel methods increasing whether guise multitask learning multisensor networks structured output data gaussian process perspective multioutput mercer kernel covariance function correlated output functions one way constructing kernels based convolution processes key problem approach efficient inference alvarez lawrence recently presented sparse approximation cps enabled efficient inference paper extend work two directions introduce concept variational inducing functions handle potential nonsmooth functions involved kernel construction consider alternative approach approximate inference based variational methods extending work titsias multiple output case demonstrate approaches prediction school marks compiler performance financial time series\",\"variational framework learning inducing variables titsias large impact gaussian process literature framework may interpreted minimizing rigorously defined kullbackleibler divergence approximating posterior processes knowledge connection thus far gone unremarked literature paper give substantial generalization literature topic give new proof result infinite index sets allows inducing points data points likelihoods depend function values discuss augmented index sets show contrary previous works marginal consistency augmentation enough guarantee consistency variational inference original model characterize extra condition guarantee obtainable finally show framework sheds light interdomain sparse approximations sparse approximations cox processes\",\"paper considers quantification prediction performance gaussian process regression standard approach base prediction error bars theoretical predictive variance lower bound mean squareerror mse approach however take account statistical model learned data show omission leads systematic underestimation prediction errors starting generalization cramerrao bound derive accurate mse bound provides measure uncertainty prediction gaussian processes improved bound easily computed illustrate using synthetic real data examples uncertainty prediction gaussian processes illustrate using synthetic real data examples\",\"spatiotemporal point process models play central role analysis spatially distributed systems several disciplines yet scalable inference remains computa tionally challenging due high resolution modelling generally required analytically intractable likelihood function exploit sparsity structure typical spatially discretised loggaussian cox process models using approximate messagepassing algorithms proposed algorithms scale well state dimension length temporal horizon moderate loss distributional accuracy hence provide flexible faster alternative nonlinear filteringsmoothing type algorithms approaches implement laplace method expectation propagation block sparse latent gaussian models infer parameters latent gaussian model using structured variational bayes approach demonstrate proposed framework simulation studies gaussian pointprocess observations use reconstruct conflict intensity dynamics afghanistan wikileaks afghan war diary\",\"gaussian process promising novel technology applied regression problem classification problem regression problem yields simple exact solutions case classification problem encounter intractable integrals paper develop new derivation transforms problem evaluating ratio multivariate gaussian orthant integrals moreover develop new monte carlo procedure evaluates integrals based aspects bootstrap sampling acceptancerejection proposed approach beneficial properties compared existing markov chain monte carlo approach simplicity reliability speed\",\"recently increasing interest methods deal multiple outputs motivated partly frameworks like multitask learning multisensor networks structured output data gaussian processes perspective problem reduces specifying appropriate covariance function whilst positive semidefinite captures dependencies data points across outputs one approach account nontrivial correlations outputs employs convolution processes latent function interpretation convolution transform establish dependencies output variables main drawbacks approach associated computational storage demands paper address issues present different sparse approximations dependent output gaussian processes constructed convolution formalism exploit conditional independencies present naturally model leads form covariance similar spirit called pitc fitc approximations single output show experimental results synthetic real data particular show results pollution prediction school exams score prediction gene expression data\",\"hyperparameters gaussian process regression gpr model specified kernel often estimated data via maximum marginal likelihood due nonconvexity marginal likelihood respect hyperparameters optimization may converge global maxima common approach tackle issue use multiple starting points randomly selected specific prior distribution result choice prior distribution may play vital role predictability approach however exists little research literature study impact prior distributions hyperparameter estimation performance gpr paper provide first empirical study problem using simulated real data experiments consider different types priors initial values hyperparameters commonly used kernels investigate influence priors predictability gpr models results reveal kernel chosen different priors initial hyperparameters significant impact performance gpr prediction despite estimates hyperparameters different true values cases\",\"work falls within context predicting value real function input locations given limited number observations function kriging interpolation technique gaussian process regression often considered tackle problem method suffers computational burden number observation points large introduce article nested kriging predictors constructed aggregating submodels based subsets observation points approach proven better theoretical properties aggregation methods found literature contrarily methods shown proposed aggregation method consistent finally practical interest proposed method illustrated simulated datasets industrial test case observations dimensional space\",\"paper propose first nonparametric bayesian model using gaussian processes make inference poisson point processes without resorting gridding domain introducing latent thinning points unlike competing models scale cubically squared memory requirement number data points model linear complexity memory requirement propose mcmc sampler show model faster accurate generates less correlated samples competing models synthetic reallife data finally show model easily handles data sizes considered thus far alternate approaches\",\"exact gaussian process regression runtime data size making intractable large many algorithms improving scaling approximate covariance lower rank matrices work exploited structure inherent particular covariance functions including gps implied markov structure equispaced inputs enable runtime however advances extended multidimensional input setting despite preponderance multidimensional applications paper introduces tests novel extensions structured gps multidimensional inputs present new methods additive gps showing novel connection classic backfitting method bayesian framework achieve optimal accuracycomplexity tradeoff extend model novel variant projection pursuit regression primary result projection pursuit gaussian process regression shows orders magnitude speedup preserving high accuracy natural second third steps include nongaussian observations higher dimensional equispaced grid methods introduce novel techniques address necessary directions thoroughly illustrate power three advances several datasets achieving close performance naive full orders magnitude less cost\",\"investigate capabilities limitations gaussian process models jointly exploring three complementary directions scalable statistically efficient inference flexible kernels iii objective functions hyperparameter learning alternative marginal likelihood approach outperforms previously reported methods standard mnist dataset performs comparatively previous kernelbased methods using rectanglesimage dataset breaks errorrate barrier models using mnistm dataset showing along way scalability method unprecedented scale models million observations classification problems overall approach represents significant breakthrough kernel methods models bridging gap deep learning approaches kernel machines\",\"develop automated variational method inference models gaussian process priors general likelihoods method supports multiple outputs multiple latent functions require detailed knowledge conditional likelihood needing evaluation blackbox function using mixture gaussians variational distribution show evidence lower bound gradients estimated efficiently using samples univariate gaussian distributions furthermore method scalable large datasets achieved using augmented prior via inducingvariable approach underpinning sparse approximations along parallel computation stochastic optimization evaluate approach quantitatively qualitatively experiments small datasets mediumscale datasets large datasets showing competitiveness different likelihood models sparsity levels largescale experiments involving prediction airline delays classification handwritten digits show method par stateoftheart hardcoded approaches scalable regression classification\",\"good sparse approximations essential practical inference gaussian processes computational cost exact methods prohibitive large datasets fully independent training conditional fitc variational free energy vfe approximations two recent popular methods despite superficial similarities approximations surprisingly different theoretical properties behave differently practice thoroughly investigate two methods regression analytically illustrative examples draw conclusions guide practical application\",\"gaussian process models form core part probabilistic machine learning considerable research effort made attacking three issues models compute efficiently number data large approximate posterior likelihood gaussian estimate covariance function parameter posteriors paper simultaneously addresses using variational approximation posterior sparse support function otherwise freeform result hybrid montecarlo sampling scheme allows nongaussian approximation function values covariance parameters simultaneously efficient computations based inducingpoint sparse gps code replicate experiment paper available shortly\",\"present blitzkriging new approach fast inference gaussian processes applicable regression optimisation classification stateoftheart stochastic inference gaussian processes large datasets scales cubically number inducing inputs variables introduced factorise model blitzkriging shares stateoftheart scaling data reduces scaling number inducing points approximately linear contrast methods blitzkriging force data conform particular structure including gridlike reduces reliance errorprone optimisation inducing point locations able learn rich covariance structure data demonstrate benefits approach real data regression timeseries prediction signalinterpolation experiments\",\"gaussian processes powerful yet analytically tractable models supervised learning gaussian process characterized mean function covariance function kernel determined model selection criterion functions compared differ parametrization fundamental structure often clear function structure choose instance decide squared exponential rational quadratic kernel based principle approximation set coding develop framework model selection rank kernels gaussian process regression experiments approximation set coding shows promise become model selection criterion competitive maximum evidence also called marginal likelihood leaveoneout crossvalidation\",\"present first fully variational bayesian inference scheme continuous gaussianprocessmodulated poisson processes point processes used variety domains including neuroscience geostatistics astronomy use hindered computational cost existing inference schemes scheme requires discretisation domain scales linearly number observed events many orders magnitude faster previous sampling based approaches resulting algorithm shown outperform standard methods synthetic examples coal mining disaster data prediction malaria incidences kenya\",\"gaussian process models often used mathematical approximations computationally expensive experiments provided kernel suitably chosen enough data available obtain reasonable fit simulator model beneficially used tasks prediction optimization montecarlobased quantification uncertainty however former conditions become unrealistic using classical gps dimension input increases one popular alternative turn generalized additive models gams relying assumption simulators response approximately decomposed sum univariate functions approach successfully applied approximation nevertheless completely compatible framework versatile applications ambition present work give insight use gps additive models integrating additivity within kernel proposing parsimonious numerical method datadriven parameter estimation first part article deals kernels naturally associated additive processes properties models based kernels second part dedicated numerical procedure based relaxation additive kernel parameter estimation finally efficiency proposed method illustrated compared approaches sobols gfunction\",\"gaussian process classification popular method number appealing properties show scale model within variational inducing point framework outperforming state art benchmark datasets importantly variational formulation exploited allow classification problems millions data points demonstrate experiments\",\"present new method estimating multivariate secondorder stationary gaussian random field grf models based sparse precision matrix selection sps algorithm proposed davanloo estimating scalar grf models theoretical convergence rates estimated betweenresponse covariance matrix estimated parameters underlying spatial correlation function established numerical tests using simulated real datasets validate theoretical findings data segmentation used handle large data sets\",\"paper proposes novel gaussian process approach fault removal timeseries data fault removal delete faulty signal data instead massages fault data assume one fault occurs one time model signal two separate nonparametric gaussian process models physical phenomenon fault order facilitate fault removal introduce markov region link kernel handling nonstationary gaussian processes kernel piecewise stationary guarantees functions generated derivatives required everywhere continuous apply kernel removal drift bias errors faulty sensor data also recovery eog artifact corrupted eeg signals\",\"paper presents bayesian generative model dependent cox point processes alongside efficient inference scheme scales point processes modelled independently handle missing data naturally infer latent structure cope large numbers observed processes novel contribution enables model work effectively higher dimensional spaces using method achieve vastly improved predictive performance real data validating structured approach\",\"method large scale gaussian process classification recently proposed based expectation propagation method allows gaussian process classifiers trained large datasets reach previous deployments shown competitive related techniques based stochastic variational inference nevertheless memory resources required scale linearly dataset size unlike variational methods severe limitation number instances large show problem avoided stochastic used train model\",\"scale gaussian processes gps large data sets introduce robust bayesian committee machine rbcm practical scalable productofexperts model largescale distributed regression unlike stateoftheart sparse approximations rbcm conceptually simple rely inducing variational parameters key idea recursively distribute computations independent computational units subsequently recombine form overall result efficient closedform inference allows straightforward parallelisation distributed computations small memory footprint rbcm independent computational graph used heterogeneous computing infrastructures ranging laptops clusters sufficient computing resources distributed model handle arbitrarily large data sets\",\"tutorial explain inference procedures developed sparse gaussian process regression gaussian process latent variable model gplvm due page limit derivation given titsias titsias lawrence brief hence getting full picture requires collecting results several different sources substantial amount algebra fillin gaps main goal thus collect results full derivations one place help speed understanding work present reparametrisation inference allows carried parallel secondary goal document therefore accompany paper opensource implementation parallel inference scheme models hope document bridge gap equations implemented code published original papers order make easier extend existing work assume prior knowledge gaussian processes variational inference also include references reading appropriate\",\"paper propose outlierrobust regularized kernelbased method linear system identification unknown impulse response modeled zeromean gaussian process whose covariance kernel given recently proposed stable spline kernel encodes information regularity exponential stability build robustness outliers model measurement noise realizations independent laplacian random variables identification problem cast bayesian framework solved new markov chain monte carlo mcmc scheme particular exploiting representation laplacian random variables scale mixtures gaussians design gibbs sampler quickly converges target distribution numerical simulations show substantial improvement accuracy estimates stateoftheart kernelbased methods\",\"consider probabilistic multinomial probit classification using gaussian process priors challenges multiclass classification integration nongaussian posterior distribution increase number unknown latent variables number target classes grows expectation propagation proven accurate method approximate inference existing approaches multinomial probit classification rely numerical quadratures independence assumptions latent values different classes facilitate computations paper propose novel nested approach require numerical quadratures approximates accurately betweenclass posterior dependencies latent values still scales linearly number classes predictive accuracy nested approach compared laplace variational bayes markov chain monte carlo mcmc approximations various benchmark data sets experiments nested consistent method respect mcmc sampling differences compared methods small classification accuracy concerned\",\"challenging task modeling multivariate time series propose new class models use dependent matern processes capture underlying structure data explain interdependencies predict unknown values although similar models proposed econometric statistics machine learning literature approach several advantages distinguish existing methods flexible provide high prediction accuracy yet complexity controlled avoid overfitting interpretability separates blackbox methods finally computational efficiency makes scalable highdimensional time series paper use several simulated real data sets illustrate advantages also briefly discuss extensions model\",\"paper presents novel approach approximate integration uncertainty noise signal variances gaussian process regression efficient straightforward approach also applied integration input dependent noise variance heteroscedasticity input dependent signal variance nonstationarity setting independent priors noise signal variances use expectation propagation inference compare results markov chain monte carlo two simulated data sets three empirical examples results show produces comparable results less computational burden\",\"multioutput gaussian processes received increasing attention last years natural mechanism extend powerful flexibility gaussian processes setup multiple output variables key point ability design kernel functions allow exploiting correlations outputs fulfilling positive definiteness requisite covariance function alternatives construct covariance functions linear model coregionalization process convolutions methods demand specification number latent gaussian process used build covariance function outputs propose paper use indian buffet process way perform model selection number latent gaussian processes type model particularly important context latent force models latent forces associated physical quantities like protein profiles latent forces mechanical systems use variational inference estimate posterior distributions variables involved show examples model performance artificial data motion capture dataset gene expression dataset\",\"large amount observational data accumulated various fields recent times growing need estimate generating processes data linear nongaussian acyclic model lingam based nongaussianity external influences proposed estimate datagenerating processes variables however results estimation biased latent classes paper first review lingam extended model well estimation procedure lingam bayesian framework propose new bayesian estimation procedure solves problem\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"3_gaussian_processes_process\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"3_gaussian_processes_process\"],\"textfont\":{\"size\":12},\"x\":[3.1064744,2.721714,2.8734367,2.4458323,2.7267408,2.3550124,2.410617,2.747764,2.3699272,2.8453171,2.6568646,3.0156152,2.96099,2.4970484,2.6046345,3.393028,3.1511612,2.9982872,3.242201,2.865803,2.5155492,3.1125774,2.787595,3.2353547,2.9002764,2.4620547,2.6584444,2.7864683,3.3735738,2.8336482,2.5184326,2.889416,3.0048392,2.900473,2.8243248,2.694011,3.3571563,2.7393656,3.0884767,3.0451818,2.6203353,3.356628,3.1812437,2.9729435,2.9868495,2.4087741,3.1033807,2.4498072,2.80855,2.5614276,3.4957926,2.8561063],\"y\":[1.2896717,1.6756502,1.7829521,1.4935347,1.5384098,2.1241028,1.5717958,1.502146,1.4962031,1.589939,1.6713067,1.4654248,1.2877495,1.7089001,1.6576324,1.3815938,1.3721697,1.1160759,1.6043489,1.5968103,1.6646478,1.567053,1.7096674,1.3772522,1.9080963,1.6889379,1.6892331,1.4880655,1.1323217,1.4417781,1.5764445,1.3055291,1.6944355,1.4758251,1.4360763,1.6229517,1.1483725,1.6039062,1.550411,1.5263907,1.80529,1.2103711,1.54081,1.1753187,1.3707385,2.0664368,1.2667415,1.6487359,1.7305562,1.3803385,1.3393946,1.5307558],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"regularized kernel methods support vector machines leastsquares support vector regression constitute important class standard learning algorithms machine learning theoretical investigations concerning asymptotic properties manly focused rates convergence last years limited asymptotic results statistical inference far serious limitation use mathematical statistics goal article fill gap based asymptotic normality many methods article derives strongly consistent estimator unknown covariance matrix limiting normal distribution way obtain asymptotically correct confidence sets psifplambda fplambda denotes minimizer regularized risk reproducing kernel hilbert space psihrightarrowmathdsrm hadamarddifferentiable functional applications include multivariate pointwise confidence sets values fplambda confidence sets gradients integrals norms\",\"propose vectorvalued regression problem whose solution equivalent reproducing kernel hilbert space rkhs embedding bayesian posterior distribution equivalence provides new understanding kernel bayesian inference moreover optimization problem induces new regularization posterior embedding estimator faster comparable performance squared regularization kernel bayes rule regularization coincides former thresholding approach used kernel pomdps whose consistency remains established theoretical work solves open problem provides consistency analysis regression settings based optimizational formulation propose flexible bayesian posterior regularization framework first time enables put regularization distribution level apply method nonparametric statespace filtering tasks extremely nonlinear dynamics show performance gains baselines\",\"consider problem learning set random samples show relevant geometric topological properties set studied analytically using concepts theory reproducing kernel hilbert spaces new kind reproducing kernel call separating kernel plays crucial role study analyzed detail prove new analytic characterization support distribution naturally leads family provably consistent regularized learning algorithms discuss stability methods respect random sampling numerical experiments show approach competitive often better state art techniques\",\"paper give new sharp generalization bound lpmkl generalized framework multiple kernel learning mkl imposes lpmixednorm regularization instead lmixednorm regularization utilize localization techniques obtain sharp learning rate bound characterized decay rate eigenvalues associated kernels larger decay rate gives faster convergence rate furthermore give minimax learning rate ball characterized lpmixednorm product space show derived learning rate lpmkl achieves minimax optimal rate lpmixednorm ball\",\"paper reviews functional aspects statistical learning theory main point consideration nature hypothesis set prior information available data within framework first discuss hypothesis set vectorial space set pointwise defined functions evaluation functional set continuous mapping based principles original theory developed generalizing notion reproduction kernel hilbert space non hilbertian sets shown hypothesis set learning machine generalized reproducing set therefore thanks general representer theorem solution learning problem still linear combination kernel furthermore way design kernels given illustrate framework examples reproducing sets kernels given\",\"nonparametric classification regression problems regularized kernel methods particular support vector machines attract much attention theoretical applied statistics abstract sense regularized kernel methods simply called svms seen regularized mestimators parameter typically infinite dimensional reproducing kernel hilbert space smooth loss functions shown difference estimator empirical svm theoretical svm asymptotically normal rate sqrtn standardized difference converges weakly gaussian process reproducing kernel hilbert space common real applications choice regularization parameter may depend data proof done application functional deltamethod showing svmfunctional suitably hadamarddifferentiable\",\"propose class nonparametric twosample tests cost linear sample size two tests given based ensemble distances analytic functions representing distributions first test uses smoothed empirical characteristic functions represent distributions second uses distribution embeddings reproducing kernel hilbert space analyticity implies differences distributions may detected almost surely finite number randomly chosen locationsfrequencies new tests consistent larger class alternatives previous lineartime tests based nonsmoothed empirical characteristic functions much faster current stateoftheart quadratictime kernelbased energy distancebased tests experiments artificial benchmarks challenging realworld testing problems demonstrate tests give better powertime tradeoff competing approaches cases better outright power even expensive quadratictime tests performance advantage retained even high dimensions cases difference distributions observable low order statistics\",\"given reproducing kernel hilbert space realvalued functions suitable measure source space subset decompose sum subspace centered functions orthogonal decomposition leads special case anova kernels functional anova representation best predictor elegantly derived either interpolation regularization framework proposed kernels appear particularly convenient analyzing ffect group variables computing sensitivity indices without recursivity\",\"kernel methods one mainstays machine learning problem kernel learning remains challenging heuristics little theory particular importance methods based estimation kernel mean embeddings probability measures characteristic kernels include commonly used ones kernel mean embedding uniquely determines probability measure used design powerful statistical testing framework includes nonparametric twosample independence tests practice however performance tests sensitive choice kernel lengthscale parameters address central issue propose new probabilistic model kernel mean embeddings bayesian kernel embedding model combining gaussian process prior reproducing kernel hilbert space containing mean embedding conjugate likelihood function thus yielding closed form posterior mean embedding posterior mean model closely related recently proposed shrinkage estimators kernel mean embeddings posterior uncertainty new interesting feature various possible applications critically purposes kernel learning model gives simple closed form marginal pseudolikelihood observed data given kernel hyperparameters marginal pseudolikelihood either optimized inform hyperparameter choice fully bayesian inference used\",\"recent years kernel density estimation exploited computer scientists model machine learning problems kernel density estimation based approaches interest due low time complexity either onlogn constructing classifier number sampling instances concerning design kernel density estimators one essential issue fast pointwise mean square error mse andor integrated mean square error imse diminish number sampling instances increases article shown proposed kernel function feasible make pointwise mse density estimator converge regardless dimension vector space provided probability density function point interest meets certain conditions\",\"new non parametric approach problem testing independence two random process developed test statistic hilbert schmidt independence criterion hsic used previously testing independence iid pairs variables asymptotic behaviour hsic established computed samples drawn random processes shown earlier bootstrap procedures worked iid case fail random processes alternative consistent estimate pvalues proposed tests artificial data realworld forex data indicate new test procedure discovers dependence missed linear approaches earlier bootstrap procedure returns elevated number false positives code available online httpsgithubcomkacperchwialkowskihsic\",\"paper presents general vectorvalued reproducing kernel hilbert spaces rkhs framework problem learning unknown functional dependency structured input space structured output space formulation encompasses vectorvalued manifold regularization coregularized multiview learning providing particular unifying framework linking two important learning approaches case least square loss function provide closed form solution obtained solving system linear equations case support vector machine svm classification formulation generalizes particular binary laplacian svm multiclass multiview settings multiclass simplex cone svm semisupervised multiview settings solution obtained solving single quadratic optimization problem standard svm via sequential minimal optimization smo approach empirical results obtained task object recognition using several challenging datasets demonstrate competitiveness algorithms compared stateoftheart methods\",\"wild bootstrap method nonparametric hypothesis tests based kernel distribution embeddings proposed bootstrap method used construct provably consistent tests apply random processes naive permutationbased bootstrap fails applies large group kernel tests based vstatistics degenerate null hypothesis nondegenerate elsewhere illustrate approach construct twosample test instantaneous independence test multiple lag independence test time series experiments wild bootstrap gives strong performance synthetic examples audio data performance benchmarking gibbs sampler\",\"kernel methods widely applied machine learning questions approximating unknown function finite sample data ensure arbitrary accuracy approximation various denseness conditions imposed selected kernel note contributes study universal characteristic cuniversal kernels first give simple direct description difference relation among three kinds universalities kernels focus translationinvariant weighted polynomial kernels simple shorter proof known characterization characteristic translationinvariant kernels presented main purpose note give delicate discussion universalities weighted polynomial kernels\",\"kernel induced random survival forests kirsf statistical learning algorithm aims improve prediction accuracy survival data random survival forests rsf cumulative hazard function predicted individual test set prediction error estimated using harrells concordance index index harrell cindex interpreted misclassification probability depend single fixed time evaluation cindex also specifically accounts censoring utilizing kernel functions kirsf achieves better results rsf many situations report show incorporate kernel functions rsf test performance kirsf compare method rsf find kirsfs performance better rsf many occasions\",\"paper deals problem nonparametric independence testing fundamental decisiontheoretic problem asks two arbitrary possibly multivariate random variables independent question comes many fields like causality neuroscience quantities like correlation test univariate linear independence natural alternatives like mutual information hard estimate due serious curse dimensionality recent approach avoiding issues estimates norms textitoperator reproducing kernel hilbert spaces rkhss main contribution strong empirical evidence employing textitshrunk operators sample size small one attain improvement power low false positive rates analyze effects stein shrinkage popular test statistic called hsic hilbertschmidt independence criterion observations provide insights two recently proposed shrinkage estimators scose fcose prove scose essentially optimal linear shrinkage method textitestimating true operator however nonlinearly shrunk fcose usually achieves greater improvements textittest power work important powerful nonparametric detection subtle nonlinear dependencies small samples\",\"estimation density derivatives versatile tool statistical data analysis naive approach first estimate density compute derivative however twostep approach work well good density estimator necessarily mean good densityderivative estimator paper give direct method approximate density derivative without estimating density proposed estimator allows analytic computationally efficient approximation multidimensional highorder density derivatives ability hyperparameters chosen objectively crossvalidation show proposed densityderivative estimator useful improving accuracy nonparametric kldivergence estimation via metric learning practical superiority proposed method experimentally demonstrated change detection feature selection\",\"kernelbased quadrature rules becoming important machine learning statistics achieve supersqrtn convergence rates numerical integration thus provide alternatives monte carlo integration challenging settings integrands expensive evaluate integrands high dimensional rules based assumption integrand certain degree smoothness expressed integrand belongs certain reproducing kernel hilbert space rkhs however assumption violated practice integrand black box function general theory established convergence kernel quadratures misspecified settings contribution proving kernel quadratures consistent even integrand belong assumed rkhs integrand less smooth assumed specifically derive convergence rates depend unknown lesser smoothness integrand degree smoothness expressed via powers rkhss via sobolev spaces\",\"propose investigate test statistics testing homogeneity reproducing kernel hilbert spaces asymptotic null distributions null hypothesis derived consistency fixed local alternatives assessed finally experimental evidence performance proposed approach artificial data speaker verification task provided\",\"robust parameter estimation well studied parametric density estimation little investigation robust density estimation nonparametric setting present robust version popular kernel density estimator kde estimators robust version kde useful since sample contamination common issue datasets robustness means nonparametric density estimate straightforward topic explore paper construct robust kde scale traditional kde project nearest weighted kde norm yields scaled projected kde spkde squared norm penalizes pointwise errors superlinearly causes weighted kde allocate weight high density regions demonstrate robustness spkde numerical experiments consistency result shows asymptotically spkde recovers uncontaminated density sufficient conditions contamination\",\"survey introduction positive definite kernels set methods inspired machine learning literature namely kernel methods first discuss properties positive definite kernels well reproducing kernel hibert spaces natural extension set functions kxcdotxinmathcalx associated kernel defined space mathcalx discuss length construction kernel functions take advantage wellknown statistical models provide overview numerous dataanalysis methods take advantage reproducing kernel hilbert spaces discuss idea combining several kernels improve performance certain tasks also provide short cookbook different kernels particularly useful certain datatypes images graphs speech segments\",\"many applications particular information systems pattern recognition machine learning cheminformatics bioinformatics name assessment uncertainty essential estimation underlying probability distribution function often form function unknown becomes necessary nonparametrically constructestimate given sample one methods choice nonparametrically estimate unknown probability distribution function given random variable defined binary space expansion estimation function rademacherwalsh polynomial basis functions paper demonstrate expansion probability distribution function estimation rademacherwalsh polynomial basis functions equivalent expansion function estimation set dirac kernel functions latter approach ameliorate computational bottleneck notational awkwardness often associated rademacherwalsh polynomial basis functions approach particular binary input space large\",\"apply wild bootstrap method lancaster threevariable interaction measure order detect factorisation joint distribution three variables forming stationary random process existing permutation bootstrap method fails iid case lancaster test found outperform existing tests cases two independent variables individually weak influence third considered jointly influence strong main contributions paper twofold first prove lancaster statistic satisfies conditions required estimate quantiles null distribution using wild bootstrap second manner proved novel simpler existing methods applied statistics\",\"paper propose family tractable kernels dense family bounded positive semidefinite functions approximate bounded kernel arbitrary precision start discussing case stationary kernels propose family spectral kernels extends existing approaches spectral mixture kernels sparse spectrum kernels extension two primary advantages firstly unlike existing spectral approaches yield infinite differentiability kernels introduce allow learning degree differentiability latent function gaussian process models functions reproducing kernel hilbert space rkhs kernel methods secondly show kernels propose require fewer parameters existing spectral kernels accuracy thereby leading faster robust inference finally generalize approach propose flexible tractable family spectral kernels prove approximate continuous bounded nonstationary kernel\",\"present work new family kernels compare positive measures arbitrary spaces xcal endowed positive kernel kappa translates naturally kernels histograms clouds points first cover case xcal euclidian focus kernels take account variance matrix mixture two measures compute similarity kernels define semigroup kernels sense use sum two measures compare spectral sense use eigenspectrum variance matrix mixture show family kernels close bonds laplace transforms nonnegativevalued functions defined cone positive semidefinite matrices present closed formulas derived special cases integral expressions focusing functions invariant addition null eigenvalue spectrum variance matrix define kernels atomic measures arbitrary spaces xcal endowed kernel kappa using directly eigenvalues centered gram matrix joined support compared measures provide explicit formulas suited applications present preliminary experiments illustrate interest approach\",\"connect shiftinvariant characteristic kernels infinitely divisible distributions mathbbrd characteristic kernels play important role machine learning applications kernel means distinguish two probability measures contribution paper twofold first show using levykhintchine formula shiftinvariant kernel given bounded continuous symmetric probability density function pdf infinitely divisible distribution mathbbrd characteristic also present closure property characteristic kernels addition pointwise product convolution second developing various kernel mean algorithms fundamental compute following values kernel mean values mpx mathcalx kernel mean rkhs inner products leftlangle rightranglemathcalh probability measures kernel gaussians computation results gaussian pdfs tractable generalize gaussian combination general cases class infinitely divisible distributions introduce conjugate kernel convolution trick pdf form expecting tractable computation least cases specific instances explore alphastable distributions rich class generalized hyperbolic distributions laplace cauchy studentt distributions included\",\"density ratio defined ratio two probability densities study inference problem density ratios apply semiparametric densityratio estimator twosample homogeneity test proposed test procedure fdivergence two probability densities estimated using densityratio estimator fdivergence estimator exploited twosample homogeneity test derive optimal estimator fdivergence sense asymptotic variance investigate relation proposed test procedure existing score test based empirical likelihood estimator numerical studies illustrate adequacy asymptotic theory finitesample inference\",\"give comprehensive theoretical characterization nonparametric estimator divergence two continuous distributions first bound rate convergence estimator showing sqrtnconsistent provided densities sufficiently smooth smooth regime show estimator asymptotically normal construct asymptotic confidence intervals establish berryesseen style inequality characterizing rate convergence normality also show estimator minimax optimal\",\"propose nonparametric sequential test aims address two practical problems pertinent online randomized experiments hypothesis test complex metrics prevent type error inflation continuous monitoring proposed test require knowledge underlying probability distribution generating data use bootstrap estimate likelihood blocks data followed mixture sequential probability ratio test validate procedure data major online ecommerce website show proposed test controls type error time good power robust misspecification distribution generating data allows quick inference online randomized experiments\",\"kernel bayes rule proposed nonparametric kernelbased method realize bayesian inference reproducing kernel hilbert spaces however demonstrate theoretically experimentally prediction result kernel bayes rule cases unnatural consider phenomenon part due fact assumptions kernel bayes rule hold general\",\"provide theoretical foundation nonparametric estimation functions random variables using kernel mean embeddings show continuous function consistent estimators mean embedding random variable lead consistent estimators mean embedding matern kernels sufficiently smooth functions also provide rates convergence results extend functions multiple random variables variables dependent require estimator mean embedding joint distribution starting point independent sufficient separate estimators mean embeddings marginal distributions either case results cover mean embeddings based iid samples well reduced set expansions terms dependent expansion points latter serves justification using expansions limit memory resources applying approach basis probabilistic programming\",\"kernel methods ubiquitous tools machine learning however often little reason common practice selecting kernel priori even universal approximating kernel selected quality finite sample estimator may greatly affected choice kernel furthermore directly applying kernel methods one typically needs compute times gram matrix pairwise kernel evaluations work dataset instances computation gram matrix precludes direct application kernel methods large datasets makes kernel learning especially difficult paper introduce bayesian nonparmetric kernellearning bank generic datadriven framework scalable learning kernels bank places nonparametric prior spectral distribution random frequencies allowing learn kernels scale large datasets show framework used large scale regression classification tasks furthermore show bank outperforms several scalable approaches kernel learning variety real world datasets\",\"derive upper bound local rademacher complexity ellpnorm multiple kernel learning yields tighter excess risk bound global approaches previous local approaches aimed analyzed case analysis covers cases leq pleqinfty assuming different feature mappings corresponding different kernels uncorrelated also show lower bound shows bound tight derive consequences regarding excess loss namely fast convergence rates order onfracalphaalpha alpha minimum eigenvalue decay rate individual kernels\",\"direct way express arbitrary dependencies datasets estimate joint distribution apply afterwards argmaxfunction obtain mode corresponding conditional distribution method practice difficult requires global optimization complicated function joint distribution fixed input variables article proposes method finding global maxima joint distribution modeled kernel density estimation experiments show advantages shortcomings resulting regression method comparison standard nadarayawatson regression technique approximates optimum expectation value\",\"statistical test independence may constructed using hilbertschmidt independence criterion hsic test statistic hsic defined distance embedding joint distribution embedding product marginals reproducing kernel hilbert space rkhs previously shown kernel used defining joint embedding characteristic embedding joint distribution feature space injective hsicbased test consistent particular sufficient product kernels individual domains characteristic joint domain note established via result lyons hsicbased independence tests consistent kernels marginals characteristic respective domains even product kernels characteristic joint domain\",\"develop approach feature elimination statistical learning kernel machines based recursive elimination featureswe present theoretical properties method show uniformly consistent finding correct feature space certain generalized assumptionswe present four case studies show assumptions met practical situations present simulation results demonstrate performance proposed approach\",\"nonparametric kernelbased method realizing bayes rule proposed based representations probabilities reproducing kernel hilbert spaces probabilities uniquely characterized mean canonical map rkhs prior conditional probabilities expressed terms rkhs functions empirical sample explicit parametric model needed quantities posterior likewise rkhs mean weighted sample estimator expectation function posterior derived rates consistency shown representative applications kernel bayes rule presented including baysian computation without likelihood filtering nonparametric statespace model\",\"introduce mondrian kernel fast random feature approximation laplace kernel suitable batch online learning admits fast kernelwidthselection procedure random features reused efficiently kernel widths features constructed sampling trees via mondrian process roy teh highlight connection mondrian forests lakshminarayanan trees also sampled via mondrian process fit independently link provides new insight relationship kernel methods random forests\",\"study density estimation problem observations generated certain dynamical systems admit unique underlying invariant lebesgue density observations drawn dynamical systems independent moreover usual mixing concepts may appropriate measuring dependence among observations employing mathcalcmixing concept measure dependence conduct statistical analysis consistency convergence kernel density estimator main results follows first show properly chosen bandwidth kernel density estimator universally consistent lnorm second establish convergence rates estimator respect several classes dynamical systems lnorm analysis density function assumed holder continuous weak assumption literature nonparametric density estimation also realistic dynamical system context last least prove convergence rates estimator linftynorm lnorm achieved density function holder continuous compactly supported bounded bandwidth selection problem kernel density estimator dynamical system also discussed study via numerical simulations\",\"prove density function gradient sufficiently smooth function omega subset mathbbrd rightarrow mathbbr obtained via random variable transformation uniformly distributed random variable increasingly closely approximated normalized power spectrum phiexpleftfracistauright free parameter tau rightarrow result shown using stationary phase approximation standard integration techniques requires proper ordering limits highlight relationship wellknown characteristic function approach density estimation detail result distinct approach\",\"paper give new generalization error bound multiple kernel learning mkl general class regularizations discuss kind regularization gives favorable predictive accuracy main target paper dense type regularizations including ellpmkl according recent numerical experiments sparse regularization necessarily show good performance compared dense type regularizations motivated fact paper gives general theoretical tool derive fast learning rates mkl applicable arbitrary mixednormtype regularizations unifying manner enables compare generalization performances various types regularizations consequence observe homogeneity complexities candidate reproducing kernel hilbert spaces rkhss affects regularization strategy ell dense preferred fact homogeneous complexity settings complexities rkhss evenly ellregularization optimal among isotropic norms hand inhomogeneous complexity settings dense type regularizations show better learning rate sparse ellregularization also show learning rate achieves minimax lower bound homogeneous complexity settings\",\"support vector machines svms naturally embody sparseness due use hinge loss functions however svms directly estimate conditional class probabilities paper propose study family coherence functions convex differentiable surrogates hinge function coherence function derived using maximumentropy principle characterized temperature parameter bridges hinge function logit function logistic regression limit coherence function zero temperature corresponds hinge function limit minimizer expected error minimizer expected error hinge loss refer use coherence function largemargin classification clearning present efficient coordinate descent algorithms training regularized cal clearning models\",\"derive new discrepancy statistic measuring differences two probability distributions based combining steins identity reproducing kernel hilbert space theory apply result test well probabilistic model fits set observations derive new class powerful goodnessoffit tests widely applicable complex high dimensional distributions even computationally intractable normalization constants theoretical empirical properties methods studied thoroughly\",\"additive models play important role semiparametric statistics paper gives learning rates regularized kernel based methods additive models learning rates compare favourably particular high dimensions recent results optimal learning rates purely nonparametric regularized kernel based quantile regression using gaussian radial basis function kernel provided assumption additive model valid additionally concrete example presented show gaussian function depending one variable lies reproducing kernel hilbert space generated additive gaussian kernel belong reproducing kernel hilbert space generated multivariate gaussian kernel variance\",\"propose nonparametric statistical test goodnessoffit given set samples test determines likely generated target density function measure goodnessoffit divergence constructed via steins method using functions reproducing kernel hilbert space test statistic based empirical estimate divergence taking form vstatistic terms log gradients target density kernel derive statistical test iid noniid samples estimate null distribution quantiles using wild bootstrap procedure apply test quantifying convergence approximate markov chain monte carlo methods statistical model criticism evaluating quality fit model complexity nonparametric density estimation\",\"many machine learning problems characterized mutual contamination models problems one observes several random samples different convex combinations set unknown base distributions interest decontaminate mutual contamination models recover base distributions either exactly permutation paper considers general setting base distributions defined arbitrary probability spaces examine decontamination problem two mutual contamination models describe popular machine learning tasks recovering base distributions permutation mixed membership model recovering base distributions exactly partial label model classification give necessary sufficient conditions identifiability mutual contamination models algorithms problems infinite finite sample cases introduce novel proof techniques based affine geometry\",\"present probabilistic viewpoint multiple kernel learning unifying wellknown regularised risk approaches recent advances approximate bayesian inference relaxations framework proposes general objective function suitable regression robust regression classification lower bound marginal likelihood contains many regularised risk approaches special cases furthermore derive efficient provably convergent optimisation algorithm\",\"despite recent progress towards efficient multiple kernel learning mkl structured output case remains open research front current approaches involve repeatedly solving batch learning problem makes inadequate large scale scenarios propose new family online proximal algorithms mkl well grouplasso variants thereof overcomes drawback show regret convergence generalization bounds proposed method experiments handwriting recognition dependency parsing testify successfulness approach\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"4_kernel_kernels_reproducing\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"4_kernel_kernels_reproducing\"],\"textfont\":{\"size\":12},\"x\":[1.0842342,1.9103284,1.4777974,0.9150286,1.4139043,1.282936,2.0330737,1.4712838,1.7716248,1.8253418,2.0473793,1.1840063,2.0174344,1.441133,1.2937014,1.9551041,2.2554898,1.1292837,1.9816396,2.2670088,1.4382756,1.5768244,2.0190246,1.5479114,1.6244111,1.8379264,2.208735,2.1142364,2.1130598,1.7484282,2.04851,1.3508179,0.926558,2.0134492,1.9935987,1.3863994,1.9300314,1.3909092,2.1788495,2.1774447,0.8853845,0.93579006,2.0644703,1.3768997,2.2076242,1.6105912,1.3063852,1.1154487,1.664286],\"y\":[2.3992972,2.2223296,2.5593762,2.7250984,2.32833,2.3895705,2.8068366,2.4180703,2.3791656,2.468501,2.8185463,2.5050929,2.8280492,2.458743,2.2003634,2.8163102,2.6373193,2.4385307,2.8075178,2.5779316,2.4551466,2.4134653,2.8571174,2.4495897,2.4209538,2.4669802,2.66222,2.6361418,2.710818,2.430376,2.5568151,2.4427402,2.7056684,2.4330647,2.803138,2.3210404,2.2937438,2.4903328,2.5685723,2.6234925,2.7404993,1.8881814,2.74707,2.4828231,2.6933947,2.6107533,2.5883744,2.6238651,2.5396116],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"histogram method powerful nonparametric approach estimating probability density function continuous variable construction histogram compared parametric approaches demands large number observations capture underlying density function thus suitable analyzing sparse data set collection units small size data paper employing probabilistic topic model develop novel bayesian approach alleviating sparsity problem conventional histogram estimation method estimates units density function mixture basis histograms number bins basis well heights determined automatically estimation procedure performed using fast easytoimplement collapsed gibbs sampling apply proposed method synthetic data showing performs well\",\"topic models popular modeling discrete data texts images videos links provide efficient way discover hidden structuressemantics massive data one core problems field posterior inference individual data instances problem particularly important streaming environments often intractable paper investigate use frankwolfe algorithm recovering sparse solutions posterior inference detailed elucidation theoretical practical aspects exhibits many interesting properties beneficial topic modeling employ design fast methods including mlfw learning latent dirichlet allocation lda large scales extensive experiments show reach predictiveness level mlfw perform tens thousand times faster existing stateoftheart methods learning lda massivestreaming data\",\"paper variable selection clustering estimation unsupervised highdimensional setting approach based fitting constrained gaussian mixture models learn number clusters set relevant variables using generalized bayesian posterior sparsity inducing prior prove sparsity oracle inequality shows procedure selects optimal parameters procedure implemented using metropolishastings algorithm based clusteringoriented greedy proposal makes convergence posterior fast\",\"develop nested hierarchical dirichlet process nhdp hierarchical topic modeling nhdp generalization nested chinese restaurant process ncrp allows word follow path topic node according documentspecific distribution shared tree alleviates rigid singlepath formulation ncrp allowing document easily express thematic borrowings random effect demonstrate algorithm million documents new york times\",\"tree structures ubiquitous data across many domains many datasets naturally modelled unobserved tree structures paper first review theory random fragmentation processes bertoin number existing methods modelling trees including popular nested chinese restaurant process ncrp define general class probability distributions trees dirichlet fragmentation process dfp novel combination theory dirichlet processes random fragmentation processes dfp presents stickbreaking construction relates ncrp way dirichlet process relates chinese restaurant process furthermore develop novel hierarchical mixture model dfp empirically compare new model similar models machine learning experiments show dfp mixture model convincingly better existing stateoftheart approaches hierarchical clustering density modelling\",\"consider problem discriminative factor analysis data general nongaussian bayesian model based ranks data proposed first introduce new maxmargin version ranklikelihood discriminative factor model developed integrating maxmargin ranklikelihood linear bayesian support vector machines also built maxmargin principle discriminative factor model extended nonlinear case mixtures local linear classifiers via dirichlet processes fully local conjugacy model yields efficient inference markov chain monte carlo variational bayes approaches extensive experiments benchmark real data demonstrate superior performance proposed model potential applications computational biology\",\"concept hierarchies formal concept analysis theoretically well grounded largely experimented methods rely line diagrams called galois lattices visualizing analysing objectattribute sets galois lattices visually seducing conceptually rich experts however present important drawbacks due concept oriented overall structure analysing show difficult non experts navigation cumbersome interaction poor scalability deep bottleneck visual interpretation even experts paper introduce semantic probes means overcome many problems extend usability application possibilities traditional fca visualization methods semantic probes visual user centred objects extract organize reduced galois subhierarchies simpler clearer provide better navigation support rich set interaction possibilities since probe driven subhierarchies limited users focus scalability control interpretation facilitated successful experiments several applications developed remaining problem finding compromise simplicity conceptual expressivity\",\"generating user interpretable multiclass predictions data rich environments many classes explanatory covariates daunting task introduce diagonal orthant latent dirichlet allocation dolda supervised topic model multiclass classification handle many classes well many covariates handle many classes use recently proposed diagonal orthant probit model johndrow together efficient horseshoe prior variable selectionshrinkage carvalho propose computationally efficient parallel gibbs sampler new model important advantage dolda learned topics directly connected individual classes without need reference class evaluate models predictive accuracy two datasets demonstrate doldas advantage interpreting generated predictions\",\"present nested chinese restaurant process ncrp stochastic process assigns probability distributions infinitelydeep infinitelybranching trees show stochastic process used prior distribution bayesian nonparametric model document collections specifically present application information retrieval documents modeled paths random tree preferential attachment dynamics ncrp leads clustering documents according sharing topics multiple levels abstraction given corpus documents posterior inference algorithm finds approximation posterior distribution trees topics allocations words levels tree demonstrate algorithm collections scientific abstracts several journals model exemplifies recent trend statistical machine learningthe use bayesian nonparametric methods infer distributions flexible data structures\",\"propose probabilistic modeling framework learning dynamic patterns collective behaviors social agents developing profiles different behavioral groups using data collected multiple information sources proposed model based hierarchical bayesian process observation finite mixture set latent groups mixture proportions group probabilities drawn randomly group associated distributions finite set outcomes moreover time evolves structure groups also changes model change group structure hidden markov model hmm fixed transition probability present efficient inference method based tensor decompositions expectationmaximization algorithm parameter estimation\",\"many modern data analysis problems involve inferences streaming data however streaming data easily amenable standard probabilistic modeling approaches assume condition finite data develop population variational bayes new approach using bayesian modeling analyze streams data approximates new type distribution population posterior combines notion population distribution data bayesian inference probabilistic model study method latent dirichlet allocation dirichlet process mixtures several largescale data sets\",\"robust bayesian models appealing alternatives standard models providing protection data contains outliers departures model assumptions historically robust models mostly developed casebycase basis examples include robust linear regression robust mixture models bursty topic models paper develop general approach robust bayesian modeling show turn existing bayesian model robust model develop generic strategy computing use method study robust variants several models including linear regression poisson regression logistic regression probabilistic topic models discuss connections methods existing approaches especially empirical bayes jamesstein estimation\",\"describe simple efficient procedure approximating levy measure textgammaalpha random variable use approximation derive finite sumrepresentation converges almost surely fergusons representation dirichlet process based arrivals homogeneous poisson process compare efficiency approximation several well known approximations dirichlet process demonstrate substantial improvement\",\"document going derive equations needed implement variational bayes estimation parameters simplified probabilistic linear discriminant analysis splda model used adapt splda one database another development data implement fully bayesian recipe approach similar bishops ppca\",\"unsupervised image segmentation aims clustering set pixels image spatially homogeneous regions introduce class bayesian nonparametric models address problem models based combination pottslike spatial smoothness component prior partitions used control number size clusters class models flexible enough include standard potts model recent pottsdirichlet process model citeorbanz importantly prior partitions introduced control global clustering structure possible penalize small large clusters necessary bayesian computation carried using original generalized swendsenwang algorithm experiments demonstrate method competitive terms rand index compared popular image segmentation methods meanshift recent alternative bayesian nonparametric models\",\"introduce supervised latent dirichlet allocation slda statistical model labelled documents model accommodates variety response types derive approximate maximumlikelihood procedure parameter estimation relies variational methods handle intractable posterior expectations prediction problems motivate research use fitted model predict response values new documents test slda two realworld problems movie ratings predicted reviews political tone amendments senate based amendment text illustrate benefits slda versus modern regularized regression well versus unsupervised lda analysis followed separate regression\",\"document going derive equations needed implement variational bayes ivector extractor used extract longer ivectors reducing risk overfittig adapt ivector extractor database another scarce development data work based patrick kennys joint factor analysis christopher bishops variational principal components\",\"introduce pitman yor diffusion tree pydt hierarchical clustering generalization dirichlet diffusion tree neal removes restriction binary branching structure generative process described shown result exchangeable distribution data points prove theoretical properties model present two inference methods collapsed mcmc sampler allows model uncertainty tree structures computationally efficient greedy bayesian search algorithm algorithms use message passing tree structure utility model algorithms demonstrated synthetic real world data continuous binary\",\"question determine number independent latent factors topics mixture models latent dirichlet allocation lda great practical importance applications exact number topics unknown depends application size data set bayesian nonparametric methods avoid problem topic number selection impracticably slow large sample sizes subject local optima develop guaranteed procedure topic number recovery necessitate learning models latent parameters beforehand procedure relies adapting results random matrix theory performance topic number recovery procedure superior hlda nonparametric method also discuss implications results sample complexity accuracy popular spectral learning algorithms lda results procedure extended spectral learning algorithms exchangeable mixture models well hidden markov models\",\"define beta diffusion tree random tree structure set leaves defines collection overlapping subsets objects known feature allocation generative process tree structure defined terms particles representing objects diffusing continuous space analogously dirichlet diffusion tree neal defines tree structure partitions nonoverlapping subsets objects unlike dirichlet diffusion tree multiple copies particle may exist diffuse along multiple branches beta diffusion tree object may therefore belong multiple subsets particles demonstrate build hierarchicallyclustered factor analysis model beta diffusion tree perform inference random tree structures markov chain monte carlo algorithm conclude several numerical experiments missing data problems data sets gene expression microarrays international development statistics intranational socioeconomic measurements\",\"investigate class feature allocation models generalize indian buffet process parameterized gibbstype random measures two existing classes contained special cases original twoparameter indian buffet process corresponding dirichlet process stable threeparameter indian buffet process corresponding pitmanyor process asymptotic behavior gibbstype partitions power laws holding number latent clusters translates analogous characteristics class gibbstype feature allocation models despite containing several different distinct subclasses properties gibbstype partitions allow develop blackbox procedure posterior inference within subclass models numerical experiments compare contrast subclasses highlight utility varying powerlaw behaviors latent features\",\"describe new method visualizing topics distributions terms automatically extracted large text corpora using latent variable models method finds significant ngrams related topic used help understand interpret underlying distribution compared usual visualization simply lists probable topical terms multiword expressions provide better intuitive impression topic approach based language model arbitrary length expressions develop new methodology based nested permutation tests find significant phrases show method outperforms standard use chi likelihood ratio tests illustrate topic presentations corpora scientific abstracts news articles\",\"present sparse treebased listbased density estimation methods binarycategorical data density estimation models higher dimensional analogies variable bin width histograms leaf tree list density constant similar flat density within bin histogram histograms however cannot easily visualized two dimensions whereas models accuracy histograms fades dimensions increase whereas models priors help generalization models sparse unlike highdimensional fixedbin histograms present three generative modeling methods first one allows user specify preferred number leaves tree within bayesian prior second method allows user specify preferred number branches within prior third method returns density lists rather trees allows user specify preferred number rules length rules within prior new approaches often yield better balance sparsity accuracy density estimates methods task present application crime analysis estimate unusual type modus operandi house breakin\",\"one core problems statistical models estimation posterior distribution topic models problem posterior inference individual texts particularly important especially dealing data streams often intractable worst case consequence existing methods posterior inference approximate guarantee neither quality convergence rate paper introduce provably fast algorithm namely online maximum posteriori estimation ope posterior inference topic models ope attractive properties existing inference approaches including theoretical guarantees quality fast rate convergence local maximalstationary point inference problem discussions ope general hence easily employed wide range contexts finally employ ope design three methods learning latent dirichlet allocation text streams large corpora extensive experiments demonstrate superior behaviors ope new learning methods\",\"among easiest ways find meaningful structure discrete data latent dirichlet allocation lda related component models applied widely simple computationally fast scalable interpretable admit nonparametric priors currently popular field network modeling relatively little work taken uncertainty data seriously bayesian sense component models introduced field recently treating node bag outgoing links introduce alternative interaction component model communities icmc whole network bag links stemming different components former finds disassortative assortative structure alternative assumes assortativity finds communitylike structures like earlier methods motivated physics dirichlet process priors efficient implementation models highly scalable demonstrated social network lastfm web site nodes million links\",\"dirichlet process mixture dpm ubiquitous flexible bayesian nonparametric statistical model however full probabilistic inference model analytically intractable computationally intensive techniques gibbs sampling required result dpmbased methods considerable potential restricted applications computational resources time inference plentiful example would practical digital signal processing embedded hardware computational resources serious premium develop simplified yet statistically rigorous approximate maximum aposteriori map inference algorithms dpms algorithm simple kmeans clustering performs experiments well gibbs sampling requiring fraction computational effort unlike related small variance asymptotics algorithm nondegenerate inherits rich get richer property dirichlet process also retains nondegenerate closedform likelihood enables standard tools crossvalidation used wellposed approximation map solution probabilistic dpm model\",\"paper considers statistical estimation problems probability distribution observed random variable invariant respect actions finite topological group shown distribution must satisfy restricted finite mixture representation specialized case distributions sphere invariant actions finite spherical symmetry group mathcal groupinvariant extension von mises fisher vmf distribution obtained mathcal ginvariant vmf parameterized location scale parameters specify distributions mean orientation concentration mean respectively using restricted finite mixture representation parameters estimated using expectation maximization maximum likelihood estimation algorithm illustrated problem mean crystal orientation estimation spherically symmetric group associated crystal form cubic octahedral hexahedral simulations experiments establish advantages extended vmf emml estimator data acquired electron backscatter diffraction ebsd microscopy polycrystalline nickel alloy sample\",\"using nonparametric methods increasingly explored bayesian hierarchical modeling way increase model flexibility although field shows lot promise inference many models including hierachical dirichlet processes hdp remain prohibitively slow one promising path forward exploit submodularity inherent indian buffet process ibp derive nearoptimal solutions polynomial time work present brief tutorial bayesian nonparametric methods especially applied topic modeling show comparison different nonparametric models current stateoftheart parametric model latent dirichlet allocation lda\",\"novel dynamic bayesian nonparametric topic model anomaly detection video proposed paper batch online gibbs samplers developed inference paper introduces new abnormality measure decision making proposed method evaluated synthetic real data comparison nondynamic model shows superiority proposed dynamic one terms classification performance anomaly detection\",\"present novel scalable bayesian approach modelling occurrence pairs symbols drawn large vocabulary observed pairs assumed generated simple popularity based selection process followed censoring using preference function basing inference wellfounded principle variational bounding using new siteindependent bounds show scalable inference procedure obtained large data sets state art results presented realworld movie viewing data\",\"introduce novel approach estimating latent dirichlet allocation lda parameters collapsed gibbs samples cgs leveraging full conditional distributions latent variable assignments efficiently average multiple samples little computational cost drawing single additional collapsed gibbs sample approach understood adapting soft clustering methodology collapsed variational bayes cvb cgs parameter estimation order get best techniques estimators straightforwardly applied output existing implementation cgs including modern accelerated variants perform extensive empirical comparisons estimators standard collapsed inference algorithms realworld data unsupervised lda priorlda supervised variant lda multilabel classification results show consistent advantage approach traditional cgs experimental conditions cvb inference majority conditions broadly results highlight importance averaging multiple samples lda parameter estimation use efficient computational techniques\",\"many practical modeling problems involve discrete data best represented draws multinomial categorical distributions example nucleotides dna sequence childrens names given state year text documents commonly modeled multinomial distributions cases expect form dependency draws nucleotide one position dna strand may depend preceding nucleotides childrens names highly correlated year year topics text may correlated dynamic dependencies naturally captured typical dirichletmultinomial formulation leverage logistic stickbreaking representation recent innovations polyagamma augmentation reformulate multinomial distribution terms latent variables jointly gaussian likelihoods enabling take advantage host bayesian inference techniques gaussian models minimal overhead\",\"dynamic topic models dtms effective discovering topics capturing evolution trends time series data posterior inference dtms existing methods batch algorithms scan full dataset update model make inexact variational approximations meanfield assumptions due lack scalable inference algorithm despite usefulness dtms captured large topic dynamics paper fills research void presents fast parallelizable inference algorithm using gibbs sampling stochastic gradient langevin dynamics make unwarranted assumptions also present metropolishastings based sampler topic assignments word token distributed environment algorithm requires little communication workers sampling almost embarrassingly parallel scales largescale applications able learn largest dynamic topic model knowledge learned dynamics topics million documents less half hour empirical results show algorithm orders magnitude faster baselines also achieves lower perplexity\",\"present new sequential monte carlo sampler coalescent based bayesian hierarchical clustering model appropriate modeling noniid data offers substantial reduction computational cost compared original sampler without resorting approximations also propose quadratic complexity approximation practice shows almost loss performance compared counterpart show byproduct formulation obtain greedy algorithm exhibits performance improvement greedy algorithms particularly small data sets order exploit correlation structure data describe incorporate gaussian process priors model flexible way model noniid data results artificial real data show significant improvements closely related approaches\",\"classical mixture gaussians model related kmeans via smallvariance asymptotics covariances gaussians tend zero negative loglikelihood mixture gaussians model approaches kmeans objective algorithm approaches kmeans algorithm kulis jordan used observation obtain novel kmeanslike algorithm gibbs sampler dirichlet process mixture instead consider applying smallvariance asymptotics directly posterior bayesian nonparametric models framework independent specific bayesian inference algorithm major advantage generalizes immediately range models beyond mixture illustrate apply framework feature learning setting beta process indian buffet process provide appropriate bayesian nonparametric prior obtain novel objective function goes beyond clustering learn penalize new groupings relax mutual exclusivity exhaustivity assumptions clustering demonstrate several algorithms scalable simple implement empirical results demonstrate benefits new framework\",\"present discrete infinite logistic normal distribution diln bayesian nonparametric prior mixed membership models diln generalization hierarchical dirichlet process hdp models correlation structure weights atoms group level derive representation diln normalized collection gammadistributed random variables study statistical properties consider applications topic modeling derive variational inference algorithm approximate posterior inference study empirical performance diln topic model four corpora comparing performance hdp correlated topic model ctm deal largescale data sets also develop online inference algorithm diln compare online hdp online lda nature magazine contains approximately articles\",\"timevarying mixture densities occur many scenarios example distributions keywords appear publications may evolve year year video frame features associated multiple targets may evolve sequence models realistically cater phenomenon must exhibit two important properties underlying mixture densities must unknown number mixtures must smoothness constraints place adjacent mixture densities traditional hierarchical dirichlet process hdp may suited first property certainly second due random measure lower hierarchies sampled independent hence facilitate temporal correlations overcome shortcomings proposed new smoothed hierarchical dirichlet process shdp key novelty model place temporal constraint amongst nearby discrete measures form symmetric kullbackleibler divergence fixed bound although constraint place involves single scalar value nonetheless allows flexibility corresponding successive measures remarkably also led infer model within stickbreaking process traditional beta distribution used stickbreaking replaced new constraint calculated present inference algorithm elaborate solutions experiment using nips keywords shown desirable effect model\",\"theory bayesian nonparametric bnp models well suited streaming data scenarios due ability adapt model complexity observed data unfortunately benefits fully realized practice existing inference algorithms either applicable streaming applications extensible bnp models special case dirichlet processes streaming inference considered however growing interest flexible bnp models building class normalized random measures nrms work within general framework present streaming variational inference algorithm nrm mixture models algorithm based assumed density filtering adf leading straightforwardly expectation propagation largescale batch inference well demonstrate efficacy algorithm clustering documents large streaming text corpora\",\"nonparametric mixture models based dirichlet process elegant alternative finite models number underlying components unknown inference models slow existing attempts parallelize inference models relied introducing approximations lead inaccuracies posterior estimate paper describe auxiliary variable representations dirichlet process hierarchical dirichlet process allow sample true posterior distributed manner show approach allows scalable inference without deterioration estimate quality accompanies existing methods\",\"observations organized groups commonalties exist amongst dependent random measures ideal choice modeling one propositions dependent random measures atoms posterior distribution shared amongst groups hence groups borrow information normalized dependent random measures prior independent increments applied derive appropriate exchangeable probability partition function eppf subsequently also deduce inference algorithm given mixture model likelihood provide necessary derivation solution framework demonstration used mixture gaussians likelihood combination dependent structure constructed linear combinations crms experiments show superior performance using framework inferred values including mixing weights number clusters respond appropriately number completely random measure used\",\"single stationary topic model latent dirichlet allocation inappropriate modeling corpora span long time periods popularity topics likely change time number models incorporate time proposed general either exhibit limited forms temporal variation require computationally expensive inference methods paper propose nonparametric topics time nptot model timevarying topics allows unbounded number topics exible distribution temporal variations topics popularity develop collapsed gibbs sampler proposed model compare existing models synthetic real document sets\",\"propose dirichlet process mixtures generalized linear models dpglm new method nonparametric regression accommodates continuous categorical inputs responses modeled generalized linear model prove conditions asymptotic unbiasedness dpglm regression mean function estimate also give examples conditions hold including models compactly supported continuous distributions model continuous covariates categorical response empirically analyze properties dpglm provides better results existing dirichlet process mixture regression models evaluate dpglm several data sets comparing modern methods nonparametric regression like cart bayesian trees gaussian processes compared existing techniques dpglm provides single model corresponding inference algorithms performs well many regression settings\",\"introduce new bayesian model hierarchical clustering based prior trees called kingmans coalescent develop novel greedy sequential monte carlo inferences operate bottomup agglomerative fashion show experimentally superiority algorithms others demonstrate approach document clustering phylolinguistics\",\"paper proposes novel dynamic hierarchical dirichlet process topic model considers dependence successive observations conventional posterior inference algorithms kind models require processing whole data several passes computationally intractable massive sequential data design batch online inference algorithms based gibbs sampling proposed model allows process sequential data incrementally updating model new observation model applied abnormal behaviour detection video sequences new abnormality measure proposed decision making proposed method compared method based non dynamic hierarchical dirichlet process also derive online gibbs sampler abnormality measure results synthetic real data show consideration dynamics topic model improves classification performance abnormal behaviour detection\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"5_dirichlet_topic_process\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"5_dirichlet_topic_process\"],\"textfont\":{\"size\":12},\"x\":[4.7274394,5.1089034,4.4601955,5.1266,4.858167,4.76701,5.2248363,5.089848,4.9230356,4.3721733,4.7777133,4.959675,4.7474875,4.8536735,4.412972,5.122802,4.778694,4.834188,5.109822,4.877139,4.741326,5.1907134,4.788329,5.081247,5.153056,4.618584,4.198065,4.9780526,5.0304766,4.7599616,5.0606327,4.967653,5.1289434,4.546997,4.5041213,4.923402,4.756372,4.7560625,4.673616,4.41955,5.151689,4.6438785,4.838465,5.0434694,4.8428874],\"y\":[2.0728102,1.7939599,2.4413598,1.85144,2.2804148,1.6040989,1.7831627,1.7386522,2.1056478,1.9338367,1.8966464,1.8182153,1.852254,1.6002235,2.4577909,1.7775558,1.5174091,2.3400736,1.7687068,2.3199923,1.8311657,1.7448361,2.3624399,1.7855287,1.9098694,1.9199212,1.9159474,1.7911196,1.8443494,1.86927,1.7376081,1.8680358,1.7483908,2.3327723,2.0765598,1.7319301,1.9545598,1.8993925,1.8898785,1.8753837,1.7563734,1.9177748,2.2872102,1.832793,1.9281217],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"neuroimaging data analysis gaussian graphical models often used model statistical dependencies across spatially remote brain regions known functional connectivity typically data collected across cohort subjects scientific objectives consist estimating population subjectspecific graphical models third objective often overlooked involves quantifying intersubject variability thus identifying regions subnetworks demonstrate heterogeneity across subjects information fundamental order thoroughly understand human connectome propose mixed neighborhood selection order simultaneously address three aforementioned objectives recasting covariance selection neighborhood selection problem able efficiently learn topology node introduce additional mixed effect component neighborhood selection order simultaneously estimate graphical model population subjects well individual subject proposed method validated empirically series simulations applied resting state data healthy subjects taken abide consortium\",\"volume collection contributions workshop machine learning interpretation neuroimaging mlini neural information processing systems nips conference modern multivariate statistical methods developed rapidly growing field machine learning increasingly applied various problems neuroimaging cognitive state detection clinical diagnosis prognosis multivariate pattern analysis methods designed examine complex relationships highdimensional signals brain images outcomes interest category stimulus type mental state subject specific mental disorder techniques contrast traditional massunivariate approaches dominated neuroimaging past treated individual imaging measurement isolation believe machine learning prominent role shaping questions neuroscience framed machinelearning mind set entering modern psychology behavioral studies also equally important practical applications fields motivate rapidly evolving line research machine learning community parallel intense interest learning brain function context rich naturalistic environments scenes efforts beyond highly specific paradigms pinpoint single function towards schemes measuring interaction natural varied scene made goal workshop pinpoint pressing issues common challenges across neuroscience neuroimaging psychology machine learning fields sketch future directions open questions light novel methodology\",\"nonparametric extension tensor regression proposed nonlinearity highdimensional tensor space broken simple local functions incorporating lowrank tensor decomposition compared naive nonparametric approaches formulation considerably improves convergence rate estimation maintaining consistency function class specific conditions estimate local functions develop bayesian estimator gaussian process prior experimental results show theoretical properties high performance terms predicting summary statistic real complex network\",\"sparse versions principal component analysis pca imposed simple yet powerful ways selecting relevant features highdimensional data unsupervised manner however several sparse principal components computed interpretation selected variables difficult since axis sparsity pattern interpreted separately overcome drawback propose bayesian procedure called globally sparse probabilistic pca gsppca allows obtain several sparse components sparsity pattern allows practitioner identify original variables relevant describe data end using roweis probabilistic interpretation pca gaussian prior loading matrix provide first exact computation marginal likelihood bayesian pca model avoid drawbacks discrete model selection simple relaxation framework presented allows find path models using variational expectationmaximization algorithm exact marginal likelihood maximized path approach illustrated real synthetic data sets particular using unlabeled microarray data gsppca infers much relevant gene subsets traditional sparse pca algorithms\",\"diffusionweighted magnetic resonance imaging dwi fiber tractography methods measure structure white matter living human brain diffusion signal modelled combined contribution many individual fascicles nerve fibers passing location white matter typically done via basis pursuit estimation exact directions limited due discretization difficulties inherent modeling dwi data shared many problems involving fitting nonparametric mixture models ekanadaham proposed approach continuous basis pursuit overcome discretization error dimensional case spikesorting propose general algorithm fits mixture models dimensionality without discretization algorithm uses principles lboost together refitting weights pruning parameters addition steps lboost accelerates algorithm assures accuracy refer resulting algorithm elastic basis pursuit ebp since expands contracts active set kernels needed show contrast existing approaches fitting mixtures boosting framework enables selection optimal biasvariance tradeoff along solution path scales highdimensional problems simulations dwi find ebp yields better parameter estimates nonnegative least squares nnls approach standard model used dwi tensor model serves basis diffusion tensor imaging dti demonstrate utility method dwi data acquired parts brain containing crossings multiple fascicles nerve fibers\",\"functional brain networks well described estimated data gaussian graphical models ggms using sparse inverse covariance estimators comparing functional connectivity subjects two populations calls comparing estimated ggms goal identify differences ggms known similar structure characterize uncertainty differences confidence intervals obtained using parametric distribution parameters sparse estimator sparse penalties enable statistical guarantees interpretable models even highdimensional lowsample settings characterizing distributions sparse models inherently challenging penalties produce biased estimator recent work invokes sparsity assumptions effectively remove bias sparse estimator lasso distributions used give confidence intervals edges ggms extension differences however case comparing ggms estimators make use assumed joint structure among ggms inspired priors brain functional connectivity derive distribution parameter differences joint penalty parameters known sparse difference leads introduce debiased multitask fused lasso whose distribution characterized efficient manner show debiased lasso multitask fused lasso used obtain confidence intervals edge differences ggms validate techniques proposed set synthetic examples well neuroimaging dataset created study autism\",\"substantial evidence indicates major psychiatric disorders associated distributed neural dysconnectivity leading strong interest using neuroimaging methods accurately predict disorder status work specifically interested multivariate approach uses features derived wholebrain resting state functional connectomes however functional connectomes reside high dimensional space complicates model interpretation introduces numerous statistical computational challenges traditional feature selection techniques used reduce data dimensionality blind spatial structure connectomes propose regularization framework structure functional connectome explicitly taken account via fused lasso graphnet regularizer method restricts loss function convex marginbased allowing nondifferentiable loss functions hingeloss used using fused lasso graphnet regularizer hingeloss leads structured sparse support vector machine svm embedded feature selection introduce novel efficient optimization algorithm based augmented lagrangian classical alternating direction method solve fused lasso graphnet regularized svm little modification also demonstrate inner subproblems algorithm solved efficiently analytic form coupling variable splitting strategy data augmentation scheme experiments simulated data resting state scans large schizophrenia dataset show proposed approach identify predictive regions spatially contiguous connectome space offering additional layer interpretability could provide new insights various disease processes\",\"principal component analysis pca exploratory tool widely used data analysis uncover dominant patterns variability within population despite ability represent data set lowdimensional space interpretability pca remains limited however neuroimaging essential uncover clinically interpretable phenotypic markers would account main variability brain images population recently alternatives standard pca approach sparse pca proposed aim limit density components nonetheless sparsity alone entirely solve interpretability problem since may yield scattered unstable components hypothesized incorporation prior information regarding structure data may lead improved relevance interpretability brain patterns therefore present simple extension popular pca framework adds structured sparsity penalties loading vectors order identify stable regions brain images accounting variability structured sparsity obtained combining total variation penalties regularization encodes higher order information structure data paper presents structured sparse pca denoted spcatv optimization framework resolution demonstrate efficiency versatility spcatv three different data sets gains spcatv unstructured approaches significantsince spcatv reveals variability within data set form intelligible brain patterns easy interpret stable across different samples\",\"combining information different sources common way improve classification accuracy braincomputer interfacing bci instance small sample settings useful integrate data subjects sessions order improve estimation quality spatial filters classifier since data different subjects may show large variability crucial weight contributions according importance many multisubject learning algorithms determine optimal weighting separate step using heuristics however without ensuring selected weights optimal respect classification work apply multiple kernel learning mkl problem mkl widely used feature fusion computer vision allows simultaneously learn classifier optimal weighting compare mkl method two baseline approaches investigate reasons performance improvement\",\"typical cohorts brain imaging studies large enough systematic testing information contained images build testable working hypotheses investigators thus rely analysis previous work sometimes formalized socalled metaanalysis brain imaging approach underlies specification regions interest rois usually selected basis coordinates previously detected effects paper propose use database images rather coordinates frame problem transfer learning learning discriminant model reference task apply different related new task facilitate statistical analysis small cohorts use sparse discriminant model selects predictive voxels reference task thus provides principled procedure define rois benefits approach twofold first uses reference database prediction provide potential biomarkers clinical setting second increases statistical power new task demonstrate set pairs functional mri experimental conditions approach gives good prediction addition specific transfer situation involving different scanners different locations show voxel selection based transfer learning leads higher detection power small cohorts\",\"highdimensional data common genomics proteomics chemometrics often contains complicated correlation structures recently partial least squares pls sparse pls methods gained attention areas dimension reduction techniques context supervised data analysis introduce framework regularized pls solving relaxation simpls optimization problem penalties pls loadings vectors approach enjoys many advantages including flexibility general penalties easy interpretation results fast computation highdimensional settings also outline extensions methods leading novel methods nonnegative pls generalized pls adaption pls structured data demonstrate utility methods simulations case study proton nuclear magnetic resonance nmr spectroscopy data\",\"improving interpretability brain decoding approaches primary interest many neuroimaging studies despite extensive studies type present formal definition interpretability brain decoding models consequence quantitative measure evaluating interpretability different brain decoding methods paper present simple definition interpretability linear brain decoding models propose combine interpretability performance brain decoding new multiobjective criterion model selection preliminary results toy data show optimizing hyperparameters regularized linear classifier based proposed criterion results informative linear models presented definition provides theoretical background quantitative evaluation interpretability linear brain decoding\",\"use machinelearning neuroimaging offers new perspectives early diagnosis prognosis brain diseases although multivariate methods capture complex relationships data traditional approaches provide irregular penalty scattered penalty predictive pattern limited relevance penalty like total variation exploits natural structure images increase spatial coherence weight map however penalization leads nonsmooth optimization problems hard minimize propose optimization framework minimizes combination penalties preserving exact penalty algorithm uses nesterovs smoothing technique approximate penalty smooth function loss penalties minimized exact accelerated proximal gradient algorithm propose original continuation algorithm uses successively smaller values smoothing parameter reach prescribed precision achieving best possible convergence rate algorithm used losses penalties algorithm applied classification problem adni dataset observe penalty necessarily improve prediction provides major breakthrough terms support recovery predictive brain regions\",\"inverse inference brain reading recent paradigm analyzing functional magnetic resonance imaging fmri data based pattern recognition statistical learning predicting cognitive variables related brain activation maps approach aims decoding brain activity inverse inference takes account multivariate information voxels currently way assess precisely cognitive information encoded activity neural populations within whole brain however relies prediction function plagued curse dimensionality since far features samples voxels fmri volumes address problem different methods proposed among others univariate feature selection feature agglomeration regularization techniques paper consider sparse hierarchical structured regularization specifically penalization use constructed tree obtained spatiallyconstrained agglomerative clustering approach encodes spatial structure data different scales regularization makes overall prediction procedure robust intersubject variability regularization used induces selection spatially coherent predictive brain regions simultaneously different scales test algorithm real data acquired study mental representation objects show proposed algorithm delineates meaningful brain regions yields well better prediction accuracy reference methods\",\"highdimensional data structured noise caused observed unobserved factors affecting multiple target variables simultaneously imposes serious challenge modeling masking often weak signal therefore explaining away structured noise multipleoutput regression paramount importance additionally assumptions correlation structure regression weights needed note formulated natural way latent variable model interesting signal noise mediated latent factors assumption signal model borrows strength noise model encouraging similar effects correlated targets introduce hyperparameter emphlatent signaltonoise ratio turns important modelling weak signals ordered infinitedimensional shrinkage prior resolves rotational unidentifiability reducedrank regression models simulations prediction experiments metabolite gene expression fmri measurement macroeconomic time series data show model equals exceeds stateoftheart performance particular outperforms standard approach assuming independent noise signal models\",\"extracting information functional magnetic resonance fmri images major area research two decades goal work present new method analysis fmri data sets capable incorporate priori available information via efficient optimization framework tests synthetic data sets demonstrate significant performance gains existing methods kind\",\"factor analysis provides linear factors describe relationships individual variables data set extend classical formulation linear factors describe relationships groups variables group represents either set related variables data set model also naturally extends canonical correlation analysis two sets way flexible previous extensions solution formulated variational inference latent variable model structural sparsity consists two hierarchical levels higher level models relationships groups whereas lower models observed variables given higher level show resulting solution solves group factor analysis problem accurately outperforming alternative factor analysis based solutions well straightforward implementations group factor analysis method demonstrated two life science data sets one brain activation systems biology illustrating applicability analysis different types highdimensional data sources\",\"despite fact consider temporal nature data classic dimensionality reduction techniques pca widely applied time series data paper introduce factor decomposition specific time series builds upon bayesian multivariate autoregressive model hence evades assumption data points mutually independent key find lowrank estimation autoregressive matrices probabilistic version factor models induces latent lowdimensional representation original data discuss possible generalisations alternatives relevant technique simultaneous smoothing dimensionality reduction illustrate potential applications apply model synthetic data set different types neuroimaging data eeg ecog\",\"imaging genetic research essentially focused discovering unique coassociation effects typically ignoring identify outliers atypical objects genetic well nongenetics variables identifying significant outliers essential challenging issue imaging genetics multiple sources data analysis therefore need examine transcription errors identified outliers first address influence function kernel mean element kernel covariance operator kernel crosscovariance operator kernel canonical correlation analysis kernel cca multiple kernel cca second propose multiple kernel cca applied two datasets third propose visualization method detect influential observations multiple sources data based kernel cca multiple kernel cca finally proposed methods capable analyzing outliers subjects usually found biomedical applications number dimension large examine outliers use stemandleaf display experiments synthesized imaging genetics data snp fmri dna methylation demonstrate proposed visualization applied effectively\",\"brain decoding involves determination subjects cognitive state associated stimulus functional neuroimaging data measuring brain activity setting cognitive state typically characterized element finite set neuroimaging data comprise voluminous amounts spatiotemporal data measuring aspect neural signal associated statistical problem one classification highdimensional data explore use functional principal component analysis mutual information networks persistent homology examining data exploratory analysis constructing features characterizing neural signal brain decoding review approach perspective incorporate features classifier based symmetric multinomial logistic regression elastic net regularization approaches illustrated application task infer brain activity measured magnetoencephalography meg type video stimulus shown subject\",\"understanding relationships different properties data whether connectome genome information disease status becoming increasingly important modern biological datasets existing approaches test whether two properties related often require unfeasibly large sample sizes real data scenarios provide insight procedure reached decision approach multiscale graph correlation mgc dependence test juxtaposes previously disparate data science techniques including knearest neighbors kernel methods support vector machines multiscale analysis wavelets methods typically require double triple number samples achieve statistical power mgc benchmark suite including highdimensional nonlinear relationships spanning polynomial linear quadratic cubic trigonometric sinusoidal circular ellipsoidal spiral geometric square diamond wshape functions dimensionality ranging moreover mgc uniquely provides simple elegant characterization potentially complex latent geometry underlying relationship providing insight maintaining computational efficiency several real data applications including brain imaging cancer genetics mgc method detect presence dependency provide specific guidance next experiment andor analysis conduct\",\"regularized variants principal components analysis especially sparse pca functional pca among useful tools analysis complex highdimensional data many examples massive data sparse functional smooth aspects may benefit regularization scheme capture forms structure example neuroimaging data brains response stimulus may restricted discrete region activation spatial sparsity exhibiting smooth response within region propose unified approach regularized pca induce sparsity smoothness row column principal components framework generalizes much previous literature sparse functional twoway sparse twoway functional pca special cases approach method permits flexible combinations sparsity smoothness lead improvements feature selection signal recovery well interpretable pca factors demonstrate efficacy method simulated data neuroimaging example eeg data\",\"understanding type inhibitory interaction plays important role drug design therefore researchers interested know whether drug competitive noncompetitive interaction particular protein targets method analyze interaction types propose factorization method macau allows combine different measurement types single tensor together proteins compounds compounds characterized high dimensional ecfp fingerprints novelty proposed method using specially designed noise injection mcmc sampler incorporate high dimensional side information millions unique ecfp compound features even large scale datasets millions compounds without side information case tensor factorization would practically futile results using public data chembl trained model identify latent subspace separating two measurement types results suggest proposed method detect competitive inhibitory activity compounds proteins\",\"propose novel classification model weak signal data building upon recent model bayesian multiview learning group factor analysis gfa instead assuming data come single gfa model allow latent clusters different gfa model producing different class distribution show sharing information across clusters sharing factors increases classification accuracy considerably shared factors essentially form flexible noise model explains away part data related classification motivation setting comes singletrial functional brain imaging data low signaltonoise ratio natural multiview setting different sensors measurement modalities eeg meg fmri possible auxiliary information views demonstrate model meg dataset\",\"genomewide interaction studies detect genegene interactions methods divided two folds single nucleotide polymorphisms snp based genebased methods basically methods based gene effective methods based single snp recent years kernel canonical correlation analysis classical kernel cca based statistic kccu proposed detect nonlinear relationship genes estimate variance kccu used resampling based methods highly computationally intensive addition classical kernel cca robust contaminated data therefore first discuss robust kernel mean element robust kernel covariance crosscovariance operators second propose method based influence function estimate variance kccu third propose nonparametric robust kccu method based robust kernel cca designed contaminated data less sensitive noise classical kernel cca finally investigate proposed methods synthesized data imaging genetic data set based gene ontology pathway analysis synthesized genetics analysis demonstrate proposed robust method shows superior performance stateoftheart methods\",\"flow cytometry often used characterize malignant cells leukemia lymphoma patients traced level individual cell typically flow cytometric data analysis performed series dimensional projections onto axes data set years clinicians determined combinations different fluorescent markers generate relatively known expression patterns specific subtypes leukemia lymphoma cancers hematopoietic system viewing series dimensional projections highdimensional nature data rarely exploited paper present means determining lowdimensional projection maintains highdimensional relationships information differing oncological data sets using machine learning techniques allow clinicians visualize data low dimension defined linear combination available markers rather time provides aid diagnosing similar forms cancer well means variable selection exploratory flow cytometric research refer method information preserving component analysis ipca\",\"clinical neuroscientific studies systematic differences two populations brain networks investigated order characterize mental diseases processes networks usually represented graphs built neuroimaging data studied means graph analysis methods typical machine learning approach study brain graphs creates classifier tests ability discriminate two populations contrast approach work propose directly test whether two populations graphs different using kernel twosample test ktst without creating intermediate classifier claim general two approaches provides similar results ktst requires much less computation additionally regime low sample size claim ktst lower frequency type error classification approach besides providing algorithmic considerations support claims show strong evidence experiments one simulation\",\"highdimensional tensors multiway data becoming prevalent areas biomedical imaging chemometrics networking bibliometrics traditional approaches finding lower dimensional representations tensor data include flattening data applying matrix factorizations principal components analysis pca employing tensor decompositions candecomp parafac tucker decompositions former lose important structure data latter higherorder pca hopca methods problematic highdimensions many irrelevant features introduce frameworks sparse tensor factorizations sparse hopca based heuristic algorithmic approaches solving penalized optimization problems related decomposition extensions approaches lead methods general regularized tensor factorizations multiway functional hopca generalizations hopca structured data illustrate utility methods dimension reduction feature selection signal recovery simulated data multidimensional microarrays functional mris\",\"sources variability experimentally derived data include measurement error addition physical phenomena interest measurement error combination systematic components originating measuring instrument random measurement errors several novel biological technologies mass cytometry singlecell rnaseq plagued systematic errors may severely affect statistical analysis data properly calibrated propose novel deep learning approach removing systematic batch effects method based residual network trained minimize maximum mean discrepancy mmd multivariate distributions two replicates measured different batches apply method mass cytometry singlecell rnaseq datasets demonstrate effectively attenuates batch effects\",\"decoding prediction brain images signals calls empirical evaluation predictive power evaluation achieved via crossvalidation method also used tune decoders hyperparameters paper review crossvalidation procedures decoding neuroimaging includes didactic overview relevant theoretical considerations practical aspects highlighted extensive empirical study common decoders withinand acrosssubject predictions multiple datasets anatomical functional mri meg simulations theory experiments outline popular leaveoneout strategy leads unstable biased estimates repeated random splits method preferred experiments outline large error bars crossvalidation neuroimaging settings typical confidence intervals nested crossvalidation tune decoders parameters avoiding circularity bias however find favorable use sane defaults particular nonsparse decoders\",\"translating potential disease biomarkers multispecies omics experiments new direction biomedical research existing methods limited simple experimental setups basic healthydiseased comparisons methods also require priori matching variables genes metabolites species however many experiments complicated multiway experimental design often involving irregularlysampled timeseries measurements instance metabolites always known matchings organisms introduce bayesian modelling framework translating multiple species results omics experiments complex multiway timeseries experimental design underlying assumption unknown matching inferred response variables multiple covariates including time\",\"introduce bayesian multitensor factorization model first bayesian formulation joint factorization multiple matrices tensors research problem generalizes joint matrixtensor factorization problem arbitrary sets tensors depth including matrices interpreted unsupervised multiview learning multiple data tensors generalized relax usual trilinear tensor factorization assumptions result factorization set tensors factors shared subsets tensors factors private individual tensors demonstrate performance existing baselines multiple tensor factorization tasks structural toxicogenomics functional neuroimaging\",\"extend multiway multivariate anovatype analysis cases one covariate view features view coming different highdimensional domains different views assumed connected paired samples common setup recent bioinformatics experiments analyze metabolite profiles different conditions disease control treatment untreated different tissues views introduce multiway latent variable model new task extending generative model bayesian canonical correlation analysis cca take multiway covariate information account population priors reducing dimensionality integrated factor analysis assumes metabolites come correlated groups\",\"extend kernelized matrix factorization fully bayesian treatment ability work multiple side information sources expressed different kernels kernel functions introduced matrix factorization integrate side information rows columns objects users recommender systems necessary making outofmatrix cold start predictions discuss specifically bipartite graph inference output matrix binary extensions general matrices straightforward extend state art two key aspects fully conjugate probabilistic formulation kernelized matrix factorization problem enables efficient variational approximation whereas fully bayesian treatments computationally feasible earlier approaches multiple side information sources included treated different kernels multiple kernel learning additionally reveals side information sources informative method outperforms alternatives predicting drugprotein interactions two data sets show framework also used solving multilabel learning problems considering samples labels two domains matrix factorization operates algorithm obtains lowest hamming loss values multilabel classification data sets compared five stateoftheart multilabel learning algorithms\",\"case control comparisons classical approach study neurological diseases however patients fall cleanly either group instead clinicians typically find patients cannot classified clearly progressed disease state subjects little said brain function basis analyses group differences describe intermediate brain function requires models interpolate disease states chosen gaussian processes regression obtain continuous spectrum brain activation extract unknown disease progression profile models incorporate spatial distribution measures activation correlation fmri trace input stimulus constitute ultrahigh multivariate regressors applied gps model fmri image phenotypes across alzheimers disease behavioural measures mmse ace etc scores obtained predictions nonobserved mmseace values overall model confirmed known reduction spatial extent activity response reading versus falsefont stimulation predictive uncertainty indicated worsening confidence intervals behavioural scores distance used training thus model indicated type patient behavioural score would need included training data improve models predictions\",\"explosion interest functional magnetic resonance imaging mri past two decades naturally accompanied many major advances understanding human connectome advances served pose novel challenges well open new avenues research one promising exciting avenues study functional mri realtime studies recently gained momentum applied wide variety settings ranging training healthy subjects selfregulate neuronal activity suggested potential treatments clinical populations date vast majority studies focused single region time due part many challenges faced estimating dynamic functional connectivity networks realtime work propose novel methodology accurately track changes functional connectivity networks realtime adapt recently proposed single algorithm estimating sparse temporally homo geneous dynamic networks applicable realtime proposed method applied motor task data human connectome project well realtime data tained exploring virtual environment show algorithm able estimate significant taskrelated changes network structure quickly enough useful future braincomputer interface applications\",\"diffusionweighted imaging dwi method currently measure connections different parts human brain vivo elucidate structure connections algorithms tracking bundles axonal fibers subcortical white matter rely local estimates fiber orientation distribution function fodf different parts brain functions describe relative abundance populations axonal fibers crossing location multiple models exist estimating fodfs quality resulting estimates quantified means suitable measure distance space fodfs however multiple distance metrics applied purpose including smoothed distances wasserstein metrics give four reasons use earth movers distance emd equipped arclength distance metric continued\",\"propose novel sparse tensor decomposition method namely tensor truncated power ttp method incorporates variable selection estimation decomposition components sparsity achieved via efficient truncation step embedded tensor power iteration method applies broad family high dimensional latent variable models including high dimensional gaussian mixture mixtures sparse regressions thorough theoretical investigation conducted particular show final decomposition estimator guaranteed achieve local statistical rate strengthen global statistical rate introducing proper initialization procedure high dimensional regimes obtained statistical rate significantly improves shown existing nonsparse decomposition methods empirical advantages ttp confirmed extensive simulated results two real applications clickthrough rate prediction highdimensional gene clustering\",\"predictive models used highdimensional brain images diagnosis clinical condition spatial regularization structured sparsity offers new perspectives context reduces risk overfitting model providing interpretable neuroimaging signatures forcing solution adhere domainspecific constraints total variation enforces spatial smoothness solution segmenting predictive regions background consider problem minimizing sum smooth convex loss nonsmooth convex penalty whose proximal operator known wide range possible complex nonsmooth convex structured penalties overlapping group lasso existing solvers either limited functions minimize practical capacity scale highdimensional imaging data nesterovs smoothing technique used minimize large number nonsmooth convex structured penalties reasonable precision requires small smoothing parameter slows convergence speed benefit versatility nesterovs smoothing technique propose first order continuation algorithm conesta automatically generates sequence decreasing smoothing parameters generated sequence maintains optimal convergence speed towards globally desired precision main contributions propose expression duality gap probe current distance global optimum order adapt smoothing parameter convergence speed provide convergence rate improvement classical proximal gradient smoothing methods demonstrate simulated highdimensional structural neuroimaging data conesta significantly outperforms many stateoftheart solvers regard convergence speed precision\",\"propose macau powerful flexible bayesian factorization method heterogeneous data model factorize set entities relations represented relational model including tensors also multiple relations entity macau also incorporate side information specifically entity relation features crucial predicting sparsely observed relations macau scales millions entity instances hundred millions observations sparse entity features millions dimensions achieve scale specially designed sampling procedure entity relation features relies primarily noise injection linear regressions show performance advanced features macau set experiments including challenging drugprotein activity prediction task\",\"central goal neuroscience understand activity nervous system related features external world features nervous system common approach model neural responses weighted combination external features vice versa structure model weights provide insight neural representations often neural inputoutput relationships sparse inputs contributing output part account sparsity structured regularizers incorporated model fitting optimization however imposing priors structured regularizers make difficult interpret learned model parameters investigate simple minimally structured model estimation method accurate unbiased estimation sparse models based bootstrapped adaptive threshold selection followed ordinary leastsquares refitting boats extensive numerical investigations show method often performs favorably compared regularizers particular variety model distributions noise levels boats accurately recovers parameters sparse models leading parsimonious explanations outputs finally apply method task decoding human speech production ecog recordings\",\"introduce factor analysis model summarizes dependencies observed variable groups instead dependencies individual variables standard factor analysis group may correspond one view set objects one many data sets tied cooccurrence set alternative variables collected statistics tables measure one property interest show assuming groupwise sparse factors active subset sets variation decomposed factors explaining relationships sets factors explaining away setspecific variation formulate assumptions bayesian model provides factors apply model two data analysis tasks neuroimaging chemical systems biology\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"6_brain_data_neuroimaging\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"6_brain_data_neuroimaging\"],\"textfont\":{\"size\":12},\"x\":[3.8317187,3.3487558,2.6061316,2.8864667,3.1957498,3.4743657,3.583435,3.1388373,3.2117918,3.3666666,3.0598903,3.3478682,3.317637,3.3795278,3.4102428,3.417025,2.72776,2.6357696,3.3622248,3.355492,3.3212595,2.9212246,2.7706313,3.334816,3.335176,3.26485,3.5509193,2.7430844,3.3381867,3.3899417,3.2602825,2.7228687,2.8133392,2.4547381,3.4288566,3.745951,3.1695228,2.6464393,3.266548,2.7974024,3.3424916,2.7975602,3.1684155],\"y\":[4.3056307,4.1793413,4.740073,4.558386,3.9747524,4.1939864,4.3242173,4.2995496,4.067566,4.1457324,4.5927534,4.1553183,4.121909,4.171243,4.1401687,4.1873055,4.4974036,4.4164104,4.556442,4.1584134,4.6065683,4.4097743,4.519011,4.1523323,4.6042695,4.62843,4.1826086,4.5722623,4.7669306,4.179702,4.8186965,4.5706177,4.5670514,4.4694405,4.148514,4.3620167,3.9847233,4.6679125,4.0639877,4.5125647,4.0710545,4.518663,4.361041],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"distributional distributionvalued data new type data arising several sources considered realizations distributional variables new set fuzzy cmeans algorithms data described distributional variables proposed algorithms use wasserstein distance distributions dissimilarity measures beside extension fuzzy cmeans algorithm distributional data considering decomposition squared wasserstein distance propose set algorithms using different automatic way compute weights associated variables well components globally clusterwise relevance weights computed clustering process introducing producttoone constraints relevance weights induce adaptive distances expressing importance variable component clustering process acting also variable selection method clustering tested proposed algorithms artificial realworld data results confirm proposed methods able better take account cluster structure data respect standard fuzzy cmeans nonadaptive distances\",\"paper propose new method predict final destination vehicle trips based initial partial trajectories first review obtained clustering trajectories describes user behaviour explain model main traffic flow patterns mixture gaussian distributions yielded density based clustering locations produces data driven grid similar points within pattern present model used predict final destination new trajectory based first locations using two step procedure first assign new trajectory clusters mot likely belongs secondly use characteristics trajectories inside clusters predict final destination finally present experimental results methods classification trajectories final destination prediction datasets timestamped gpslocation taxi trips test methods two different datasets assess capacity method adapt automatically different subsets\",\"show objective function conventional kmeans clustering expressed frobenius norm difference data matrix low rank approximation data matrix short show kmeans clustering matrix factorization problem notes meant reference intended provide guided tour towards result often mentioned seldom made explicit literature\",\"improve current instabilitybased methods selection number clusters cluster analysis developing normalized cluster instability measure corrects distribution cluster sizes previously unaccounted driver cluster instability show normalized instability measure outperforms current instabilitybased measures across whole sequence possible especially overcomes limitations context large also compare first time modelbased modelfree approaches determine clusterinstability find performance comparable make method available rpackage verbcstab\",\"propose novel method multiple clustering assumes coclustering structure partitions rows columns data matrix view new method applicable highdimensional data based nonparametric bayesian approach number views number featuresubject clusters inferred datadriven manner simultaneously model different distribution families gaussian poisson multinomial distributions cluster block makes method applicable datasets consisting numerical categorical variables biomedical data typically clustering solutions based variational inference mean field approximation apply proposed method synthetic real data show method outperforms multiple clustering methods recovering true cluster structures computation time finally apply method depression dataset true cluster structure available useful inferences drawn possible clustering structures data\",\"several application domains highdimensional observations collected analysed search naturally occurring data clusters might provide insights nature problem paper describe new approach partitioning highdimensional data assumption within cluster data approximated well linear subspace estimated means principal component analysis pca proposed algorithm predictive subspace clustering psc partitions data clusters simultaneously estimating clusterwise pca parameters algorithm minimises objective function depends upon new measure influence pca models penalised version algorithm also described carrying simultaneous subspace clustering variable selection convergence psc discussed detail extensive simulation results comparisons competing methods presented comparative performance psc assessed six real gene expression data sets psc often provides stateofart results\",\"present methodology clustering objects described multivariate time series several sequences realvalued random variables clustering methodology leverages copulas distributions encoding dependence structure several random variables take fully account dependence information clustering need distance copulas work compare renowned distances distributions fisherrao geodesic distance related divergences optimal transport discuss advantages disadvantages applications methodology found clustering financial assets tutorial experiments implementation reproducible research found wwwdatagrapplecomtech\",\"mean shift clustering finds modes data probability density identifying zero points density gradient since require fix number clusters advance mean shift popular clustering algorithm various application fields typical implementation mean shift first estimate density kernel density estimation compute gradient however since good density estimation necessarily imply accurate estimation density gradient indirect twostep approach reliable paper propose method directly estimate gradient logdensity without going density estimation proposed method gives global solution analytically thus computationally efficient develop meanshiftlike fixedpoint algorithm find modes density clustering mean shift one need set number clusters advance empirically show proposed clustering method works much better mean shift especially highdimensional data experimental results indicate proposed method outperforms existing clustering methods\",\"adjusted chance measures widely used compare partitionsclusterings data set particular adjusted rand index ari based paircounting adjusted mutual information ami based shannon information theory popular clustering community nonetheless open problem best application scenarios measure guidelines literature usage sparse result users often resort using generalized information theoretic measures based tsallis entropy shown link paircounting shannon measures paper aim bridge gap adjustment measures based paircounting measures based information theory solve key technical challenge analytically computing expected value variance generalized measures allows propose adjustments generalized measures reduce well known adjusted clustering comparison measures special cases using theory generalized measures able propose following guidelines using ari ami external validation indices ari used reference clustering large equal sized clusters ami used reference clustering unbalanced exist small clusters\",\"distancebased hierarchical clustering methods widely used unsupervised data analysis authors take account uncertainty distance data incorporate statistical model uncertainty corruption noise pairwise distances investigate problem estimating unknown parameters measurements specifically focus single linkage hierarchical clustering slhc study geometry prove fairly reasonable conditions probability distribution governing measurements slhc equivalent maximum partial profile likelihood estimation mpple information contained data ignored time show direct evaluation slhc maximum likelihood estimation mle pairwise distances yields consistent estimator consequently full mle expected perform better slhc getting correct results ground truth metric\",\"standard clustering problems data points represented vectors stacking together one forms data matrix row column cluster structure paper consider class binary matrices arising many applications exhibit row column cluster structure goal exactly recover underlying row column clusters observing small fraction noisy entries first derive lower bound minimum number observations needed exact cluster recovery propose three algorithms different running time compare number observations needed successful cluster recovery analytical results show smooth timedata tradeoffs one gradually reduce computational complexity increasingly observations available\",\"explore performance several automatic bandwidth selectors originally designed density gradient estimation databased procedures nonparametric modal clustering key tool obtain clustering density gradient estimators mean shift algorithm allows obtain partition data sample also whole space results simulation study suggest methods considered like cross validation plug bandwidth selectors useful cluster analysis via mean shift algorithm\",\"consider problem clustering sequence multinomial observations way model selection criterion propose form penalty term model selection procedure approach subsumes conventional aic bic criteria also extends conventional criteria way applicable also sequence sparse multinomial observations even within cluster number multinomial trials may different different observations addition preliminary estimation step maximum likelihood estimation generally maximum estimation propose use reduced rank projection combination nonnegative factorization motivate approach showing model selection criterion preliminary estimation step yield consistent estimates simplifying assumptions also illustrate approach numerical experiments using real simulated data\",\"derive statistical model estimation dendrogram single linkage hierarchical clustering slhc takes account uncertainty noise corruption measurements separation data focus estimation hierarchy partitions afforded dendrogram rather heights latter concept estimating dendrogram structure introduced approximate maximum likelihood estimator mle dendrogram structure described ideas illustrated simple monte carlo simulation least small data sets suggests method outperforms slhc presence noise\",\"popular method selecting number clusters based stability arguments one chooses number clusters corresponding clustering results stable recent years series papers analyzed behavior method theoretical point view however results technical difficult interpret nonexperts paper give highlevel overview existing literature clustering stability addition presenting results slightly informal accessible way relate discuss different implications\",\"investigations performed using clustering methods data mining timeseries data smart meters problem identify patterns trends energy usage profiles commercial industrial customers hour periods group similar profiles tested method energy usage data provided several power utilities results show accurate grouping accounts similar energy usage patterns potential method utilized energy efficiency programs\",\"determination cluster centers generally depends scale use analyze data clustered inappropriate scale usually leads unreasonable cluster centers thus unreasonable results study first consider similarity elements data connectivity nodes undirected graph present concept connection center regard cluster center data based definition determination cluster centers assignment class simple natural effective one crucial finding cluster centers different scales obtained easily different powers similarity matrix change power small large leads dynamic evolution cluster centers local microscopic global microscopic process evolution number categories changes discontinuously means presented method automatically skip unreasonable number clusters suggest appropriate observation scales provide corresponding cluster results\",\"symbolic data analysis based special descriptions data symbolic objects descriptions preserve detailed information units clusters usual representations mean values special kind symbolic object representation frequency probability distributions modal values representation enables consider clustering process variables measurement types time paper clustering criterion function sos proposed representative cluster composed distributions variables values cluster corresponding leaders clustering method based result also shown corresponding agglomerative hierarchical method generalized wards formula holds methods compatible solving clustering optimization problem leaders method efficiently solves clustering problems large number units agglomerative method applied alone smaller data set could applied leaders obtained compatible nonhierarchical clustering method combination two compatible methods enables decide upon right number clusters basis corresponding dendrogram proposed methods applied different data sets paper results clustering ess data presented\",\"construct framework studying clustering algorithms includes two key ideas persistence functoriality first encodes idea output clustering scheme carry multiresolution structure second idea one able compare results clustering algorithms one varies data set example adding points applying functions show within framework one prove theorem analogous one kleinberg one obtains existence uniqueness theorem instead nonexistence result explore properties unique scheme stability convergence established\",\"present technique clustering categorical data generating many dissimilarity matrices averaging begin demonstrating technique low dimensional categorical data comparing several techniques proposed give conditions method yield good results general method extends high dimensional categorical data equal lengths ensembling many choices explanatory variables context compare method two methods finally extend method high dimensional categorical data vectors unequal length using alignment techniques equalize lengths give examples show method continues provide good results particular better context genome sequences clusterings suggested phylogenetic trees\",\"density mathbb highdensity cluster connected component geq lambda lambda set highdensity clusters forms hierarchy called cluster tree present two procedures estimating cluster tree given samples first robust variant single linkage algorithm hierarchical clustering second based knearest neighbor graph samples give finitesample convergence rates algorithms also imply consistency derive lower bounds sample complexity cluster tree estimation finally study tree pruning procedure guarantees milder conditions usual remove clusters spurious recovering salient\",\"paper propose new fuzzy clustering algorithm based modeseeking framework given dataset mathbbrd define regions high density call cluster cores consider random walk neighborhood graph built top data points designed attracted high density regions strength attraction controlled temperature parameter beta membership point given cluster probability random walk hit corresponding cluster core many properties random walks hitting times commute distances etcdots shown enventually encode purely local information number data points grows show regularization introduced use cluster cores solves issue empirically show choice beta influences behavior algorithm small values beta result close hard modeseeking whereas beta close result similar output fuzzy spectral clustering finally demonstrate scalability approach providing fuzzy clustering protein configuration dataset containing million data points dimensions\",\"dendrograms used data analysis ultrametric spaces hence objects nonarchimedean geometry known exist padic representation dendrograms completed point infinity viewed subtrees bruhattits tree associated padic projective line implications certain moduli spaces known algebraic geometry padic parameter spaces families dendrograms stochastic classification also handled within framework end calculate topology hidden part dendrogram\",\"paper formulate general terms approach prove strong consistency empirical risk minimisation inductive principle applied prototype distance based clustering approach motivated divisive informationtheoretic feature clustering model probabilistic space kullbackleibler divergence may regarded special case within clustering minimisation framework also propose clustering regularization restricting creation additional clusters significant essentially different comparing existing clusters\",\"mixture gaussians fit single curved heavytailed cluster report data contains many clusters produce appropriate clusterings introduce model warps latent mixture gaussians produce nonparametric cluster shapes possibly lowdimensional latent mixture model allows summarize properties highdimensional clusters density manifolds describing data number manifolds well shape dimension manifold automatically inferred derive simple inference scheme model analytically integrates mixture parameters warping function show model effective density estimation performs better infinite gaussian mixture models recovering true number clusters produces interpretable summaries highdimensional datasets\",\"investigate role initialization stability kmeans clustering algorithm opposed papers consider actual kmeans algorithm ignore property getting stuck local optima interested actual clustering costs solution analyze different initializations lead local optimum lead different local optima enables prove reasonable select number clusters based stability scores\",\"paper discusses package implements pattern sequence based forecasting psf algorithm developed univariate time series forecasting algorithm successfully applied many different fields psf algorithm consists two major parts clustering prediction clustering part includes selection optimum number clusters labels time series data reference clusters prediction part includes functions like optimum window size selection specific patterns prediction future values reference past pattern sequences psf package consists various functions implement psf algorithm also contains function automates functions obtain optimized prediction results aim package promote psf algorithm ease implementation minimum efforts paper describes functions psf package syntax also provides simple example usage finally usefulness package discussed comparing autoarima ets wellknown time series forecasting functions available cran repository\",\"gaussian mixture models gmm found many applications density estimation data clustering however model adapt well curved strongly nonlinear data recently appeared improvement called acagmm active curve axis gaussian mixture model fits gaussians along curves using emlike expectation maximization approach using ideas standing behind acagmm build alternative active function model clustering advantages acagmm particular naturally defined arbitrary dimensions enables easy adaptation clustering complicated datasets along predefined family functions moreover need external methods determine number clusters automatically reduces number groups online\",\"paper proposes original approach cluster multicomponent data sets including estimation number clusters construction minimal spanning tree prims algorithm assumption vertices approximately distributed according poisson distribution number clusters estimated thresholding prims trajectory corresponding cluster centroids computed order initialize generalized lloyds algorithm also known kmeans allows circumvent initialization problems results derived evaluating false positive rate cluster detection algorithm help approximations relevant euclidean spaces metrics used measuring similarity multidimensional data points based symmetrical divergences use informational divergences together proposed method leads better results compared clustering methods problem astrophysical data processing applications method multihyperspectral imagery domain satellite view paris image mars planet also presented order demonstrate usefulness divergences problem method informational divergence similarity measure compared method using classical metrics astrophysics application also compare method spectral clustering algorithms\",\"describe many vantage points baire metric use clustering data use preprocessing structuring data order support search retrieval operations cases proceed directly clusters directly determine distances show hierarchical clustering read directly one pass data offer insights also practical implications precision data measurement mechanism treating multidimensional data including high dimensional data use random projections\",\"many situations interest lies identifying clusters one might expect available variables carry information groups furthermore data quality outliers missing entries might present serious sometimes hardtoassess problem large complex datasets paper show small proportion atypical observations might serious adverse effects solutions found sparse clustering algorithm witten tibshirani propose robustification sparse kmeans algorithm based trimmed kmeans algorithm cuestaalbertos proposal also able handle datasets missing values illustrate use method microarray data cancer patients able identify strong biological clusters much reduced number genes simulation studies show outliers data robust sparse kmeans algorithm performs better competing methods terms selection features also identified clusters robust sparse kmeans algorithm implemented package rskc publicly available cran repository\",\"traditionally practitioners initialize kmeans algorithm centers chosen uniformly random randomized initialization uneven weights kmeans recently used improve performance strategy cost runtime consider kmeans problem semisupervised information data prelabeled seek label rest according minimum cost solution extending kmeans algorithm analysis account labels derive improved theoretical bound expected cost observe improved performance simulated real data examples analysis provides theoretical justification roughly linear semisupervised clustering algorithm\",\"consider problem sparse clustering assumed subset features useful clustering purposes framework cosa method friedman meulman subsequently improved form sparse kmeans method witten tibshirani natural simpler hillclimbing approach introduced new method shown competitive two methods others\",\"fundamental aim clustering algorithms partition data points consider tasks discovered partition allowed vary covariate space time one approach would use fragmentationcoagulation processes markov processes restricted linear tree structured covariate spaces define partitionvalued process arbitrary covariate space using gaussian processes use process construct multitask clustering model partitions datapoints similar way across multiple data sources time series model network data allows cluster assignments vary time describe sampling algorithms inference apply method defining cancer subtypes based different types cellular characteristics finding regulatory modules gene expression data multiple human populations discovering time varying community structure social network\",\"conceptual framework cluster analysis viewpoint padic geometry introduced describing space dendrograms datapoints relating moduli space padic riemannian spheres punctures using method recently applied murtagh method embeds dendrogram subtree bruhattits tree associated padic numbers goes back cornelissen padic geometry explaining definitions concept classifiers discussed context moduli spaces upper bounds number hidden vertices dendrograms given\",\"logdensity gradient estimation fundamental statistical problem possesses various practical applications clustering measuring nongaussianity naive twostep approach first estimating density taking loggradient unreliable accurate density estimate necessarily lead accurate logdensity gradient estimate cope problem method directly estimate logdensity gradient without density estimation explored demonstrated work much better twostep method objective paper improve performance direct method multidimensional cases idea regard problem logdensity gradient estimation dimension task apply regularized multitask learning direct logdensity gradient estimator experimentally demonstrate usefulness proposed multitask method logdensity gradient estimation modeseeking clustering\",\"datasets mixture numerical categorical attributes routinely encountered many application domains work examine approach clustering datasets using homogeneity analysis homogeneity analysis determines euclidean representation data analyzed leveraging large body tools techniques data euclidean representation experiments conducted part study suggest approach useful analysis exploration big datasets mixture numerical categorical attributes\",\"many modern data mining applications concerned analysis datasets observations described paired highdimensional vectorial representations views typical examples found web mining genomics applications article present algorithm data clustering multiple views multiview predictive partitioning mvpp relies novel criterion predictive similarity data points assume within cluster dependence multivariate views modelled using twoblock partial least squares tbpls regression model performs dimensionality reduction particularly suitable highdimensional settings proposed mvpp algorithm partitions data withincluster predictive ability views maximised proposed objective function depends measure predictive influence points tbpls model derived extension press statistic commonly used ordinary least squares regression using simulated data compare performance mvpp competing multiview clustering methods rely upon geometric structures points ignore predictive relationship two views stateofart results obtained benchmark web mining datasets\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"7_clustering_cluster_clusters\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"7_clustering_cluster_clusters\"],\"textfont\":{\"size\":12},\"x\":[2.6738465,2.8100293,2.2554176,2.6137605,2.6343975,2.5657752,2.6612713,2.8599296,2.6364706,2.6516225,2.2856872,2.8828256,2.5486693,2.594367,2.6099339,2.6947305,2.6953363,2.738992,2.6189,2.6354258,3.1420734,2.9155278,2.4733138,2.6022644,2.8751645,2.5354333,0.334061,2.875214,2.5416648,2.449521,2.5413299,2.366642,2.4356294,3.1097584,2.479161,2.8758898,2.7130866,2.6242137,2.5936143],\"y\":[5.946071,5.874779,6.1033745,6.2249517,5.9501476,5.9587154,5.965073,5.659764,6.0591884,6.032438,6.04564,5.655231,6.1114373,5.8240256,6.257661,5.9903045,6.199127,6.0365047,6.2175927,5.874167,6.167213,6.11447,5.5905347,6.0399675,5.6325464,6.2457557,0.7541322,5.6348934,5.7852564,5.6379685,5.9526577,6.1290216,6.045846,6.017133,5.7068024,5.6699977,5.622271,5.8560534,5.804966],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"learn structure markov network two groups random variables joint observations since modelling learning full structure may hard learning links two groups directly may preferable option introduce novel concept called emphpartitioned ratio whose factorization directly associates markovian properties random variables across two groups simple oneshot convex optimization procedure proposed learning emphsparse factorizations partitioned ratio theoretically guaranteed recover correct intergroup structure mild conditions performance proposed method experimentally compared state art structure learning methods using roc curves real applications analyzing bipartisanship congress pairwise dnatimeseries alignments also reported\",\"develop square root graphical models sqr novel class parametric graphical models provides multivariate generalizations univariate exponential family distributions previous multivariate graphical models yang allow positive dependencies exponential poisson generalizations however many realworld datasets variables clearly positive dependencies example airport delay time new yorkmodeled exponential distributionis positively related delay time boston motivation give example model class derived univariate exponential distribution allows almost arbitrary positive negative dependencies mild condition parameter matrixa condition akin positive definiteness gaussian covariance matrix poisson generalization allows positive negative dependencies without constraints parameter values also develop parameter estimation methods using nodewise regressions ell regularization likelihood approximation methods using sampling finally demonstrate exponential generalization synthetic dataset realworld dataset airport delay times\",\"analyzing understanding structure complex relational data important many applications including analysis connectivity human brain networks prominent patterns different scales calling hierarchically structured model propose two nonparametric bayesian hierarchical network models based gibbs fragmentation tree priors demonstrate ability capture nested patterns simulated networks real networks demonstrate detection hierarchical structure show predictive performance par state art envision methods employed exploratory analysis large scale complex networks example model human brain connectivity\",\"consider structure learning problem graphical models call loosely connected markov random fields number short paths pair nodes small present new conditional independence test based algorithm learning underlying graph structure novel maximization step algorithm ensures true edges detected correctly even short cycles graph number samples required algorithm clog size graph constant depends parameters model show several previously studied models examples loosely connected markov random fields algorithm achieves lower computational complexity previously designed algorithms individual cases also get new results general graphical models particular algorithm learns general ising models erdosrenyi random graph correctly running time onp\",\"present procedure effective estimation entropy mutual information smallsample data apply problem inferring highdimensional gene association networks specifically develop jamessteintype shrinkage estimator resulting procedure highly efficient statistically well computationally despite simplicity show outperforms eight entropy estimation procedures across diverse range sampling scenarios datagenerating models even cases severe undersampling illustrate approach analyzing coli gene expression data computing entropybased geneassociation network gene expression data computer program available implements proposed shrinkage estimator\",\"introduce network maximal correlation nmc multivariate measure nonlinear association among random variables nmc defined via optimization infers transformations variables maximizing aggregate inner products transformed variables finite discrete jointly gaussian random variables characterize solution nmc optimization using basis expansion functions appropriate basis functions finite discrete variables propose algorithm based alternating conditional expectation determine nmc moreover propose distributed algorithm compute approximation nmc large dense graphs using graph partitioning finite discrete variables show probability discrepancy greater given level nmc nmc computed using empirical distributions decays exponentially fast sample size grows jointly gaussian variables show conditions nmc optimization instance maxcut problem illustrate application nmc inference graphical model bijective functions jointly gaussian variables finally show nmcs utility data application learning nonlinear dependencies among genes cancer dataset\",\"propose new method detecting changes markov network structure two sets samples instead naively fitting two markov network models separately two data sets figuring difference emphdirectly learn network structure change estimating ratio markov network models densityratio formulation naturally allows introduce sparsity network structure change highly contributes enhancing interpretability furthermore computation normalization term critical bottleneck naive approach remarkably mitigated also give dual formulation optimization problem reduces computation cost largescale markov networks experiments demonstrate usefulness method\",\"consider two connected aspects maximum likelihood estimation parameter highdimensional discrete graphical models existence maximum likelihood estimate mle computation data sparse many zeros contingency table maximum likelihood estimate parameter may exist fienberg rinaldo shown mle exists iff data vector belongs face socalled marginal cone spanned rows design matrix model identifying faces highdimension challenging paper take local approach show one face albeit possibly smallest one identified looking collection marginal graphical models generated induced subgraphs giildotsk first contribution second contribution concerns composite maximum likelihood estimate dimension problem large estimating parameters given graphical model maximum likelihood challenging impossible traditional approach problem local use composite likelihood based local conditional likelihoods recent development components composite likelihood marginal likelihoods centred around first show estimates obtained consensus local conditional marginal likelihoods identical study asymptotic properties composite maximum likelihood estimate dimension model sample size infinity\",\"study graph estimation density estimation high dimensions using family density estimators based forest structured undirected graphical models density estimation assume true distribution corresponds forest rather form kernel density estimates bivariate univariate marginals apply kruskals algorithm estimate optimal forest held data prove oracle inequality excess risk resulting estimator relative risk best forest graph estimation consider problem estimating forests restricted tree sizes prove finding maximum weight spanning forest restricted tree size nphard develop approximation algorithm problem viewing tree size complexity parameter select forest using data splitting prove bounds excess risk structure selection consistency procedure experiments simulated data microarray data indicate methods practical alternative gaussian graphical models\",\"introduce truncated gaussian graphical model tggm novel framework designing statistical models nonlinear learning tggm gaussian graphical model ggm subset variables truncated nonnegative truncated variables assumed latent integrated induce marginal model show variables marginal model nongaussian distributed expected relations nonlinear use expectationmaximization break inference nonlinear model sequence tggm inference problems efficiently solved using properties numerical methods multivariate gaussian distributions use tggm design models nonlinear regression classification performances models demonstrated extensive benchmark datasets compared stateoftheart competing results\",\"propose new class semiparametric exponential family graphical models analysis high dimensional mixed data different existing mixed graphical models allow nodewise conditional distributions semiparametric generalized linear models unspecified base measure functions thus one advantage method unnecessary specify type node method convenient apply practice proposed model consider problems parameter estimation hypothesis testing high dimensions particular propose symmetric pairwise score test presence single edge graph compared existing methods hypothesis tests approach takes account symmetry parameters inferential results invariant respect different parametrizations edge thorough numerical simulations real data example provided back results\",\"theory graphical models matured three decades provide backbone several classes models used myriad applications genetic mapping diseases credit risk evaluation reliability computer security etc despite generic applicability wide adoptance constraints imposed undirected graphical models bayesian networks also recognized unnecessarily stringent certain circumstances observation led proposal several generalizations aim relaxed constraints models impose local contextspecific dependence structures consider additional class models termed stratified graphical models develop method bayesian learning models deriving analytical expression marginal likelihood data specific subclass decomposable stratified models nonreversible markov chain monte carlo approach used identify models highly supported posterior distribution model space method illustrated compared ordinary graphical models application several real synthetic datasets\",\"loglinear models popular workhorses analyzing contingency tables loglinear parameterization interaction model expressive direct parameterization based probabilities leading powerful way defining restrictions derived marginal conditional contextspecific independence however parameter estimation often simpler direct parameterization provided model enjoys certain decomposability properties introduce cyclical projection algorithm obtaining maximum likelihood estimates loglinear parameters arbitrary contextspecific graphical loglinear model needs satisfy criteria decomposability illustrate lifting restriction decomposability makes models expressive additional contextspecific independencies embedded real data identified also shown contextspecific graphical model correspond nonhierarchical loglinear parameterization concise interpretation observation pave way development nonhierarchical loglinear models largely neglected due believed lack interpretability\",\"consider structure discovery undirected graphical models observational data inferring likely structures examples complex task often requiring formulation priors sophisticated inference procedures popular methods rely estimating penalized maximum likelihood precision matrix however approaches structure recovery indirect consequence datafit term penalty difficult adapt domainspecific knowledge inference computationally demanding contrast may easier generate training samples data arise graphs desired structure properties propose leverage latter source information training data learn function parametrized neural network maps empirical covariance matrices estimated graph structures learning function brings two benefits implicitly models desired structure sparsity properties form suitable priors tailored specific problem edge structure discovery rather maximizing data likelihood applying framework find learnable graphdiscovery method trained synthetic data generalizes well identifying relevant edges synthetic real data completely unknown training time find genetics brain imaging simulation data obtain performance generally superior analytical methods\",\"recent methods estimating sparse undirected graphs realvalued data high dimensional problems rely heavily assumption normality show use semiparametric gaussian copulaor nonparanormalfor high dimensional inference additive models extend linear models replacing linear functions set onedimensional smooth functions nonparanormal extends normal transforming variables smooth functions derive method estimating nonparanormal study methods theoretical properties show works well many examples\",\"investigate generic problem learning pairwise exponential family graphical models pairwise sufficient statistics defined global mapping function mercer kernels subclass pairwise graphical models allow flexibly capture complex interactions among variables beyond pairwise product propose two ellnorm penalized maximum likelihood estimators learn model parameters iid samples first one joint estimator estimates parameters simultaneously second one nodewise conditional estimator estimates parameters individually node estimators show proper conditions extra flexibility gained model comes almost cost statistical computational efficiency demonstrate advantages model stateoftheart methods synthetic real datasets\",\"study problem learning sparse structure changes two markov networks rather fitting two markov networks separately two sets data figuring differences recent work proposed learn changes emphdirectly via estimating ratio two markov network models paper give sufficient conditions emphsuccessful change detection respect sample size dimension data number changed edges using unbounded density ratio model prove true sparse changes consistently identified omegad log fracmm omeganp exponentially decaying upperbound learning error sample complexity improved minnp omegad log fracmm boundedness density ratio model assumed theoretical guarantee applied wide range discretecontinuous markov networks\",\"gaussian graphical models widely used represent conditional dependence among random variables paper propose novel estimator data arising group gaussian graphical models dependent motivating example modeling gene expression collected multiple tissues individual multivariate outcome affected dependencies acting level specific tissues also level whole body existing methods assume independence among graphs applicable case estimate multiple dependent graphs decompose problem two graphical layers systemic layer affects outcomes thereby induces cross graph dependence categoryspecific layer represents graphspecific variation propose graphical technique estimates layers jointly establish estimation consistency selection sparsistency proposed estimator confirm simulation method superior simple onestep method apply technique mouse genomics data obtain biologically plausible results\",\"challenging problem estimating highdimensional graphical models choose regularization parameter datadependent way standard techniques include kfold crossvalidation kcv akaike information criterion aic bayesian information criterion bic though methods work well lowdimensional problems suitable high dimensional settings paper present stars new stabilitybased method choosing regularization parameter high dimensional inference undirected graphs method clear interpretation use least amount regularization simultaneously makes graph sparse replicable random sampling interpretation requires essentially conditions mild conditions show stars partially sparsistent terms graph estimation high probability true edges included selected model even graph size diverges sample size empirically performance stars compared stateoftheart model selection procedures including kcv aic bic synthetic data real microarray dataset stars outperforms competing procedures\",\"main contribution article new prior distribution directed acyclic graphs gives larger weight sparse graphs distribution intended structured bayesian networks structure given ordered block model nodes graph objects fall categories blocks blocks natural ordering presence relationship two objects denoted arrow object lower category object higher category models considered introduced kemp relational data extended multivariate data mansinghka prior graph structures presented explicit formula number nodes layer graph follow hoppe ewens urn model consider situation nodes graph represent random variables whose joint probability distribution factorises along dag describe monte carlo schemes finding optimal aposteriori structure given data matrix compare performance mansinghka also uniform prior\",\"paper propose semiparametric approach named nonparanormal skeptic efficiently robustly estimating high dimensional undirected graphical models achieve modeling flexibility consider gaussian copula graphical models nonparanormal proposed liu achieve estimation robustness exploit nonparametric rankbased correlation coefficient estimators including spearmans rho kendalls tau high dimensional settings prove nonparanormal skeptic achieves optimal parametric rate convergence graph parameter estimation celebrating result suggests gaussian copula graphical models used safe replacement popular gaussian graphical models even data truly gaussian besides theoretical analysis also conduct thorough numerical simulations compare different estimators graph recovery performance ideal noisy settings proposed methods applied largescale genomic dataset illustrate empirical usefulness language software package huge implementing proposed methods available comprehensive archive network httpcran rprojectorg\",\"paper proposes unified framework quantify local global inferential uncertainty high dimensional nonparanormal graphical models particular consider problems testing presence single edge constructing uniform confidence subgraph due presence unknown marginal transformations propose pseudo likelihood based inferential approach sharp contrast existing high dimensional score test method method free tuning parameters given initial estimator extends scope existing likelihood based inferential framework furthermore propose ustatistic multiplier bootstrap method construct confidence subgraph show constructed subgraph contained true graph probability greater given nominal level compared existing methods constructing confidence subgraphs method rely gaussian subgaussian assumptions theoretical properties proposed inferential methods verified thorough numerical experiments real data analysis\",\"consider problem changepoint detection multivariate timeseries multivariate distribution observations supposed follow graphical model whose graph parameters affected abrupt changes throughout time demonstrate possible perform exact bayesian inference whenever one considers simple class undirected graphs called spanning trees possible structures able integrate graph segmentation spaces time combining classical dynamic programming algebraic results pertaining spanning trees particular show quantities posterior distributions changepoints posterior edge probabilities time efficiently obtained illustrate results synthetic experimental data arising biology neuroscience\",\"gaussian graphical models ggms popular tools studying network structures however many modern applications gene network discovery social interactions analysis often involve highdimensional noisy data outliers heavier tails gaussian distribution paper propose trimmed graphical lasso robust estimation sparse ggms method guards outliers implicit trimming mechanism akin popular least trimmed squares method used linear regression provide rigorous statistical analysis estimator highdimensional setting contrast existing approaches robust sparse ggms estimation lack statistical guarantees theoretical results complemented experiments simulated real gene expression data demonstrate value approach\",\"tree structured graphical models powerful expressing long range hierarchical dependency among many variables widely applied different areas computer science statistics however existing methods parameter estimation inference structure learning mainly rely gaussian discrete assumptions restrictive many applications paper propose new nonparametric methods based reproducing kernel hilbert space embeddings distributions recover latent tree structures estimate parameters perform inference high dimensional continuous nongaussian variables usefulness proposed methods illustrated thorough numerical results\",\"paper presents foundational theoretical results distributed parameter estimation undirected probabilistic graphical models introduces general condition composite likelihood decompositions models guarantees global consistency distributed estimators provided local estimators consistent\",\"real world systems typically feature variety different dependency types topologies complicate model selection probabilistic graphical models introduce ensembleofforests model generalization ensembleoftrees model model enables structure learning markov random fields mrf multiple connected components arbitrary potentials present two approximate inference techniques model demonstrate performance synthetic data results suggest ensembleofforests approach accurately recover sparse possibly disconnected mrf topologies even presence nongaussian dependencies andor low sample size applied ensembleofforests model learn structure perturbed signaling networks immune cells found frequently exhibit nongaussian dependencies disconnected mrf topologies summary expect ensembleofforests model enable mrf structure learning high dimensional real world settings governed nontrivial dependencies\",\"graphical models commonly used tools modeling multivariate random variables exist many convenient multivariate distributions gaussian distribution continuous data mixed data presence discrete variables combination continuous discrete variables poses new challenges statistical modeling paper propose semiparametric model named latent gaussian copula model binary mixed data observed binary data assumed obtained dichotomizing latent variable satisfying gaussian copula distribution nonparanormal distribution latent gaussian model assumption latent variables multivariate gaussian special case proposed model novel rankbased approach proposed latent graph estimation latent principal component analysis theoretically proposed methods achieve rates convergence precision matrix estimation eigenvector estimation latent variables observed similar conditions consistency graph structure recovery feature selection leading eigenvectors established performance proposed methods numerically assessed simulation studies usage methods illustrated genetic dataset\",\"present novel kway highdimensional graphical model called generalized root model grm explicitly models dependencies variable sets size standard pairwise graphical model model based taking kth root original sufficient statistics univariate exponential family positive sufficient statistics including poisson exponential distributions recent work square root graphical sqr models inouye restricted pairwise dependencieswe give conditions parameters needed normalization using radial conditionals similar pairwise case inouye particular show poisson grm restrictions parameters exponential grm restriction akin negative definiteness develop simple general learning algorithm based lregularized nodewise regressions also present general way numerically approximating log partition function associated derivatives grm univariate node conditionalsin contrast inouye provided algorithm estimating exponential sqr illustrate grm model word counts poisson grm show associated ksized variable sets finish discussing methods reducing parameter space various situations\",\"bayesian networks convenient graphical expressions high dimensional probability distributions representing complex relationships large number random variables employed extensively areas bioinformatics artificial intelligence diagnosis risk management recovery structure network data prime importance purposes modeling analysis prediction recovery algorithms literature assume either discrete continuous gaussian data general continuous data discretization usually employed often destroys structure one recover friedman goldszmidt suggest approach based minimum description length principle chooses discretization preserves information original data set however one difficult impossible implement even moderately sized networks paper provide extremely efficient search strategy allows one use friedman goldszmidt discretization practice\",\"consider discrete graphical models markov respect graph propose two distributed marginal methods estimate maximum likelihood estimate canonical parameter model methods based relaxation marginal likelihood obtained considering density variables represented vertex neighborhood two methods differ size neighborhood show estimates consistent obtained larger neighborhood smaller asymptotic variance ones obtained smaller neighborhood\",\"bnlearn package includes several algorithms learning structure bayesian networks either discrete continuous variables constraintbased scorebased algorithms implemented use functionality provided snow package improve performance via parallel computing several network scores conditional independence algorithms available learning algorithms independent use advanced plotting options provided rgraphviz package\",\"high dimensions propose analyze aggregation estimator precision matrix gaussian graphical models estimator called graphical exponential screening ges linearly combines suitable set individual estimators different underlying graphs balances estimation error sparsity study risk aggregation estimator show comparable best estimator based single graph chosen oracle numerical performance method investigated using simulated real datasets comparison stateofart estimation procedures\",\"undirected graphical models known markov networks popular wide variety applications ranging statistical physics computational biology traditionally learning network structure done assumption chordality ensures efficient scoring methods used general nonchordal graphs intractable normalizing constants renders calculation bayesian scores difficult beyond smallscale systems recently surge interest towards use regularized pseudolikelihood methods structural learning largescale markov network models approach avoids assumption chordality currently available methods typically necessitate use tuning parameter adapt level regularization particular dataset optimized example crossvalidation introduce bayesian version pseudolikelihood scoring markov networks enables automatic regularization marginalization nuisance parameters model prove consistency resulting mpl estimator network structure via comparison pseudo information criterion identification mploptimal network prescanned graph space considered greedy hill climbing exact pseudoboolean optimization algorithms find reasonable sample sizes hill climbing approach often identifies networks negligible distance restricted global optimum using synthetic existing benchmark networks marginal pseudolikelihood method shown generally perform favorably recent popular inference methods markov networks\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"8_graphical_models_structure\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"8_graphical_models_structure\"],\"textfont\":{\"size\":12},\"x\":[4.8011475,4.929214,4.636289,4.997382,4.331645,4.711035,4.770281,4.7816663,4.651921,4.916304,4.649001,4.9735537,4.9976315,4.5045724,4.4888797,4.96435,4.793432,4.4831357,4.553994,5.1114535,4.5003133,4.710863,4.995438,4.454857,4.913551,4.8061137,4.4670067,4.3985586,4.943454,4.8445196,4.766333,4.9837356,4.5046606,4.8485193,4.74073],\"y\":[4.866119,4.1288342,4.853404,4.8836784,4.641595,4.6260834,4.8714848,4.120445,4.2123666,3.9961836,4.1524053,4.2101035,4.100424,4.768401,4.19173,4.088896,4.8946476,4.304386,4.3612595,4.776212,4.185647,4.118719,4.61074,4.4022083,4.063307,4.123759,4.7284436,4.0904384,4.1077633,4.675406,4.1395993,4.7295613,4.192516,4.742972,4.4105806],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"interested learning causal relationships pairs random variables purely observational data effectively address task stateoftheart relies strong assumptions regarding mechanisms mapping causes effects invertibility existence additive noise hold limited situations contrary short paper proposes learn perform causal inference directly data without need feature engineering particular pose causality kernel mean embedding classification problem inputs samples arbitrary probability distributions pairs random variables labels types causal relationships validate performance method synthetic realworld data stateoftheart moreover submitted algorithm chalearns fast causation coefficient challenge competition fastest code prize ranked third overall leaderboard\",\"information geometric causal inference igci new approach distinguish cause effect two variables based independence assumption input distribution causal mechanism phrased terms orthogonality information space describe two intuitive reinterpretations approach makes igci accessible broader audience moreover show described independence related hypothesis unsupervised learning semisupervised learning works predicting cause effect vice versa\",\"causal inference relies structure graph often directed acyclic graph dag different graphs may result different causal inference statements different intervention distributions quantify differences propose pre distance dags structural intervention distance sid sid based graphical criterion quantifies closeness two dags terms corresponding causal inference statements therefore wellsuited evaluating graphs used computing interventions instead dags also possible compare cpdags completed partially directed acyclic graphs represent markov equivalence classes since differs significantly popular structural hamming distance shd sid constitutes valuable additional measure discuss properties distance provide efficient implementation software code available first authors homepage package construction\",\"controlled interventions provide direct source information learning causal effects particular doseresponse curve learned varying treatment level observing corresponding outcomes however interventions expensive timeconsuming observational data treatment controlled known mechanism sometimes available strong assumptions observational data allows estimation doseresponse curves estimating curves nonparametrically hard sample sizes controlled interventions may small observational case large number measured confounders may need marginalized paper introduce hierarchical gaussian process prior constructs distribution doseresponse curve learning observational data reshapes distribution nonparametric affine transform learned controlled interventions function composition different sources shown speedup learning demonstrate thorough sensitivity analysis application modeling effect therapy cognitive skills premature infants\",\"learning causal effect observational data straightforward possible without assumptions hidden common causes treatment outcome cannot blocked measurements one possibility use instrumental variable principle possible assumptions discover whether variable structurally instrumental target causal effect rightarrow current frameworks somewhat lacking general assumptions instrumental variable discovery problem challenging variable tested instrument isolation groups different variables might require different conditions considered instrument moreover identification constraints might hard detect statistically paper give theoretical characterization instrumental variable discovery highlighting identifiability problems solutions need nongaussianity assumptions fit within existing methods\",\"present new methods estimate causal effects retrospectively micro data assistance machine learning ensemble approach overcomes two important limitations conventional methods like regression modeling matching ambiguity pertinent retrospective counterfactuals potential misspecification overfitting otherwise biasprone inefficient use large identifying covariate set estimation causal effects method targets analysis toward well defined retrospective intervention effect rie based hypothetical population interventions applies machine learning ensemble allows data guide controlled fashion use large identifying covariate set illustrate analysis policy options reducing excombatant recidivism colombia\",\"structural equation models bayesian networks widely used analyze causal relations continuous variables frameworks linear acyclic models typically used model datagenerating process variables recently shown use nongaussianity identifies full structure linear acyclic model causal ordering variables connection strengths without using prior knowledge network structure case conventional methods however existing estimation methods based iterative search algorithms may converge correct solution finite number steps paper propose new direct method estimate causal ordering connection strengths based nongaussianity contrast previous methods algorithm requires algorithmic parameters guaranteed converge right solution within small fixed number steps data strictly follows model\",\"causal inference deals identifying random variables cause control random variables recent advances topic causal inference based tools statistical estimation machine learning resulted practical algorithms causal inference causal inference potential significant impact medical research prevention control diseases identifying factors impact economic changes name however promising applications causal inference often ones involve sensitive personal data users need kept private medical records personal finances etc therefore need development causal inference methods preserve data privacy study problem inferring causality using current popular causal inference framework additive noise model anm simultaneously ensuring privacy users framework provides differential privacy guarantees variety anm variants run extensive experiments demonstrate techniques practical easy implement\",\"consider learn causal ordering variables linear nongaussian acyclic model called lingam several existing methods shown consistently estimate causal ordering assuming model assumptions correct estimation results could distorted assumptions actually violated paper propose new algorithm learning causal orders robust one typical violation model assumptions latent confounders demonstrate effectiveness method using artificial data\",\"paper addresses problem inferring sparse causal networks modeled multivariate autoregressive mar processes conditions derived group lasso glasso procedure consistently estimates sparse network structure key condition involves false connection score particular show consistent recovery possible even number observations network far less number parameters describing network provided false connection score less one false connection score also demonstrated useful metric recovery nonasymptotic regimes conditions suggest modified glasso procedure tends improve false connection score reduce chances reversing direction causal influence computational experiments real network based electrocorticogram ecog simulation study demonstrate effectiveness approach\",\"given data sampled number variables one often interested underlying causal relationships form directed acyclic graph general case without interventions variables possible identify graph markov equivalence class however situations one find true causal graph observational data example structural equation models additive noise nonlinear edge functions current methods achieving rely nonparametric independence tests one problems null hypothesis independence one would like get evidence take different approach work using penalized likelihood score model selection practically feasible many settings advantage yielding natural ranking candidate models making smoothness assumptions probability density space prove consistency penalized maximum likelihood estimator also present empirical results simulated scenarios real twodimensional data sets causeeffect pairs obtain similar results stateoftheart methods\",\"propose method infer causal structures containing discrete continuous variables idea select causal hypotheses conditional density every variable given causes becomes smooth define family smooth densities conditional densities second order exponential models maximizing conditional entropy subject first second statistical moments variables take values proper subsets conditionals induce different families joint distributions even markovequivalent graphs consider case one binary one realvalued variable method distinguish cause effect using example describe sometimes causal hypothesis must rejected peffectcause pcause share algorithmic information untypical chosen independently way method spirit faithfulnessbased causal inference also rejects nongeneric mutual adjustments among dagparameters\",\"consider learning possible causal direction two observed variables presence latent confounding variables several existing methods shown consistently estimate causal direction assuming linear type nonlinear relationship latent confounders however estimation results could distorted either assumption actually violated paper first propose new linear nongaussian acyclic structural equation model individualspecific effects allows latent confounders considered propose empirical bayesian approach estimating possible causal direction using new model demonstrate effectiveness method using artificial realworld data\",\"notion causality used many situations dealing uncertainty consider problem whether causality identified given data set generated discrete random variables rather continuous ones particular nonbinary data thus far known causality identified except rare cases paper present necessary sufficient condition integer modular acyclic additive noise iman two variables addition relate bivariate multivariate causal identifiability explicit manner develop practical algorithm find order variables parent sets demonstrate performance applications artificial data real world body motion data comparisons conventional methods\",\"many statistical methods proposed estimate causal models classical situations fewer variables observations number variables number observations however modern datasets including gene expression data need highdimensional causal modeling challenging situations orders magnitude variables observations paper propose method find exogenous variables linear nongaussian causal model requires much smaller sample sizes conventional methods works even key idea identify variables exogenous based nongaussianity instead estimating entire structure model exogenous variables work triggers activate causal chain model identification leads efficient experimental designs better understanding causal mechanism present experiments artificial data realworld gene expression data evaluate method\",\"linear nongaussian structural equation model called lingam identifiable model exploratory causal analysis previous methods estimate causal ordering variables connection strengths based single dataset however many application domains data obtained different conditions multiple datasets obtained rather single dataset paper present new method jointly estimate multiple lingams assumption models share causal ordering may different connection strengths differently distributed variables simulations new method estimates models accurately estimating separately\",\"discovery nonlinear causal relationship additive nongaussian noise models attracted considerable attention recently high flexibility paper propose novel causal inference algorithm called leastsquares independence regression lsir lsir learns additive noise model minimization estimator squaredloss mutual information inputs residuals notable advantage lsir existing approaches tuning parameters kernel width regularization parameter naturally optimized crossvalidation allowing avoid overfitting datadependent fashion experiments realworld datasets show lsir compares favorably stateoftheart causal inference method\",\"describe method inferring linear causal relations among multidimensional variables idea use asymmetry distributions cause effect occurs covariance matrix cause structure matrix mapping cause effect independently chosen method works stochastic deterministic causal relations provided dimensionality sufficiently high experiments enough applicable gaussian well nongaussian data\",\"provide theoretical empirical evidence type asymmetry causes effects present related via linear models contaminated additive nongaussian noise assuming causes effects distribution show distribution residuals linear fit anticausal direction closer gaussian distribution residuals causal direction gaussianization effect characterized reduction magnitude highorder cumulants increment differential entropy residuals problem nonlinear causal inference addressed performing embedding expanded feature space relation causes effects assumed linear effectiveness method discriminate causes effects based type asymmetry illustrated variety experiments using different measures gaussianity proposed method shown competitive stateoftheart techniques causal inference\",\"consider learning causal ordering variables linear nongaussian acyclic model called lingam several existing methods shown consistently estimate causal ordering assuming model assumptions correct estimation results could distorted assumptions actually violated paper propose new algorithm learning causal orders robust one typical violation model assumptions latent confounders key idea detect latent confounders testing independence estimated external influences find subsets parcels include variables affected latent confounders demonstrate effectiveness method using artificial data simulated brain imaging data\",\"consider problem structure learning bowfree acyclic path diagrams baps baps viewed generalization linear gaussian dag models allow certain hidden variables present first method problem using greedy scorebased search algorithm also prove necessary sufficient conditions distributional equivalence baps used algorithmic proach compute nearly equivalent model structures allows infer lower bounds causal effects also present applications real simulated datasets using publicly available rpackage\",\"one fundamental problems causal inference estimation causal effect variables confounded difficult observational study one direct evidence confounders adjusted introduce novel approach estimating causal effects exploits observational conditional independencies suggest weak paths unknown causal graph widely used faithfulness condition spirtes relaxed allow varying degrees path cancellations imply conditional independencies rule existence confounding causal paths outcome posterior distribution bounds average causal effect via linear programming approach bayesian inference claim approach used regular practice along default tools observational studies\",\"consider problem learning causal directed acyclic graphs observational joint distribution one use graphs predict outcome interventional experiments data often available show observational distribution follows structural equation model additive noise structure directed acyclic graph becomes identifiable distribution mild conditions constitutes interesting alternative traditional methods assume faithfulness identify markov equivalence class graph thus leaving edges undirected provide practical algorithms finitely many samples resit regression subsequent independence test two methods based independence score prove resit correct population setting provide empirical evaluation\",\"widely applied approach causal inference nonexperimental time series often referred linear granger causal analysis regress present past interpret regression matrix hatb causally however unmeasured time series influences approach lead wrong causal conclusions distinct one would draw one additional information paper take different approach assume together hidden forms first order vector autoregressive var process transition matrix argue valid interpret causally instead hatb examine conditions important parts identifiable almost identifiable essentially sufficient conditions nongaussian independent noise influence present two estimation algorithms tailored towards conditions respectively evaluate synthetic realworld data discuss check model using\",\"article contains detailed proofs additional examples related uai submission learning sparse causal models nphard describes fci algorithm method sound complete causal model discovery presence latent confounders andor selection bias worst case polynomial complexity order number independence tests sparse graphs nodes bounded node degree algorithm adaptation wellknown fci algorithm spirtes also sound complete worst case complexity exponential\",\"inferring causal structure set random variables finite sample joint distribution important problem science recently methods using additive noise models suggested approach case continuous variables many situations however variables interest discrete even finitely many states work extend notion additive noise models cases prove whenever joint distribution probxy admits model one direction yfxn independent admit reversed model xgytilde tilde independent long model chosen generic way based deliberations propose efficient new algorithm able distinguish cause effect finite sample discrete variables extensive experimental study show algorithm works synthetic real data sets\",\"paper aims justifying lwf amp chain graphs showing represent arbitrary independence models specifically show every chain graph inclusion optimal wrt intersection independence models represented set directed acyclic graphs conditioning implies independence model represented chain graph accounted set causal models subject selection bias turn accounted system switches different regimes configurations\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"9_causal_variables_acyclic\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"9_causal_variables_acyclic\"],\"textfont\":{\"size\":12},\"x\":[6.430534,6.4421363,6.2128654,6.526428,6.4647975,6.5048904,6.486545,6.4492316,6.5400705,6.182026,6.226881,6.4127393,6.5547857,6.4319663,6.438913,6.5185432,6.4789686,6.4608855,6.4679885,6.5326447,6.190464,6.426142,6.178652,6.48098,6.2155895,6.408992,6.0958014,6.3985353],\"y\":[6.1638227,6.1692586,6.0716906,6.189264,6.193438,6.2098207,6.2399745,6.1782,6.2752223,6.063235,6.079068,6.141576,6.281507,6.163633,6.180277,6.2535114,6.2081823,6.1729364,6.1923885,6.264245,6.03981,6.1915627,6.0405526,6.2147064,6.0753336,6.1464887,6.0428867,6.164541],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"paper consider problem stochastic optimization bandit feedback model generalize gpucb algorithm srinivas arbitrary kernels search spaces use notion localized chaining control supremum gaussian process provide novel optimization scheme based computation covering numbers theoretical bounds obtain cumulative regret generic present convergence rates gpucb algorithm finally algorithm shown empirically efficient natural competitors simple complex input spaces\",\"present informationtheoretic framework solving global blackbox optimization problems also blackbox constraints particular interest efficiently solve problems decoupled constraints subsets objective constraint functions may evaluated independently example objective evaluated cpu constraints evaluated independently gpu problems require acquisition function separated contributions individual function evaluations develop one acquisition function call predictive entropy search constraints pesc pesc approximation expected information gain criterion compares favorably alternative approaches based improvement several synthetic realworld problems addition consider problems mix functions fast slow evaluate problems require balancing amount time spent metacomputation pesc actual evaluation target objective take bounded rationality approach develop partial update pesc trades accuracy speed propose method adaptively switching partial full updates pesc allows interpolate versions pesc efficient terms function evaluations efficient terms wallclock time overall demonstrate pesc effective algorithm provides promising direction towards unified solution constrained bayesian optimization\",\"address online linear optimization problem actions forecaster represented binary vectors goal understand magnitude minimax regret worst possible set actions study problem three different assumptions feedback full information partial information models socalled semibandit bandit problems consider linfty ltype restrictions losses assigned adversary formulate general strategy using bregman projections top potentialbased gradient descent generalizes ones studied series papers gyorgy dani abernethy cesabianchi lugosi helmbold warmuth koolen uchiya kale audibert bubeck provide simple proofs recover previous results propose new upper bounds semibandit game moreover derive lower bounds three feedback assumptions exception bandit game upper lower bounds tight constant factor finally answer question asked koolen showing exponentially weighted average forecaster suboptimal linfty adversaries\",\"simulated annealing popular method approaching solution global optimization problem existing results performance apply discrete combinatorial optimization optimization variables assume finite set possible values introduce new general formulation simulated annealing allows one guarantee finitetime performance optimization functions continuous variables results hold universally optimization problem bounded domain establish connection simulated annealing uptodate theory convergence markov chain monte carlo methods continuous domains work inspired concept finitetime learning known accuracy confidence developed statistical learning theory\",\"address problem synthetic gene design using bayesian optimization main issue designing gene design space defined terms long strings characters different lengths renders optimization intractable propose threestep approach deal issue first use gaussian process model emulate behavior cell inputs model use set biologically meaningful gene features allows define optimal gene designs rules based model outputs define multitask acquisition function optimize simultaneously severals aspects interest finally define evaluation function allow rank sets candidate gene sequences coherent optimal design strategy illustrate performance approach real gene design experiment mammalian cells\",\"popularity bayesian optimization methods efficient exploration parameter spaces lead series papers applying gaussian processes surrogates optimization functions however proposed approaches allow exploration parameter space occur sequentially often desirable simultaneously propose batches parameter values explore particularly case large parallel processing facilities available facilities could computational physical facets process optimized biological experiments many experimental set ups allow several samples simultaneously processed batch methods however require modeling interaction evaluations batch expensive complex scenarios investigate simple heuristic based estimate lipschitz constant captures important aspect interaction local repulsion negligible computational overhead resulting algorithm compares well running time much elaborate alternatives approach assumes function interest lipschitz continuous function wraploop around acquisition function used collect batches points certain size minimizing nonparallelizable computational effort speedup method respect previous approaches significant set computationally expensive experiments\",\"consider mnkclassical problem controller activating sampling sequentially finite number geq populations specified unknown distributions time horizon time ldots controller wishes select population sample goal sampling population optimizes score function distribution maximizing expected sum outcomes minimizing variability define class textituniformly fast sampling policies show mild regularity conditions asymptotic lower bound expected total number suboptimal population activations provide sufficient conditions ucb policy asymptotically optimal since attains lower bound explicit solutions provided number examples interest including general score functionals unconstrained pareto distributions potentially infinite mean uniform distributions unknown support additional results bandits normal distributions also provided\",\"paper index policies minimizing frequentist regret stochastic multiarmed bandit model inspired bayesian view problem main contribution prove bayesucb algorithm relies quantiles posterior distributions asymptotically optimal reward distributions belong onedimensional exponential family large class prior distributions also show bayesian literature gives new insight kind exploration rates could used frequentist ucbtype algorithms indeed approximations bayesian optimal solution finite horizon gittins indices provide justification klucb klucbh algorithms whose asymptotic optimality also established\",\"present glasses global optimisation lookahead stochastic simulation expectedloss search majority global optimisation approaches use myopic considering impact next function value nonmyopic approaches exist able consider handful future evaluations novel algorithm glasses permits consideration dozens evaluations future done approximating ideal lookahead loss function expensive evaluate cheaper alternative future steps algorithm simulated beforehand expectation propagation algorithm used compute expected value losswe show farhorizon planning thus enabled leads substantive performance gains empirical tests\",\"focusing bound constrained global optimization problems whose objective functions computationally expensive blackbox functions multiple local minima recently popular metric stochastic response surface msrs algorithm proposed citeregissrbf based adaptive sequential learning based response surfaces revisited extended better performance case higher dimensional problems specifically propose new way generate candidate points next function evaluation point picked according metric criteria based new definition distance prove global convergence corresponding correspondingly adaptive implementation msrs named sosa presented sosa likely perturb sensitive coordinates generating candidate points instead perturbing coordinates simultaneously numerical experiments synthetic problems real problems demonstrate advantages new algorithm compared many state art alternatives\",\"provide yet another proof existence calibrated forecasters two merits first valid arbitrary finite number outcomes second short simple follows direct application blackwells approachability theorem carefully chosen vectorvalued payoff function convex target set proof captures essence existing proofs based approachability proof foster case binary outcomes highlights intrinsic connection approachability calibration\",\"present pesmo bayesian method identifying pareto set multiobjective optimization problems functions expensive evaluate central idea pesmo choose evaluation points maximally reduce entropy posterior distribution pareto set critically pesmo multiobjective acquisition function decomposed sum objectivespecific acquisition functions enables algorithm used emphdecoupled scenarios objectives evaluated separately perhaps different costs decoupling capability also makes possible identify difficult objectives require evaluations pesmo also offers gains efficiency cost scales linearly number objectives comparison exponential cost methods compare pesmo related methods multiobjective bayesian optimization synthetic realworld problems results show pesmo produces better recommendations smaller number evaluations objectives decoupled evaluation lead improvements performance particularly number objectives large\",\"study problem allocating stocks dark pools propose analyze optimal approach allocations continuousvalued allocations allowed also propose modification case integervalued allocations possible extend previous work problem adversarial scenarios also improving results iid setup resulting algorithms efficient perform well simulations stochastic adversarial inputs\",\"propose extension concept expected improvement criterion commonly used kriging based optimization extend complex kriging models models using derivatives target field application cfd problems objective function extremely expensive evaluate theory also used fields\",\"work presents pesmoc predictive entropy search multiobjective bayesian optimization constraints informationbased strategy simultaneous optimization multiple expensivetoevaluate blackbox functions presence several constraints pesmoc hence used solve wide range optimization problems iteratively pesmoc chooses input location evaluate objective functions constraints maximally reduce entropy pareto set corresponding optimization problem constraints considered pesmoc assumed similar properties objective functions typical bayesian optimization problems known expression prevents gradient computation evaluation considered expensive resulting observations may corrupted noise constraints arise plethora expensive blackbox optimization problems carry synthetic experiments illustrate effectiveness pesmoc sample objectives constraints gaussian process prior results obtained show pesmoc able provide better recommendations smaller number evaluations strategy based random search\",\"methods decisiontheoretic online learning based hedge algorithm takes parameter called learning rate previous analyses learning rate carefully tuned obtain optimal worstcase performance leading suboptimal performance easy instances example exists action significantly better others propose new way setting learning rate adapts difficulty learning problem worst case procedure still guarantees optimal performance easy instances achieves much smaller regret particular adaptive method achieves constant regret probabilistic setting exists action average obtains strictly smaller loss actions also provide simulation study comparing approach existing methods\",\"inventory control unknown demand distribution considered emphasis placed case involving discrete nonperishable items focus adaptive policy every period uses much possible optimal newsvendor ordering quantity empirical distribution learned period policy assessed using regret criterion measures price paid ambiguity demand distribution periods guarantees latters separation critical newsvendor parameter betabhb constant upper bound regret found without prior information demand distribution show regret grow faster rate tepsilon epsilon view known lower bound almost best one could hope simulation studies involving along policies also conducted\",\"unknown constraints arise many types expensive blackbox optimization problems several methods proposed recently performing bayesian optimization constraints based expected improvement heuristic however lead pathologies used constraints example case decoupled constraintsie one independently evaluate objective constraintsei encounter pathology prevents exploration additionally computing requires current best solution may exist none data collected far satisfy constraints contrast informationbased approaches suffer failure modes paper present new informationbased method called predictive entropy search constraints pesc analyze performance pesc show compares favorably eibased approaches synthetic benchmark problems well several realworld examples demonstrate pesc effective algorithm provides promising direction towards unified solution constrained bayesian optimization\",\"many retailers today employ inventory management systems based reorder point policies rely assumption decreases product inventory levels result product sales unfortunately usually happens small random quantities product get lost stolen broken without record time passes consequence shoplifting usual retailers handling large varieties inexpensive products grocery stores turn time discrepancies lead stock freezing problems situations system believes stock reorder point actual stock zero replenishments sales occur motivated issues model interaction sales losses replenishments inventory levels dynamic bayesian network dbn inventory levels unobserved hidden variables wish estimate present expectationmaximization algorithm estimate parameters sale loss distributions relies solving onedimensional dynamic program estep solving two separate onedimensional nonlinear programs mstep\",\"consider firm sells products periods without knowing demand function firm sequentially sets prices earn revenue learn underlying demand function simultaneously natural heuristic problem commonly used practice greedy iterative least squares gils time period gils estimates demand linear function price applying least squares set prior prices realized demands price maximizes revenue given estimated demand function used next time period performance measured regret expected revenue loss optimal oracle pricing policy demand function known recently den boer zwart keskin zeevi demonstrated gils suboptimal introduced algorithms integrate forced price dispersion gils achieve asymptotically optimal performance paper consider dynamic pricing problem datarich environment particular assume firm knows expected demand particular price historical data period setting price firm access extra information demand covariates may predictive demand prove setting gils achieves asymptotically optimal regret order logt also show following surprising result original dynamic pricing problem den boer zwart keskin zeevi inclusion set covariates gils potential demand covariates even though could carry information would make gils asymptotically optimal validate results via extensive numerical simulations synthetic real data sets\",\"study restless bandit associated extremely simple scalar kalman filter model discrete time certain assumptions prove problem indexable sense whittle index nondecreasing function relevant belief state spite long history problem appears first proof use results schurconvexity mechanical words particular binary strings intimately related palindromes\",\"paper studies deviations regret stochastic multiarmed bandit problem total number plays known beforehand agent audibert exhibit policy probability least regret policy order logn also shown property shared popular ucb policy auer work first answers open question extends negative result anytime policy second contribution paper design anytime robust policies specific multiarmed bandit problems restrictions put set possible distributions different arms\",\"thompson sampling demonstrated many complex bandit models however theoretical guarantees available parametric multiarmed bandit still limited bernoulli case extend proving asymptotic optimality algorithm using jeffreys prior dimensional exponential family bandits proof builds previous work also makes extensive use closed forms kullbackleibler divergence fisher information thus jeffreys prior available exponential family allow give finite time exponential concentration inequality posterior distributions exponential families may interest right moreover analysis covers distributions optimistic algorithm yet proposed including heavytailed exponential families\",\"paper devoted regret lower bounds classical model stochastic multiarmed bandit wellknown result lai robbins extended burnetas katehakis established presence logarithmic bound consistent policies relax notion consistence exhibit generalisation logarithmic bound also show non existence logarithmic bound general case hannan consistency get results study variants popular upper confidence bounds ucb policies byproduct prove impossible design adaptive policy would select best two algorithms taking advantage properties environment\",\"consider bayesian optimization expensivetoevaluate blackbox objective function also access cheaper approximations objective general approximations arise applications reinforcement learning engineering natural sciences subject inherent unknown bias model discrepancy caused inadequate internal model deviates reality vary domain making utilization approximations nontrivial task present novel algorithm provides rigorous mathematical treatment uncertainties arising model discrepancies noisy observations optimization decisions rely value information analysis extends knowledge gradient factor setting multiple information sources vary cost sampling decision maximizes predicted benefit per unit cost conduct experimental evaluation demonstrates method consistently outperforms stateoftheart techniques finds designs considerably higher objective value additionally inflicts less cost exploration process\",\"important problem sequential decisionmaking uncertainty use limited data compute safe policy policy guaranteed perform least well given baseline strategy paper develop analyze new modelbased approach compute safe policy access inaccurate dynamics model system known accuracy guarantees proposed robust method uses inaccurate model directly minimize negative regret wrt baseline policy contrary existing approaches minimizing regret allows one improve baseline policy states accurate dynamics seamlessly fall back baseline policy otherwise show formulation nphard propose approximate algorithm empirical results several domains show even relatively simple approximate algorithm significantly outperform standard approaches\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"10_optimization_policy_regret\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"10_optimization_policy_regret\"],\"textfont\":{\"size\":12},\"x\":[2.15344,2.5808213,2.1240015,2.7310863,2.7913337,2.7854018,2.134534,2.1359813,2.619641,2.5961158,2.0604954,2.6399527,2.5413148,2.6429942,2.6170573,2.353811,2.1332355,2.6353137,2.1252685,2.1206338,2.1302812,2.1128974,2.1253068,2.125751,2.4388335,2.2300217,2.3725202],\"y\":[-0.53140175,-0.18333983,-0.5197822,-0.24506198,0.0145991435,-0.033198275,-0.48290676,-0.5412304,-0.23005371,-0.17213972,-0.24997991,-0.18978818,-0.21183343,-0.19529021,-0.17703992,-0.26383084,-0.3053005,-0.17254402,-0.25385404,-0.24521422,-0.54505277,-0.5290156,-0.5268417,-0.52934885,-0.28072923,-0.14690216,-0.29796463],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"rapid overlay chemical structures rocs standard tool calculation shape chemical color similarity rocs uses unweighted sums combine many aspects similarity yielding parameterfree models virtual screening report decompose rocs color force field color components color atom overlaps novel color similarity features weighted systemspecific manner machine learning algorithms crossvalidation experiments additional features significantly improve virtual screening performance roc auc scores relative standard rocs\",\"consider standard binary classification problem performance binary classifier based training data characterized excess risk study bahadurs type exponential bounds minimax accuracy confidence function based excess risk study quantity depends complexity class distributions characterized exponents entropies class regression functions class bayes classifiers corresponding distributions class also study dependence margin parameters classification problem\",\"supervised learning active research area numerous applications diverse fields data analytics computer vision speech audio processing image understanding cases loss functions used machine learning assume symmetric noise models seek estimate unknown function parameters however loss functions quantile quantile huber generalize symmetric ell huber losses asymmetric setting fixed quantile parameter paper propose jointly infer quantile parameter unknown function parameters asymmetric quantile huber quantile losses explore various properties quantile huber loss implement convexity certificate used check convexity quantile parameter loss convex respect parameter function prove biconvex function quantile parameters propose algorithm jointly estimate results synthetic real data demonstrate proposed approach automatically recover quantile parameter corresponding noise also provide improved recovery function parameters illustrate potential framework extend gradient boosting machines quantile losses automatically estimate quantile parameter iteration\",\"currently machine learning plays important role lives individual activities numerous people accordingly become necessary design machine learning algorithms ensure discrimination biased views unfair treatment result decision making predictions made via machine learning work introduce novel empirical risk minimization erm framework supervised learning neutralized erm nerm ensures classifiers obtained guaranteed neutral respect viewpoint hypothesis specifically given viewpoint hypothesis nerm works find target hypothesis minimizes empirical risk simultaneously identifying target hypothesis neutral viewpoint hypothesis within nerm framework derive theoretical bound empirical generalization neutrality risks furthermore realization nerm linear classification derive maxmargin algorithm neutral support vector machine svm experimental results show neutral svm shows improved classification performance real datasets without sacrificing neutrality guarantee\",\"concept refinement probability elicitation considered proper scoring rules taking directions axioms probability refinement clarified using hilbert space interpretation reformulated underlying data distribution setting connections maximal marginal diversity conditional entropy considered used derive measures provide arbitrarily tight bounds bayes error refinement also reformulated classifier output setting connections calibrated classifiers proper margin losses established\",\"study losses binary classification class probability estimation extend understanding margin losses general composite losses composition proper loss link function characterise margin losses proper composite losses explicitly show determine symmetric loss full half one partial losses introduce intrinsic parametrisation composite binary losses give complete characterisation relationship proper losses classification calibrated losses also consider question best surrogate binary loss introduce precise notion best show exist situations two convex surrogate losses incommensurable provide complete explicit characterisation convexity composite binary losses terms link function weight function associated proper loss make composite loss characterisation suggests new ways surrogate tuning finally appendix present new algorithmindependent results relationship properness convexity robustness misclassification noise binary losses show convex proper losses nonrobust misclassification noise\",\"consider training probabilistic classifiers case large number classes number classes assumed large perform exact normalisation classes account consider simple approach directly approximates likelihood show simple approach works well toy problems competitive recently introduced alternative nonlikelihood based approximations furthermore relate approach simple ranking objective leads suggest specific setting optimal threshold ranking objective\",\"classification important topic statistics machine learning great potential many real applications paper investigate two popular large margin classification methods support vector machine svm distance weighted discrimination dwd two contexts highdimensional lowsample size data imbalanced data unified family classification machines flexible assortment machine flame proposed within dwd svm special cases flame family helps identify similarities differences svm dwd well known many classifiers overfit data highdimensional setting others sensitive imbalanced data class larger sample size overly influences classifier pushes decision boundary towards minority class svm resistant imbalanced data issue overfits highdimensional data sets showing undesired datapiling phenomena dwd method proposed improve svm highdimensional setting decision boundary sensitive imbalanced ratio sample sizes flame family helps understand intrinsic connection svm dwd improves methods providing better tradeoff sensitivity imbalanced data overfitting highdimensional data several asymptotic properties flame classifiers studied simulations real data applications investigated illustrate usefulness flame classifiers\",\"one limiting factors using support vector machines svms large scale applications superlinear computational requirements terms number training samples address issue several approaches train svms many small chunks large data sets separately proposed literature far however almost approaches empirically investigated addition motivation always based computational requirements work consider localized svm approach based upon partition input space local svm derive general oracle inequality apply oracle inequality least squares regression using gaussian kernels deduce local learning rates essentially minimax optimal standard smoothness assumptions regression function gives first motivation using local svms based computational requirements theoretical predictions generalization performance introduce datadependent parameter selection method local svm approach show method achieves learning rates finally present larger scale experiments localized svm showing achieves essentially test performance global svm fraction computational requirements addition turns computational requirements local svms similar vanilla random chunk approach achieved test errors significantly better\",\"security issues crucial number machine learning applications especially scenarios dealing human activity rather natural phenomena information ranking spam detection malware detection etc expected cases learning algorithms deal manipulated data aimed hampering decision making although previous work addressed handling malicious data context supervised learning little known behavior anomaly detection methods scenarios contribution analyze performance particular method online centroid anomaly detection presence adversarial noise analysis addresses following securityrelated issues formalization learning attack processes derivation optimal attack analysis efficiency constraints derive bounds effectiveness poisoning attack centroid anomaly different conditions bounded unbounded percentage traffic bounded false positive rate bounds show whereas poisoning attack effectively staged unconstrained case made arbitrarily difficult strict upper bound attackers gain external constraints properly used experimental evaluation carried real http exploit traces confirms tightness theoretical bounds practicality protection mechanisms\",\"present surrogate regret bounds arbitrary surrogate losses context binary classification labeldependent costs bounds relate classifiers risk assessed respect surrogate loss costsensitive classification risk two approaches surrogate regret bounds developed first direct generalization bartlett focus marginbased losses costinsensitive classification second adopts framework steinwart based calibration functions nontrivial surrogate regret bounds shown exist precisely surrogate loss satisfies calibration condition easily verified many common losses apply theory class uneven margin losses characterize losses properly calibrated uneven hinge squared error exponential sigmoid losses treated detail\",\"introduce efficient method training linear ranking support vector machine method combines cutting plane optimization redblack tree based approach subgradient calculations omsmlogm time complexity number training examples average number nonzero features per example best previously known training algorithms achieve efficiency restricted special cases whereas proposed approach allows real valued utility scores training data experiments demonstrate superior scalability proposed approach compared fastest existing ranksvm implementations\",\"apply informationbased complexity analysis support vector machine svm algorithms goal comprehensive continuous algorithmic analysis algorithms involves complexity measures higher order operations certain optimizations considered primitive purposes measuring complexity consider classes information operators algorithms made scaled families investigate utility scaling complexities minimize error look division statistical learning information algorithmic components complexities applications support vector machine svm general machine learning algorithms give applications svm algorithms graded linear higher order components give example biomedical informatics\",\"introduce new nearestprototype classifier prototype vector machine pvm arises combinatorial optimization problem cast variant set cover problem propose two algorithms approximating solution pvm selects relatively small number representative points used classification contains special case method compatible dissimilarity measure making amenable situations data embedded underlying feature space using noneuclidean metric desirable indeed demonstrate much studied zip code data pvm reap benefits problemspecific metric example pvm outperforms highly successful tangent distance retaining fewer half data points example highlights strengths pvm yielding lowerror highly interpretable model additionally apply pvm protein classification problem kernelbased distance used\",\"sparse classifiers support vector machines svm efficient testphases classifier characterized subset samples called support vectors svs rest samples non svs influence classification result however advantage sparsity fully exploited training phases generally difficult know sample turns beforehand paper introduce new approach called safe sample screening enables identify subset nonsvs screen prior training phase approach different existing heuristic approaches sense screened samples guaranteed nonsvs optimal solution investigate advantage safe sample screening approach intensive numerical experiments demonstrate substantially decrease computational cost stateoftheart svm solvers libsvm current big data era believe safe sample screening would great practical importance since data size reduced without sacrificing optimality final solution\",\"last years many different performance measures introduced overcome weakness natural metric accuracy among matthews correlation coefficient recently gained popularity among researchers machine learning also several application fields bioinformatics nonetheless novel functions proposed literature show confusion entropy recently introduced classifier performance measure multiclass problems strong monotone relation multiclass generalization classical metric matthews correlation coefficient computational evidence support claim provided together outline theoretical explanation\",\"study highdimensional asymptotic performance limits binary supervised classification problems class conditional densities gaussian unknown means covariances number signal dimensions scales faster number labeled training samples show bayes error namely minimum attainable error probability complete distributional knowledge equally likely classes arbitrarily close zero yet limiting minimax error probability every supervised learning algorithm better random coin toss contrast related studies classification difficulty bayes error made vanish hold constant taking highdimensional limits contrast vcdimension based minimax lower bounds consider worst case error probability distributions fixed bayes error worst case family gaussian distributions constant bayes error also show nontrivial asymptotic minimax error probability attained parametric subsets zero measure suitable measure space results expose fundamental importance prior knowledge suggest unless impose strong structural constraints sparsity parametric space supervised learning may ineffective high dimensional small sample settings\",\"novel linear classification method possesses merits support vector machine svm distanceweighted discrimination dwd proposed article proposed distanceweighted support vector machine method viewed hybrid svm dwd finds classification direction minimizing mainly dwd loss determines intercept term svm manner show method inheres merit dwd hence overcomes datapiling overfitting issue svm hand new method subject imbalanced data issue main advantage svm dwd uses unusual loss combines hinge loss svm dwd loss trick axillary hyperplane several theoretical properties including fisher consistency asymptotic normality dwsvm solution developed use simulated examples show new method compete dwd svm classification performance interpretability real data application establishes usefulness approach\",\"distance weighted discrimination dwd marginbased classifier interesting geometric motivation dwd originally proposed superior alternative support vector machine svm however dwd yet popular compared svm main reasons twofold first stateoftheart algorithm solving dwd based secondordercone programming socp svm quadratic programming problem much efficient solve second current statistical theory dwd mainly focuses linear dwd highdimensionlowsamplesize setting datapiling learning theory svm mainly focuses bayes risk consistency kernel svm fact bayes risk consistency dwd presented open problem original dwd paper work advance current understanding dwd computational theoretical perspectives propose novel efficient algorithm solving dwd algorithm several hundred times faster existing stateoftheart algorithm based socp addition algorithm handle generalized dwd socp algorithm works well special dwd generalized dwd furthermore consider natural kernel dwd reproducing kernel hilbert space establish bayes risk consistency kernel dwd compare dwd svm several benchmark data sets show two comparable classification accuracy dwd equipped new algorithm much faster compute svm\",\"paper generalizes important result pacbayesian literature binary classification case ensemble methods structured outputs prove generic version cbound upper bound risk models expressed weighted majority vote based first second statistical moments votes margin bound may advantageously applied complex outputs multiclass labels multilabel allow consider margin relaxations results open way develop new ensemble methods structured output prediction pacbayesian guarantees\",\"prove new fast learning rates onevsall multiclass plugin classifiers trained either exponentially strongly mixing data data generated converging drifting distribution two typical scenarios training data iid learning rates obtained multiclass version tsybakovs margin assumption type lownoise assumption depend number classes results general include previous result binaryclass plugin classifiers iid data special case contrast previous works least squares svms binaryclass setting results retain optimal learning rate iid case\",\"numbers numerical vectors account large portion data however recently amount string data generated increased dramatically consequently classifying string data common problem many fields widely used approach problem convert strings numerical vectors using string kernels subsequently apply support vector machine works numerical vector space however nononetoone conversion involves loss information makes impossible evaluate using probability theory generalization error learning machine considering given data train test machine strings generated according probability laws study approach classification problem constructing classifier works set strings evaluate generalization error classifier theoretically probability theory strings required therefore first extend limit theorem asymptotic behavior consensus sequence strings counterpart mean numerical vectors demonstrated probability theory metric space strings developed one authors colleague previous study using obtained result demonstrate learning machine classifies strings asymptotically optimal manner furthermore demonstrate usefulness machine practical data analysis applying predicting proteinprotein interactions using amino acid sequences\",\"existing binary classification methods target optimization overall classification risk may fail serve realworld applications cancer diagnosis users concerned risk misclassifying one specific class neymanpearson paradigm introduced context novel statistical framework handling asymmetric type iii error priorities seeks classifiers minimal type error constrained type error user specified level article first attempt construct classifiers guaranteed theoretical performance paradigm highdimensional settings based fundamental neymanpearson lemma used plugin approach construct nptype classifiers naive bayes models proposed classifiers satisfy oracle inequalities natural paradigm counterparts oracle inequalities classical binary classification besides desirable theoretical properties also demonstrated numerical advantages prioritized error control via simulation real data studies\",\"main goal article address bipartite ranking issue perspective functional data analysis fda given training set independent realizations possibly sampled secondorder random function locally smooth autocorrelation structure binary label randomly assigned objective learn scoring function optimal roc curve based linearnonlinear waveletbased approximations shown select compact finite dimensional representations input curves adaptively order build accurate ranking rules using recent advances ranking problem multivariate data binary feedback beyond theoretical considerations performance learning methods functional bipartite ranking proposed paper illustrated numerical experiments\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"11_svm_dwd_losses\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"11_svm_dwd_losses\"],\"textfont\":{\"size\":12},\"x\":[0.8236729,0.31785852,0.27877304,0.70235837,0.32132354,0.2458955,0.3929037,0.76907676,0.85220224,0.57583266,0.20730683,0.8122792,0.81447,0.91405773,0.81952643,0.3722401,0.45227304,0.8667092,0.86348945,0.2022723,0.6424021,0.77085644,0.33192703,1.0804523,0.60125667],\"y\":[1.4556946,1.6710246,1.6658424,1.3833169,1.6429402,1.7218378,1.6825687,1.499133,1.7172917,1.5782173,1.7566116,1.7738532,1.5441855,1.6087095,1.7526741,1.5919241,1.669851,1.6744179,1.655638,1.5228797,1.6354781,1.4148908,1.6233513,1.9947805,1.6348797],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"study probability measures induced set functions constraints measures arise variety realworld settings prior knowledge resource limitations pragmatic considerations impose constraints consider task rapidly sampling constrained measures develop fast markov chain samplers first main result mcmc sampling strongly rayleigh measures present sharp polynomial bounds mixing time corollary result yields fast mixing sampler determinantal point processes dpps yielding knowledge first provably fast mcmc sampler dpps since inception four decades ago beyond measures develop mcmc samplers probabilistic models hard constraints identify sufficient conditions chains mix rapidly illustrate claims empirically verifying dependence mixing times key factors governing theoretical bounds\",\"partition functions probability distributions important quantities model evaluation comparisons present new method compute partition functions complex multimodal distributions distributions often sampled using simulated tempering augments target space auxiliary inverse temperature variable method exploits multinomial probability law inverse temperatures provides estimates partition function terms simple quotient raoblackwellized marginal inverse temperature probability estimates updated sampling show method interesting connections several alternative popular methods offers significant advantages particular empirically find new method provides accurate estimates annealed importance sampling calculating partition functions large restricted boltzmann machines rbm moreover method sufficiently accurate track training validation loglikelihoods learning rbms minimal computational cost\",\"recently stochastic gradient markov chain monte carlo sgmcmc methods proposed scaling monte carlo computations large data problems whilst approaches proven useful many applications vanilla sgmcmc might suffer poor mixing rates random variables exhibit strong couplings target densities big scale differences study propose novel sgmcmc method takes local geometry account using ideas quasinewton optimization methods second order methods directly approximate inverse hessian using limited history samples gradients method uses dense approximations inverse hessian keeping time memory complexities linear dimension problem provide formal theoretical analysis show proposed method asymptotically unbiased consistent posterior expectations illustrate effectiveness approach synthetic real datasets experiments two challenging applications show method achieves fast convergence rates similar riemannian approaches time low computational requirements similar diagonal preconditioning approaches\",\"datasets growing size complexity creating demand rich models quantification uncertainty bayesian methods excellent fit demand scaling bayesian inference challenge response challenge considerable recent work based varying assumptions model structure underlying computational resources importance asymptotic correctness result zoo ideas clear overarching principles paper seek identify unifying principles patterns intuitions scaling bayesian inference review existing work utilizing modern computing resources mcmc variational approximation techniques taxonomy ideas characterize general principles proven successful designing scalable inference procedures comment path forward\",\"modern scale data brought new challenges bayesian inference particular conventional mcmc algorithms computationally expensive large data sets promising approach solve problem embarrassingly parallel mcmc epmcmc first partitions data multiple subsets runs independent sampling algorithms subset subset posterior draws aggregated via combining rules obtain final approximation existing epmcmc algorithms limited approximation accuracy difficulty resampling article propose new epmcmc algorithm part solves problems new algorithm applies random partition trees combine subset posterior draws distributionfree easy resample adapt multiple scales provide theoretical justification extensive experiments illustrating empirical performance\",\"consider problem adaptive stratified sampling monte carlo integration noisy function given finite budget noisy evaluations function tackle paper problem adapting function time number samples stratum partition precisely interesting refine partition domain area noise function variations function heterogeneous hand refined stratification optimal indeed refined stratification difficult adjust allocation samples stratification sample points noise variations function larger provide paper algorithm selects online among large class partitions partition provides optimal tradeoff allocates samples almost optimally partition\",\"applying standard markov chain monte carlo mcmc algorithms large data sets computationally expensive calculation acceptance probability creation informed proposals usually require iteration whole data set recently proposed stochastic gradient langevin dynamics sgld method circumvents problem generating proposals based subset data skipping acceptreject step using decreasing stepsizes sequence deltamm geq appropriate lyapunov conditions provide article rigorous mathematical framework analysing algorithm prove verifiable assumptions algorithm consistent satisfies central limit theorem clt asymptotic biasvariance decomposition characterized explicit functional stepsizes sequence deltamm geq leverage analysis give practical recommendations notoriously difficult tuning algorithm asymptotically optimal use stepsize sequence type deltam asymp leading algorithm whose mean squared error mse decreases rate mathcalom\",\"regular particle filter algorithm sequential monte carlo smc methods initial weights traditionally dependent proposed distribution posterior distribution current timestamp sampled sequence target posterior distribution previous timestamp technically correct leads algorithms usually practical issues degeneracy particles eventually collapse onto single particle paper propose evaluate using means clustering attack even take advantage degeneracy specifically propose stochastic smc algorithm initializes set means providing initial centers chosen collapsed particles fight degeneracy adjust regular smc weights mediated cluster proportions correct retain expectation experimentally demonstrate approach better performance vanilla algorithms\",\"consider problem adaptive stratified sampling monte carlo integration differentiable function given finite number evaluations function construct sampling scheme samples often regions function oscillates allocating samples well spread domain notion shares similitude low discrepancy prove estimate returned algorithm almost similarly accurate estimate optimal oracle strategy would know variations function everywhere would return provide finitesample analysis\",\"consider continuous time markovian processes populations individual agents interact stochastically according kinetic rules despite increasing prominence models fields ranging biology smart cities bayesian inference systems remains challenging continuous time discrete state systems potentially infinite statespace propose novel efficient algorithm joint state parameter posterior sampling population markov jump processes introduce class pseudomarginal sampling algorithms based random truncation method enables principled treatment infinite state spaces extensive evaluation number benchmark models shows approach achieves considerable savings compared state art methods retaining accuracy fast convergence also present results synthetic biology data set showing potential practical usefulness work\",\"hamiltonian monte carlo hmc exploits hamiltonian dynamics construct efficient proposals markov chain monte carlo mcmc paper present generalization hmc exploits textitnoncanonical hamiltonian dynamics refer algorithm magnetic hmc since dimensions subset dynamics map onto mechanics charged particle coupled magnetic field establish theoretical basis use noncanonical hamiltonian dynamics mcmc construct symplectic leapfroglike integrator allowing implementation magnetic hmc finally exhibit several examples noncanonical dynamics lead improved mixing magnetic hmc relative ordinary hmc\",\"propose restricted collapsed draw rcd sampler general markov chain monte carlo sampler simultaneous draws hierarchical chinese restaurant process hcrp restriction models require simultaneous draws hierarchical dirichlet process restriction infinite hidden markov models ihmm difficult enjoy benefits markergthe hcrp due combinatorial explosion calculating distributions coupled draws constructing proposal seating arrangements partitioning stochastically accepts proposal metropolishastings algorithm rcd sampler makes accurate sampling complex combination draws retaining efficiency hcrp representation based rcd sampler developed series sophisticated sampling algorithms ihmms including blocked gibbs sampling beam sampling splitmerge sampling outperformed conventional ihmm samplers experiments\",\"many random processes simulated output deterministic model accepting random inputs model usually describes complex mathematical physical stochastic system randomness introduced input variables model statistics output event known input variables chosen specific way output prescribed statistics probability distribution input random variables directly known dictated implicitly statistics output random variables problem usually intractable classical sampling methods based markov chain monte carlo propose novel method sample random inputs models introducing modification standard metropolishastings algorithm example consider system described stochastic differential equation sde demonstrate sample paths random process satisfying sde generated technique\",\"estimation normalizing constants fundamental step probabilistic model comparison sequential monte carlo methods may used task advantage inherently parallelizable however standard choice using fixed number particles iteration suboptimal steps contribute disproportionately variance estimate introduce adaptive version resamplemove algorithm particle set adaptively expanded whenever better approximation intermediate distribution needed algorithm builds expression optimal number particles corresponding minimum variance found ideal conditions benchmark results challenging gaussian process classification restricted boltzmann machine applications show adaptive resamplemove arm estimates normalizing constant smaller variance using less computational resources either resamplemove fixed number particles annealed importance sampling advantage annealed importance sampling arm easier tune\",\"adaptive monte carlo schemes developed last years usually seek ensure ergodicity sampling process line mcmc tradition poses constraints possible terms adaptation general case ergodicity guaranteed adaptation diminished certain rate importance sampling approaches offer way circumvent limitation design sampling algorithms keep adapting present gradient informed variant smc special case population monte carlo static problems\",\"monte carlo sampling algorithms extremely widelyused technique estimate expectations functions especially high dimensions control variates powerful technique reduce error estimates conventional form rely accurate approximation priori stacked monte carlo stackmc recently introduced technique designed overcome limitation fitting control variate data samples done naively forming control variate data would result overfitting typically worsening algorithms performance stackmc uses insample outsample techniques remove overfitting crucially postprocessing technique requiring additional samples applied data generated estimator preliminary experiments demonstrated stackmc improved estimates expectations used postprocess samples produces simple sampling estimator substantially extend earlier work provide indepth analysis stackmc algorithm use construct improved version original algorithm lower estimation error perform experiments stackmc several additional kinds estimators demonstrating improved performance samples generated via importance sampling latinhypercube sampling quasimonte carlo sampling also show extend stackmc combine multiple fitting functions apply discrete input spaces\",\"propose novel sampling framework inference probabilistic models active learning approach converges quickly wallclock time markov chain monte carlo mcmc benchmarks central challenge probabilistic inference numerical integration average ensembles models unknown hyperparameters example compute marginal likelihood partition function mcmc provided approaches numerical integration deliver stateoftheart inference suffer sample inefficiency poor convergence diagnostics bayesian quadrature techniques offer modelbased solution problems uptake hindered prohibitive computation costs introduce warped model probabilistic integrands likelihoods known nonnegative permitting cheap active learning scheme optimally select sample locations algorithm demonstrated offer faster convergence seconds relative simple monte carlo annealed importance sampling synthetic realworld examples\",\"probabilistic programming languages simplify development machine learning techniques inference sufficiently scalable unfortunately bayesian parameter estimation highly coupled models regressions statespace models still scales poorly mcmc transition takes linear time number observations paper describes sublineartime algorithm making metropolishastings updates latent variables probabilistic programs approach generalizes recently introduced approximate techniques instead subsampling data items assumed independent subsamples edges dynamically constructed graphical model thus applies broader class problems interoperates generalpurpose inference techniques empirical results including confirmation sublinear pertransition scaling presented bayesian logistic regression nonlinear classification via joint dirichlet process mixtures parameter estimation stochastic volatility models state estimation via particle mcmc three applications use implementation requires lines probabilistic code\",\"large matrix factorisation problems develop distributed markov chain monte carlo mcmc method based stochastic gradient langevin dynamics sgld call parallel sgld psgld psgld favourable scaling properties increasing data size comparable terms computational requirements optimisation methods based stochastic gradient descent psgld achieves high performance exploiting conditional independence structure models subsample data systematic manner allow parallelisation distributed computation provide convergence proof algorithm verify superior performance various architectures graphics processing units shared memory multicore systems multicomputer clusters\",\"manifold markov chain monte carlo algorithms introduced sample effectively challenging target densities exhibiting multiple modes strong correlations algorithms exploit local geometry parameter space thus enabling chains achieve faster convergence rate measured number steps however acquiring local geometric information often increase computational complexity per step extent sampling highdimensional targets becomes inefficient terms total computational time paper analyzes computational complexity manifold langevin monte carlo proposes geometric adaptive monte carlo sampler aimed balancing benefits exploiting local geometry computational cost achieve high effective sample size given computational cost suggested sampler discretetime stochastic process random environment random environment allows switch local geometric adaptive proposal kernels help schedule exponential schedule put forward enables frequent use geometric information early transient phases chain saving computational time late stationary phases average complexity manually set depending need geometric exploitation posed underlying model\",\"propose kernel hamiltonian monte carlo kmc gradientfree adaptive mcmc algorithm based hamiltonian monte carlo hmc target densities classical hmc option due intractable gradients kmc adaptively learns targets gradient structure fitting exponential family model reproducing kernel hilbert space computational costs reduced two novel efficient approximations gradient asymptotically exact kmc mimics hmc terms sampling efficiency offers substantial mixing improvements stateoftheart gradient free samplers support claims experimental studies toy realworld applications including approximate bayesian computation exactapproximate mcmc\",\"hamiltonian monte carlo hmc popular markov chain monte carlo mcmc algorithm generates proposals metropolishastings algorithm simulating dynamics hamiltonian system however hmc sensitive large time discretizations performs poorly mismatch spatial geometry target distribution scales momentum distribution particular mass matrix hmc hard tune well order alleviate problems propose relativistic hamiltonian monte carlo version hmc based relativistic dynamics introduce maximum velocity particles also derive stochastic gradient versions algorithm show resulting algorithms bear interesting relationships gradient clipping rmsprop adagrad adam popular optimisation methods deep learning based develop relativistic stochastic gradient descent taking zerotemperature limit relativistic stochastic gradient hamiltonian monte carlo experiments show relativistic algorithms perform better classical newtonian variants adam\",\"contrastive divergence algorithm achieved notable success training energybased models including restricted boltzmann machines played key role emergence deep learning idea algorithm approximate intractable term exact gradient loglikelihood function using short markov chain monte carlo mcmc runs approximate gradient computationallycheap biased whether algorithm provides asymptotically consistent estimate still open questions paper studies asymptotic properties algorithm canonical exponential families special cases energybased model suppose algorithm runs mcmc transition steps iteration iteratively generates sequence parameter estimates thetatt given iid data sample xiin sim pthetastar conditions commonly obeyed algorithm practice prove existence bounded limit point time average left sumst thetas right infty consistent estimate true parameter thetastar proof based fact thetatt homogenous markov chain conditional data sample xiin chain meets fosterlyapunov drift criterion converges random walk around maximum likelihood estimate range random walk shrinks zero rate mathcalosqrtn sample size infty\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"12_carlo_monte_sampling\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"12_carlo_monte_sampling\"],\"textfont\":{\"size\":12},\"x\":[3.7252111,3.6439815,3.7199986,3.8633301,3.8352609,3.664013,3.7504573,3.6902432,3.6532757,4.065213,3.8226717,3.9157197,3.869746,3.6358383,3.7007346,3.5573008,3.7449481,3.8610995,3.7326458,3.7514937,3.8553727,3.7872207,3.8754547,3.7704885],\"y\":[-0.10106155,-0.08861378,0.25311118,0.77959675,0.7453108,-0.13733095,0.0709235,-0.01834261,-0.17444092,0.20787138,0.13786726,-0.04661945,0.09022047,-0.08638005,-0.123136856,-0.15881753,-0.019388154,0.64891744,0.15371361,-0.08487401,0.07531212,0.08481548,0.26962787,0.10775139],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"recursive partitioning approaches producing treelike models long standing staple predictive modeling last decade mostly sublearners within state art ensemble methods like boosting random forest however fundamental flaw partitioning splitting rule commonly used tree building methods precludes treating different types variables equally clearly manifests methods inability properly utilize categorical variables large number categories ubiquitous new age big data variables often informative current tree methods essentially leave choice either using exposing models severe overfitting propose conceptual framework splitting using leaveoneout loo cross validation selecting splitting variable performing regular split case following carts approach selected variable important consequence approach categorical variables many categories safely used tree building chosen contribute predictive power demonstrate extensive simulation real data analysis novel splitting approach significantly improves performance single tree models ensemble methods utilize trees importantly design algorithm loo splitting variable selection reasonable assumptions increase overall computational complexity compared cart twoclass classification regression tasks approach carries increased computational burden replacing ologn factor cart splitting rule search term\",\"paper examines use residual bootstrap bias correction machine learning regression methods accounting bias important obstacle recent efforts develop statistical inference machine learning methods demonstrate empirically proposed bootstrap bias correction lead substantial improvements bias predictive accuracy context ensembles trees show correction approximated double cost training original ensemble without introducing additional variance method shown improve testset accuracy random forests example problems uci repository\",\"random survival forests rsf powerful method risk prediction rightcensored outcomes biomedical research rsf use logrank split criterion form ensemble survival trees common approach evaluate prediction accuracy rsf model harrells concordance index survival data index conceptually strategy implies split criterion rsf different evaluation criterion interest discrepancy overcome using harrells node splitting evaluation compare difference two split criteria analytically simulation studies respect preference unbalanced splits termed endcut preference ecp specifically show logrank statistic stronger ecp compared index simulation studies help two medical data sets demonstrate accuracy rsf predictions measured harrells improved logrank statistic replaced index node splitting especially true situations censoring rate fraction informative continuous predictor variables high conversely logrank splitting preferable noisy scenarios cbased logrank splitting implemented rpackage ranger recommend harrells split criterion use smaller scale clinical studies logrank split criterion use largescale omics studies\",\"ensemble regression trees become popular statistical tools estimation conditional mean given set predictors however quantile regression trees ensembles yet garnered much attention despite increasing popularity linear quantile regression model work proposes bayesian quantile additive regression trees model shows good predictive performance illustrated using simulation studies real data applications extension tackle binary classification problems also considered\",\"big data comes various ways types shapes forms sizes indeed almost areas science technology medicine public health economics business linguistics social science bombarded ever increasing flows data begging analyzed efficiently effectively paper propose rough idea possible taxonomy big data along commonly used tools handling particular category bigness dimensionality input space sample size usually main ingredients characterization data bigness specific statistical machine learning technique used handle particular big data set depend category falls within bigness taxonomy large small data sets instance require different set tools large small variety among tools discuss preprocessing standardization imputation projection regularization penalization compression reduction selection kernelization hybridization parallelization aggregation randomization replication sequentialization indeed important emphasize right away socalled free lunch theorem applies sense universally superior method outperforms methods categories bigness also important stress fact simplicity sense ockhams razor non plurality principle parsimony tends reign supreme comes massive data conclude comparison predictive performance commonly used methods data sets\",\"article proposed several approaches post processing large ensemble prediction models rules results simulations show post processing methods considered promising used techniques developed estimation quantitative traits markers benchmark bostob housingdata set simulations cases produced models better prediction performance example ones produced random forest rulefit algorithms\",\"variable selection highdimensional linear models received lot attention lately mostly context lregularization part attraction variable selection effect parsimonious models obtained suitable interpretation terms predictive power however regularized linear models often slightly inferior machine learning procedures like tree ensembles tree ensembles hand lack usually formal way variable selection difficult visualize garrotestyle convex penalty trees ensembles particular random forests proposed penalty selects functional groups nodes trees could simple monotone functions individual predictor variables yields parsimonious function fit lends easily visualization interpretation predictive power maintained least level original tree ensemble key feature method tree ensemble fitted tuning parameter needs selected empirical performance demonstrated wide array datasets\",\"tree ensembles random forests boosted trees renowned high prediction performance however interpretability critically limited due enormous complexity study present method make complex tree ensemble interpretable simplifying model specifically formalize simplification tree ensembles model selection problem given complex tree ensemble aim obtaining simplest representation essentially equivalent original one end derive bayesian model selection algorithm optimizes simplified model maintaining prediction performance numerical experiments several datasets showed complicated tree ensembles reasonably approximated interpretable\",\"becoming increasingly important machine learning methods make predictions interpretable well accurate many practical applications interest features feature interactions relevant prediction task present novel method selective bayesian forest classifier strikes balance predictive power interpretability simultaneously performing classification feature selection feature interaction detection visualization builds parsimonious yet flexible models using treestructured bayesian networks samples ensemble models using markov chain monte carlo build feature selection dividing trees two groups according relevance outcome interest method performs competitively classification feature selection benchmarks low high dimensions includes visualization tool provides insight relevant features interactions\",\"estimating strength dependency two variables fundamental exploratory analysis many applications data mining example nonlinear dependencies two continuous variables explored maximal information coefficient mic categorical variables dependent target class selected using gini gain random forests nonetheless dependency measures estimated finite samples interpretability quantification accuracy ranking dependencies become challenging dependency estimates equal variables independent cannot compared computed different sample size inflated chance variables categories paper propose framework adjust dependency measure estimates finite samples adjustments simple applicable dependency measure helpful improving interpretability quantifying dependency improving accuracy task ranking dependencies particular demonstrate approach enhances interpretability mic used proxy amount noise variables gain accuracy ranking variables splitting procedure random forests\",\"article proposes multinomial probit bayesian additive regression trees mpbart multinomial probit extension bart bayesian additive regression trees chipman mpbart flexible allow inclusion predictors describe observed units well available choice alternatives two simulation studies four real data examples show mpbart exhibits good predictive performance comparison discrete choice multiclass classification methods implement mpbart developed package mpbart available freely cran repositories\",\"paper introduces develops novel variable importance score function context ensemble learning demonstrates appeal theoretically empirically proposed score function simple straightforward counterpart proposed context random forest avoiding permutations design computationally efficient random forest variable importance function like random forest variable importance function score handles regression classification seamlessly one distinct advantage proposed score fact offers natural cut zero positive scores indicating importance significance negative scores deemed indications insignificance extra advantage proposed score lies fact works well beyond ensemble trees seamlessly used base learners random subspace learning context examples simulated real demonstrate proposed score compete mostly favorably random forest score\",\"given two possible treatments may exist subgroups benefit greater one treatment problem relevant field marketing treatments may correspond different ways selling product similarly relevant field public policy treatments may correspond specific government programs finally personalized medicine field wholly devoted understanding subgroups individuals benefit particular medical treatments present computationally fast treebased method abtree treatment effect differentiation unlike methods abtree specifically produces decision rules optimal treatment assignment perindividual basis treatment choices selected maximizing overall occurrence desired binary outcome conditional set covariates poster present methodology tree growth pruning show performance results applied simulated data well real data\",\"accurate predictions typically obtained learning machines complex feature spaces induced kernels unfortunately decision rules hardly accessible humans cannot easily used gain insights application domain therefore one often resorts linear models combination variable selection thereby sacrificing predictive power presumptive interpretability introduce feature importance ranking measure firm retrospective analysis arbitrary learning machines allows achieve excellent predictive performance superior interpretation contrast standard raw feature weighting firm takes underlying correlation structure features account thereby able discover relevant features even appearance training data entirely prevented noise desirable properties firm investigated analytically illustrated simulations\",\"characterize study variable importance vimp pairwise variable associations binary regression trees key component involves node mean squared error quantity refer maximal subtree theory naturally extends single trees ensembles trees applies methods like random forests useful importance values random forests used screen variables example used filter high throughput genomic data bioinformatics little theory exists properties\",\"regression trees becoming increasingly popular omnibus predicting tools basis numerous modern statistical learning ensembles part popularity ability create regression prediction without ever specifying structure mean model however method implicitly assumes homogeneous variance across entire explanatoryvariable space unknown algorithm behaves faced heteroscedastic data study assess performance popular regressiontree algorithm singlevariable setting simple stepfunction model heteroscedasticity use simulation show locations splits hence ability accurately predict means adversely influenced change variance identify pruning algorithm main concern although effects splitting algorithm may meaningful applications\",\"testament success theory random forests long outpaced application practice paper take step towards narrowing gap providing consistency result online random forests\",\"tree ensembles random forest boosted trees renowned high prediction performance whereas interpretability critically limited paper propose post processing method improves model interpretability tree ensembles learning complex tree ensembles standard way approximate simpler model interpretable human obtain simpler model derive algorithm minimizing divergence complex ensemble synthetic experiment showed complicated tree ensemble approximated reasonably interpretable\",\"motivation work improve performance standard stacking approaches ensembles composed simple heterogeneous base models integration generation selection stages regression problems propose two extensions standard stacking approach first extension combine set standard stacking approaches ensemble ensembles using twostep ensemble learning regression setting second extension consists two parts initial part diversity mechanism injected original training data set systematically generating different training subsets partitions corresponding ensembles ensembles final part measuring quality different partitions ensembles maxmin rulebased selection algorithm used select appropriate ensemblepartition make final prediction show based experiments broad range data sets second extension performs better best standard stacking approaches good oracle databases best base model selected crossvalidation data set addition second extension performs better two stateoftheart ensemble methods regression good third stateoftheart ensemble method\",\"data analysis machine learning become integrative part modern scientific methodology offering automated procedures prediction phenomenon based past observations unraveling underlying patterns data providing insights problem yet caution avoid using machine learning blackbox tool rather consider methodology rational thought process entirely dependent problem study particular use algorithms ideally require reasonable understanding mechanisms properties limitations order better apprehend interpret results accordingly goal thesis provide indepth analysis random forests consistently calling question every part algorithm order shed new light learning capabilities inner workings interpretability first part work studies induction decision trees construction ensembles randomized trees motivating design purpose whenever possible contributions follow original complexity analysis random forests showing good computational performance scalability along indepth discussion implementation details contributed within scikitlearn second part work analyse discuss interpretability random forests eyes variable importance measures core contributions rests theoretical characterization mean decrease impurity variable importance measure prove derive properties case multiway totally randomized trees asymptotic conditions consequence work analysis demonstrates variable importances\",\"consider problem learning forest nonlinear decision rules general loss functions standard methods employ boosted decision trees adaboost exponential loss friedmans gradient boosting general loss contrast traditional boosting algorithms treat tree learner black box method propose directly learns decision forests via fullycorrective regularized greedy search using underlying forest structure method achieves higher accuracy smaller models gradient boosting adaboost exponential loss many datasets\",\"paper examines experimental perspective random forests increasingly used statistical method classification regression problems introduced leo breiman first aims confirming known sparse advice using random forests proposing complementary remarks standard problems well high dimensional ones number variables hugely exceeds sample size main contribution paper twofold provide insights behavior variable importance index based random forests addition propose investigate two classical issues variable selection first one find important variables interpretation second one restrictive try design good prediction model strategy involves ranking explanatory variables using random forests score importance stepwise ascending variable introduction strategy\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"13_trees_ensembles_ensemble\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"13_trees_ensembles_ensemble\"],\"textfont\":{\"size\":12},\"x\":[-0.26146498,-0.3085554,-0.08916004,-0.3078359,0.77199334,-0.3459877,-0.37224373,-0.33729625,-0.37051153,-0.3254751,-0.0921266,-0.39954832,-0.10385701,-0.38196734,-0.3760265,-0.26655957,-0.36873057,-0.37378317,-0.25688562,-0.31634724,-0.27629554,-0.40498376,-0.2528931],\"y\":[0.59057915,0.59460294,0.6989244,0.6298738,0.7756085,0.61620486,0.58485323,0.61347204,0.5684698,0.5918227,0.68361497,0.5145047,0.6473804,0.57910955,0.51339024,0.6450041,0.53775984,0.6167937,0.6197545,0.5394482,0.8685557,0.50566137,0.6152449],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"genomewide association study gwas correlates marker variation trait variation sample individuals study subject genotyped multitude snps single nucleotide polymorphisms spanning genome assume subjects unrelated collected random trait values normally distributed transformed normality past decade researchers remarkably successful applying gwas analysis hundreds traits massive amount data produced studies present unique computational challenges penalized regression lasso mcp penalties capable selecting handful associated snps millions potential snps unfortunately model selection corrupted false positives false negatives obscuring genetic underpinning trait paper introduces iterative hard thresholding iht algorithm gwas analysis continuous traits parallel implementation iht accommodates snp genotype compression exploits multiple cpu cores graphics processing units gpus allows statistical geneticists leverage commodity desktop computers gwas analysis avoid supercomputing evaluate iht performance simulated real gwas data conclude reduces false positive false negative rates remaining competitive computational time penalized regression source code freely available httpsgithubcomklkeysihtjl\",\"portable wearable wireless electrocardiogram ecg systems potential used pointofcare cardiovascular disease diagnostic systems wearable wireless ecg systems require automatic detection cardiovascular disease even primary care automation ecg diagnostic systems improve efficiency ecg diagnosis reduce minimal training requirement local healthcare workers however fully automatic myocardial infarction disease detection algorithms well developed paper presents novel automatic classification algorithm using second order ordinary differential equation ode time varying coefficients simultaneously captures morphological dynamic feature highly correlated ecg signals effectively estimating unobserved state variables parameters second order ode accuracy classification significantly improved estimated time varying coefficients second order ode used input support vector machine svm classification proposed method applied ptb diagnostic ecg database within physionet overall sensitivity specificity classification accuracy lead ecgs binary classifications respectively also found even using one lead ecg signals reach accuracy high multiclass classification challenging task developed ode approach lead ecgs coupled multiclass svm reached accuracy classifying subgroups healthy controls\",\"advances modern sensing sequencing technologies generate deluge high dimensional spacetemporal physiological nextgeneration sequencing ngs data physiological traits observed either continuous random functions dense grid referred functionvalued traits physiological ngs data highly correlated data inherent order spacing functional nature ignored traditional summarybased univariate multivariate regression methods designed quantitative genetic analysis scalar trait common variants capture morphological dynamic features data utilize dependent structure propose functional linear model flm trait curve modeled response function genetic variation genomic region gene modeled functional predictor genetic effects modeled function time genomic position flmf genetic analysis functionvalued trait gwas ngs data extensive simulations demonstrate flmf correct type error rates much higher power detect association existing methods flmf applied sleep data starr county health studies oxygen saturation measured seconds average individuals found genes significantly associated oxygen saturation functional trait pvalues ranging results clearly demonstrate flmf substantially outperforms traditional genetic models scalar trait\",\"paper concerned problems interaction screening nonlinear classification highdimensional setting propose twostep procedure iissqda first step innovated interaction screening iis approach based transforming original pdimensional feature vector proposed second step sparse quadratic discriminant analysis sqda proposed selecting important interactions main effects simultaneously conducting classification iis approach screens important interactions examining features instead twoway interactions order theory shows proposed method enjoys sure screening property interaction selection highdimensional setting growing exponentially sample size selection classification step establish sparse inequality estimated coefficient vector qda prove classification error procedure upperbounded oracle classification error plus smaller order term extensive simulation studies real data analysis show proposal compares favorably existing methods interaction selection highdimensional classification\",\"introduce new approach variable selection called predictive correlation screening predictor design predictive correlation screening pcs implements false positive control selected variables well suited small sample sizes scalable high dimensions establish asymptotic bounds familywise error rate fwer resultant mean square error linear predictor selected variables apply predictive correlation screening following twostage predictor design problem experimenter wants learn multivariate predictor gene expressions based successive biological samples assayed mrna arrays assays whole genome samples assays selects small number variables using predictive correlation screening reduce assay cost subsequently assays selected variables remaining samples learn predictor coefficients show superiority predictive correlation screening relative lasso correlation learning sometimes popularly referred literature marginal regression simple thresholding terms performance computational complexity\",\"often wish predict large number variables depend well observed variables structured prediction methods essentially combination classification graphical modeling combining ability graphical models compactly model multivariate data ability classification methods perform prediction using large sets input features tutorial describes conditional random fields popular probabilistic method structured prediction crfs seen wide application natural language processing computer vision bioinformatics describe methods inference parameter estimation crfs including practical issues implementing large scale crfs assume previous knowledge graphical modeling tutorial intended useful practitioners wide variety fields\",\"regularized logistic regression become workhorse data mining bioinformatics widely used many classification problems particularly ones many features however regularization typically selects many features socalled false positives unavoidable paper demonstrate analyze aggregation method sparse logistic regression high dimensions approach linearly combines estimators suitable set logistic models different underlying sparsity patterns balance predictive ability model interpretability numerical performance proposed aggregation method investigated using simulation studies also analyze published genomewide casecontrol dataset evaluate usefulness aggregation method multilocus association mapping\",\"propose novel application simultaneous orthogonal matching pursuit somp procedure sparsistant variable selection ultrahigh dimensional multitask regression problems screening variables introduced citefansis efficient highly scalable way remove many irrelevant variables set variables retaining relevant variables somp applied problems hundreds thousands variables number variables reduced manageable size computationally demanding procedure used identify relevant variables regression outputs knowledge first attempt utilize relatedness multiple outputs perform fast screening relevant variables main theoretical contribution prove asymptotically somp guaranteed reduce ultrahigh number variables sample size without losing true relevant variables also provide formal evidence modified bayesian information criterion bic used efficiently determine number iterations somp provide empirical evidence benefit variable selection using multiple regression outputs jointly opposed performing variable selection output separately finite sample performance somp demonstrated extensive simulation studies genetic association mapping problem keywords adaptive lasso greedy forward regression orthogonal matching pursuit multioutput regression multitask learning simultaneous orthogonal matching pursuit sure screening variable selection\",\"realizations stochastic process often observed temporal data functional data growing interests classification dynamic functional data basic feature functional data functional data infinite dimensions highly correlated essential issue classifying dynamic functional data effectively reduce dimension explore dynamic feature however statistical methods dynamic data classification directly used rich dynamic features data propose use second order ordinary differential equation ode model dynamic process principal differential analysis estimate constant timevarying parameters ode examine differential dynamic properties dynamic system across different conditions including stability transientresponse determine dynamic systems maintain functions performance broad range random internal external perturbations use parameters ode features classifiers proof principle proposed methods applied classifying normal abnormal qrs complexes electrocardiogram ecg data analysis great clinical values diagnosis cardiovascular diseases show odebased classification methods qrs complex classification outperform currently widely used neural networks fourier expansion coefficients functional data features expect dynamic modelbased classification methods may open new avenue functional data classification\",\"machine learning methods used discover complex nonlinear relationships biological medical data however sophisticated learning models computationally unfeasible data millions features introduce first feature selection method nonlinear learning problems scale large ultrahigh dimensional biological data specifically scale novel hilbertschmidt independence criterion lasso hsic lasso handle millions features tens thousand samples proposed method guaranteed find optimal subset maximally predictive features minimal redundancy yielding higher predictive power improved interpretability effectiveness demonstrated applications classify phenotypes based module expression human prostate cancer patients detect enzymes among protein structures achieve high accuracy one million features dimensionality reduction algorithm implemented commodity cloud computing platforms dramatic reduction features may lead ubiquitous deployment sophisticated prediction models mobile health care applications\",\"thesis responds challenges using large number thousands features regression classification problems two situations high dimensional features arise one high dimensional measurements available example gene expression data produced microarray techniques computational reasons people may select small subset features modelling data looking relevant features predicting response based measure correlation response training data although used commonly procedure make response appear predictable actually chapter propose bayesian method avoid selection bias application naive bayes models mixture models high dimensional features also arise consider highorder interactions number parameters increase exponentially order considered chapter propose method compressing group parameters single one exploiting fact many predictor variables derived highorder interactions values training cases number compressed parameters may converged considering highest possible order apply compression method logistic sequence prediction models logistic classification models use simulated data real data test methods chapters\",\"many complex diseases wide variety ways individual manifest disease challenge personalized medicine develop tools accurately predict trajectory individuals disease turn enable clinicians optimize treatments represent individuals disease trajectory continuousvalued continuoustime function describing severity disease time propose hierarchical latent variable model individualizes predictions disease trajectories model shares statistical strength across observations different resolutionsthe population subpopulation individual level describe algorithm learning population subpopulation parameters offline online procedure dynamically learning individualspecific parameters finally validate model task predicting course interstitial lung disease leading cause death among patients autoimmune disease scleroderma compare approach stateoftheart demonstrate significant improvements predictive accuracy\",\"paper treats problem screening variables high correlations high dimensional data many fewer samples variables focus thresholdbased correlation screening methods three related applications screening variables large correlations within single treatment autocorrelation screening screening variables large crosscorrelations two treatments crosscorrelation screening screening variables persistently large autocorrelations two treatments persistentcorrelation screening novelty correlation screening identifies smaller number variables highly correlated others compared identifying number correlation parameters correlation screening suffers phase transition phenomenon correlation threshold decreases number discoveries increases abruptly obtain asymptotic expressions mean number discoveries phase transition thresholds function number samples number variables joint sample distribution also show weak dependency condition number discoveries dominated poisson random variable giving asymptotic expression false positive rate correlation screening approach bears tremendous dividends terms type strength asymptotic results obtained also overcomes major hurdles faced existing methods literature correlation screening naturally scalable high dimension numerical results strongly validate theory presented paper illustrate application correlation screening methodology large scale geneexpression dataset revealing influential variables exhibit significant amount correlation multiple treatments\",\"reshef reshef recently published paper present method called maximal information coefficient mic detect forms statistical dependence pairs variables sample size goes infinity method praised also criticized lack power finite samples seek modify mic higher power detecting associations limited sample sizes present generalized mean information coefficient gmic generalization mic incorporates tuning parameter used modify complexity association favored measure define gmic prove maintains several key asymptotic properties mic increased power mic demonstrated using simulation eight different functional relationships sixty different noise levels results compared pearson correlation distance correlation mic simulation results suggest generally gmic slightly lower power distance correlation measure achieves higher power mic many forms underlying association functional relationships gmic surpasses statistics calculated preliminary results suggest choosing moderate value tuning parameter gmic yield test robust across underlying relationships gmic promising new method mitigates power issues suffered mic possible expense equitability nonetheless distance correlation simulations powerful many forms underlying relationships minimum work motivates consideration maximal informationbased nonparametric exploration mine methods statistical tests independence\",\"data sets many features observations independent screening based univariate regression models leads computationally convenient variable selection method recent efforts shown case generalized linear models independent screening may suffice capture relevant features high probability even ultrahigh dimension unclear whether formal sure screening property attainable response rightcensored survival time propose computationally efficient independent screening method survival data viewed natural survival equivalent correlation screening state conditions method admits sure screening property within general class singleindex hazard rate models ultrahigh dimensional features iterative variant also described combines screening penalized regression order handle complex feature covariance structures methods evaluated simulation studies application real gene expression dataset\",\"predicting individuals risk experiencing future clinical outcome statistical task important consequences practicing clinicians public health experts modern observational databases electronic health records ehrs provide alternative longitudinal cohort studies traditionally used construct risk models bringing opportunities challenges large sample sizes detailed covariate histories enable use sophisticated machine learning techniques uncover complex associations interactions observational databases often messy high levels missing data incomplete patient followup paper propose adaptation wellknown naive bayes machine learning approach classification timetoevent outcomes subject censoring compare predictive performance method cox proportional hazards model commonly used risk prediction healthcare populations illustrate application prediction cardiovascular risk using ehr dataset large midwest integrated healthcare system\",\"technique formal concept analysis applied dataset describing traits rodents goal identifying zoonotic disease carriersor species carrying infections spillover cause human disease concepts identified among species together provide rulesofthumb intrinsic biological features rodents carry zoonotic diseases offer utility better targeting field surveillance efforts search novel disease carriers wild\",\"date instability prognostic predictors sparse high dimensional model hinders clinical adoption received little attention stable prediction often overlooked favour performance yet stability prevails key adopting models critical areas healthcare study proposes stabilization scheme detecting higher order feature correlations using linear model basis prediction achieve feature stability regularising latent correlation features latent higher order correlation among features modelled using autoencoder network stability enhanced combining recent technique uses feature graph augmenting external unlabelled data training autoencoder network experiments conducted heart failure cohort australian hospital stability measured using consistency index feature subsets signaltonoise ratio model parameters methods demonstrated significant improvement feature stability model estimation stability compared baselines\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"14_screening_correlation_features\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"14_screening_correlation_features\"],\"textfont\":{\"size\":12},\"x\":[-0.8508429,-0.5284075,-0.8750978,-0.65617234,-0.82346964,-0.7221773,-0.7487747,-0.6667719,-0.5753654,-0.7099468,-0.74837935,-0.71993953,-0.9325021,-0.5177133,-0.8356498,-0.55506444,-0.7498099,-0.57919323,-0.71084875],\"y\":[2.7765412,2.4605312,2.757328,2.8137913,2.8892834,2.69995,2.5870898,2.9936597,2.4690943,2.6515336,2.6942244,2.6290855,2.834829,1.2986294,2.8378992,2.3681192,2.6679492,2.4995222,2.60717],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"distance metric plays important role nearest neighbor classification usually euclidean distance metric assumed mahalanobis distance metric optimized improve performance paper study problem embedding arbitrary metric spaces euclidean space goal improve accuracy classifier propose solution appealing framework regularization reproducing kernel hilbert space prove representerlike theorem classification embedding function determined solving semidefinite program interesting connection softmargin linear binary support vector machine classifier although main focus paper present general theoretical framework metric embedding setting demonstrate performance proposed method benchmark datasets show performs better mahalanobis metric learning algorithm terms leaveoneout generalization errors\",\"local linear embedding algorithm lle nonlinear dimensionreducing technique widely used due computational simplicity intuitive approach lle first linearly reconstructs input point nearest neighbors preserves neighborhood relations lowdimensional embedding show reconstruction weights computed lle capture highdimensional structure neighborhoods lowdimensional manifold structure consequently weight vectors highly sensitive noise moreover causes lle converge linear projection input opposed nonlinear embedding goal overcome problems propose compute weight vectors using lowdimensional neighborhood representation prove theoretically straightforward computationally simple modification lle reduces lles sensitivity noise modification also removes need regularization number neighbors larger dimension input present numerical examples demonstrating perturbation linear projection problems improved outputs using lowdimensional neighborhood representation\",\"maximum variance unfolding one main methods nonlinear dimensionality reduction study large sample limit providing specific rates convergence standard assumptions find consistent underlying submanifold isometric convex subset provide simple examples fails consistent\",\"research manifold learning within density ridge estimation framework shown great potential recent work estimation denoising manifolds building intuitive welldefined notion principal curves surfaces however problem unwrapping unfolding manifolds received relatively little attention within density ridge approach despite integral part manifold learning general paper proposes two novel algorithms unwrapping manifolds based estimated principal curves surfaces one multidimensional manifolds respectively methods unwrapping founded realization principal curves principal surfaces inherent local maxima probability density function following observation coordinate systems follow shape manifold computed following integral curves gradient flow kernel density estimate manifold furthermore since integral curves gradient flow kernel density estimate inherently local propose stitch together local coordinate systems using parallel transport along manifold provide numerical experiments real synthetic data illustrates clear intuitive unwrapping results comparable stateoftheart manifold learning algorithms\",\"recent years manifold learning become increasingly popular tool performing nonlinear dimensionality reduction led development numerous algorithms varying degrees complexity aim recover man ifold geometry using either local global features data building laplacian eigenmap diffusionmaps framework propose new paradigm offers guarantee reasonable assumptions manifo learning algorithm preserve geometry data set approach based augmenting output embedding algorithms geometric informatio embodied riemannian metric manifold provide algorithm estimating riemannian metric data demonstrate possible application approach variety examples\",\"propose novel method introducing structure existing machine learning techniques developing structurebased similarity distance measures learn structural information lowdimensional structure data captured solving nonlinear lowrank representation problem show lowrank representation kernelized closedform solution allows separation independent manifolds robust noise representation similarity observations based nonlinear structure computed incorporated existing feature transformations dimensionality reduction techniques machine learning methods experimental results synthetic real data sets show performance improvements clustering anomaly detection use structural similarity\",\"present procrustes measure novel measure based procrustes rotation enables quantitative comparison output manifoldbased embedding algorithms lle roweis saul isomap tenenbaum measure also serves natural tool choosing dimensionreduction parameters also present two novel dimensionreduction techniques attempt minimize suggested measure compare results techniques results existing algorithms finally suggest simple iterative method used improve output existing algorithms\",\"density modeling notoriously difficult high dimensional data one approach problem search lower dimensional manifold captures main characteristics data recently gaussian process latent variable model gplvm successfully used find low dimensional manifolds variety complex data gplvm consists set points low dimensional latent space stochastic map observed space show interpreted density model observed space however gplvm trained density model therefore yields bad density estimates propose new training strategy obtain improved generalisation performance better density estimates comparative evaluations several benchmark data sets\",\"introduce locally linear latent variable model lllvm probabilistic model nonlinear manifold discovery describes joint distribution observations manifold coordinates locally linear maps conditioned set neighbourhood relationships model allows straightforward variational optimisation posterior distribution coordinates locally linear maps latent space observation space given data thus lllvm encapsulates localgeometry preserving intuitions underlie nonprobabilistic methods locally linear embedding lle probabilistic semantics make easy evaluate quality hypothesised neighbourhood relationships select intrinsic dimensionality manifold construct outofsample extensions combine manifold model additional probabilistic models capture structure coordinates within manifold\",\"paper autoassociative models proposed candidates generalization principal component analysis show models dedicated approximation dataset manifold word manifold refers topology properties structure approximating manifold built projection pursuit algorithm step algorithm dimension manifold incremented theoretical properties provided particular show step algorithm mean residuals norm increased moreover also established algorithm converges finite number steps particular autoassociative models exhibited compared classical pca neural networks models implementation aspects discussed show numerous cases optimization procedure required illustrations simulated real data presented\",\"learning low dimensional structure multidimensional data canonical problem machine learning one common approach suppose observed data close lowerdimensional smooth manifold rich variety manifold learning methods available allow mapping data points manifold however clear lack probabilistic methods allow learning manifold along generative distribution observed data best attempt gaussian process latent variable model gplvm identifiability issues lead poor performance solve issues proposing novel coulomb repulsive process corp locations points manifold inspired physical models electrostatic interactions among particles combining process prior mapping function yields novel electrostatic electrogp process focusing simple case onedimensional manifold develop efficient inference algorithms illustrate substantially improved performance variety experiments including filling missing frames video\",\"paper presents new framework manifold learning based sequence principal polynomials capture possibly nonlinear nature data proposed principal polynomial analysis ppa generalizes pca modeling directions maximal variance means curves instead straight lines contrarily previous approaches ppa reduces performing simple univariate regressions makes computationally feasible robust moreover ppa shows number interesting analytical properties first ppa volumepreserving map turn guarantees existence inverse second inverse obtained closed form invertibility important advantage learning methods permits understand identified features input domain data physical meaning moreover allows evaluate performance dimensionality reduction sensible inputdomain units volume preservation also allows easy computation information theoretic quantities reduction multiinformation transform third analytical nature ppa leads clear geometrical interpretation manifold allows computation frenetserret frames local features generalized curvatures point space fourth analytical jacobian allows computation metric induced data thus generalizing mahalanobis distance properties demonstrated theoretically illustrated experimentally performance ppa evaluated dimensionality redundancy reduction synthetic real datasets uci repository\",\"class schoenberg transformations embedding euclidean distances higher dimensional euclidean spaces presented derived theorems positive definite conditionally negative definite matrices original results arc lengths angles curvature transformations proposed visualized artificial data sets classical multidimensional scaling simple distancebased discriminant algorithm illustrates theory intimately connected gaussian kernels machine learning\",\"present contribution suggests use multidimensional scaling mds algorithm visualization tool manifoldvalued elements visualization tool kind useful signal processing machine learning whenever learningadaptation algorithms insist highdimensional parameter manifolds\",\"analyze performance class manifoldlearning algorithms find output minimizing quadratic form normalization constraints class consists locally linear embedding lle laplacian eigenmap local tangent space alignment ltsa hessian eigenmaps hlle diffusion maps present prove conditions manifold necessary success algorithms finite sample case limit case analyzed show simple manifolds necessary conditions violated hence algorithms cannot recover underlying manifolds finally present numerical results demonstrate claims\",\"modern data analyst must cope data encoded various forms vectors matrices strings graphs consequently statistical machine learning models tailored different data encodings important focus data encoded normalized vectors direction important magnitude specifically consider highdimensional vectors lie either surface unit hypersphere real projective plane data briefly review common mathematical models prevalent machine learning also outlining technical aspects software applications open mathematical challenges\",\"manifold learning dimensionality reduction techniques ubiquitous science engineering computationally expensive procedures applied large data sets similarities expensive compute date little work done investigate tradeoff computational resources quality learned representations present theoretical experimental explorations question particular consider laplacian eigenmaps embeddings based kernel matrix explore embeddings behave kernel matrix corrupted occlusion noise main theoretical result shows modest noise occlusion assumptions high probability recover good approximation laplacian eigenmaps embedding based uncorrupted kernel matrix results also show regularization aid approximation experimentally explore effects noise occlusion laplacian eigenmaps embeddings two realworld data sets one speech processing one neuroscience well synthetic data set\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"15_manifold_embedding_manifolds\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"15_manifold_embedding_manifolds\"],\"textfont\":{\"size\":12},\"x\":[1.4711126,1.6195433,1.7032446,1.7855446,1.6935848,1.6119392,1.6040462,1.8605245,1.8788058,1.6722015,1.7966434,1.6651387,1.5604672,1.7091765,1.685168,1.6274525,1.7913091,1.6903471],\"y\":[3.383526,4.1052637,4.032894,4.0012736,4.055756,4.2529674,4.1148844,3.9431467,3.9736094,4.1836367,3.9815574,4.0203304,3.8490548,4.0166574,4.114782,3.9188802,3.9501858,3.994024],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"variational inference powerful concept underlies many iterative approximation algorithms expectation propagation meanfield methods belief propagations central themes school perceived unifying framework lectures manfred opper introduce archetypal example expectation propagation establishing connection approximation methods corrections expansion expectation propagation explained finally advanced inference topics applications explored final sections\",\"stochastic variational inference relatively well known scaling inference bayesian probabilistic models related methods also offer ways circumnavigate approximation analytically intractable expectations key challenge either setting controlling variance gradient estimates recent work shown continuous latent variables particularly multivariate gaussians achieved using gradient log posterior paper apply idea gamma distributed latent variables given gamma variational distributions enabling straightforward black box variational inference models sparsity nonnegativity appropriate demonstrate method recently proposed gamma process model network data well novel sparse factor analysis outperform generic sampling algorithms approach using gaussian variational distributions transformed variables\",\"approximate bayesian computation abc framework performing likelihoodfree posterior inference simulation models stochastic variational inference svi appealing alternative inefficient sampling approaches commonly used abc however svi highly sensitive variance gradient estimators problem exacerbated approximating likelihood draw upon recent advances variance reduction likelihoodfree inference using deterministic simulations produce low variance gradient estimators variational lowerbound exploiting automatic differentiation libraries avoid nearly modelspecific derivations demonstrate performance three problems compare existing svi algorithms results demonstrate correctness efficiency algorithm\",\"derive novel variational expectation maximization approach based truncated posterior distributions truncated distributions proportional exact posteriors within subsets discrete state space equal zero otherwise treatment distributions subsets variational parameters distinguishes approach previous variational approaches specific structure truncated distributions allows deriving novel mathematically grounded results turn used formulate novel efficient algorithms optimize parameters probabilistic generative models centrally find variational lower bounds correspond truncated distributions given concise efficiently computable expressions update equations model parameters remain standard form based findings show efficient easily applicable metaalgorithms formulated guarantee monotonic increase variational bound example applications derived framework provide novel theoretical results learning procedures latent variable models well mixture models furthermore show truncated variation naturally interpolates standard full posteriors based maximum aposteriori state map approach therefore regarded generalization popular hard approach towards similarly efficient method capture true posterior structure\",\"article describe model derivation implementation variational bayesian inference linear logistic regression without automatic relevance determination dual function acting tutorial derivation variational bayesian inference simple models well documenting providing brief examples matlaboctave functions implement inference functions freely available online\",\"variational inference lies core many stateoftheart algorithms improve approximation posterior beyond parametric families proposed include mcmc steps variational lower bound work explore idea using steps hamiltonian monte carlo hmc algorithm efficient mcmc method particular incorporate acceptance step hmc algorithm guaranteeing asymptotic convergence true posterior additionally introduce extensions hmc algorithm geared towards faster convergence theoretical advantages modifications reflected performance improvements experimental results\",\"laplace approximation calls computation second derivatives likelihood maximum maximum found emalgorithm convenient way compute derivatives likelihood gradient obtained emauxiliary hessian obtained gradient pearlmutter trick\",\"mean field variational bayes mfvb popular posterior approximation method due fast runtime largescale data sets however well known major failing mfvb underestimates uncertainty model variables sometimes severely provides information model variable covariance generalize linear response methods statistical physics deliver accurate uncertainty estimates model variablesboth individual variables coherently across variables call method linear response variational bayes lrvb mfvb posterior approximation exponential family lrvb simple analytic form even nonconjugate models indeed make assumptions form true posterior demonstrate accuracy scalability method range models simulated real data\",\"variational inference scalable technique approximate bayesian inference deriving variational inference algorithms requires tedious modelspecific calculations makes difficult automate propose automatic variational inference algorithm automatic differentiation variational inference advi user provides bayesian model dataset nothing else make conjugacy assumptions support broad class models algorithm automatically determines appropriate variational family optimizes variational objective implement advi stan code available probabilistic programming framework compare advi mcmc sampling across hierarchical generalized linear models nonconjugate matrix factorization mixture model train mixture model quarter million images advi use variational inference model write stan\",\"highlight pitfall applying stochastic variational inference general bayesian networks global random variables approximated exponential family distribution natural gradient steps commonly starting unit length step size averaged convergence useful insight scaling initial step sizes lost approximation factorizes across general bayesian network care must taken ensure practical convergence experimentally investigate much baby wellscaled steps thrown bath water exact gradients\",\"meanfield variational methods widely used approximate posterior inference many probabilistic models typical application meanfield methods approximately compute posterior coordinateascent optimization algorithm model conditionally conjugate coordinate updates easily derived closed form however many models interestlike correlated topic model bayesian logistic regressionare nonconjuate models meanfield methods cannot directly applied practitioners develop variational algorithms casebycase basis paper develop two generic methods nonconjugate models laplace variational inference delta method variational inference methods several advantages allow easily derived variational algorithms wide class nonconjugate models extend unify existing algorithms derived specific models work well realworld datasets studied methods correlated topic model bayesian logistic regression hierarchical bayesian logistic regression\",\"introduce overdispersed blackbox variational inference method reduce variance monte carlo estimator gradient blackbox variational inference instead taking samples variational distribution use importance sampling take samples overdispersed distribution exponential family variational approximation approach general since readily applied exponential family distribution typical choice variational approximation run experiments two nonconjugate probabilistic models show method effectively reduces variance overhead introduced computation proposal parameters importance weights negligible find overdispersed importance sampling scheme provides lower variance blackbox variational inference even latter uses twice number samples results faster convergence blackbox inference procedure\",\"reparameterization gradient become widely used method obtain monte carlo gradients optimize variational objective however technique easily apply commonly used distributions beta gamma without approximations practical applications reparameterization gradient fit gaussian distributions paper introduce generalized reparameterization gradient method extends reparameterization gradient wider class variational distributions generalized reparameterizations use invertible transformations latent variables lead transformed distributions weakly depend variational parameters results new monte carlo gradients combine reparameterization gradients score function gradients demonstrate approach variational inference two complex probabilistic models generalized reparameterization effective even single sample variational distribution enough obtain lowvariance gradient\",\"propose secondorder hessian hessianfree based optimization method variational inference inspired gaussian backpropagation argue quasinewton optimization developed well accomplished generalizing gradient computation stochastic backpropagation via reparametrization trick lower complexity illustrative example apply approach problems bayesian logistic regression variational autoencoder vae additionally compute bounds estimator variance intractable expectations family lipschitz continuous function method practical scalable model free demonstrate method several realworld datasets provide comparisons stochastic gradient methods show substantial enhancement convergence rates\",\"blackbox alpha bbalpha new approximate inference method based minimization alphadivergences bbalpha scales large datasets implemented using stochastic gradient descent bbalpha applied complex probabilistic models little effort since requires input likelihood function gradients gradients easily obtained using automatic differentiation changing divergence parameter alpha method able interpolate variational bayes alpha rightarrow algorithm similar expectation propagation alpha experiments probit regression neural network regression classification problems show bbalpha nonstandard settings alpha alpha usually produces better predictions alpha rightarrow alpha\",\"introduce local expectation gradients general purpose stochastic variational inference algorithm constructing stochastic gradients sampling variational distribution algorithm divides problem estimating stochastic gradients multiple variational parameters smaller subtasks subtask exploits intelligently information coming relevant part variational distribution achieved performing exact expectation single random variable mostly correlates variational parameter interest resulting raoblackwellized estimate low variance work efficiently continuous discrete random variables furthermore proposed algorithm interesting similarities gibbs sampling time unlike gibbs sampling trivially parallelized\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"16_variational_inference_gradient\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"16_variational_inference_gradient\"],\"textfont\":{\"size\":12},\"x\":[3.8458018,3.993645,4.0249953,4.1508803,4.117115,3.9804683,3.7557867,4.0458884,4.0678735,3.833087,4.0496807,4.0873523,4.0607905,3.7554939,4.0008883,4.112784,3.9926581],\"y\":[1.4677888,1.0591677,0.7919539,1.0498506,1.068179,0.7719682,0.49840862,1.0224303,0.9622762,0.83317745,1.0299326,0.93152624,0.9378269,0.44112378,0.89776194,0.9579954,0.92008555],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"improve recently published results resources restricted boltzmann machines rbm deep belief networks dbn required make universal approximators show distribution set binary vectors length arbitrarily well approximated rbm hidden units minimal number pairs binary vectors differing one entry union contains support set important cases number half cardinality support set construct dbn nnb logn hidden layers width capable approximating distribution arbitrarily well confirms conjecture presented roux bengio\",\"significant success reported recently using deep neural networks classification large networks computationally intensive even training implementing trained networks hardware chips limited precision synaptic weights may improve speed energy efficiency several orders magnitude thus enabling integration small lowpower electronic devices motivation develop computationally efficient learning algorithm multilayer neural networks binary weights assuming hidden neurons fanout one algorithm derived within bayesian probabilistic online setting shown work well synthetic realworld problems performing comparably algorithms realvalued weights retaining computational tractability\",\"deep gaussian processes provide flexible approach probabilistic modelling data using either supervised unsupervised learning tractable inference approximations marginal likelihood model must made original approach approximate inference models used variational compression allow approximate variational marginalization hidden variables leading lower bound marginal likelihood model damianou lawrence paper extend idea nested variational compression resulting lower bound likelihood easily parallelized adapted stochastic variational inference\",\"learning deep models using bayesian methods generated significant attention recently largely feasibility modern bayesian methods yield scalable learning inference maintaining measure uncertainty model parameters stochastic gradient mcmc algorithms sgmcmc family diffusionbased sampling methods largescale bayesian learning sgmcmc multivariate stochastic gradient thermostats msgnht augment parameter interest momentum thermostat variable maintain stationary distributions target posterior distributions number variables continuoustime diffusion increases numerical approximation error becomes practical bottleneck better use numerical integrator desirable end propose use efficient symmetric splitting integrator msgnht instead traditional euler integrator demonstrate proposed scheme accurate robust converges faster properties demonstrated desirable bayesian deep learning extensive experiments two canonical models deep extensions demonstrate proposed scheme improves general bayesian posterior sampling particularly deep models\",\"study mixtures factorizing probability distributions represented visible marginal distributions stochastic layered networks take perspective kernel transitions distributions gives unified picture distributed representations arising deep belief networks dbn networks without lateral connections describe combinatorial geometric properties set kernels products kernels realizable dbns network parameters vary describe explicit classes probability distributions including exponential families learned dbns use submodels bound maximal expected kullbackleibler approximation errors dbns depending number hidden layers units contain\",\"recurrent neural networks rnns stand forefront many recent developments deep learning yet major difficulty models tendency overfit dropout shown fail applied recurrent layers recent results intersection bayesian modelling deep learning offer bayesian interpretation common deep learning techniques dropout grounding dropout approximate bayesian inference suggests extension theoretical results offering insights use dropout rnn models apply new variational inference based dropout technique lstm gru models assessing language modelling sentiment analysis tasks new approach outperforms existing techniques best knowledge improves single model stateoftheart language modelling penn treebank test perplexity extends arsenal variational tools deep learning\",\"deep belief networks powerful way model complex probability distributions however learning structure belief network particularly one hidden units difficult indian buffet process used nonparametric bayesian prior directed structure belief network single infinitely wide hidden layer paper introduce cascading indian buffet process cibp provides nonparametric prior structure layered directed belief network unbounded depth width yet allows tractable inference use cibp prior nonlinear gaussian belief network unit additionally vary behavior discrete continuous representations provide markov chain monte carlo algorithms inference belief networks explore structures learned several image data sets\",\"corrupting input hidden layers deep neural networks dnns multiplicative noise often drawn bernoulli distribution dropout provides regularization significantly contributed deep learnings success however understanding multiplicative corruptions prevent overfitting difficult due complexity dnns functional form paper show gaussian prior placed dnns weights applying multiplicative noise induces gaussian scale mixture reparameterized circumvent problematic likelihood function analysis proceed using typeii maximum likelihood procedure derive closedform expression revealing regularization evolves function networks weights results show multiplicative noise forces weights become either sparse invariant rescaling find analysis implications model compression naturally reveals weight pruning rule starkly contrasts commonly used signaltonoise ratio snr snr prunes weights large variances seeing noisy approach recognizes robustness retains empirically demonstrate approach strong advantage snr heuristic competitive retraining soft targets produced teacher model\",\"show neural network arbitrary depth nonlinearities dropout applied every weight layer mathematically equivalent approximation well known bayesian model interpretation might offer explanation dropouts key properties robustness overfitting interpretation allows reason uncertainty deep learning allows introduction bayesian machinery existing deep learning frameworks principled way document appendix main paper dropout bayesian approximation representing model uncertainty deep learning gal ghahramani\",\"since learning typically slow boltzmann machines need restrict connections within hidden layers however resulting states hidden units exhibit statistical dependencies based observation propose using regularization upon activation possibilities hidden units restricted boltzmann machines capture loacal dependencies among hidden units regularization encourages hidden units many groups inactive given observed data also makes hidden units within group compete modeling observed data thus regularization rbms yields sparsity group hidden unit levels call rbms trained regularizer emphsparse group rbms proposed sparse group rbms applied three tasks modeling patches natural images modeling handwritten digits pretaining deep networks classification task furthermore illustrate regularizer also applied deep boltzmann machines lead sparse group deep boltzmann machines adapted mnist data set twolayer sparse group boltzmann machine achieves error rate knowledge best published result permutationinvariant version mnist task\",\"deep gaussian processes dgps multilayer hierarchical generalisations gaussian processes gps formally equivalent neural networks multiple infinitely wide hidden layers dgps probabilistic nonparametric arguably flexible greater capacity generalise provide better calibrated uncertainty estimates alternative deep models focus paper scalable approximate bayesian learning networks paper develops novel efficient extension probabilistic backpropagation stateoftheart method training bayesian neural networks used train dgps new method leverages recently proposed method scaling expectation propagation called stochastic expectation propagation method able automatically discover useful input warping expansion compression therefore flexible form bayesian kernel design demonstrate success new method supervised learning several realworld datasets showing typically outperforms regression never much worse\",\"large multilayer neural networks trained backpropagation recently achieved stateoftheart results wide range problems however using backprop neural net learning still disadvantages tune large number hyperparameters data lack calibrated probabilistic predictions tendency overfit training data principle bayesian approach learning neural networks problems however existing bayesian techniques lack scalability large dataset network sizes work present novel scalable method learning bayesian neural networks called probabilistic backpropagation pbp similar classical backpropagation pbp works computing forward propagation probabilities network backward computation gradients series experiments ten realworld datasets show pbp significantly faster techniques offering competitive predictive abilities experiments also show pbp provides accurate estimates posterior variance network weights\",\"dropout recently emerged powerful simple method training neural networks preventing coadaptation stochastically omitting neurons dropout currently grounded explicit modelling assumptions far precluded adoption bayesian modelling using bayesian entropic reasoning show dropout interpreted optimal inference constraints demonstrate analytically tractable regression model providing bayesian interpretation mechanism regularizing preventing coadaptation well connection bayesian techniques also discuss two general approximate techniques applying bayesian dropout general models one based analytical approximation stochastic variational techniques techniques applied baysian logistic regression problem shown improve performance model become misspecified framework roots dropout theoretically justified practical tool statistical modelling allowing bayesians tap benefits dropout training\",\"effective training deep neural networks suffers two main issues first parameter spaces models exhibit pathological curvature recent methods address problem using adaptive preconditioning stochastic gradient descent sgd methods improve convergence adapting local geometry parameter space second issue overfitting typically addressed early stopping however recent work demonstrated bayesian model averaging mitigates problem posterior sampled using stochastic gradient langevin dynamics sgld however rapidly changing curvature renders default sgld methods inefficient propose combining adaptive preconditioners sgld support idea give theoretical properties asymptotic convergence predictive risk also provide empirical results logistic regression feedforward neural nets convolutional neural nets demonstrating preconditioned sgld method gives stateoftheart performance models\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"17_deep_dropout_networks\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"17_deep_dropout_networks\"],\"textfont\":{\"size\":12},\"x\":[2.623874,2.6073737,3.2201283,3.0207453,2.7469082,2.9327157,2.846611,2.5616498,2.8664587,2.5014582,2.8558266,2.646547,2.913269,3.016837,2.8114572],\"y\":[0.7414996,0.6475331,0.86883354,0.6717829,0.77040154,0.70071614,0.7177116,0.7496289,0.69555247,0.78876173,0.76112264,0.5688214,0.7087824,0.57624197,0.71195644],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"give improved constants data dependent variance sensitive confidence bounds called empirical bernstein bounds extend inequalities hold uniformly classes functionswhose growth function polynomial sample size bounds lead consider sample variance penalization novel learning method takes account empirical variance loss function give conditions sample variance penalization effective particular present bound excess risk incurred method using argue situations excess risk method order excess risk empirical risk minimization order sqrtn show experimental results confirm theory finally discuss potential application results sample compression schemes\",\"article derive concentration inequalities crossvalidation estimate generalization error stable predictors context risk assessment notion stability first introduced citedewa extended citekea citebe citekuniy characterize class predictors infinite dimension particular covers knearest neighbors rules bayesian algorithm citekea boosting general loss functions class predictors considered use formalism introduced citedud cover large variety crossvalidation procedures including leaveoneout crossvalidation kfold crossvalidation holdout crossvalidation split sample leaveupsilonout crossvalidation particular give simple rule choose crossvalidation depending stability class predictors special case uniform stability interesting consequence number elements test set required grow infinity consistency crossvalidation procedure special case particular interest leaveoneout crossvalidation emphasized\",\"analyze generalization robustness batched weighted average algorithm vgeometrically ergodic markov data algorithm good alternative empirical risk minimization algorithm latter suffers overfitting optimizing empirical risk hard generalization algorithm prove pacstyle bound training sample size expected lloss converge optimal loss training data vgeometrically ergodic markov chains robustness show training target variables values contain bounded noise generalization bound algorithm deviates range noise results applied regression problem classification problem case exists unknown deterministic target hypothesis\",\"highdimensional estimation prediction methods propose minimize cost function empirical risk written sum losses associated data point paper focus case nonconvex losses practically important still poorly understood classical empirical process theory implies uniform convergence empirical risk population risk uniform convergence implies consistency resulting mestimator ensure latter computed efficiently order capture complexity computing mestimators propose study landscape empirical risk namely stationary points properties establish uniform convergence gradient hessian empirical risk population counterparts soon number samples becomes larger number unknown parameters modulo logarithmic factors consequently good properties population risk carried empirical risk establish onetoone correspondence stationary points demonstrate several problems nonconvex binary classification robust regression gaussian mixture model result implies complete characterization landscape empirical risk convergence properties descent algorithms extend analysis highdimensional setting number parameters exceeds number samples provide characterization empirical risk landscape nearly informationtheoretically minimal condition namely number samples exceeds sparsity unknown parameters vector modulo logarithmic factors suitable uniform convergence result takes place apply result nonconvex binary classification robust regression highdimension\",\"model selection crucial issue machinelearning wide variety penalisation methods possibly data dependent complexity penalties recently introduced purpose however empirical performance generally well documented literature goal paper investigate extent recent techniques successfully used tuning regularisation kernel parameters support vector regression svr complexity measure regression trees cart task traditionally solved via vfold crossvalidation vfcv gives efficient results reasonable computational cost disadvantage however vfcv procedure known provide asymptotically suboptimal risk estimate number examples tends infinity recently penalisation procedure called vfold penalisation proposed improve vfcv supported theoretical arguments report extensive set experiments comparing vfold penalisation vfcv svrcart calibration several benchmark datasets highlight cases vfcv vfold penalisation provide poor estimates risk respectively introduce modified penalisation technique reduce estimation error\",\"study paper consequences using mean absolute percentage error mape measure quality regression models show finding best model mape equivalent weighted mean absolute error mae regression also show asumptions universal consistency empirical risk minimization remains possible using mape\",\"article derive concentration inequalities crossvalidation estimate generalization error empirical risk minimizers general setting prove sanitycheck bounds spirit citekr textquotedbllefttextitbounds showing worstcase error estimate much worse training error estimate textquotedblright general loss functions class predictors finite vcdimension considered closely follow formalism introduced citedud cover large variety crossvalidation procedures including leaveoneout crossvalidation fold crossvalidation holdout crossvalidation split sample leaveupsilonout crossvalidation particular focus proving consistency various crossvalidation procedures point interest crossvalidation procedure terms rate convergence estimation curve transition phases depending crossvalidation procedure percentage observations test sample gives simple rule choose crossvalidation interesting consequence size test sample required grow infinity consistency crossvalidation procedure\",\"article derive concentration inequalities crossvalidation estimate generalization error subagged estimators classification regressor general loss functions class predictors finite infinite vcdimension considered slightly generalize formalism introduced citedud cover large variety crossvalidation procedures including leaveoneout crossvalidation kfold crossvalidation holdout crossvalidation split sample leaveupsilonout crossvalidation bigskip noindent interesting consequence probability upper bound bounded minimum hoeffdingtype bound vapniktype bounds thus smaller even small learning set finally give simple rule subbag predictor bigskip\",\"vapnikchervonenkis dimension fundamental measure generalization capacity learning algorithms however apart special cases hard impossible calculate analytically vapnik proposed technique estimating dimension empirically approach behaves well simulations could used bound generalization risk classifiers bounds estimation error dimension rectify omission providing high probability concentration results proposed estimator deriving corresponding generalization bounds\",\"study prediction estimation problems using empirical risk minimization relative general convex loss function obtain sharp error rates even concentration false restricted example heavytailed scenarios results show error rate depends two parameters one captures intrinsic complexity class essentially leads error rate noisefree realizable problem measures interactions class members target loss dominant problem far realizable also explain one may deal outliers choosing loss way calibrated intrinsic complexity class noiselevel problem latter measured distance target class\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"18_crossvalidation_risk_empirical\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"18_crossvalidation_risk_empirical\"],\"textfont\":{\"size\":12},\"x\":[-0.06878926,0.060116183,0.29393348,0.14240007,0.015233015,0.22730456,0.05549031,-0.0030569797,-0.046087846,0.22592847,0.0902472],\"y\":[1.9076664,2.1291068,2.0576146,2.162146,2.5689917,2.2028277,2.178908,2.0833085,1.9274858,2.1173544,2.133541],\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"shapes\":[{\"line\":{\"color\":\"#CFD8DC\",\"width\":2},\"type\":\"line\",\"x0\":3.2328130915760998,\"x1\":3.2328130915760998,\"y0\":-0.6268106818199157,\"y1\":7.515946793556213},{\"line\":{\"color\":\"#9E9E9E\",\"width\":2},\"type\":\"line\",\"x0\":-1.0723774045705796,\"x1\":7.538003587722779,\"y0\":3.4445680558681486,\"y1\":3.4445680558681486}],\"annotations\":[{\"showarrow\":false,\"text\":\"D1\",\"x\":-1.0723774045705796,\"y\":3.4445680558681486,\"yshift\":10},{\"showarrow\":false,\"text\":\"D2\",\"x\":3.2328130915760998,\"xshift\":10,\"y\":7.515946793556213}],\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eDocuments and Topics\\u003c\\u002fb\\u003e\",\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":750,\"xaxis\":{\"visible\":false},\"yaxis\":{\"visible\":false}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f5033b66-8dcc-4b9b-ac11-c2a290b02589');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 85
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example output with 10,000+ datapoints from cs.AI category:\n",
        "![](https://raw.githubusercontent.com/dnxv/BERTopic/refs/heads/main/outputs/embedding-BAAI-bge-base%2Cmin_size_50_1.jpg)"
      ],
      "metadata": {
        "id": "9MnB3oZ0cYLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-compute embeddings"
      ],
      "metadata": {
        "id": "N5NuqtP_cYLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare embeddings\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embedding_model.encode(abstracts, show_progress_bar=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:00:47.108623Z",
          "iopub.execute_input": "2025-05-13T21:00:47.109015Z",
          "iopub.status.idle": "2025-05-13T21:01:37.204189Z",
          "shell.execute_reply.started": "2025-05-13T21:00:47.108984Z",
          "shell.execute_reply": "2025-05-13T21:01:37.203319Z"
        },
        "id": "4fy0y2TFcYLq",
        "outputId": "5cd9ea34-d95a-426d-fdbc-d5a85c0f6a4f",
        "colab": {
          "referenced_widgets": [
            "a0975823266c4c3787a73ff5814ac877",
            "a2dca892bcc34f47836d0467ea7123fc",
            "393248a126574b02b73bf9299934b610",
            "cdd54c527262459a9a59b4ee7fd3f657",
            "c4e1fe83aadf4542abd118fb426b6e35",
            "c02638d6a9b44518a096c90cbe7ee098",
            "6d48fcfd6c134c8590f6df91991f6736",
            "d1433340d99b47bab41c396bf1360d6f",
            "bbbf8a01037b4121a135433a3e75865e",
            "58af5b76ddc14d01b813a6805198ef73",
            "40a94900fb164231bba8ab32de7ac8f4"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/51 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0975823266c4c3787a73ff5814ac877"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 86
    },
    {
      "cell_type": "code",
      "source": [
        "from umap import UMAP\n",
        "\n",
        "reduced_embeddings = UMAP(n_neighbors=30, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
        "\n",
        "df_plot = pd.DataFrame({\n",
        "    \"x1\": [point[0] for point in reduced_embeddings],\n",
        "    \"x2\": [point[1] for point in reduced_embeddings],\n",
        "    \"docs\": abstracts,\n",
        "})\n",
        "\n",
        "df_plot[\"docs_short\"] = df_plot[\"docs\"].str[:100] + \"...\"\n",
        "df_plot.head(10)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:37.205503Z",
          "iopub.execute_input": "2025-05-13T21:01:37.205801Z",
          "iopub.status.idle": "2025-05-13T21:01:42.737239Z",
          "shell.execute_reply.started": "2025-05-13T21:01:37.205774Z",
          "shell.execute_reply": "2025-05-13T21:01:42.736562Z"
        },
        "id": "SnEXvP1NcYLq",
        "outputId": "7ffc83d0-f1f0-4df4-b9f1-44e9cd1f40c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         x1        x2                                               docs  \\\n",
              "0 -0.657060  4.779006  consider problem binary classification one par...   \n",
              "1 -1.518607  3.958432  distance metric plays important role nearest n...   \n",
              "2 -3.145614  6.410346  dendrograms used data analysis ultrametric spa...   \n",
              "3 -3.203511  6.571202  conceptual framework cluster analysis viewpoin...   \n",
              "4 -4.546526  5.217755  present analyse three online algorithms learni...   \n",
              "5 -2.277890  4.325052  recent years kernel density estimation exploit...   \n",
              "6 -0.375601  3.811363  thesis responds challenges using large number ...   \n",
              "7 -3.622922  2.599043  simulated annealing popular method approaching...   \n",
              "8 -4.468881  6.162722  present nested chinese restaurant process ncrp...   \n",
              "9 -2.742345  3.328779  provide selfcontained proof theorem relating p...   \n",
              "\n",
              "                                          docs_short  \n",
              "0  consider problem binary classification one par...  \n",
              "1  distance metric plays important role nearest n...  \n",
              "2  dendrograms used data analysis ultrametric spa...  \n",
              "3  conceptual framework cluster analysis viewpoin...  \n",
              "4  present analyse three online algorithms learni...  \n",
              "5  recent years kernel density estimation exploit...  \n",
              "6  thesis responds challenges using large number ...  \n",
              "7  simulated annealing popular method approaching...  \n",
              "8  present nested chinese restaurant process ncrp...  \n",
              "9  provide selfcontained proof theorem relating p...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-673ab740-3700-47db-b462-b18fa3691047\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>docs</th>\n",
              "      <th>docs_short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.657060</td>\n",
              "      <td>4.779006</td>\n",
              "      <td>consider problem binary classification one par...</td>\n",
              "      <td>consider problem binary classification one par...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.518607</td>\n",
              "      <td>3.958432</td>\n",
              "      <td>distance metric plays important role nearest n...</td>\n",
              "      <td>distance metric plays important role nearest n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-3.145614</td>\n",
              "      <td>6.410346</td>\n",
              "      <td>dendrograms used data analysis ultrametric spa...</td>\n",
              "      <td>dendrograms used data analysis ultrametric spa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-3.203511</td>\n",
              "      <td>6.571202</td>\n",
              "      <td>conceptual framework cluster analysis viewpoin...</td>\n",
              "      <td>conceptual framework cluster analysis viewpoin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-4.546526</td>\n",
              "      <td>5.217755</td>\n",
              "      <td>present analyse three online algorithms learni...</td>\n",
              "      <td>present analyse three online algorithms learni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-2.277890</td>\n",
              "      <td>4.325052</td>\n",
              "      <td>recent years kernel density estimation exploit...</td>\n",
              "      <td>recent years kernel density estimation exploit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.375601</td>\n",
              "      <td>3.811363</td>\n",
              "      <td>thesis responds challenges using large number ...</td>\n",
              "      <td>thesis responds challenges using large number ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-3.622922</td>\n",
              "      <td>2.599043</td>\n",
              "      <td>simulated annealing popular method approaching...</td>\n",
              "      <td>simulated annealing popular method approaching...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-4.468881</td>\n",
              "      <td>6.162722</td>\n",
              "      <td>present nested chinese restaurant process ncrp...</td>\n",
              "      <td>present nested chinese restaurant process ncrp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-2.742345</td>\n",
              "      <td>3.328779</td>\n",
              "      <td>provide selfcontained proof theorem relating p...</td>\n",
              "      <td>provide selfcontained proof theorem relating p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-673ab740-3700-47db-b462-b18fa3691047')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-673ab740-3700-47db-b462-b18fa3691047 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-673ab740-3700-47db-b462-b18fa3691047');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5b2ce1b0-a5a7-4a6d-82a9-ebf18ebafecf\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5b2ce1b0-a5a7-4a6d-82a9-ebf18ebafecf')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5b2ce1b0-a5a7-4a6d-82a9-ebf18ebafecf button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_plot",
              "summary": "{\n  \"name\": \"df_plot\",\n  \"rows\": 1601,\n  \"fields\": [\n    {\n      \"column\": \"x1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1601,\n        \"samples\": [\n          -1.599982738494873,\n          0.8505657911300659,\n          -2.602396011352539\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"x2\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1601,\n        \"samples\": [\n          4.268630027770996,\n          5.366730213165283,\n          4.1065521240234375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"docs\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1601,\n        \"samples\": [\n          \"propose likelihood ratio based inferential framework high dimensional semiparametric generalized linear models framework addresses variety challenging problems high dimensional data analysis including incomplete data selection bias heterogeneous multitask learning work three main contributions develop regularized statistical chromatography approach infer parameter interest proposed semiparametric generalized linear model without need estimating unknown base measure function propose new framework construct postregularization confidence regions tests low dimensional components high dimensional parameters unlike existing postregularization inferential methods approach based novel directional likelihood particular framework naturally handles generic regularized estimators nonconvex penalty functions used infer least false parameters misspecified models iii develop new concentration inequalities normal approximation results ustatistics unbounded kernels independent interest demonstrate consequences general theory using example missing data problem extensive simulation studies real data analysis provided illustrate proposed approach\",\n          \"manuscript consider problem jointly estimating multiple graphical models high dimensions assume data collected subjects consists possibly dependent observations graphical models subjects vary assumed change smoothly corresponding measure closeness subjects propose kernel based method jointly estimating graphical models theoretically double asymptotic framework dimension increase provide explicit rate convergence parameter estimation characterizes strength one borrow across different individuals impact data dependence parameter estimation empirically experiments synthetic real resting state functional magnetic resonance imaging rsfmri data illustrate effectiveness proposed method\",\n          \"propose novel algebraic framework treating probability distributions represented cumulants mean covariance matrix example consider unsupervised learning problem finding subspace several probability distributions agree instead minimizing objective function involving estimated cumulants show treating cumulants elements polynomial ring directly solve problem lower computational cost higher accuracy moreover algebraic viewpoint probability distributions allows invoke theory algebraic geometry demonstrate compact proof identifiability criterion\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"docs_short\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1596,\n        \"samples\": [\n          \"problem finding overlapping communities networks gained much attention recently optimizationbased ap...\",\n          \"propose paper differentiable learning loss time series building upon celebrated dynamic time warping...\",\n          \"estimating state dynamical system series noisecorrupted observations fundamental many areas science ...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "execution_count": 87
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import plotly.io as pio\n",
        "\n",
        "total_docs = len(df_plot)\n",
        "fig = px.scatter(df_plot, x=\"x1\", y=\"x2\",  hover_data=[\"docs_short\"])\n",
        "fig.update_traces(marker=dict(line=dict(width=0.5, color='white')))\n",
        "fig.update_layout(\n",
        "    title=f\"Abstracts\",\n",
        "    title_font_size=20\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:42.738168Z",
          "iopub.execute_input": "2025-05-13T21:01:42.7384Z",
          "iopub.status.idle": "2025-05-13T21:01:42.823966Z",
          "shell.execute_reply.started": "2025-05-13T21:01:42.738382Z",
          "shell.execute_reply": "2025-05-13T21:01:42.822852Z"
        },
        "id": "E2N4eCHHcYLq",
        "outputId": "e371e5b9-75d4-425c-bcc3-cb9e4587d8f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"5393b769-10f5-4a26-88fb-a8883d073648\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5393b769-10f5-4a26-88fb-a8883d073648\")) {                    Plotly.newPlot(                        \"5393b769-10f5-4a26-88fb-a8883d073648\",                        [{\"customdata\":[[\"consider problem binary classification one particular cost choose classify observation present simpl...\"],[\"distance metric plays important role nearest neighbor classification usually euclidean distance metr...\"],[\"dendrograms used data analysis ultrametric spaces hence objects nonarchimedean geometry known exist ...\"],[\"conceptual framework cluster analysis viewpoint padic geometry introduced describing space dendrogra...\"],[\"present analyse three online algorithms learning discrete hidden markov models hmms compare baldicha...\"],[\"recent years kernel density estimation exploited computer scientists model machine learning problems...\"],[\"thesis responds challenges using large number thousands features regression classification problems ...\"],[\"simulated annealing popular method approaching solution global optimization problem existing results...\"],[\"present nested chinese restaurant process ncrp stochastic process assigns probability distributions ...\"],[\"provide selfcontained proof theorem relating probabilistic coherence forecasts nondomination rival f...\"],[\"changepoints abrupt variations generative parameters data sequence online detection changepoints use...\"],[\"characterize study variable importance vimp pairwise variable associations binary regression trees k...\"],[\"monograph deals adaptive supervised classification using tools borrowed statistical mechanics inform...\"],[\"dimensionality reduction topic recent interest paper present classification constrained dimensionali...\"],[\"among easiest ways find meaningful structure discrete data latent dirichlet allocation lda related c...\"],[\"propose investigate test statistics testing homogeneity reproducing kernel hilbert spaces asymptotic...\"],[\"using knearest neighbors method one often ignores uncertainty choice account uncertainty holmes adam...\"],[\"flow cytometry often used characterize malignant cells leukemia lymphoma patients traced level indiv...\"],[\"simple computationally efficient scheme treestructured vector quantization presented unlike previous...\"],[\"analyze performance class manifoldlearning algorithms find output minimizing quadratic form normaliz...\"],[\"present procrustes measure novel measure based procrustes rotation enables quantitative comparison o...\"],[\"problem supervised classification discrimination functional data considered special interest popular...\"],[\"propose new sparsitysmoothness penalty highdimensional generalized additive models combination spars...\"],[\"local linear embedding algorithm lle nonlinear dimensionreducing technique widely used due computati...\"],[\"construct framework studying clustering algorithms includes two key ideas persistence functoriality ...\"],[\"consider principal component analysis pca decomposable gaussian graphical models exploit prior infor...\"],[\"many problems lowlevel computer vision image processing denoising deconvolution tomographic reconstr...\"],[\"present new online boosting algorithm adapting weights boosted classifier yields closer approximatio...\"],[\"report derive nonnegative series expansion jensenshannon divergence jsd two probability distribution...\"],[\"consider problem jointly estimating parameters well structure binary valued markov random fields con...\"],[\"direct way express arbitrary dependencies datasets estimate joint distribution apply afterwards argm...\"],[\"present procedure effective estimation entropy mutual information smallsample data apply problem inf...\"],[\"paper examines experimental perspective random forests increasingly used statistical method classifi...\"],[\"mincut clustering based minimizing one two heuristic costfunctions proposed shi malik spawned tremen...\"],[\"autoencoder neural network implemented estimate missing data genetic algorithm implemented network o...\"],[\"obtain index complexity random sequence allowing role measure classical probability theory played fu...\"],[\"explosion interest statistical models analyzing network data considerable interest class exponential...\"],[\"article introduces new algorithm reconstructing epsilonmachines data well decisional states defined ...\"],[\"runtime kernel partial least squares kpls compute fit quadratic number examples however necessity ob...\"],[\"present first treebased regressor whose convergence rate depends intrinsic dimension data namely ass...\"],[\"recent methods estimating sparse undirected graphs realvalued data high dimensional problems rely he...\"],[\"many statistical methods proposed estimate causal models classical situations fewer variables observ...\"],[\"consider empirical risk minimization problem linear supervised learning regularization structured sp...\"],[\"prove criterion markov equivalence provided zhao may involve set features graph exponential number v...\"],[\"present new boosting algorithm motivated large margins theory boosting give experimental evidence ne...\"],[\"variable selection highdimensional linear models received lot attention lately mostly context lregul...\"],[\"accurate predictions typically obtained learning machines complex feature spaces induced kernels unf...\"],[\"selecting important features nonlinear kernel spaces difficult challenge classification regression p...\"],[\"introduce new bayesian model hierarchical clustering based prior trees called kingmans coalescent de...\"],[\"describe new method visualizing topics distributions terms automatically extracted large text corpor...\"],[\"network models popular modeling representing complex relationships dependencies observed variables d...\"],[\"give improved constants data dependent variance sensitive confidence bounds called empirical bernste...\"],[\"discuss meanfield theory cellular automata model metalearning metalearning process combining outcome...\"],[\"investigate role initialization stability kmeans clustering algorithm opposed papers consider actual...\"],[\"introduce new nearestprototype classifier prototype vector machine pvm arises combinatorial optimiza...\"],[\"section incorrect removed submissions rewritten version posted future...\"],[\"propose extension concept expected improvement criterion commonly used kriging based optimization ex...\"],[\"bnlearn package includes several algorithms learning structure bayesian networks either discrete con...\"],[\"present work new family kernels compare positive measures arbitrary spaces xcal endowed positive ker...\"],[\"present extension sparse pca sparse dictionary learning sparsity patterns dictionary elements struct...\"],[\"describe method inferring linear causal relations among multidimensional variables idea use asymmetr...\"],[\"paper proposes original approach cluster multicomponent data sets including estimation number cluste...\"],[\"propose dirichlet process mixtures generalized linear models dpglm new method nonparametric regressi...\"],[\"last years due growing ubiquity unlabeled data much effort spent machine learning community develop ...\"],[\"many inference problems involving questions optimality ask maximum minimum finite set unknown quanti...\"],[\"paper reviews functional aspects statistical learning theory main point consideration nature hypothe...\"],[\"minimum description length mdl principle states optimal model given data set compresses best due pra...\"],[\"propose method infer causal structures containing discrete continuous variables idea select causal h...\"],[\"inferring causal structure set random variables finite sample joint distribution important problem s...\"],[\"slow feature analysis sfa method extracting slowly varying driving forces quickly varying nonstation...\"],[\"recently increasing interest methods deal multiple outputs motivated partly frameworks like multitas...\"],[\"survey introduction positive definite kernels set methods inspired machine learning literature namel...\"],[\"article addresses modeling reverberant recording environments context underdetermined convolutive bl...\"],[\"paper consider problem hypersparse aggregation namely given dictionary functions look optimal aggreg...\"],[\"extend multiway multivariate anovatype analysis cases one covariate view features view coming differ...\"],[\"interest multioutput kernel methods increasing whether guise multitask learning multisensor networks...\"],[\"study losses binary classification class probability estimation extend understanding margin losses g...\"],[\"provide yet another proof existence calibrated forecasters two merits first valid arbitrary finite n...\"],[\"deep belief networks powerful way model complex probability distributions however learning structure...\"],[\"study graph estimation density estimation high dimensions using family density estimators based fore...\"],[\"assume data independently sampled mixture distribution unit ball ddimensional euclidean space compon...\"],[\"independent component analysis ica aims decomposing observed random vector statistically independent...\"],[\"security issues crucial number machine learning applications especially scenarios dealing human acti...\"],[\"introduce supervised latent dirichlet allocation slda statistical model labelled documents model acc...\"],[\"study problem allocating stocks dark pools propose analyze optimal approach allocations continuousva...\"],[\"propose novel algorithm greedy forward feature selection regularized leastsquares rls regression cla...\"],[\"class schoenberg transformations embedding euclidean distances higher dimensional euclidean spaces p...\"],[\"present contribution suggests use multidimensional scaling mds algorithm visualization tool manifold...\"],[\"paper formulate general terms approach prove strong consistency empirical risk minimisation inductiv...\"],[\"paper consider sparse identifiable linear latent variable factor linear bayesian network models pars...\"],[\"introduce efficient method training linear ranking support vector machine method combines cutting pl...\"],[\"johnsonlindenstrauss lemma allows projection points pdimensional euclidean space onto kdimensional e...\"],[\"improve recently published results resources restricted boltzmann machines rbm deep belief networks ...\"],[\"challenging problem estimating highdimensional graphical models choose regularization parameter data...\"],[\"density modeling notoriously difficult high dimensional data one approach problem search lower dimen...\"],[\"define class euclidean distances weighted graphs enabling perform thermodynamic soft graph clusterin...\"],[\"popular method selecting number clusters based stability arguments one chooses number clusters corre...\"],[\"distributions permutations arise applications ranging multiobject tracking ranking instances difficu...\"],[\"supervised linear feature extraction achieved fitting reduced rank multivariate model paper studies ...\"],[\"support vector machines svms special kernel based methods belong successful learning methods since d...\"],[\"last years many different performance measures introduced overcome weakness natural metric accuracy ...\"],[\"propose simple kernel based nearest neighbor approach handwritten digit classification distance actu...\"],[\"kernel induced random survival forests kirsf statistical learning algorithm aims improve prediction ...\"],[\"since learning typically slow boltzmann machines need restrict connections within hidden layers howe...\"],[\"sharply characterize performance different penalization schemes problem selecting relevant variables...\"],[\"sparse coding consists representing signals sparse linear combinations atoms selected dictionary con...\"],[\"present surrogate regret bounds arbitrary surrogate losses context binary classification labeldepend...\"],[\"finding sparse solutions underdetermined systems linear equations fundamental problem signal process...\"],[\"modeling data linear combinations elements learned dictionary focus much recent research machine lea...\"],[\"nonparametric kernelbased method realizing bayes rule proposed based representations probabilities r...\"],[\"nonparametric classification regression problems regularized kernel methods particular support vecto...\"],[\"study problem learning sparse linear regression vector additional conditions structure sparsity patt...\"],[\"consider problem learning binary classifier training set positive unlabeled examples inductive trans...\"],[\"despite recent progress towards efficient multiple kernel learning mkl structured output case remain...\"],[\"group lasso penalized regression method used regression problems covariates partitioned groups promo...\"],[\"developed efficient algorithm maximum likelihood joint tracking association problem strong clutter g...\"],[\"density ratio defined ratio two probability densities study inference problem density ratios apply s...\"],[\"article derive concentration inequalities crossvalidation estimate generalization error empirical ri...\"],[\"performance lasso well understood assumptions standard linear model homoscedastic noise however seve...\"],[\"often wish predict large number variables depend well observed variables structured prediction metho...\"],[\"article derive concentration inequalities crossvalidation estimate generalization error stable predi...\"],[\"article derive concentration inequalities crossvalidation estimate generalization error subagged est...\"],[\"goal crossdomain object matching cdom find correspondence two sets objects different domains unsuper...\"],[\"translating potential disease biomarkers multispecies omics experiments new direction biomedical res...\"],[\"propose novel algorithm solve expectation propagation relaxation bayesian inference continuousvariab...\"],[\"study problem estimating temporally varying coefficient varying structure vcvs graphical model under...\"],[\"propose novel application simultaneous orthogonal matching pursuit somp procedure sparsistant variab...\"],[\"present revival interest bistatic radar systems research area gained momentum given strategic advant...\"],[\"applications related airborne radars simulation always played important role mainly two fold reason ...\"],[\"propose work new family kernels variablelength time series work builds upon vector autoregressive va...\"],[\"structural equation models bayesian networks widely used analyze causal relations continuous variabl...\"],[\"existing approaches analyzing asymptotics graph laplacians typically assume wellbehaved kernel funct...\"],[\"collaborative convex framework factoring data matrix nonnegative product sparse coefficient matrix p...\"],[\"paper treats problem screening variables high correlations high dimensional data many fewer samples ...\"],[\"unsupervised discovery latent representations addition useful density modeling visualisation explora...\"],[\"propose active set selection framework gaussian process classification cases dataset large enough re...\"],[\"investigate learning rate multiple kernel leaning mkl elasticnet regularization consists ellregulari...\"],[\"derive upper bound local rademacher complexity ellpnorm multiple kernel learning yields tighter exce...\"],[\"present probabilistic viewpoint multiple kernel learning unifying wellknown regularised risk approac...\"],[\"gaussian process models often used mathematical approximations computationally expensive experiments...\"],[\"present two sets theoretical results grouped lasso overlap jacob obozinski vert linear regression se...\"],[\"present discrete infinite logistic normal distribution diln bayesian nonparametric prior mixed membe...\"],[\"purpose sufficient dimension reduction sdr find lowdimensional subspace input features sufficient pr...\"],[\"theoretically investigate convergence rate support consistency correctly identifying subset nonzero ...\"],[\"paper give new sharp generalization bound lpmkl generalized framework multiple kernel learning mkl i...\"],[\"discovery nonlinear causal relationship additive nongaussian noise models attracted considerable att...\"],[\"paper autoassociative models proposed candidates generalization principal component analysis show mo...\"],[\"nonparametric methods widely applicable statistical inference problems since rely modeling assumptio...\"],[\"many realworld problems encountered several disciplines deal modeling timeseries containing differen...\"],[\"linear nongaussian structural equation model called lingam identifiable model exploratory causal ana...\"],[\"inverse inference brain reading recent paradigm analyzing functional magnetic resonance imaging fmri...\"],[\"compare risk ridge regression simple variant ordinary least squares one simply projects data onto fi...\"],[\"define discuss first sparse coding algorithm based closedform updates continuous latent variables un...\"],[\"data sets many features observations independent screening based univariate regression models leads ...\"],[\"address online linear optimization problem actions forecaster represented binary vectors goal unders...\"],[\"response problem vidyasagar state criterion pac learnability concept class mathscr family nonatomic ...\"],[\"propose restricted collapsed draw rcd sampler general markov chain monte carlo sampler simultaneous ...\"],[\"number recent work studied effectiveness feature selection using lasso known restricted isometry pro...\"],[\"paper addresses problem inferring sparse causal networks modeled multivariate autoregressive mar pro...\"],[\"due space limitations submission source separation clustering phaselocked subspaces accepted publica...\"],[\"introduce pitman yor diffusion tree pydt hierarchical clustering generalization dirichlet diffusion ...\"],[\"given reproducing kernel hilbert space realvalued functions suitable measure source space subset dec...\"],[\"nonnegative matrix factorization nmf common tool audio source separation learning nmf large audio da...\"],[\"study problem estimating data sparse approximation inverse covariance matrix estimating sparsity con...\"],[\"describe simple efficient procedure approximating levy measure textgammaalpha random variable use ap...\"],[\"many areas machine learning becomes necessary find eigenvector decompositions large matrices discuss...\"],[\"paper studies deviations regret stochastic multiarmed bandit problem total number plays known before...\"],[\"many data mining applications collection sufficiently large datasets time consuming expensive hand i...\"],[\"propose novel algebraic framework treating probability distributions represented cumulants mean cova...\"],[\"present method estimate block membership nodes random graph generated stochastic blockmodel use embe...\"],[\"sparse bayesian learning sbl gaussian scale mixtures gsms used model sparsityinducing priors realize...\"],[\"concave regularization methods provide natural procedures sparse recovery however difficult analyze ...\"],[\"performance orthogonal matching pursuit omp variable selection analyzed random designs contrasted de...\"],[\"consider problem learning forest nonlinear decision rules general loss functions standard methods em...\"],[\"response variables nominal populations crossclassified respect multiple polytomies questions often a...\"],[\"present probabilistic model natural images based gaussian scale mixtures simple multiscale represent...\"],[\"modelling real world complexity music challenge machine learning address task modeling melodic seque...\"],[\"introduce factor analysis model summarizes dependencies observed variable groups instead dependencie...\"],[\"many nonparametric regressors recently shown converge rates depend intrinsic dimension data regresso...\"],[\"increasing body evidence suggesting exact nearest neighbour search highdimensional spaces affected c...\"],[\"propose method called ideal regression approximating arbitrary system polynomial equations system pa...\"],[\"consider gaussian process formulation multiple kernel learning problem goal select convex combinatio...\"],[\"propose scalable efficient statistically motivated computational framework graphical lasso friedman ...\"],[\"propose novel method introducing structure existing machine learning techniques developing structure...\"],[\"methods decisiontheoretic online learning based hedge algorithm takes parameter called learning rate...\"],[\"popular cubic smoothing spline estimate regression function arises minimizer penalized sum squares s...\"],[\"vapnikchervonenkis dimension fundamental measure generalization capacity learning algorithms however...\"],[\"paper give new generalization error bound multiple kernel learning mkl general class regularizations...\"],[\"paper addresses problem segmenting timeseries respect changes mean value variance first case time da...\"],[\"consider standard binary classification problem performance binary classifier based training data ch...\"],[\"gaussian process models also called kriging models often used mathematical approximations expensive ...\"],[\"describe many vantage points baire metric use clustering data use preprocessing structuring data ord...\"],[\"gaussian probability densities omnipresent applied mathematics gaussian cumulative probabilities har...\"],[\"informationmaximization clustering learns probabilistic classifier unsupervised manner mutual inform...\"],[\"asymptotic pseudotrajectory approach stochastic approximation benaim hofbauer sorin extended asynchr...\"],[\"exact inference linear regression model spike slab priors often intractable expectation propagation ...\"],[\"unbalanced data arises many learning tasks clustering multiclass data hierarchical divisive clusteri...\"],[\"article proposed several approaches post processing large ensemble prediction models rules results s...\"],[\"paper devoted regret lower bounds classical model stochastic multiarmed bandit wellknown result lai ...\"],[\"paper develops general theoretical framework analyze structured sparse recovery problems using notat...\"],[\"many situations interest lies identifying clusters one might expect available variables carry inform...\"],[\"many modern data mining applications concerned analysis datasets observations described paired highd...\"],[\"propose new yet natural algorithm learning graph structure general discrete graphical models aka mar...\"],[\"paper propose semiparametric approach named nonparanormal skeptic efficiently robustly estimating hi...\"],[\"highdimensional tensors multiway data becoming prevalent areas biomedical imaging chemometrics netwo...\"],[\"properties data frequently seen vary depending sampled situations usually changes along time evoluti...\"],[\"several application domains highdimensional observations collected analysed search naturally occurri...\"],[\"present alternating augmented lagrangian method convex optimization problems cost function sum two t...\"],[\"regularized kernel methods support vector machines leastsquares support vector regression constitute...\"],[\"paper describes novel method approximate polynomial coefficients regression functions particular int...\"],[\"introduce new discriminant analysis method empirical discriminant analysis eda binary classification...\"],[\"existing methods sparse channel estimation typically provide estimate computed solution maximizing o...\"],[\"consider learn causal ordering variables linear nongaussian acyclic model called lingam several exis...\"],[\"support vector machines svms naturally embody sparseness due use hinge loss functions however svms d...\"],[\"consider problem learning set random samples show relevant geometric topological properties set stud...\"],[\"highdimensional data common genomics proteomics chemometrics often contains complicated correlation ...\"],[\"paper study statistical properties semisupervised learning considered important problem community ma...\"],[\"paper propose novel framework construction sparsityinducing priors particular define priors mixture ...\"],[\"present new sequential monte carlo sampler coalescent based bayesian hierarchical clustering model a...\"],[\"consider structure learning problem graphical models call loosely connected markov random fields num...\"],[\"study problem prediction evolving graph data formulate problem minimization convex objective encoura...\"],[\"hierarchical parametric models consisting observable latent variables widely used unsupervised learn...\"],[\"paper considers problem robust subspace recovery given set points mathbbrd many lie ddimensional sub...\"],[\"mixture gaussians fit single curved heavytailed cluster report data contains many clusters produce a...\"],[\"compute expected value kullbackleibler divergence various fundamental statistical models respect can...\"],[\"consider probabilistic multinomial probit classification using gaussian process priors challenges mu...\"],[\"case control comparisons classical approach study neurological diseases however patients fall cleanl...\"],[\"recently sparsitybased algorithms proposed superresolution spectrum estimation however achieve adequ...\"],[\"important topic systems biology developing statistical methods automatically find causal relations g...\"],[\"single stationary topic model latent dirichlet allocation inappropriate modeling corpora span long t...\"],[\"maximum variance unfolding one main methods nonlinear dimensionality reduction study large sample li...\"],[\"given two graphs graph matching problem align two vertex sets minimize number adjacency disagreement...\"],[\"offer novel view adaboost statistical setting propose bayesian model binary classification label noi...\"],[\"paper consider problem link prediction timeevolving graphs assume certain graph features node degree...\"],[\"consider problems detection localization contiguous block weak activation large matrix small number ...\"],[\"exact gaussian process regression runtime data size making intractable large many algorithms improvi...\"],[\"meanfield variational methods widely used approximate posterior inference many probabilistic models ...\"],[\"typical cohorts brain imaging studies large enough systematic testing information contained images b...\"],[\"many applications data analysis rely decomposition data matrix lowrank sparse component existing met...\"],[\"graph clustering involves task dividing nodes clusters edge density higher within clusters opposed a...\"],[\"consider problem adaptive stratified sampling monte carlo integration differentiable function given ...\"],[\"multitask sparse feature learning aims improve generalization performance exploiting shared features...\"],[\"many real world network problems often concern multivariate nodal attributes image textual multiview...\"],[\"given time series graphs fixed vertex set represents actors edge vertex vertex time represents exist...\"],[\"study mixtures factorizing probability distributions represented visible marginal distributions stoc...\"],[\"extend kernelized matrix factorization fully bayesian treatment ability work multiple side informati...\"],[\"prove density function gradient sufficiently smooth function omega subset mathbbrd rightarrow mathbb...\"],[\"study inference learning based sparse coding model spikeandslab prior standard sparse coding model u...\"],[\"statistical inference graphs burgeoning field applied theoretical statistics communities well throug...\"],[\"many random processes simulated output deterministic model accepting random inputs model usually des...\"],[\"nonparametric mixture models based dirichlet process elegant alternative finite models number underl...\"],[\"paper prove probabilistic continuous complexity conjecture continuous complexity theory states compl...\"],[\"lately several suggestions parametrized distances graph generalize shortest path distance commute ti...\"],[\"model selection crucial issue machinelearning wide variety penalisation methods possibly data depend...\"],[\"classical mixture gaussians model related kmeans via smallvariance asymptotics covariances gaussians...\"],[\"apply informationbased complexity analysis support vector machine svm algorithms goal comprehensive ...\"],[\"problems machine learning involve noisy input data classification methods reached limiting accuracie...\"],[\"technical note considers problems blind sparse learning inference electrogram egm signals atrial fib...\"],[\"new geometricallymotivated algorithm nonnegative matrix factorization developed applied discovery la...\"],[\"partitioning graph groups vertices within group densely connected vertices assigned different groups...\"],[\"propose approach multivariate nonparametric regression generalizes reduced rank regression linear mo...\"],[\"suppose two large multidimensional data sets noisy measurements underlying random process principle ...\"],[\"propose spectral clustering method based local principal components analysis pca performing local pc...\"],[\"expectation propagation provides framework approximate inference model consideration latent gaussian...\"],[\"develop nested hierarchical dirichlet process nhdp hierarchical topic modeling nhdp generalization n...\"],[\"dictionary learning proven powerful tool many image processing tasks atoms typically defined small i...\"],[\"study highdimensional asymptotic performance limits binary supervised classification problems class ...\"],[\"identifying homogeneous subgroups variables challenging high dimensional data analysis highly correl...\"],[\"shown aictype criteria asymptotically efficient selectors tuning parameter nonconcave penalized regr...\"],[\"measurements made satellite remote sensing moderate resolution imaging spectroradiometer modis globa...\"],[\"present two graphbased algorithms multiclass segmentation highdimensional data algorithms use diffus...\"],[\"testament success theory random forests long outpaced application practice paper take step towards n...\"],[\"spectral clustering graphbased semisupervised learning ssl algorithms sensitive graphs constructed d...\"],[\"work develops generic framework called bagofpaths bop link network data analysis central idea assign...\"],[\"gaussian process promising novel technology applied regression problem classification problem regres...\"],[\"introduce new approach variable selection called predictive correlation screening predictor design p...\"],[\"concept hierarchies formal concept analysis theoretically well grounded largely experimented methods...\"],[\"concept refinement probability elicitation considered proper scoring rules taking directions axioms ...\"],[\"consider problem adaptive stratified sampling monte carlo integration noisy function given finite bu...\"],[\"fundamental aim clustering algorithms partition data points consider tasks discovered partition allo...\"],[\"propose novel approach nonlinear regression using twolayer neural network model structure sparsityfa...\"],[\"consider learning causal ordering variables linear nongaussian acyclic model called lingam several e...\"],[\"popular sparse estimation methods based ellrelaxation lasso dantzig selector require knowledge varia...\"],[\"study low rank matrix tensor completion propose novel algorithms employ adaptive sampling schemes ob...\"],[\"develop approach feature elimination statistical learning kernel machines based recursive eliminatio...\"],[\"scaled complex wishart distribution widely used model multilook full polarimetric sar data whose ade...\"],[\"propose new method detecting changes markov network structure two sets samples instead naively fitti...\"],[\"introduce randomized dependence coefficient rdc measure nonlinear dependence random variables arbitr...\"],[\"multiple multivariate data sets derive conditions generalized canonical correlation analysis gcca im...\"],[\"consider following signal recovery problem given measurement matrix phiin mathbbrntimes noisy observ...\"],[\"dasgupta shulman showed tworound variant algorithm learn mixture gaussian distributions near optimal...\"],[\"propose original model inferring team strengths using markov random field used generate historical e...\"],[\"spatiotemporal point process models play central role analysis spatially distributed systems several...\"],[\"consider problem joint modelling metabolic signals gene expression systems biology applications prop...\"],[\"consider problem vertex classification graphs constructed latent position model shown previously app...\"],[\"study adaptive estimation copula correlation matrix sigma semiparametric elliptical copula model con...\"],[\"correlation matrices play key role many multivariate methods graphical model estimation factor analy...\"],[\"recent years manifold learning become increasingly popular tool performing nonlinear dimensionality ...\"],[\"systems biomedicine experimenter encounters different potential sources variation data individual sa...\"],[\"introduce general constructive setting density ratio estimation problem solution multidimensional in...\"],[\"optimal transportation distances fundamental family parameterized distances histograms despite appea...\"],[\"causal inference relies structure graph often directed acyclic graph dag different graphs may result...\"],[\"paper generalizes beta divergence beyond classical form associated power variance functions tweedie ...\"],[\"strategy early stopping regularization technique based choosing stopping time iterative algorithm fo...\"],[\"problem lowrank matrix estimation recently received lot attention due challenging applications lot w...\"],[\"consider robust covariance estimation group symmetry constraints nongaussian covariance estimation t...\"],[\"provide theoretical analysis statistical computational properties penalized mestimators formulated s...\"],[\"adaptive filtering algorithms operating reproducing kernel hilbert spaces demonstrated superiority l...\"],[\"introduce supersparse linear integer models slim tool create scoring systems binary classification d...\"],[\"study sparse principal component analysis high dimensional vector autoregressive time series doubly ...\"],[\"vector autoregressive var model powerful tool modeling complex time series exploited many fields how...\"],[\"gaussian process latent variable model gplvm nonlinear probabilistic method embedding high dimension...\"],[\"estimation dependencies multiple variables central problem analysis financial time series common app...\"],[\"interaction transitivity sparsity two common features empirical networks implies local regions large...\"],[\"community detection graphs subject many algorithms recent methods want optimize modularity function ...\"],[\"thompson sampling demonstrated many complex bandit models however theoretical guarantees available p...\"],[\"paper propose study family sparsityinducing penalty functions since penalty functions related kineti...\"],[\"project investigate idea reducing dimensionality datasets using borel isomorphism purpose subsequent...\"],[\"introduce application group lasso design experiments note trying explain experimental design group l...\"],[\"big data bring new opportunities modern society challenges data scientists one hand big data hold gr...\"],[\"hierarchical probabilistic models gaussian mixture models widely used unsupervised learning tasks mo...\"],[\"propose general framework reducedrank modeling matrixvalued data applying generalized nuclear norm p...\"],[\"canonical correlation analysis cca one popular methods frequency recognition steadystate visual evok...\"],[\"reshef reshef recently published paper present method called maximal information coefficient mic det...\"],[\"describe ways define calculate lnorm signal subspaces less sensitive outlying data lcalculated subsp...\"],[\"paper considers problem subspace clustering noise specifically study behavior sparse subspace cluste...\"],[\"spectral clustering sensitive graphs constructed data particularly proximal imbalanced clusters pres...\"],[\"regularized variants principal components analysis especially sparse pca functional pca among useful...\"],[\"informally call stochastic process learnable admits generalization error approaching zero probabilit...\"],[\"present numerical algorithm nonnegative matrix factorization nmf problems noisy separability nmf pro...\"],[\"theory graphical models matured three decades provide backbone several classes models used myriad ap...\"],[\"consider problem learning causal directed acyclic graphs observational joint distribution one use gr...\"],[\"article proposes multinomial probit bayesian additive regression trees mpbart multinomial probit ext...\"],[\"standard clustering problems data points represented vectors stacking together one forms data matrix...\"],[\"vertex clustering stochastic blockmodel graph wide applicability subject extensive research thispape...\"],[\"important aspect classifier error rate quantifies predictive capacity thus accuracy error estimation...\"],[\"proposed new statistical dependency measure called copula dependency coefficientcdc two sets variabl...\"],[\"significant success reported recently using deep neural networks classification large networks compu...\"],[\"novel linear classification method possesses merits support vector machine svm distanceweighted disc...\"],[\"classification important topic statistics machine learning great potential many real applications pa...\"],[\"present robust alternative principal component analysis pca called elliptical component analysis eca...\"],[\"mixed linear regression involves recovery two unknown vectors unlabeled linear measurements sample c...\"],[\"present new algorithms compute mean set empirical probability measures optimal transport metric mean...\"],[\"substantial evidence indicates major psychiatric disorders associated distributed neural dysconnecti...\"],[\"article describe model derivation implementation variational bayesian inference linear logistic regr...\"],[\"kernel methods widely applied machine learning questions approximating unknown function finite sampl...\"],[\"consider discrete graphical models markov respect graph propose two distributed marginal methods est...\"],[\"introduce computationally effective algorithm linear model selection consisting three steps screenin...\"],[\"combining information different sources common way improve classification accuracy braincomputer int...\"],[\"latent force models lfm principled approaches incorporating solutions differential equations within ...\"],[\"consider learning possible causal direction two observed variables presence latent confounding varia...\"],[\"explore performance several automatic bandwidth selectors originally designed density gradient estim...\"],[\"incorporating spatial information hyperspectral unmixing procedures shown positive effects due inher...\"],[\"kernel leastmeansquare klms algorithm appealing tool online identification nonlinear systems due sim...\"],[\"manuscript consider problem jointly estimating multiple graphical models high dimensions assume data...\"],[\"propose family multivariate gaussian process models correlated outputs based assuming likelihood fun...\"],[\"propose new stochastic dual coordinate ascent technique applied wide range regularized learning prob...\"],[\"analyzing understanding structure complex relational data important many applications including anal...\"],[\"machine learning methods clustering classification rely distance function describe relationships dat...\"],[\"stochastic blockmodels among prominent statistical models cluster analysis complex networks clusters...\"],[\"propose method inferring conditional indepen dence graph cig highdimensional discretetime gaus sian ...\"],[\"work compute lower lipschitz bounds ellp pooling operators infty well ellp pooling operators precede...\"],[\"symmetric binary matrices representing relations among entities commonly collected many areas focus ...\"],[\"investigate generic problem learning pairwise exponential family graphical models pairwise sufficien...\"],[\"random graphs distributed according stochastic blockmodels special case latent position graphs adjac...\"],[\"robust tensor recovery plays instrumental role robustifying tensor decompositions multilinear data a...\"],[\"given data sampled number variables one often interested underlying causal relationships form direct...\"],[\"nonorthogonal joint diagonalization njd free prewhitening widely studied context blind source separa...\"],[\"performance spectral clustering considerably improved via regularization demonstrated empirically am...\"],[\"present exploration rich theoretical connections several classes regularized models network flows re...\"],[\"question determine number independent latent factors topics mixture models latent dirichlet allocati...\"],[\"paper aims justifying lwf amp chain graphs showing represent arbitrary independence models specifica...\"],[\"detection anomalous activity graphs statistical problem arises many applications network surveillanc...\"],[\"paper addresses problem filtering statespace model standard approaches filtering assume probabilisti...\"],[\"real world systems typically feature variety different dependency types topologies complicate model ...\"],[\"concerned approximation problem symmetric positive semidefinite matrix due motivation class nonlinea...\"],[\"paper study nonconvex penalization using bernstein functions since bernstein function concave nonsmo...\"],[\"main goal article address bipartite ranking issue perspective functional data analysis fda given tra...\"],[\"probabilistic models often parameters translated scaled permuted otherwise transformed without chang...\"],[\"modeling structure complex networks using bayesian nonparametrics makes possible specify flexible mo...\"],[\"paper propose outlierrobust regularized kernelbased method linear system identification unknown impu...\"],[\"consider problem clustering sequence multinomial observations way model selection criterion propose ...\"],[\"archetypal analysis represents set observations convex combinations pure patterns archetypes origina...\"],[\"multitask learning shown significantly enhance performance multiple related learning tasks variety s...\"],[\"consider forwardbackward greedy algorithms solving sparse feature selection problems general convex ...\"],[\"main goal paper propose novel method perform matrix completion online motivated wide variety applica...\"],[\"new shrinkagebased construction developed compressible vector boldsymbolxinmathbbrn cases components...\"],[\"recently number mostly ellnorm regularized least squares type deterministic algorithms proposed addr...\"],[\"work consider problem detecting anomalous spatiotemporal behavior videos approach learn normative mu...\"],[\"survey present compare different approaches estimate mutual information data analyse general depende...\"],[\"tree structured graphical models powerful expressing long range hierarchical dependency among many v...\"],[\"visual rendering graphs key task mapping complex network data although graph drawing algorithms emph...\"],[\"undirected graphical models known markov networks popular wide variety applications ranging statisti...\"],[\"paper proposes novel scheme reducedrank gaussian process regression method based approximate series ...\"],[\"notion causality used many situations dealing uncertainty consider problem whether causality identif...\"],[\"laplace approximation calls computation second derivatives likelihood maximum maximum found emalgori...\"],[\"recent paper shown lasso algorithm exhibits nearideal behavior following sense suppose eta satisfies...\"],[\"sparse classifiers support vector machines svm efficient testphases classifier characterized subset ...\"],[\"paper demonstrate tempering markov chain monte carlo samplers bayesian models recursively subsamplin...\"],[\"paper variable selection clustering estimation unsupervised highdimensional setting approach based f...\"],[\"paper presents general vectorvalued reproducing kernel hilbert spaces rkhs framework problem learnin...\"],[\"inductive probabilistic classification rule must generally obey principles bayesian predictive infer...\"],[\"tutorial explain inference procedures developed sparse gaussian process regression gaussian process ...\"],[\"practical model building processes often timeconsuming many different models must trained validated ...\"],[\"information geometric causal inference igci new approach distinguish cause effect two variables base...\"],[\"consider problem dictionary learning assumption observed signals represented sparse linear combinati...\"],[\"new non parametric approach problem testing independence two random process developed test statistic...\"],[\"propose new high dimensional semiparametric principal component analysis pca method named copula com...\"],[\"deep learning methods attempt learn generic features unsupervised fashion large unlabelled data set ...\"],[\"paper concerned obtaining distributionfree concentration inequalities mixture independent bernoulli ...\"],[\"ksupport norm regularizer successfully applied sparse vector prediction problems show belongs genera...\"],[\"present nonparametric prior reversible markov chains use completely random measures specifically gam...\"],[\"lasso computationally efficient regression regularization procedure produce sparse estimators number...\"],[\"synthesis model signals represented sparse combinations atoms dictionary dictionary learning describ...\"],[\"present supervisedlearning algorithm graph data set graphs arbitrary twicedifferentiable loss functi...\"],[\"nonnegative matrix factorization nmf shown identifiable separability assumption columnsor rows input...\"],[\"paper consider low rank matrix estimation using either matrixversion dantzig selector hatalambdad ma...\"],[\"motivation work improve performance standard stacking approaches ensembles composed simple heterogen...\"],[\"connect shiftinvariant characteristic kernels infinitely divisible distributions mathbbrd characteri...\"],[\"bayesian networks convenient graphical expressions high dimensional probability distributions repres...\"],[\"paper considers problem estimating structure multiple related directed acyclic graph dag models buil...\"],[\"propose method inferring conditional independence graph cig highdimensional gaussian vector time ser...\"],[\"given iid observations unknown absolute continuous distribution defined domain omega propose nonpara...\"],[\"address structured covariance estimation elliptical distributions assuming covariance priori known b...\"],[\"describe simple efficient permutation based procedure selecting penalty parameter lasso procedure in...\"],[\"predicting individuals risk experiencing future clinical outcome statistical task important conseque...\"],[\"mean shift clustering finds modes data probability density identifying zero points density gradient ...\"],[\"paper presents novel approach approximate integration uncertainty noise signal variances gaussian pr...\"],[\"focus interpolation method referred bayesian reconstruction paper whereas standard interpolation met...\"],[\"graphical models commonly used tools modeling multivariate random variables exist many convenient mu...\"],[\"propose novel nonparametric adaptive anomaly detection algorithm high dimensional data based ranksvm...\"],[\"study novel splinelike basis name falling factorial basis bearing many similarities classic truncate...\"],[\"first encountered pacbayesian concentration inequalities seemed rather disconnected good oldfashione...\"],[\"additive models play important role semiparametric statistics paper gives learning rates regularized...\"],[\"propose new two stage algorithm ling large scale regression problems ling risk well known ridge regr...\"],[\"bayesian mixture models widely applied unsupervised learning exploratory data analysis markov chain ...\"],[\"one fundamental problems causal inference estimation causal effect variables confounded difficult ob...\"],[\"numbers numerical vectors account large portion data however recently amount string data generated i...\"],[\"linear dimensionality reduction methods cornerstone analyzing high dimensional data due simple geome...\"],[\"density mathbb highdensity cluster connected component geq lambda lambda set highdensity clusters fo...\"],[\"nonparametric regression massive numbers samples features increasingly important problem big setting...\"],[\"paper deals problem nonparametric independence testing fundamental decisiontheoretic problem asks tw...\"],[\"study problem lowrank tensor factorization presence missing data ask following question many sampled...\"],[\"paper presents novel algorithms exploit intrinsic algebraic combinatorial structure matrix completio...\"],[\"paper presents foundational theoretical results distributed parameter estimation undirected probabil...\"],[\"analyze generalization robustness batched weighted average algorithm vgeometrically ergodic markov d...\"],[\"subsampling methods recently proposed speed least squares estimation large scale settings however al...\"],[\"propose loco algorithm largescale ridge regression distributes features across workers cluster impor...\"],[\"despite fact consider temporal nature data classic dimensionality reduction techniques pca widely ap...\"],[\"crowdsourcing popular paradigm effectively collecting labels low cost dawidskene estimator widely us...\"],[\"present technique significantly speeding alternating least squares als gradient descent two widely u...\"],[\"consider statistical well algorithmic aspects solving largescale leastsquares problems using randomi...\"],[\"consider problem grouping multiple graphs several clusters using singular value thesholding nonnegat...\"],[\"natural approach analyze interaction data form whatconnectstowhatwhen create timeseries rather seque...\"],[\"paper propose new fuzzy clustering algorithm based modeseeking framework given dataset mathbbrd defi...\"],[\"consider class optimization problems arising computationally intensive lregularized mestimators func...\"],[\"estimation density derivatives versatile tool statistical data analysis naive approach first estimat...\"],[\"study problem learning sparse structure changes two markov networks rather fitting two markov networ...\"],[\"canonical correlation analysis cca widely used statistical tool well established theory favorable pe...\"],[\"use machinelearning neuroimaging offers new perspectives early diagnosis prognosis brain diseases al...\"],[\"many graph clustering quality functions suffer resolution limit inability find small clusters large ...\"],[\"paper presents bayesian generative model dependent cox point processes alongside efficient inference...\"],[\"data analysis machine learning become integrative part modern scientific methodology offering automa...\"],[\"matrix factorization become common approach collaborative filtering due ease implementation scalabil...\"],[\"introduce means automating machine learning big data tasks performing scalable stochastic bayesian o...\"],[\"contribution deals generalized symmetric fastica algorithm domain independent component analysis ica...\"],[\"large amount observational data accumulated various fields recent times growing need estimate genera...\"],[\"recently focus penalized loglikelihood covariance estimation sparse inverse covariance precision mat...\"],[\"paper generalizes important result pacbayesian literature binary classification case ensemble method...\"],[\"prove new fast learning rates onevsall multiclass plugin classifiers trained either exponentially st...\"],[\"define beta diffusion tree random tree structure set leaves defines collection overlapping subsets o...\"],[\"large number algorithms machine learning principal component analysis pca nonlinear kernel extension...\"],[\"wild bootstrap method nonparametric hypothesis tests based kernel distribution embeddings proposed b...\"],[\"paper introduces kernelbased information criterion kic model selection regression analysis novel ker...\"],[\"fastica algorithm one popular iterative algorithms domain linear independent component analysis desp...\"],[\"applying standard markov chain monte carlo mcmc algorithms large data sets computationally expensive...\"],[\"loglinear models popular workhorses analyzing contingency tables loglinear parameterization interact...\"],[\"present novel scalable bayesian approach modelling occurrence pairs symbols drawn large vocabulary o...\"],[\"paper studies ordered weighted owl norm regularization sparse estimation problems strongly correlate...\"],[\"practical bayesian optimization must often search structures differing numbers parameters instance m...\"],[\"network metrics form fundamental part network analysis toolbox used quantitatively measure different...\"],[\"interested learning causal relationships pairs random variables purely observational data effectivel...\"],[\"provide theoretical empirical evidence type asymmetry causes effects present related via linear mode...\"],[\"variational inference powerful concept underlies many iterative approximation algorithms expectation...\"],[\"diffusionweighted magnetic resonance imaging dwi fiber tractography methods measure structure white ...\"],[\"significant attention given minimizing penalized least squares criterion estimating sparse solutions...\"],[\"principal components analysis widely used technique dimension reduction characterization variability...\"],[\"paper introduces linear statespace model timevarying dynamics time dependency obtained forming state...\"],[\"recent years rank aggregation received significant attention machine learning community goal problem...\"],[\"propose novel graphical model selection gms scheme highdimensional stationary time series discrete t...\"],[\"distributed learning probabilistic models multiple data repositories minimum communication increasin...\"],[\"study prediction estimation problems using empirical risk minimization relative general convex loss ...\"],[\"consider task fitting regression model involving interactions among potentially large set covariates...\"],[\"classical approach inverse problems based optimization misfit function despite computational appeal ...\"],[\"focusing bound constrained global optimization problems whose objective functions computationally ex...\"],[\"introduce locally linear latent variable model lllvm probabilistic model nonlinear manifold discover...\"],[\"paper propose first nonparametric bayesian model using gaussian processes make inference poisson poi...\"],[\"regularized logistic regression become workhorse data mining bioinformatics widely used many classif...\"],[\"portable wearable wireless electrocardiogram ecg systems potential used pointofcare cardiovascular d...\"],[\"realizations stochastic process often observed temporal data functional data growing interests class...\"],[\"paper focus developing efficient sensitivity analysis methods computationally expensive objective fu...\"],[\"highdimensional data structured noise caused observed unobserved factors affecting multiple target v...\"],[\"advances modern sensing sequencing technologies generate deluge high dimensional spacetemporal physi...\"],[\"motivated largescale collaborativefiltering applications present noncommuting latent factor nclf ten...\"],[\"paper introduce new optimization formulation sparse regression compressed sensing called clot combin...\"],[\"give comprehensive theoretical characterization nonparametric estimator divergence two continuous di...\"],[\"motivated generating personalized recommendations using ordinal preference data study question learn...\"],[\"present first fully variational bayesian inference scheme continuous gaussianprocessmodulated poisso...\"],[\"propose novel sampling framework inference probabilistic models active learning approach converges q...\"],[\"use statistical learning methods construct adaptive state estimator nonlinear stochastic systems opt...\"],[\"dirichlet process mixture dpm ubiquitous flexible bayesian nonparametric statistical model however f...\"],[\"article contains detailed proofs additional examples related uai submission learning sparse causal m...\"],[\"variational inference algorithms proven successful bayesian analysis large data settings recent adva...\"],[\"probabilistic programming languages simplify development machine learning techniques inference suffi...\"],[\"gaussian process classification popular method number appealing properties show scale model within v...\"],[\"paper considers statistical estimation problems probability distribution observed random variable in...\"],[\"classical stochastic gradient methods well suited minimizing expectedvalue objective functions howev...\"],[\"widely applied approach causal inference nonexperimental time series often referred linear granger c...\"],[\"robust parameter estimation well studied parametric density estimation little investigation robust d...\"],[\"article derive new stepsize adaptation normalized least mean square algorithm nlms describing task l...\"],[\"diffusionweighted imaging dwi method currently measure connections different parts human brain vivo ...\"],[\"factor analysis provides linear factors describe relationships individual variables data set extend ...\"],[\"several problems network intrusion community detection disease outbreak described observations attri...\"],[\"article considers problem multigroup classification setting number variables larger number observati...\"],[\"theory bayesian nonparametric bnp models well suited streaming data scenarios due ability adapt mode...\"],[\"deep gaussian processes provide flexible approach probabilistic modelling data using either supervis...\"],[\"propose likelihood ratio based inferential framework high dimensional semiparametric generalized lin...\"],[\"community detection fundamental problem network analysis made challenging overlaps communities often...\"],[\"matching datasets multiple modalities become important task data analysis existing methods often rel...\"],[\"introduce bayesian multitensor factorization model first bayesian formulation joint factorization mu...\"],[\"two recently introduced criteria estimation generative models based reduction binary classification ...\"],[\"highdimensional data often lie lowdimensional subspaces corresponding different classes belong findi...\"],[\"given iid observations random vector highdimensional vector lowdimensional index variable study prob...\"],[\"recent studies literature paid much attention sparsity linear classification tasks one motivation im...\"],[\"propose new class semiparametric exponential family graphical models analysis high dimensional mixed...\"],[\"propose robust inferential procedure assessing uncertainties parameter estimation highdimensional li...\"],[\"provide general theory expectationmaximization algorithm inferring high dimensional latent variable ...\"],[\"consider problem uncertainty assessment low dimensional components high dimensional models specifica...\"],[\"recently theoretical guarantees obtained matrix completion nonuniform sampling regime particular sam...\"],[\"big data comes various ways types shapes forms sizes indeed almost areas science technology medicine...\"],[\"consider problem unveiling implicit network structure node interactions user interactions social net...\"],[\"paper concerned problems interaction screening nonlinear classification highdimensional setting prop...\"],[\"measuring dependence two random variables important critical many applied areas variable selection b...\"],[\"certain situations shall undoubtedly common big data era datasets available massive computing statis...\"],[\"using bayesian approach consider problem recovering sparse signals additive sparse dense noise typic...\"],[\"item response theory irt models categorical response data widely used analysis educational data comp...\"],[\"using nonparametric methods increasingly explored bayesian hierarchical modeling way increase model ...\"],[\"bayesian optimization powerful tool finetuning hyperparameters wide variety machine learning models ...\"],[\"consider setting linear regression high dimension focus problem constructing adaptive honest confide...\"],[\"graphical models provide powerful tools uncover complicated patterns multivariate data commonly used...\"],[\"motivated problems arise number applications online marketing explosives detection observations usua...\"],[\"statistical test independence may constructed using hilbertschmidt independence criterion hsic test ...\"],[\"paper introduces develops novel variable importance score function context ensemble learning demonst...\"],[\"abtesting popular technique web companies since makes possible accurately predict impact modificatio...\"],[\"laplacian mixture models identify overlapping regions influence unlabeled graph network data scalabl...\"],[\"inference learning graphical models wellstudied problems statistics machine learning found many appl...\"],[\"sparse representation classifier src utilized various classification problems makes use minimization...\"],[\"propose novel sparse tensor decomposition method namely tensor truncated power ttp method incorporat...\"],[\"gaussian mixture models gmm found many applications density estimation data clustering however model...\"],[\"multioutput gaussian processes mogp probability distributions vectorvalued functions previously used...\"],[\"explosion interest functional magnetic resonance imaging mri past two decades naturally accompanied ...\"],[\"careful tuning regularization parameter indispensable many machine learning tasks significant impact...\"],[\"paper proposes unified framework quantify local global inferential uncertainty high dimensional nonp...\"],[\"scale gaussian processes gps large data sets introduce robust bayesian committee machine rbcm practi...\"],[\"labeled stochastic block model random graph model representing networks community structure interact...\"],[\"challenging task modeling multivariate time series propose new class models use dependent matern pro...\"],[\"reciprocating interactions represent central feature human exchanges target various recent experimen...\"],[\"present novel algorithm westfallyoung light detecting patterns itemsets subgraphs statistically sign...\"],[\"introduce develop novel approach outlier detection based adaptation random subspace learning propose...\"],[\"given set pairwise comparisons classical ranking problem computes single ranking best represents pre...\"],[\"unknown constraints arise many types expensive blackbox optimization problems several methods propos...\"],[\"annealed importance sampling ais common algorithm estimate partition functions useful stochastic mod...\"],[\"large multilayer neural networks trained backpropagation recently achieved stateoftheart results wid...\"],[\"bayesian optimization effective methodology global optimization functions expensive evaluations reli...\"],[\"consider problem noisy bit matrix completion exact rank constraint true underlying matrix instead ob...\"],[\"paper examine problem approximating general linear dimensionality reduction ldr operator represented...\"],[\"present derivation kullback leibler kldivergence also known relative entropy von mises fisher vmf di...\"],[\"consider problem multivariate regression setting relevant predictors could shared among different re...\"],[\"paper considers problem matrix completion observed entries noisy contain outliers begins introducing...\"],[\"many high dimensional sparse learning problems formulated nonconvex optimization popular approach so...\"],[\"introduce local expectation gradients general purpose stochastic variational inference algorithm con...\"],[\"multivariate categorical data occur many applications machine learning one main difficulties vectors...\"],[\"standard sparse pseudoinput approximations gaussian process cannot handle complex functions well spa...\"],[\"high dimensions propose analyze aggregation estimator precision matrix gaussian graphical models est...\"],[\"concerned obtaining novel concentration inequalities missing mass total probability mass outcomes ob...\"],[\"structured sparsity recently emerged statistics machine learning signal processing promising paradig...\"],[\"estimation response functions important task dynamic medical imaging task arises example dynamic ren...\"],[\"novel concentration inequalities obtained missing mass total probability mass outcomes observed samp...\"],[\"multioutput gaussian processes received increasing attention last years natural mechanism extend pow...\"],[\"lowrank representationlrr significant method segmenting data generated union subspaces however known...\"],[\"approximate bayesian computation abc likelihoodfree monte carlo methods abc methods use comparison s...\"],[\"learn structure markov network two groups random variables joint observations since modelling learni...\"],[\"consider problem embedding unweighted directed knearest neighbor graphs lowdimensional euclidean spa...\"],[\"present new approach estimating interdependence industries economy applying data science solutions e...\"],[\"consider inference structure undirected graphical model exact bayesian framework specifically aim ac...\"],[\"brain decoding involves determination subjects cognitive state associated stimulus functional neuroi...\"],[\"presence weak overall correlation may useful investigate correlation significantly substantially pro...\"],[\"consider two connected aspects maximum likelihood estimation parameter highdimensional discrete grap...\"],[\"consider problem estimating undirected trianglefree graphs high dimensional distributions trianglefr...\"],[\"main contribution article new prior distribution directed acyclic graphs gives larger weight sparse ...\"],[\"variational framework learning inducing variables titsias large impact gaussian process literature f...\"],[\"consider problem discriminative factor analysis data general nongaussian bayesian model based ranks ...\"],[\"infinite hidden markov models ihmms attractive nonparametric generalization classical hidden markov ...\"],[\"modern scientific research massive datasets huge numbers observations frequently encountered facilit...\"],[\"hypergraph partitioning lies heart number problems machine learning network sciences many algorithms...\"],[\"address problem synthetic gene design using bayesian optimization main issue designing gene design s...\"],[\"introduce novel approach estimating latent dirichlet allocation lda parameters collapsed gibbs sampl...\"],[\"paper develops exact linear relationship leading eigenvector unnormalized modularity matrix eigenvec...\"],[\"central goal neuroscience understand activity nervous system related features external world feature...\"],[\"study theoretical properties learning dictionary signals mathbf xiin mathbb via lminimization assume...\"],[\"range fields including geosciences molecular biology robotics computer vision one encounters problem...\"],[\"present unified framework lowrank matrix estimation nonconvex penalties first prove proposed estimat...\"],[\"present vectorspace markov random fields vsmrfs novel class undirected graphical models variable bel...\"],[\"consider statistical algorithmic aspects solving largescale leastsquares problems using randomized s...\"],[\"empirically evaluate stochastic annealing strategy bayesian posterior optimization variational infer...\"],[\"recurring problem building probabilistic latent variable models regularization model selection insta...\"],[\"popularity bayesian optimization methods efficient exploration parameter spaces lead series papers a...\"],[\"models complex systems often formalized sequential software simulators computationally intensive pro...\"],[\"paper examines use residual bootstrap bias correction machine learning regression methods accounting...\"],[\"large matrix factorisation problems develop distributed markov chain monte carlo mcmc method based s...\"],[\"show neural network arbitrary depth nonlinearities dropout applied every weight layer mathematically...\"],[\"paper propose family tractable kernels dense family bounded positive semidefinite functions approxim...\"],[\"introduce new class nonstationary kernels derive covariance functions novel family stochastic proces...\"],[\"becoming increasingly important machine learning methods make predictions interpretable well accurat...\"],[\"subdifferential convex functions singular spectrum real matrices widely studied matrix analysis opti...\"],[\"propose kernel hamiltonian monte carlo kmc gradientfree adaptive mcmc algorithm based hamiltonian mo...\"],[\"renewed interest formulating integration inference problem motivated obtaining full distribution num...\"],[\"recent years increased interest statistical analysis data multiple types relations among set entitie...\"],[\"modern scale data brought new challenges bayesian inference particular conventional mcmc algorithms ...\"],[\"corrupting input hidden layers deep neural networks dnns multiplicative noise often drawn bernoulli ...\"],[\"variational inference scalable technique approximate bayesian inference deriving variational inferen...\"],[\"learning low dimensional structure multidimensional data canonical problem machine learning one comm...\"],[\"introduce gamsel generalized additive model selection penalized likelihood approach fitting sparse g...\"],[\"past years robust pca established standard tool reliable lowrank approximation matrices presence out...\"],[\"gaussian process models form core part probabilistic machine learning considerable research effort m...\"],[\"mean field variational bayes mfvb popular posterior approximation method due fast runtime largescale...\"],[\"latent block model lbm flexible probabilistic tool describe interactions node sets bipartite network...\"],[\"paper presents novel spectral algorithm additive clustering designed identify overlapping communitie...\"],[\"paper propose online algorithm compute matrix factorizations proposed algorithm updates dictionary m...\"],[\"propose class nonparametric twosample tests cost linear sample size two tests given based ensemble d...\"],[\"statistical dependencies independent component analysis ica cannot remove often provide rich informa...\"],[\"many practical modeling problems involve discrete data best represented draws multinomial categorica...\"],[\"nonparametric extension tensor regression proposed nonlinearity highdimensional tensor space broken ...\"],[\"present technique clustering categorical data generating many dissimilarity matrices averaging begin...\"],[\"given family probability measures space probability measures hilbert space goal paper highlight one ...\"],[\"factorial hidden markov models fhmms powerful tools modeling sequential data learning fhmms yields c...\"],[\"finding statistically significant highorder interaction features predictive modeling important chall...\"],[\"taking account highorder interactions among covariates valuable many practical regression problems h...\"],[\"kernel methods ubiquitous tools machine learning however often little reason common practice selecti...\"],[\"factorized information criterion fic recently developed information criterion based novel model sele...\"],[\"study gaussian process regression model context training data noise input output presence two source...\"],[\"new bayesian approach linear system identification proposed series recent papers main idea frame lin...\"],[\"paper compares classical parametric methods recently developed bayesian methods system identificatio...\"],[\"paper proposes novel gaussian process approach fault removal timeseries data fault removal delete fa...\"],[\"kernel bayes rule proposed nonparametric kernelbased method realize bayesian inference reproducing k...\"],[\"dictionaryaided sparse regression approach recently emerged promising alternative hyperspectral unmi...\"],[\"many statistical methods network data parameterize edgeprobability attributing latent traits vertice...\"],[\"random survival forests rsf powerful method risk prediction rightcensored outcomes biomedical resear...\"],[\"nonnegative matrix factorization nmf aims factorize matrix two optimized nonnegative matrices approp...\"],[\"networks capture intuition relationships world describe friendships facebook users interactions fina...\"],[\"linear regression models depend directly design matrix properties techniques efficiently estimate mo...\"],[\"consider factoring lowrank tensors presence outlying slabs problem important practice data collected...\"],[\"highlight pitfall applying stochastic variational inference general bayesian networks global random ...\"],[\"variational methods recently considered scaling training process gaussian process classifiers large ...\"],[\"introduce incremental variational inference apply latent dirichlet allocation lda incremental variat...\"],[\"approximate bayesian computation abc using sequential monte carlo method provides comprehensive plat...\"],[\"density matrices positively semidefinite hermitian matrices unit trace describe state quantum system...\"],[\"many modern data analysis problems involve inferences streaming data however streaming data easily a...\"],[\"methods transfer learning try combine knowledge several related tasks domains improve performance te...\"],[\"adaptive monte carlo schemes developed last years usually seek ensure ergodicity sampling process li...\"],[\"paper consider statistical problem learning linear model noisy samples existing work focused approxi...\"],[\"capturing dependence structure multivariate extreme events major concern many fields involving manag...\"],[\"recent decades seen interest prediction problems bayesian methodology used ubiquitously sampling app...\"],[\"one limiting factors using support vector machines svms large scale applications superlinear computa...\"],[\"symbolic data analysis based special descriptions data symbolic objects descriptions preserve detail...\"],[\"paper introduce novel framework making exact nonparametric bayesian inference latent functions parti...\"],[\"work studies class algorithms learning sideinformation emerge extending generative models embedded c...\"],[\"logdensity gradient estimation fundamental statistical problem possesses various practical applicati...\"],[\"typical goal supervised dimension reduction find lowdimensional subspace input space projected input...\"],[\"study statistical calibration adjusting features computational model observable controllable associa...\"],[\"study largescale spatial systems contain exogenous variables environmental factors significant predi...\"],[\"consider problem structure learning bowfree acyclic path diagrams baps baps viewed generalization li...\"],[\"consider problem minimizing sum functions convex parameter set mathcalc subset mathbbrp ngg pgg regi...\"],[\"dropout recently emerged powerful simple method training neural networks preventing coadaptation sto...\"],[\"existing binary classification methods target optimization overall classification risk may fail serv...\"],[\"bayesian optimization recently emerged popular efficient tool global optimization hyperparameter tun...\"],[\"present novel approach fully nonstationary gaussian process regression gpr three key parameters nois...\"],[\"interested solving multiple measurement vector mmv problem instances underlying sparsity pattern exh...\"],[\"distance weighted discrimination dwd marginbased classifier interesting geometric motivation dwd ori...\"],[\"general approach anomaly detection novelty detection consists estimating high density regions minimu...\"],[\"stochastic variational inference relatively well known scaling inference bayesian probabilistic mode...\"],[\"text investigates relations two wellknown family algorithms matrix factorisations recursive linear f...\"],[\"large sample size brings computation bottleneck modern data analysis subsampling one efficient strat...\"],[\"paper focus stochastic block model sbma probabilistic tool describing interactions nodes network usi...\"],[\"study paper consequences using mean absolute percentage error mape measure quality regression models...\"],[\"online passiveaggressive learning class online marginbased algorithms suitable wide range realtime p...\"],[\"propose secondorder hessian hessianfree based optimization method variational inference inspired gau...\"],[\"study propose automatic learning method variables selection based lasso epidemiology context one aim...\"],[\"using proper model characterize time series crucial making accurate predictions work use timevarying...\"],[\"study restless bandit associated extremely simple scalar kalman filter model discrete time certain a...\"],[\"propose macau powerful flexible bayesian factorization method heterogeneous data model factorize set...\"],[\"paper addresses problem scalable optimization lregularized conditional gaussian graphical models con...\"],[\"tree structures ubiquitous data across many domains many datasets naturally modelled unobserved tree...\"],[\"paper develop statistical theory implementation deep learning models show elegant variable splitting...\"],[\"given iid samples unknown continuous density hyperrectangle attempt learn piecewise constant functio...\"],[\"briefly review recent progress techniques modeling analyzing hyperspectral images movies particular ...\"],[\"consider continuous time markovian processes populations individual agents interact stochastically a...\"],[\"consider problem transforming samples one continuous source distribution samples another target dist...\"],[\"estimation probabilities network edges observed adjacency matrix important applications predicting m...\"],[\"latent space model family random graphs assigns realvalued vectors nodes graph edge probabilities de...\"],[\"hierarchical learning models mixture models bayesian networks widely employed unsupervised learning ...\"],[\"consider mnkclassical problem controller activating sampling sequentially finite number geq populati...\"],[\"consider sequential decision making problems binary classification scenario learner takes active rol...\"],[\"paper introduce novel online time series forecasting model refer pmgp filter show model equivalent g...\"],[\"consider problem extracting lowdimensional linear latent variable structure highdimensional random v...\"],[\"order identify important variables involved making optimal treatment decision proposed penalized lea...\"],[\"address problem detecting changes multivariate datastreams investigate intrinsic difficulty changede...\"],[\"robust bayesian models appealing alternatives standard models providing protection data contains out...\"],[\"introduce multivariate stochastic volatility model asset returns imposes restrictions structure vola...\"],[\"paper exact linear relation leading eigenvectors modularity matrix singular vectors uncentered data ...\"],[\"paper consider problem stochastic optimization bandit feedback model generalize gpucb algorithm srin...\"],[\"early stopping well known approach reduce time complexity performing training model selection large ...\"],[\"propose novel method multiple clustering assumes coclustering structure partitions rows columns data...\"],[\"present glasses global optimisation lookahead stochastic simulation expectedloss search majority glo...\"],[\"inventory control unknown demand distribution considered emphasis placed case involving discrete non...\"],[\"present sparse treebased listbased density estimation methods binarycategorical data density estimat...\"],[\"estimating strength dependency two variables fundamental exploratory analysis many applications data...\"],[\"present blitzkriging new approach fast inference gaussian processes applicable regression optimisati...\"],[\"laplacian eigenvectors graph constructed data set used many spectral manifold learning algorithms di...\"],[\"kernel methods obtain superb performance terms accuracy various machine learning tasks since effecti...\"],[\"gaussian graphical models ggms popular tools studying network structures however many modern applica...\"],[\"paper study nonconvex penalization using bernstein functions whose firstorder derivatives completely...\"],[\"dimensionality reduction methods common field high dimensional data analysis typically algorithms di...\"],[\"life sciences experts generally use empirical knowledge recode variables choose interactions perform...\"],[\"currently machine learning plays important role lives individual activities numerous people accordin...\"],[\"propose framework perform streaming covariance selection approach employs regularization constraints...\"],[\"learning causal effect observational data straightforward possible without assumptions hidden common...\"],[\"blackbox alpha bbalpha new approximate inference method based minimization alphadivergences bbalpha ...\"],[\"method large scale gaussian process classification recently proposed based expectation propagation m...\"],[\"deep gaussian processes dgps multilayer hierarchical generalisations gaussian processes gps formally...\"],[\"supervised learning active research area numerous applications diverse fields data analytics compute...\"],[\"regular particle filter algorithm sequential monte carlo smc methods initial weights traditionally d...\"],[\"compute approximate solutions regularized linear regression using regularization also known lasso in...\"],[\"present scalable gaussian process model identifying characterizing smooth multidimensional changepoi...\"],[\"present convex approach probabilistic segmentation modeling time series data approach builds upon re...\"],[\"present pesmo bayesian method identifying pareto set multiobjective optimization problems functions ...\"],[\"clinical neuroscientific studies systematic differences two populations brain networks investigated ...\"],[\"speaker recognition scenarios find conversations recorded simultaneously multiple channels case inte...\"],[\"consider convexconcave saddle point problems separable structure nonstrongly convex functions propos...\"],[\"document going derive equations needed implement variational bayes estimation parameters simplified ...\"],[\"latent variable timeseries models among heavily used tools machine learning applied statistics model...\"],[\"stateoftheart speaker recognition relays models need large amount training data models successful ta...\"],[\"document going derive equations needed implement variational bayes ivector extractor used extract lo...\"],[\"distancebased hierarchical clustering methods widely used unsupervised data analysis authors take ac...\"],[\"derive statistical model estimation dendrogram single linkage hierarchical clustering slhc takes acc...\"],[\"paper aims achieving good estimator gradient function highdimensional space often functions sensitiv...\"],[\"present informationtheoretic framework solving global blackbox optimization problems also blackbox c...\"],[\"understanding type inhibitory interaction plays important role drug design therefore researchers int...\"],[\"adjusted chance measures widely used compare partitionsclusterings data set particular adjusted rand...\"],[\"stochastic variational inference collapsed models recently successfully applied large scale topic mo...\"],[\"stochastic variational inference collapsed models recently successfully applied large scale topic mo...\"],[\"neuroimaging data analysis gaussian graphical models often used model statistical dependencies acros...\"],[\"investigate class feature allocation models generalize indian buffet process parameterized gibbstype...\"],[\"topic models popular modeling discrete data texts images videos links provide efficient way discover...\"],[\"one core problems statistical models estimation posterior distribution topic models problem posterio...\"],[\"recursive partitioning approaches producing treelike models long standing staple predictive modeling...\"],[\"stochastic block model sbm widely used random graph model networks communities despite recent burst ...\"],[\"learning hidden markov model hmm sequen tial observations often complemented realvalued summary resp...\"],[\"recurrent neural networks rnns stand forefront many recent developments deep learning yet major diff...\"],[\"causal inference deals identifying random variables cause control random variables recent advances t...\"],[\"propose novel classification model weak signal data building upon recent model bayesian multiview le...\"],[\"investigate properties estimators obtained minimization uprocesses lasso penalty highdimensional set...\"],[\"consider inverse problem reconstructing posterior measure trajec tories diffusion process discrete t...\"],[\"show objective function conventional kmeans clustering expressed frobenius norm difference data matr...\"],[\"learning deep models using bayesian methods generated significant attention recently largely feasibi...\"],[\"effective training deep neural networks suffers two main issues first parameter spaces models exhibi...\"],[\"histogram method powerful nonparametric approach estimating probability density function continuous ...\"],[\"propose novel class timevarying nonparanormal graphical models allows model high dimensional heavyta...\"],[\"functional brain networks well described estimated data gaussian graphical models ggms using sparse ...\"],[\"compressed sensing order recover sparse nearly sparse vector possibly noisy measurements popular app...\"],[\"study fundamental tradeoffs computational tractability statistical accuracy general family hypothesi...\"],[\"introduce general framework estimation inverse covariance precision matrices heterogeneous populatio...\"],[\"aim paper provide new method learning relationships data obtained independently unlike existing meth...\"],[\"paper index policies minimizing frequentist regret stochastic multiarmed bandit model inspired bayes...\"],[\"paper novel approach coding nominal data proposed given nominal data rank form complex number assign...\"],[\"many complex diseases wide variety ways individual manifest disease challenge personalized medicine ...\"],[\"present method variable selection sparse generalized additive model method doesnt assume specific fu...\"],[\"nongaussian component analysis ngca aimed identifying linear subspace projected data follows nongaus...\"],[\"paper presents new framework manifold learning based sequence principal polynomials capture possibly...\"],[\"signal processing problems involve challenging task multidimensional probability density function pd...\"],[\"generating user interpretable multiclass predictions data rich environments many classes explanatory...\"],[\"traditionally practitioners initialize kmeans algorithm centers chosen uniformly random randomized i...\"],[\"regularized discriminant analysis rda proposed friedman widely popular classifier lacks interpretabi...\"],[\"hearing aid algorithms need tuned fitted match impairment specific patient lack fundamental fitting ...\"],[\"problem learning sparse model conceptually interpreted process identifying active featuressamples op...\"],[\"propose nonparametric statistical test goodnessoffit given set samples test determines likely genera...\"],[\"unsupervised image segmentation aims clustering set pixels image spatially homogeneous regions intro...\"],[\"derive new discrepancy statistic measuring differences two probability distributions based combining...\"],[\"recently stochastic gradient markov chain monte carlo sgmcmc methods proposed scaling monte carlo co...\"],[\"extracting underlying lowdimensional space highdimensional signals often reside long center numerous...\"],[\"mixture experts moe model popular neural network architecture nonlinear regression classification cl...\"],[\"paper study predictive pattern mining problems goal construct predictive model based subset predicti...\"],[\"discovering statistically significant patterns databases important challenging problem main obstacle...\"],[\"circular variables arise multitude datamodelling contexts ranging robotics social sciences largely o...\"],[\"datasets growing size complexity creating demand rich models quantification uncertainty bayesian met...\"],[\"best knowledge general wellfounded robust methods statistical unsupervised learning unsupervised met...\"],[\"sampling replacement occurs many settings machine learning notably bagging ensemble technique valida...\"],[\"dynamic topic models dtms effective discovering topics capturing evolution trends time series data p...\"],[\"many machine learning problems characterized mutual contamination models problems one observes sever...\"],[\"propose segmented ihmm sihmm hierarchical infinite hidden markov model ihmm supports simple efficien...\"],[\"introduce new approach amortizing inference directed graphical models learning heuristic approximati...\"],[\"consider problem sparse clustering assumed subset features useful clustering purposes framework cosa...\"],[\"paper present framework fitting multivariate hawkes processes largescale problems number events obse...\"],[\"consider bayesian optimization expensivetoevaluate blackbox objective function also access cheaper a...\"],[\"apply wild bootstrap method lancaster threevariable interaction measure order detect factorisation j...\"],[\"nongaussian component analysis ngca unsupervised linear dimension reduction method extracts lowdimen...\"],[\"introduce overdispersed blackbox variational inference method reduce variance monte carlo estimator ...\"],[\"partition functions probability distributions important quantities model evaluation comparisons pres...\"],[\"kernel methods one mainstays machine learning problem kernel learning remains challenging heuristics...\"],[\"note compares two recently published machine learning methods constructing flexible tractable famili...\"],[\"generalised degrees freedom gdf defined jasa represent sensitivity model fits perturbations data com...\"],[\"develop square root graphical models sqr novel class parametric graphical models provides multivaria...\"],[\"paper investigates phase retrieval problem aims recover signal magnitudes linear measurements develo...\"],[\"manifold learning dimensionality reduction techniques ubiquitous science engineering computationally...\"],[\"consider problem maximizing unknown function compact convex set using observations possible observe ...\"],[\"linear autoregressive models serve basic representations discrete time stochastic processes differen...\"],[\"contrastive divergence algorithm achieved notable success training energybased models including rest...\"],[\"many real world graphs graphs molecules exhibit structure multiple different scales existing kernels...\"],[\"propose general modeling inference framework composes probabilistic graphical models deep learning m...\"],[\"document travel may expect short snippets document also travel introduce general framework incorpora...\"],[\"propose new class metrics sets vectors functions used various stages data mining including explorato...\"],[\"patent lawsuits costly timeconsuming ability forecast patent litigation time litigation allows compa...\"],[\"one objectively measure performance individual offensive lineman nfl existing literature proposes va...\"],[\"investigations performed using clustering methods data mining timeseries data smart meters problem i...\"],[\"multiplayer online battle arena moba games among played digital games world games teams players figh...\"],[\"consider problem changepoint detection multivariate timeseries multivariate distribution observation...\"],[\"paper present unified analysis matrix completion general lowdimensional structural constraints induc...\"],[\"quantitatively assessing relationships latent variables observed variables important understanding d...\"],[\"extremes play special role anomaly detection beyond inference simulation purposes probabilistic tool...\"],[\"many retailers today employ inventory management systems based reorder point policies rely assumptio...\"],[\"nonparametric detection existence anomalous structure network investigated nodes corresponding anoma...\"],[\"research manifold learning within density ridge estimation framework shown great potential recent wo...\"],[\"population migration valuable information leads proper decision urbanplanning strategy massive inves...\"],[\"estimation normalizing constants fundamental step probabilistic model comparison sequential monte ca...\"],[\"study sparse nonnegative least squares snnls problem snnls occurs naturally wide variety application...\"],[\"matrix generalized inverse gaussian mathcalmgig distribution arises naturally settings distribution ...\"],[\"recent years structured matrix recovery problems gained considerable attention real world applicatio...\"],[\"consider statistical inverse learning problem observe image function linear operator iid random desi...\"],[\"due challenging applications collaborative filtering matrix completion problem widely studied past y...\"],[\"article present elitist particle filter based evolutionary strategies epfes efficient approach nonli...\"],[\"timevarying mixture densities occur many scenarios example distributions keywords appear publication...\"],[\"introduce sparse random projection important dimensionreduction tool machine learning estimation dis...\"],[\"consider firm sells products periods without knowing demand function firm sequentially sets prices e...\"],[\"present methodology clustering objects described multivariate time series several sequences realvalu...\"],[\"sparse generalized eigenvalue problem gep plays pivotal role large family highdimensional statistica...\"],[\"modern data analyst must cope data encoded various forms vectors matrices strings graphs consequentl...\"],[\"gaussian graphical models ggms probabilistic tools choice analyzing conditional dependencies variabl...\"],[\"separating short jobs long known technique improve scheduling performance paper describe method deve...\"],[\"distributional distributionvalued data new type data arising several sources considered realizations...\"],[\"controlled interventions provide direct source information learning causal effects particular dosere...\"],[\"recent popularity graphical clustering methods increased focus information samples show learning clu...\"],[\"biological systems often modelled different levels abstraction depending particular aimsresources st...\"],[\"study paper consequences using mean absolute percentage error mape measure quality regression models...\"],[\"multivariate analysis mva comprises family wellknown methods feature extraction exploit correlations...\"],[\"paper propose new method predict final destination vehicle trips based initial partial trajectories ...\"],[\"simple interpretation matrix completion problem introduced based statistical models combined wellkno...\"],[\"structural equation models sems widely adopted inference causal interactions complex networks recent...\"],[\"present new method estimating multivariate secondorder stationary gaussian random field grf models b...\"],[\"given two possible treatments may exist subgroups benefit greater one treatment problem relevant fie...\"],[\"volume collection contributions workshop machine learning interpretation neuroimaging mlini neural i...\"],[\"many complex ecosystems formed multiple microbial taxa involve intricate interactions amongst variou...\"],[\"present method based orthogonal symmetric nonnegative matrix trifactorization normalized laplacian m...\"],[\"study problem estimating parameters regression model set observations consisting response predictor ...\"],[\"rapid growth crowdsourcing platforms become easy relatively inexpensive collect dataset labeled mult...\"],[\"sparse versions principal component analysis pca imposed simple yet powerful ways selecting relevant...\"],[\"extend stochastic gradient variational bayes perform posterior inference weights stickbreaking proce...\"],[\"consider structure discovery undirected graphical models observational data inferring likely structu...\"],[\"kernelbased quadrature rules becoming important machine learning statistics achieve supersqrtn conve...\"],[\"many applications desirable extract relevant aspects data principled way information bottleneck meth...\"],[\"extend traditional worstcase minimax analysis stochastic convex optimization introducing localized f...\"],[\"dictionary learning cuttingedge area imaging processing recently led stateoftheart results many sign...\"],[\"hyperparameters gaussian process regression gpr model specified kernel often estimated data via maxi...\"],[\"consider problem robustifying highdimensional structured estimation robust techniques key realworld ...\"],[\"propose datadriven coarsegraining formulation context equilibrium statistical mechanics contrast exi...\"],[\"generalized canonical correlation analysis gcca aims finding latent lowdimensional common structure ...\"],[\"stochastic variational inference svi stateoftheart algorithm scaling variational inference largedata...\"],[\"predictive models used highdimensional brain images diagnosis clinical condition spatial regularizat...\"],[\"imaging genetic research essentially focused discovering unique coassociation effects typically igno...\"],[\"genomewide interaction studies detect genegene interactions methods divided two folds single nucleot...\"],[\"present novel kway highdimensional graphical model called generalized root model grm explicitly mode...\"],[\"propose nonconvex estimator joint multivariate regression precision matrix estimation high dimension...\"],[\"introduce truncated gaussian graphical model tggm novel framework designing statistical models nonli...\"],[\"rapid overlay chemical structures rocs standard tool calculation shape chemical color similarity roc...\"],[\"clustering one important unsupervised problems machine learning statistics among many existing algor...\"],[\"monte carlo sampling algorithms extremely widelyused technique estimate expectations functions espec...\"],[\"vast majority neural network literature focuses predicting point values given set response variables...\"],[\"multivariate normal density monotonic function distance mean ellipsoidal shape due underlying euclid...\"],[\"paper considers quantification prediction performance gaussian process regression standard approach ...\"],[\"paper develop method learning nonlinear systems multiple outputs inputs begin modelling errors nomin...\"],[\"introduce network maximal correlation nmc multivariate measure nonlinear association among random va...\"],[\"good sparse approximations essential practical inference gaussian processes computational cost exact...\"],[\"decoding prediction brain images signals calls empirical evaluation predictive power evaluation achi...\"],[\"introduce mondrian kernel fast random feature approximation laplace kernel suitable batch online lea...\"],[\"regression trees becoming increasingly popular omnibus predicting tools basis numerous modern statis...\"],[\"tree ensembles random forest boosted trees renowned high prediction performance whereas interpretabi...\"],[\"paper discusses package implements pattern sequence based forecasting psf algorithm developed univar...\"],[\"improving interpretability brain decoding approaches primary interest many neuroimaging studies desp...\"],[\"paper study problem recovering group sparse vector small number linear measurements past common appr...\"],[\"consider training probabilistic classifiers case large number classes number classes assumed large p...\"],[\"study twolevel multiview learning two views pacbayesian framework approach sometimes referred late f...\"],[\"propose probabilistic modeling framework learning dynamic patterns collective behaviors social agent...\"],[\"observations organized groups commonalties exist amongst dependent random measures ideal choice mode...\"],[\"novel dynamic bayesian nonparametric topic model anomaly detection video proposed paper batch online...\"],[\"paper proposes novel dynamic hierarchical dirichlet process topic model considers dependence success...\"],[\"approximate bayesian computation abc framework performing likelihoodfree posterior inference simulat...\"],[\"deep learning methods multitask neural networks recently applied ligandbased virtual screening drug ...\"],[\"contagions spread popular news stories infectious diseases propagate cascades dynamic networks unobs...\"],[\"consider learning highdimensional multiresponse linear models structured parameters exploiting noise...\"],[\"tree ensembles random forests boosted trees renowned high prediction performance however interpretab...\"],[\"problem finding overlapping communities networks gained much attention recently optimizationbased ap...\"],[\"communitybased question answering cqa sites play important role addressing health information needs ...\"],[\"probabilistic programming languages represent complex data intermingled models lines code efficient ...\"],[\"given graph vertices deemed interesting priori vertex nomination task order remaining vertices nomin...\"],[\"paper propose bayesian nonparametric approach modelling sparse timevarying networks positive paramet...\"],[\"propose vectorvalued regression problem whose solution equivalent reproducing kernel hilbert space r...\"],[\"present general framework classifying partially observed dynamical systems based idea learning model...\"],[\"additive nonparametric regression models provide attractive tool variable selection high dimensions ...\"],[\"ensemble regression trees become popular statistical tools estimation conditional mean given set pre...\"],[\"hamiltonian monte carlo hmc exploits hamiltonian dynamics construct efficient proposals markov chain...\"],[\"present new methods estimate causal effects retrospectively micro data assistance machine learning e...\"],[\"machine learning science discovering statistical dependencies data use dependencies perform predicti...\"],[\"hierarchical probabilistic models mixture models used cluster analysis models two types variables ob...\"],[\"study density estimation problem observations generated certain dynamical systems admit unique under...\"],[\"important problem sequential decisionmaking uncertainty use limited data compute safe policy policy ...\"],[\"spectral embedding uses eigenfunctions discrete laplacian weighted graph obtain coordinates embeddin...\"],[\"work falls within context predicting value real function input locations given limited number observ...\"],[\"concentration inequalities indispensable tools studying generalization capacity learning models hoef...\"],[\"propose method performs anomaly detection localisation within heterogeneous data using pairwise undi...\"],[\"highdimensional estimation prediction methods propose minimize cost function empirical risk written ...\"],[\"mixture models gamma inversegamma distributed mixture components useful medical image tissue segment...\"],[\"preterm births occur alarming rate preemies higher risk infant mortality developmental retardation l...\"],[\"prove central limit theorem components eigenvectors corresponding largest eigenvalues normalized lap...\"],[\"nonlinear similarity measures defined kernel space correntropy extract higherorder statistics data o...\"],[\"study probability measures induced set functions constraints measures arise variety realworld settin...\"],[\"study problem demixing pair sparse signals noisy nonlinear observations superposition mathematically...\"],[\"genomewide association study gwas correlates marker variation trait variation sample individuals stu...\"],[\"recently general method analyzing statistical accuracy algorithm developed applied simple latent var...\"],[\"boosting combines weak biased learners obtain effective learning algorithms classification predictio...\"],[\"factorial hidden markov models fhmms powerful models sequential data scale well long sequences propo...\"],[\"machine learning methods used discover complex nonlinear relationships biological medical data howev...\"],[\"paper considers emphvolume minimization volminbased structured matrix factorization smf volmin facto...\"],[\"datasets mixture numerical categorical attributes routinely encountered many application domains wor...\"],[\"exploration hydrocarbon resources highly complicated expensive process various geological geochemica...\"],[\"kalman filter used variety applications computing posterior distribution latent states state space m...\"],[\"technique formal concept analysis applied dataset describing traits rodents goal identifying zoonoti...\"],[\"improve current instabilitybased methods selection number clusters cluster analysis developing norma...\"],[\"unscented transformation efficient method solve state estimation problem nonlinear dynamic system ut...\"],[\"kernelbased kmeans clustering gained popularity due simplicity power implicit nonlinear representati...\"],[\"manifold markov chain monte carlo algorithms introduced sample effectively challenging target densit...\"],[\"incrementalonline state dynamic learning method proposed identification nonlinear gaussian state spa...\"],[\"gaussian graphical models widely used represent conditional dependence among random variables paper ...\"],[\"sparse coding core building block many data analysis machine learning pipelines typically solved rel...\"],[\"develop automated variational method inference models gaussian process priors general likelihoods me...\"],[\"work presents pesmoc predictive entropy search multiobjective bayesian optimization constraints info...\"],[\"principal component analysis pca exploratory tool widely used data analysis uncover dominant pattern...\"],[\"develop automated variational inference method bayesian structured prediction problems gaussian proc...\"],[\"hamiltonian monte carlo hmc popular markov chain monte carlo mcmc algorithm generates proposals metr...\"],[\"sparse subspace clustering ssc elegant approach unsupervised segmentation data points cluster locate...\"],[\"understanding relationships different properties data whether connectome genome information disease ...\"],[\"extend adaptive regression spline model incorporating saturation natural requirement function extend...\"],[\"many applications particular information systems pattern recognition machine learning cheminformatic...\"],[\"softmax representation probabilities categorical variables plays prominent role modern machine learn...\"],[\"variational inference lies core many stateoftheart algorithms improve approximation posterior beyond...\"],[\"scale data growing every day reducing dimensionality aka sketching highdimensional data emerged task...\"],[\"date instability prognostic predictors sparse high dimensional model hinders clinical adoption recei...\"],[\"bayesian networks bns graphical models useful representing highdimensional probability distributions...\"],[\"gaussian processes powerful yet analytically tractable models supervised learning gaussian process c...\"],[\"motivated electricity consumption metering extend existing nonnegative matrix factorization nmf algo...\"],[\"constrained adaptive filtering algorithms inculding constrained least mean square clms constrained a...\"],[\"reparameterization gradient become widely used method obtain monte carlo gradients optimize variatio...\"],[\"propose nonparametric sequential test aims address two practical problems pertinent online randomize...\"],[\"generative adversarial networks gans successful deep generative models gans based twoplayer minimax ...\"],[\"derive novel variational expectation maximization approach based truncated posterior distributions t...\"],[\"extracting information functional magnetic resonance fmri images major area research two decades goa...\"],[\"study statistical inference distributionally robust solution methods stochastic optimization problem...\"],[\"sources variability experimentally derived data include measurement error addition physical phenomen...\"],[\"paper consider clustering data assumed come one finitely many pointed convex polyhedral cones model ...\"],[\"propose communicationefficient distributed estimation method sparse linear discriminant analysis lda...\"],[\"density matrices positively semidefinite hermitian matrices unit trace describe states quantum syste...\"],[\"quantitative modeling posttranscriptional regulation process challenging problem systems biology mec...\"],[\"importance sampling widely used machine learning statistics power limited restriction using simple p...\"],[\"propose unified framework estimating lowrank matrices nonconvex optimization based gradient descent ...\"],[\"investigate capabilities limitations gaussian process models jointly exploring three complementary d...\"],[\"provide theoretical foundation nonparametric estimation functions random variables using kernel mean...\"],[\"determination cluster centers generally depends scale use analyze data clustered inappropriate scale...\"],[\"effective accurate model selection important problem modern data analysis one major challenges compu...\"],[\"independent component analysis ica powerful method blind source separation based assumption sources ...\"],[\"goal twosample tests assess whether two samples sim sim drawn distribution perhaps intriguingly one ...\"],[\"recent advances bayesian learning largescale data witnessed emergence stochastic gradient mcmc algor...\"],[\"present sparse estimation dictionary learning framework compressed fiber sensing based probabilistic...\"],[\"gradient matching gaussian processes promising tool learning parameters ordinary differential equati...\"],[\"independent component analysis ica popular method blind source separation bss diverse set applicatio...\"],[\"paper develop new sequential regression modeling approach data streams data streams commonly found a...\"],[\"datasets containing large samples timetoevent data arising several small heterogeneous groups common...\"],[\"introduce mixture model censored durations cmix develop maximum likelihood inference joint estimatio...\"],[\"propose parallelizable sparse inverse formulation gaussian process spingp temporal models uses spars...\"],[\"paper investigate link state space models gaussian processes time series modeling forecasting partic...\"],[\"directed networks pervasive nature engineered systems often underlying complex behavior observed bio...\"],[\"many natural systems neurons firing brain basketball teams traversing court give rise time series da...\"],[\"despite fundamental nature inhomogeneous poisson process theory application stochastic processes att...\"],[\"stochastic gradient descent sgd algorithm widely used statistical estimation largescale data due com...\"],[\"gpflow gaussian process library uses tensorflow core computations python front end distinguishing fe...\"],[\"paper proposes subspace decomposition method based overcomplete dictionary sparse representation cal...\"],[\"nonnegative matrix factorization nmf popular tool data exploration bayesian nmf promises also charac...\"],[\"propose geometric algorithm topic learning inference built convex geometry topics arising latent dir...\"],[\"regularised canonical correlation analysis recently extended two sets variables multiblock method re...\"],[\"exploiting fact arrival processes exhibit cyclic behaviour propose simple procedure estimating inten...\"],[\"propose methodology explore measure pairwise correlations exist variables dataset methodology levera...\"],[\"analysis nonstationary time series great importance many scientific fields physics neuroscience rece...\"],[\"consider problem constructing diffusion operators high dimensional data address counterfactual funct...\"],[\"propose new method discovering causal relationships temporal data based notion causal compression en...\"],[\"kernel dependence measures yield accurate estimates nonlinear relations random variables also endors...\"],[\"semisupervised unsupervised systems provide operators invaluable support tremendously reduce operato...\"],[\"many machine learning algorithms require precise estimates covariance matrices sample covariance mat...\"],[\"introduce semiparametric bayesian model survival analysis model centred parametric baseline hazard u...\"],[\"tensor decomposition important technique capturing highorder interactions among multiway data multil...\"],[\"article study spectral methods community detection based alphaparametrized normalized modularity mat...\"],[\"consider learning algorithms general source condition polynomial decay eigenvalues integral operator...\"],[\"game theory finds nowadays broad range applications engineering machine learning however derivativef...\"],[\"vision precision medicine use individual patient characteristics inform personalized treatment plan ...\"],[\"investigate kernel regularization methods achieve minimax convergence rates source condition regular...\"],[\"spectral dimensionality reduction frequently used identify lowdimensional structure highdimensional ...\"],[\"proposed complex populations arise genomics studies may exhibit dependencies among observations well...\"],[\"ability track moving vehicle crucial importance numerous applications task often approached importan...\"],[\"paper combine two important extensions ordinary least squares regression regularization optimal scal...\"],[\"propose method finding alternate features missing lasso optimal solution ordinary lasso problem one ...\"],[\"work brings together two powerful concepts gaussian processes variational approach sparse approximat...\"],[\"years ensemble methods become staple machine learning similarly generalized linear models glms becom...\"],[\"widespread need techniques discover structure time series data recently introduced techniques automa...\"],[\"consider problem estimating expected value information knowledge gradient bayesian learning problems...\"],[\"present wrightfisher indian buffet process wfibp probabilistic model timedependent data assumed gene...\"],[\"decision makers doctors judges make crucial decisions recommending treatments patients granting bail...\"],[\"proceedings nips workshop interpretable machine learning complex systems held barcelona spain decemb...\"],[\"increasing availability vehicle gps data created potentially transformative opportunities traffic ma...\"],[\"construction synthetic complexvalued signals realvalued observations important step many time series...\"],[\"paper consider problem learning highdimensional tensor regression problems lowrank structure one cor...\"],[\"variational inference provides powerful tool approximate probabilistic ference complex structured mo...\"],[\"aim create framework transfer learning using latent factor models learn dependence structure larger ...\"],[\"introduce novel approach parallelizing mcmc inference models spatially determined conditional indepe...\"],[\"noise injection efficient technique mitigate overfitting neural networks nns bernoulli procedure imp...\"],[\"supervised topic models help clinical researchers find interpretable cooccurence patterns count data...\"],[\"bagging device intended reducing prediction error learning algorithms simplest form bagging draws bo...\"],[\"focus maximum regularization parameter anisotropic totalvariation denoising corresponds minimum valu...\"],[\"paper presents algorithm unsupervised learning latent variable models unlabeled sets data base techn...\"],[\"stochastic variational inference svi paradigm combines variational inference natural gradients stoch...\"],[\"spectral analysis neighborhood graphs one widely used techniques exploratory data analysis applicati...\"],[\"many popular network models rely assumption vertex exchangeability distribution graph invariant rela...\"],[\"paper frames causal structure estimation machine learning task idea treat indicators causal relation...\"],[\"tuning parameter selection critical importance kernel ridge regression date data driven tuning metho...\"],[\"introduce novel multivariate random process producing bernoulli outputs per dimension possibly forma...\"],[\"typical applications bayesian optimization minimal assumptions made objective function optimized tru...\"],[\"provide way infer existence topological circularity highdimensional data sets mathbbrd projection ma...\"],[\"propose communicationefficient distributed estimation inference methods transelliptical graphical mo...\"],[\"joint blind source separation jbss emerging datadriven technique multiset datafusion paper jbss addr...\"],[\"personalized treatment patients based tissuespecific cancer subtypes strongly increased efficacy cho...\"],[\"study problem estimating lowrank matrices linear measurements aka matrix sensing nonconvex optimizat...\"],[\"dynamic mode decomposition dmd emerged powerful tool analyzing dynamics nonlinear systems experiment...\"],[\"workshop explores interface cognitive neuroscience recent advances fields aim reproduce human perfor...\"],[\"recent years seen increasing popularity learning sparse emphchanges markov networks changes structur...\"],[\"gaussian graphical model graphical representation dependence structure gaussian random vector recogn...\"],[\"propose generic framework based new stochastic variancereduced gradient descent algorithm accelerati...\"],[\"article large dimensional performance analysis kernel least squares support vector machines lssvms p...\"],[\"processes governing lives use part automatic decision step based feature vector derived applicant al...\"],[\"canonical correlation analysis cca multivariate statistical technique finding linear relationship tw...\"],[\"regression models increasingly built using datasets follow design experiment instead data gathered a...\"],[\"gaussian processes gps proven powerful tools various areas machine learning however applications gps...\"],[\"random forest missing data algorithms attractive approach dealing missing data desirable properties ...\"],[\"estimation individual treatment effect observational data complicated due challenges confounding sel...\"],[\"stability important aspect classification procedure unstable predictions potentially reduce users tr...\"],[\"clustering central approach unsupervised learning clustering applied fundamental analysis quantitati...\"],[\"consider demixing problem two highdimensional vectors nonlinear observations number observations far...\"],[\"random sinusoidal features popular approach speeding kernelbased inference large datasets prior infe...\"],[\"heavytailed distributions widely used robust mixture modelling due possessing thick tails computatio...\"],[\"subset selection multiple linear regression aims choose subset candidate explanatory variables trade...\"],[\"given functional data survival process timedependent covariates derive smooth convex representation ...\"],[\"prototypal analysis introduced overcome two shortcomings archetypal analysis sensitivity outliers no...\"],[\"forward regression statistical model selection estimation procedure inductively selects covariates a...\"],[\"paper presents novel datadriven technique based spatiotemporal pattern network stpn energypower pred...\"],[\"common problem disciplines applied statistics research astrostatistics estimating posterior distribu...\"],[\"consumer demand response important research industry problem seeks categorize predict modify consume...\"],[\"paper proposes hierarchical feature extractor nonstationary streaming time series based concept swit...\"],[\"work robust clustering algorithm stationary time series proposed algorithm based use estimated spect...\"],[\"propose framework modeling estimating state controlled dynamical systems agent affect system actions...\"],[\"data collections become larger exploratory regression analysis becomes important challenging observa...\"],[\"statistical downscaling global climate models gcms allows researchers study local climate change eff...\"],[\"flow cytometry highthroughput technology used quantify multiple surface intracellular markers level ...\"],[\"many modern data sets sampled error complex highdimensional surfaces methods tensor product splines ...\"],[\"study additive models built trend filtering additive models whose components regularized discrete to...\"],[\"estimating state dynamical system series noisecorrupted observations fundamental many areas science ...\"],[\"paper introduces method efficiently inferring highdimensional distributed quantity observations quan...\"],[\"saga fast incremental gradient method finite sum problem effectiveness tested vast applications pape...\"],[\"background statistical mechanics results dauphin choromanska suggest local minima high error exponen...\"],[\"hypothesis tests models whose dimension far exceeds sample size formulated much like classical stude...\"],[\"many statistical learning problems posed minimization sum two convex functions one typically composi...\"],[\"note answer question lecue showing column normalization random matrix iid entries need lead good spa...\"],[\"propose unified framework solve general lowrank plus sparse matrix recovery problems based matrix fa...\"],[\"partial least squares pls methods heavily exploited analyse association two blocs data powerful appr...\"],[\"paper propose pckid novel robust kernel function spectral clustering specifically designed handle in...\"],[\"learning rates leastsquares regression typically expressed terms lnorms paper extend rates norms str...\"],[\"present approach deep estimation discrete conditional probability distributions models several appli...\"],[\"consider problem estimating regression function common situation number features small interpretabil...\"],[\"microwavebased breast cancer detection proposed complementary approach compensate drawbacks existing...\"],[\"present model random simple graphs degree distribution obeys power law heavytailed attain behavior e...\"],[\"pseudolikelihood method one popular algorithms learning sparse binary pairwise markov networks paper...\"],[\"introduce novel kernel models inputdependent couplings across multiple latent processes pairwise joi...\"],[\"training gaussian processbased models typically involves computational bottleneck due inverting cova...\"],[\"propose novel method semisupervised learning ssl based datadriven distributionally robust optimizati...\"],[\"study fundamental class regression models called second order linear model slm slm extends linear mo...\"],[\"consider modification covariance function gaussian processes correctly account known linear constrai...\"],[\"computing partition function important statistical inference task arising applications graphical mod...\"],[\"paper studies new bayesian algorithm joint reconstruction classification reflectance confocal micros...\"],[\"topic models one popular methods learning representations text major challenge change topic model re...\"],[\"yield curve forecasting important problem finance work explore use gaussian processes conjunction dy...\"],[\"propose paper differentiable learning loss time series building upon celebrated dynamic time warping...\"],[\"study two procedures reversemode forwardmode computing gradient validation error respect hyperparame...\"],[\"inspired importance diversity biological system built heterogeneous system could achieve goal archit...\"],[\"deep generative models wildly successful learning coherent latent representations continuous data vi...\"],[\"discuss bayesian formulation coarsegraining pdes coefficients material parameters exhibit random fin...\"],[\"goal paper design sequential strategies lead efficient optimization unknown function assumption fini...\"],[\"study dual volume sampling method selecting columns short wide matrix probability selection proporti...\"],[\"density ratio estimation vital tool machine learning statistical community however due unbounded nat...\"],[\"present mlrmbo flexible comprehensive toolbox modelbased optimization mbo also known bayesian optimi...\"],[\"indian buffet process based models elegant way discovering underlying features within data set infer...\"],[\"show dbscan estimate connected components lambdadensity level set lambda given iid samples unknown d...\"],[\"present study proposes deep learning model named deepsleepnet automatic sleep stage scoring based ra...\"],[\"propose novel bayesian optimization approach blackbox functions environmental variable whose value d...\"],[\"gaussian process model vectorvalued function shown useful multioutput prediction existing method mod...\"],[\"present method conditional time series forecasting based adaptation recent deep convolutional wavene...\"],[\"propose probabilistic model aggregate answers respondents answering multiplechoice questions model a...\"],[\"goal data clustering partition data points groups minimize given objective function existing cluster...\"],[\"propose data aggregationbased algorithm monotonic convergence global optimum generalized version lno...\"],[\"work addresses various open questions theory active learning nonparametric classification contributi...\"],[\"graphbased semisupervised learning one popular methods machine learning theoretical properties bound...\"],[\"bandit methods blackbox optimisation bayesian optimisation used variety applications including hyper...\"],[\"investigate coresets succinct small summaries large data sets solutions found summary provably compe...\"],[\"massive amount available data potentially used discover patters machine learning challenge kernel ba...\"],[\"kernel embeddings distributions maximum mean discrepancy mmd resulting distance distributions useful...\"],[\"present first treatment arc length gaussian process single output dimension gps commonly used tasks ...\"],[\"maximum correntropy criterion mcc recently successfully applied robust regression classification ada...\"],[\"inferring correct answers binary tasks based multiple noisy answers unsupervised manner emerged cano...\"],[\"train statistical mixture model massive data set work show construct coresets mixtures gaussians cor...\"],[\"growing interest applying machine learning methods electronic medical records emr across different i...\"],[\"paper deals problem largescale linear supervised learning settings large number continuous features ...\"],[\"integrating visual linguistic information single multimodal representation unsolved problem widereac...\"],[\"existing strategies finitearmed stochastic bandits mostly depend parameter scale must known advance ...\"],[\"consider explorationexploitation tradeoff linear quadratic control problems state dynamics linear co...\"],[\"implemented several multilabel classification algorithms machine learning package mlr implemented me...\"],[\"scenario realtime monitoring hospital patients highquality inference patients health status using in...\"],[\"development computed tomography image reconstruction methods significantly reduce patient radiation ...\"],[\"objectives discussions fairness criminal justice risk assessments typically lack conceptual precisio...\"],[\"analyzing multivariate time series data important predict future events changes complex systems fina...\"],[\"consider generalization lowrank matrix completion case data belongs algebraic variety data point sol...\"],[\"highdimensional andor nonparametric regression problems regularization penalization used control mod...\"],[\"spectral clustering popular versatile clustering method based relaxation normalised graph cut object...\"],[\"tradeoff cost acquiring processing data uncertainty due lack data fundamental machine learning basic...\"],[\"objective work perform margin assessment human breast tissue optical coherence tomography oct images...\"],[\"many applied settings empirical economics involve simultaneous estimation large number parameters pa...\"],[\"accurate reliable predictions infectious disease dynamics valuable public health organizations plan ...\"],[\"exciting branch machine learning research focuses methods learning optimizing integrating unknown fu...\"],[\"ensure interpretability extracted sources tensor decomposition introduce paper dictionarybased tenso...\"],[\"recently shown many existing quasinewton algorithms formulated learning algorithms capable learning ...\"],[\"study model one target variable correlated vector xxxd predictor variables potential causes describe...\"],[\"cluster analysis high dimensional data benefit properties high dimensionality informally expressed w...\"],[\"integrative analysis disparate data blocks measured common set experimental subjects major challenge...\"],[\"least absolute shrinkage selection operator lasso method adapted recently networkstructured datasets...\"],[\"superresolution classical problem image processing numerous applications remote sensing image enhanc...\"],[\"temporal group lasso example multitask regularized regression approach prediction response variables...\"],[\"paper describes structuring data constructing plots explore forest classification models interactive...\"],[\"paper study problem noisy tensor completion tensors admit canonical polyadic candecompparafac decomp...\"],[\"study strictly proper scoring rules reproducing kernel hilbert space propose general kernel scoring ...\"],[\"computing accurate estimates fourier transform analog signals discrete data points important many fi...\"],[\"standard interpretation importanceweighted autoencoders maximize tighter lower bound marginal likeli...\"],[\"bayesian optimization emerged last years effective approach optimizing blackbox functions direct que...\"],[\"matrix factorisation methods decompose multivariate observations linear combinations latent feature ...\"],[\"frankwolfe algorithm widely used solving nuclear norm constrained problems since require projections...\"],[\"show kmeans lloyds algorithm obtained special case truncated variational approximations applied gaus...\"],[\"boosting gradient descent algorithms one popular method machine learning paper novel boostingtype al...\"],[\"consider problem accelerating distributed optimization multiagent networks sequentially adding edges...\"],[\"propose novel adaptive importance sampling algorithm incorporates stein variational gradient decent ...\"],[\"variational inference approximates posterior distribution probabilistic model parameterized density ...\"],[\"study problem interactively learning binary classifier using noisy labeling pairwise comparison orac...\"],[\"gaussian processes gps powerful nonparametric function estimators however applications largely limit...\"],[\"consider problem estimating consensus community structure combining information multiple layers mult...\"],[\"stein variational gradient descent svgd deterministic sampling algorithm iteratively transports set ...\"],[\"current work characterizes users vod streaming space userpersonas based tenure timeline temporal beh...\"],[\"give convergence guarantees estimating coefficients symmetric mixture two linear regressions expecta...\"],[\"paper new bayesian model sparse linear regression spatiotemporal structure proposed incorporates str...\"],[\"daytime hypoglycemia accurately predicted achieve normoglycemia avoid disastrous situations hypoglyc...\"],[\"sales forecasting plays prominent role business planning business strategy value importance advance ...\"],[\"emergence thriving development social networks huge number short texts accumulated need processed in...\"],[\"address problem defining group sparse formulation principal components analysis pca equivalent formu...\"],[\"consider two networks overlapping nonidentical vertex sets given vertices interest first network see...\"],[\"consider class misspecified dynamical models governing term approximately known assumption observati...\"],[\"paper aims formulating issue ranking multivariate unlabeled observations depending degree abnormalit...\"],[\"possible perform linear regression datasets whose labels shuffled respect inputs explore question pr...\"],[\"present semiparametric spectral modeling complete larval drosophila mushroom body connectome motivat...\"],[\"semiparametric nonlinear regression model presence latent variables introduced latent variables corr...\"],[\"many unsupervised kernel methods rely estimation kernel covariance operator kernel kernel crosscovar...\"],[\"healthcare applications temporal variables encode movement health status longitudinal patient evolut...\"],[\"sparse mapping key methodology many highdimensional scientific problems multiple tasks share set rel...\"],[\"propose set convex low rank inducing norms coupled matrices tensors hereafter coupled tensors shares...\"],[\"probability distributions produced crossentropy loss ordinal classification problems possess undesir...\"],[\"order achieve stateoftheart performance modern machine learning techniques require careful data prep...\"],[\"kernel methods popular clustering due generality discriminating power however show many kernel clust...\"],[\"paper present novel method coclustering unsupervised learning approach aims discovering homogeneous ...\"],[\"develop new method called discriminated hub graphical lasso dhgl based hub graphical lasso hgl provi...\"],[\"consider problem efficient randomized dimensionality reduction normpreservation guarantees specifica...\"],[\"analyzing underlying structure multiple timesequences provides insights understanding social network...\"],[\"gradient matching promising tool learning parameters state dynamics ordinary differential equations ...\"],[\"paper introduces kernel mixture network new method nonparametric estimation conditional probability ...\"],[\"sparse pseudopoint approximations gaussian process models provide suite methods support deployment g...\"],[\"recently blanchet kang murhy blanchet kang showed several machine learning algorithms squareroot las...\"],[\"datadriven distributionally robust optimization dddro via optimal transport shown encompass wide ran...\"],[\"inference latent feature models bayesian nonparametric setting generally difficult especially high d...\"],[\"present work deals active sampling graph nodes representing training data binary classification grap...\"],[\"dropoutbased regularization methods regarded injecting random noise predefined magnitude different p...\"],[\"present accelerated algorithm hierarchical density based clustering new algorithm improves upon hdbs...\"],[\"highdimensional classification settings wish seek balance high power ensuring control desired loss f...\"],[\"waggle dance honeybees perform astonishing way communicating location food source years discovery re...\"],[\"consider problem estimation lowrank matrix limited number noisy rankone projections particular propo...\"],[\"increasing size complexity scientific data could dramatically enhance discovery prediction basic sci...\"],[\"identifying set homogeneous clusters heterogeneous dataset one important classes problems statistica...\"],[\"actorcritic methods solve reinforcement learning problems updating parameterized policy known actor ...\"],[\"study unsupervised generative modeling terms optimal transport problem true unknown data distributio...\"],[\"various problems data analysis statistical genetics call recovery columnsparse lowrank matrix noisy ...\"],[\"many distributed learning problems heterogeneous loading computing machines may harm overall perform...\"],[\"deep generative models provide powerful tools distributions complicated manifolds natural images man...\"],[\"dropout used practical tool obtain uncertainty estimates large vision models reinforcement learning ...\"],[\"work develop fast saliency detection method applied differentiable image classifier train masking mo...\"],[\"paper analyzes use convolutional neural networks brain tumor segmentation images address problem usi...\"],[\"traditionally community detection graphs solved using spectral methods posterior inference probabili...\"],[\"volume data generated internet social networks increasing every day clear need efficient ways extrac...\"],[\"consider parametric exponential families dimension real line study variant textitboundary crossing p...\"],[\"gaussian processes gps good choice function approximation flexible robust overfitting provide wellca...\"],[\"exponential family distributions highly useful machine learning since calculation performed efficien...\"],[\"langevin diffusion commonly used tool sampling given distribution work establish target density log ...\"],[\"generative adversarial networks gans become widely popular framework generative modelling highdimens...\"],[\"present new model predictive state recurrent neural networks psrnns filtering prediction dynamical s...\"],[\"deep learning applies hierarchical layers hidden variables construct nonlinear high dimensional pred...\"],[\"build autoencoding sequential monte carlo aesmc method model proposal learning based maximizing lowe...\"],[\"paper consider problem fair statistical inference involving outcome variables examples include class...\"],[\"bayesian neural networks bnns recently received increasing attention ability provide wellcalibrated ...\"],[\"gaussian processes gps distributions arbitrary functions continuous domain generalized multioutput c...\"],[\"numerous social medical engineering biological challenges framed graphbased learning tasks propose n...\"],[\"gaussian process state space model gpssm nonlinear dynamical system unknown transition andor measure...\"],[\"recent papers formulated problem learning graphs data inverse covariance estimation graph laplacian ...\"],[\"minimizing empirical risk popular training strategy learning tasks data may noisy heavytailed one ma...\"],[\"ability compare two degenerate probability distributions two probability distributions supported two...\"],[\"study problem detecting change points cps characterized subset dimensions multidimensional sequence ...\"],[\"sparse coding core building block many data analysis machine learning pipelines typically solved rel...\"],[\"short article revisits ideas introduced arxiv arxiv simple setup sheds lights connexions variational...\"],[\"chemical space large brute force searches new interesting molecules infeasible highthroughput virtua...\"],[\"piecewise linearquadratic plq penalties widely used develop models statistical inference signal proc...\"],[\"paper propose new volumepreserving flow show performs similarly linear general normalizing flow idea...\"],[\"present distributionally robust optimization dro approach estimate robustified regression plane line...\"],[\"consider stationary autoregressive processes coefficients restricted ellipsoid includes autoregressi...\"],[\"time series analysis used understand predict dynamic processes including evolving demands business w...\"],[\"multiple instance dictionary learning approach dictionary learning using functions multiple instance...\"],[\"present efficient blockdiagonal proximation gaussnewton matrix feedforward neural networks result in...\"],[\"bayesian optimization methods useful optimizing functions expensive evaluate lack analytical express...\"],[\"latent feature modeling allows capturing latent structure responsible generating observed properties...\"],[\"stochastic gradient mcmc sgmcmc algorithms proven useful scaling bayesian inference large datasets a...\"],[\"present probabilistic framework overlapping community discovery link prediction relational data give...\"],[\"twosample hypothesis testing problem studied challenging scenario high dimensional data sets small s...\"],[\"infinitesimal jackknife recently applied random forest estimate prediction variance theorems verifie...\"],[\"propose bayesian nonparametric mixture model prediction information extraction tasks efficient infer...\"],[\"machinelearned models often described black boxes many realworld applications however models may sac...\"],[\"paper present method determine global horizontal irradiance ghi power measurements one systems locat...\"],[\"two proteins homologous common evolutionary origin binary classification problem identify proteins c...\"],[\"paper describes expectation propagation method multiclass classification gaussian processes scales w...\"],[\"adopt data structure form cover trees iteratively apply approximate nearest neighbour ann searches f...\"],[\"present two deep generative models based variational autoencoders improve accuracy drug response pre...\"],[\"paper introduces youtubem video understanding challenge hosted kaggle competition also describes app...\"],[\"statistical dimensionality reduction common rely assumption high dimensional data tend concentrate n...\"],[\"bayesian neural networks bnns latent variables probabilistic models automatically identify complex s...\"],[\"power supply renewable resources global rise forecasted renewable generation surpass types generatio...\"],[\"study problem learning latent variables gaussian graphical models existing methods problem assume pr...\"],[\"learning children animals occurs effortlessly largely without obvious supervision successes automati...\"],[\"restricted isometry property rip universal tool data recovery explore implication rip framework gene...\"],[\"neural network based generative models discriminative components powerful approach semisupervised le...\"],[\"interpreting gradient methods fixedpoint iterations provide detailed analysis methods minimizing con...\"],[\"rising topic computational journalism enhance diversity news served subscribers foster exploration b...\"],[\"propose nuclear norm penalty alternative ridge penalty regularized multinomial regression convex rel...\"],[\"devising course treatment patient doctors often little quantitative evidence base decisions beyond m...\"],[\"tomal introduced notion phalanxes context rareclass detection twoclass classification problems phala...\"],[\"modes ridges probability density function behind observed data useful geometric features modeseeking...\"],[\"propose ksparse exhaustive search esk method ksparse approximate exhaustive search method aesk selec...\"],[\"given full partial information collection points lie close union several subspaces subspace clusteri...\"],[\"deep neural networks dnns excellent representative power state art classifiers many tasks however of...\"],[\"develop model interactions nodes dynamic network counted non homogeneous poisson processes block mod...\"],[\"sparsity learning known grouping structure received considerable attention due wide modern applicati...\"],[\"tick statistical learning library python particular emphasis timedependent models point processes to...\"],[\"given observation highdimensional ornsteinuhlenbeck process continuous time proceed inference drift ...\"],[\"propose method estimating coefficients multivariate regression clustering structure response variabl...\"],[\"paper presents approach automation interpretable feature selection internet things analytics iota us...\"],[\"propose novel theoreticallygrounded acquisition function batch bayesian optimization informed insigh...\"],[\"study tested interaction effect multimodal datasets using novel method called kernel method detectin...\"],[\"nowadays unprecedented penetration renewable distributed energy resources ders necessity efficient e...\"],[\"study learning problems involving arbitrary classes functions distributions targets proper learning ...\"],[\"provide two main contributions pacbayesian theory domain adaptation objective learn source distribut...\"],[\"propose simple method combines neural networks gaussian processes proposed method estimate uncertain...\"],[\"neymanscott classic example estimation problem partiallyconsistent posterior standard estimation met...\"],[\"soil moisture active passive smap mission delivered valuable sensing surface soil moisture since how...\"],[\"propose simple algorithm train stochastic neural networks draw samples given target distributions pr...\"],[\"study present multiclass graphical bayesian predictive classifier incorporates uncertainty model sel...\"],[\"mobile technologies offer opportunities higher resolution monitoring health conditions opportunity s...\"],[\"paper presents systematic review stateoftheart approaches identify patient cohorts using electronic ...\"],[\"bayesian nonparametrics class probabilistic models model size inferred data recently developed metho...\"],[\"orthogonal matching pursuit omp orthogonal least squares ols widely used sparse signal reconstructio...\"],[\"new technologies recording activity large neural populations complex behavior provide exciting oppor...\"],[\"paper generalized multivariate studentt mixture model developed classification clustering low probab...\"],[\"twosample feature selection problem finding features describe difference two probability distributio...\"],[\"transfer learning aims improve learning target domain borrowing knowledge related different source d...\"],[\"real data often contain anomalous cases also known outliers may spoil resulting analysis may also co...\"],[\"disease classification crucial element biomedical research recent studies demonstrated machine learn...\"],[\"tensor train decomposition provides spaceefficient representation higherorder tensors despite advant...\"],[\"quick accurate medical diagnosis crucial successful treatment disease using machine learning algorit...\"],[\"recurrent major mood episodes subsyndromal mood instability cause substantial disability patients bi...\"],[\"introduce new algorithm approximate inference combines reparametrization markov chain monte carlo va...\"],[\"consider problem estimating regression function common situation number features small interpretabil...\"],[\"knearestneighbor knn ensembles exist despite efficacy approach regression classification outlier det...\"],[\"gaussian process regression generally scale beyond thousands data points without applying sort kerne...\"],[\"consider structured matrix factorization model one factor restricted columns lying unit simplex simp...\"],[\"consider demixing problem two structured highdimensional vectors limited number nonlinear observatio...\"],[\"fundamental question data analysis machine learning signal processing compare data points choice dis...\"],[\"malaria serious infectious disease responsible half million deaths yearly worldwide major cause mort...\"],[\"bayesian models mix multiple dirichlet prior parameters called multidirichlet priors paper gaining p...\"],[\"choosing bestperforming optimizers portfolio optimization algorithms usually difficult complex task ...\"],[\"although various distributed machine learning schemes proposed recently pure linear models fully non...\"],[\"present novel approach estimating conditional probability tables based joint rather independent esti...\"],[\"prediction disease onset patient survey lifestyle data quickly becoming important tool diagnosing di...\"],[\"comment fact gradient ascent logistic regression connection perceptron learning algorithm logistic l...\"],[\"graphical lasso popular method learning structure undirected graphical model based regularization te...\"],[\"sparse alphanorm regularization many datarich applications marketing economics alphanorm contrast la...\"],[\"consistency doubly robust estimators relies consistent estimation least one two nuisance regression ...\"],[\"interpretability prediction mechanisms respect underlying prediction problem often unclear several s...\"],[\"smallball method introduced way obtaining high probability isomorphic lower bound quadratic empirica...\"],[\"two fundamental problems unsupervised learning efficient inference latentvariable models robust dens...\"],[\"stochastic gradient markov chain monte carlo sgmcmc developed flexible family scalable bayesian samp...\"],[\"solve key biomedical problems experimentalists routinely measure millions billions features dimensio...\"],[\"reliable uncertainty estimation time series prediction critical many fields including physics biolog...\"],[\"ability many powerful machine learning algorithms deal large data sets without compromise often hamp...\"],[\"propose new framework hamiltonian monte carlo hmc truncated probability distributions smooth underly...\"],[\"discovering correlation one variable another variable fundamental scientific practical interest exis...\"],[\"multiple linear regression setting propose general framework termed weighted orthogonal components r...\"],[\"assessing heterogeneous treatment effects become growing interest advancing precision medicine indiv...\"],[\"model high dimensional data gaussian methods widely used since remain tractable yield parsimonious m...\"],[\"consider problem sequentially making decisions rewarded successes failures predicted unknown relatio...\"],[\"compare two statistical models three binary random variables one mixture model product mixtures mode...\"],[\"introduce new approach functional causal modeling observational data called causal generative neural...\"],[\"automatic chemical design framework generating novel molecules optimized properties original scheme ...\"],[\"consider graphical model multivariate normal vector associated node underlying graph estimate graphi...\"],[\"principal component analysis pca popular perform dimension reduction selection number significant co...\"],[\"paper investigates theoretical foundations metric learning focused three key questions fully address...\"],[\"kernelbased learning algorithms widely used machine learning problems make use similarity object pai...\"],[\"bayesian graphical models useful tool understanding dependence relationships among many variables pa...\"],[\"stochastic principal component analysis spca become popular dimensionality reduction strategy large ...\"],[\"renewable distributed energy resources ders penetrate power grid accelerating speed essential operat...\"],[\"edge partition model epm fundamental bayesian nonparametric model extracting overlapping structure b...\"],[\"consider fundamental problem inferring causal direction two univariate numeric random variables obse...\"],[\"paper presents new approach nonparametric cluster analysis called adaptive weights clustering awc id...\"],[\"paper presents technique reducedorder markov modeling compact representation timeseries data work sy...\"],[\"propose bayesian regression method accounts multiway interactions arbitrary orders among predictor v...\"],[\"high throughput screening compounds chemicals essential part drug discovery involving thousands mill...\"],[\"consider problem reconstructing signals images periodic nonlinearities problems design measurement s...\"],[\"feature selection highdimensional data small proportion relevant features poses severe challenge sta...\"],[\"present new method forecasting systems multiple interrelated time series method learns forecast mode...\"],[\"consider problem solving largescale quadratically constrained quadratic program problems occur natur...\"],[\"many machine learning problems characterized mutual contamination models problems one observes sever...\"],[\"work constructs hypothesis test detecting whether datagenerating function rightarrow belongs specifi...\"],[\"parametric point process model developed modeling based assumption sequential observations often sha...\"],[\"deep convolutional neural networks cnns based approaches stateoftheart various computer vision tasks...\"],[\"lay theoretical foundations new database release mechanisms allow thirdparties construct consistent ...\"],[\"uncertainty analysis form probabilistic forecasting significantly improve decision making processes ...\"],[\"inferring predictive maps multiple input multiple output variables tasks innumerable applications da...\"],[\"understanding player behavior fundamental game data science video games evolve players interact game...\"],[\"emergence mobile games caused paradigm shift videogame industry game developers disposal plethora in...\"],[\"reducing user attrition churn broad challenge faced several industries mobile social games decreasin...\"],[\"propose new algorithms topic modeling number topics unknown approach relies analysis concentration m...\"],[\"identifying changes generative process sequential data known changepoint detection become increasing...\"],[\"paper propose framework automatic classification patients multimodal genetic brain imaging data opti...\"],[\"reliable measures statistical dependence could useful tools learning independent features performing...\"],[\"introduce methodology efficiently computing lower bound empowerment allowing used unsupervised cost ...\"],[\"superposition temporal point processes studied many years although usefulness models practical appli...\"],[\"modified cholesky decomposition commonly used precision matrix estimation given specified order rand...\"],[\"paper considers problem brain disease classification based connectome data connectome network repres...\"],[\"propose first fullyadaptive algorithm pure exploration linear banditsthe task find arm largest expec...\"],[\"new social economic activities massively exploit big data machine learning algorithms inference peop...\"],[\"time series forecasting widely used multitude domains paper present four models predict stock price ...\"],[\"machine learning data mining linear models widely used model response parametric linear functions pr...\"],[\"consider novel stochastic multiarmed bandit problem called good arm identification gai good arm defi...\"],[\"robustness outliers central issue realworld machine learning applications replacing model heavytaile...\"],[\"tensor decomposition methods popular tools learning latent variables given lowerorder moments data h...\"],[\"inverse covariance matrix provides considerable insight understanding statistical models multivariat...\"],[\"characteristics numerical patterns feature vector transform domain perturbation model differ signifi...\"],[\"paper problem onebit compressed sensing obcs formulated problem probably approximately correct pac l...\"],[\"propose analyze two new mcmc sampling algorithms vaidya walk john walk generating samples uniform di...\"],[\"paper present technique using bootstrap estimate operating characteristics variability certain types...\"],[\"introduce approach based givens representation posterior inference statistical models orthogonal mat...\"],[\"many matching tracking sorting ranking problems require probabilistic reasoning possible permutation...\"],[\"understanding developing correlation measure detect general dependencies imperative statistics machi...\"],[\"consider unknown smooth function rightarrow mathbbr say given noisymod samples fxi etaimod etai deno...\"],[\"work presents new classifier specifically designed fully interpretable technique determines probabil...\"],[\"study introduce new technique symbolic regression guarantees global optimality achieved formulating ...\"],[\"consider classifiers highdimensional data strongly spiked eigenvalue sse model first show highdimens...\"],[\"reduced modeling computationally demanding dynamical system aims approximating trajectories optimizi...\"],[\"paper raises implicit manifold learning perspective generative adversarial networks gans studying su...\"],[\"deep generative models provide systematic way learn nonlinear data distributions set latent variable...\"],[\"many decisions healthcare business policy domains made without support rigorous evidence due cost co...\"],[\"article carries large dimensional analysis standard regularized discriminant analysis classifiers de...\"],[\"informationtheoretic bayesian optimisation techniques demonstrated stateoftheart performance tacklin...\"],[\"modeling sequential data become important practice applications autonomous driving virtual sensors w...\"],[\"profiling cellular phenotypes microscopic imaging provide meaningful biological information resultin...\"],[\"sparse regularization ell regularization quite powerful widely used strategy high dimensional learni...\"],[\"powerful approach understanding neural population dynamics extract lowdimensional trajectories popul...\"],[\"mechanistic models singleneuron dynamics extensively studied computational neuroscience however iden...\"],[\"goal extract meaningful transformations raw images varying thickness lines handwriting lighting port...\"],[\"paper presents novel twostep approach fundamental problem learning optimal map one distribution anot...\"],[\"wellestablished methods solution stochastic partial differential equations spdes typically struggle ...\"],[\"establish consistency algorithm mondrian forests randomized classification algorithm implemented onl...\"],[\"gaussian multiplicative noise commonly used stochastic regularisation technique training determinist...\"],[\"one iteration standard kmeans lloyds algorithm standard gaussian mixture models gmms scales linearly...\"],[\"novel python framework bayesian optimization known gpflowopt introduced package based popular gpflow...\"],[\"stein variational gradient descent svgd recently proposed particlebased bayesian inference method at...\"],[\"model criticism usually carried assessing replicated data generated fitted model looks similar obser...\"],[\"network clustering reveals organization network corresponding complex system elements represented ve...\"],[\"nonparametric family conditional distributions introduced generalizes conditional exponential famili...\"],[\"given vertex interest network vertex nomination problem seeks find corresponding vertex interest exi...\"],[\"use covariance kernels ubiquitous field spatial statistics kernels allow data mapped highdimensional...\"],[\"recently crowdsourcing emerged effective paradigm humanpowered large scale problem solving various d...\"],[\"compression neural networks become highly studied topic recent years main reason demand industrial s...\"],[\"nonnegative matrix factorization nmf technique finding latent representations data method applied co...\"],[\"inpatient care large share total health care spending making analysis inpatient utilization patterns...\"],[\"technical report explores estimation methodologies hyperparameters markov random field gaussian hidd...\"],[\"algorithm one many important tools field statistics often used imputing missing data widespread appl...\"],[\"problem domain generalization labeled training data sets several related prediction problems goal ma...\"],[\"calls arms build interpretable models express wellfounded discomfort machine learning software agent...\"],[\"research interpretability machine learning systems focuses development rigorous notion interpretabil...\"],[\"classical approaches granger causality detection repose upon linear time series assumptions many int...\"],[\"paper scale mixture normal distributions model developed classification clustering data outliers mis...\"],[\"present efficient alternating direction method multipliers admm algorithm segmenting multivariate no...\"],[\"estimation optimal treatment regimes considerable interest precision medicine work propose causal kn...\"],[\"modern machine learning systems image classifiers rely heavily large scale data sets training data s...\"],[\"modern aircraft may require order thousands custom shims fill gaps structural components airframe ar...\"],[\"present causal generative neural networks cgnns learn functional causal models observational data cg...\"],[\"paper investigates asymptotic behaviors gradient descent algorithms particularly accelerated gradien...\"],[\"proceedings nips workshop machine learning developing world held long beach california usa december...\"],[\"proceedings nips symposium interpretable machine learning held long beach california usa december...\"],[\"many problem settings parameter vectors merely sparse dependent way nonzero coefficients tend cluste...\"],[\"tremendous interest precision medicine means improve patient outcomes tailoring treatment individual...\"],[\"develop model using deep learning techniques natural language processing unstructured text medical r...\"],[\"independent component analysis ica technique unsupervised exploration multichannel data widely used ...\"],[\"stochastic gradient markov chain monte carlo sgmcmc increasingly popular bayesian learning due abili...\"],[\"propose framework general probabilistic multistep time series regression specifically exploit expres...\"],[\"develop riemannian stein variational gradient descent rsvgd bayesian inference method generalizes st...\"],[\"interpretation deep learning models challenge due size complexity often opaque internal state additi...\"],[\"missing data expected issue large amounts data collected several imputation techniques proposed tack...\"],[\"propose new sampling method thermostatassisted continuouslytempered hamiltonian monte carlo bayesian...\"],[\"paper study effects different prior likelihood choices bayesian matrix factorisation focusing small ...\"],[\"identifying altered pathways associated specific cancer types potentially bring significant impact c...\"],[\"introduce new approach topic modeling supervised survival analysis specifically build recent work un...\"],[\"work analyze problem adoption mobile money pakistan using call detail records major telecom company ...\"],[\"variety machine learning taskseg matrix factorization topic modelling feature allocationcan viewed l...\"],[\"problems outliers detection robust regression highdimensional setting fundamental statistics numerou...\"],[\"bayesian optimization modelbased approach gradientfree blackbox function optimization typically powe...\"],[\"paper study general problem optimizing convex function set times matrices subject rank constraints h...\"],[\"performing inference simulators generally intractable runtime means cannot compute marginal likeliho...\"],[\"derive novel sensitivity analysis input variables predictive epistemic aleatoric uncertainty use bay...\"],[\"study problem recovering structured signal mathbfx highdimensional data mathbfyifmathbfaitmathbfx no...\"],[\"subset selection multiple linear regression aims construct regression model minimizes errors selecti...\"],[\"consider problem clustering longestleg path distance llpd metric informative elongated irregularly s...\"],[\"investigate optimization two probabilistic generative models binary latent variables using novel var...\"],[\"classification dissimilarity space become active research area since provides possibility learn data...\"],[\"point forecasting univariate time series challenging problem extensive work conducted however nonpar...\"],[\"paper provides estimation inference methods conditional average treatment effects cate characterized...\"],[\"computing partition function discrete graphical model fundamental inference challenge since computat...\"],[\"basis adaptation homogeneous chaos spaces rely suitable rotation underlying gaussian germ several ro...\"],[\"deep gaussian processes dgp hierarchical generalizations gaussian processes proven work effectively ...\"],[\"paper deals inference prediction multiple correlated time series one also choice using candidate poo...\"],[\"formulate supervised learning problem referred continuous ranking continuous realvalued label assign...\"],[\"gaussian process priors commonly used aerospace design performing bayesian optimization nonetheless ...\"],[\"gene expression data represents unique challenge predictive model building small number samples comp...\"],[\"motivations using variational inference neural networks differ significantly latent variable models ...\"],[\"paper design nonparametric online algorithm estimating triggering functions multivariate hawkes proc...\"],[\"decision trees algorithms use gain function select best split trees induction function crucial obtai...\"],[\"need reason uncertainty large complex multimodal datasets become increasingly common across modern s...\"],[\"independent component analysis ica widely used bss method uniquely achieve source recovery subject s...\"],[\"introduce new unsupervised learning problem clustering widesense stationary ergodic stochastic proce...\"],[\"common method generalizing binary multiclass classification error correcting code ecc eccs may optim...\"],[\"gradient descent optimization requires choose learning rate deeper deeper models tuning learning rat...\"],[\"large scale online inference problems update strategy critical performance derive adaptive scan gibb...\"],[\"modern vehicles equipped increasingly complex sensors sensors generate large volumes data provide op...\"],[\"marketing analytics diverse field academic researchers practitioners coming range backgrounds includ...\"],[\"vanishing ideal set polynomials takes zero value given data points originally proposed computer alge...\"],[\"receiver operating characteristic roc analysis widely used evaluating diagnostic systems recent stud...\"],[\"stochastic bandit problem goal maximize unknown function via sequence noisy evaluations typically ob...\"],[\"fundamental task general density estimation keen interest machine learning work attempt systematical...\"],[\"statespace models ssms highly expressive model class learning patterns time series data system ident...\"],[\"causal inference using observational data challenging especially bivariate case minimum description ...\"],[\"gaussian process models provide powerful tool prediction computationally prohibitive using large dat...\"],[\"study logistic modelbased active learning procedure binary classification problems adopt batch subje...\"],[\"topic models bayesian models frequently used capture latent structure certain corpora documents imag...\"],[\"melanoma deadliest form skin cancer computer systems assist melanoma detection widespread clinical p...\"],[\"paper taskrelated fmri problem treated matrix factorization formulation focused dictionary learning ...\"],[\"correlated component analysis proposed dmochowski tool investigating brain process similarity respon...\"],[\"multitude methods perform multiset correlated component analysis mcca including require iterative so...\"],[\"consider problem model selection gaussian markov fields sample deficient scenario benchmark informat...\"],[\"learning using privileged information attractive problem setting helps many learning scenarios real ...\"],[\"study safe screening metric learning distance metric learning optimize metric set triplets one defin...\"],[\"theoretically discuss deep neural networks dnns performs better models cases investigating statistic...\"],[\"simple framework probabilistic multiview graph embedding pmvge proposed multiview feature learning m...\"],[\"consider multitask learning simultaneously learns related prediction tasks improve generalization pe...\"],[\"consider learning multiagent hawkes processes model containing multiple hawkes processes shared endo...\"],[\"provide comprehensive overview tooling modeling nongaussian likelihoods using state space methods st...\"],[\"modeling complex conditional distributions critical variety settings despite long tradition research...\"],[\"suppose one particular block stochastic block model interest block labels observed vertices network ...\"],[\"modeling variability tensor decomposition methods one challenges source separation one possible solu...\"],[\"paper presents novel approach direct covariance function learning bayesian optimisation particular e...\"],[\"scaling bayesian optimization high dimensions challenging task global optimization highdimensional a...\"],[\"paper propose new algorithm streaming principal component analysis limited memory small devices cann...\"],[\"independent component analysis ica one basic tools data analysis aims find coordinate system compone...\"],[\"study optimal covariate balance causal inferences observational data rich covariates complex relatio...\"],[\"introduce novel generative formulation deep probabilistic models implementing soft constraints funct...\"],[\"classical approaches granger causality detection assume linear dynamics many interactions realworld ...\"],[\"measuring divergence two distributions essential machine learning statistics various applications in...\"],[\"many popular dimensionality reduction procedures outofsample extensions allow practitioner apply lea...\"],[\"show training deep network using batch normalization equivalent approximate inference bayesian model...\"],[\"study problem recovery matrices simultaneously low rank row andor column sparse matrices appear rece...\"],[\"paper first work propose network predict structured uncertainty distribution synthesized image previ...\"],[\"paper considers generation prediction intervals pis neural networks quantifying uncertainty regressi...\"],[\"multioutput regression models must exploit dependencies outputs maximise predictive performance appl...\"],[\"convex sparsityinducing regularizations ubiquitous highdimensional machine learning solving resultin...\"],[\"present novel model architecture leverages deep learning tools perform exact bayesian inference sets...\"],[\"consider problem learning fair decision systems complex scenarios sensitive attribute might affect d...\"],[\"distributional approaches valuebased reinforcement learning model entire distribution returns rather...\"],[\"present causal gaussian process convolution model cgpcm doubly nonparametric model causal spectrally...\"],[\"propose novel approach parameter estimation simulatorbased statistical models intractable likelihood...\"],[\"determinantal point processes dpps enable modeling repulsion provide diverse sets points repulsion e...\"],[\"predictive models generalize well distributional shift often desirable sometimes crucial building ro...\"],[\"multiresolution gaussian process gained increasing attention viable approach towards improving quali...\"],[\"modern supervised machine learning algorithms involve hyperparameters set running options setting hy...\"],[\"latent variable models hidden binary units appear various applications learning models particular pr...\"],[\"full length article draft version problem number topics topic modeling discussed proposed idea renyi...\"],[\"optimal transport theory informally described using words french mathematician gaspard monge worker ...\"],[\"paper presents novel formulation solution orbit determination finite time horizons learning problem ...\"],[\"develop theory nonlinear dimensionality reduction nldr number nldr methods developed limited underst...\"],[\"propose practical extensions bayesian optimization solving dynamic problems model dynamic objective ...\"],[\"consider unknown smooth function rightarrow mathbbr say given noisy mod samples fxi etaimod etai den...\"],[\"paper presents approximate confidence intervals function parameters banach space based bootstrap alg...\"],[\"ordinary stochastic neural networks mostly rely expected values weights make predictions whereas ind...\"],[\"provide sharp empirical estimates expectation variance normal approximation class statistics whose v...\"],[\"present first framework gaussianprocessmodulated poisson processes temporal data appear form panel c...\"],[\"conventional ode modelling coefficients equation driving system state forward time estimated however...\"],[\"propose simulation method multidimensional hawkes processes based superposition theory point process...\"],[\"paper propose tackle problem reducing discrepancies multiple domains referred multisource domain ada...\"],[\"new causal discovery method structural agnostic modeling sam presented paper leveraging conditional ...\"],[\"zeroinflated datasets excess zero outputs commonly encountered problems climate rare event modelling...\"],[\"probabilistic graphical models key tool machine learning applications computing partition function n...\"],[\"uplift modeling aimed estimating incremental impact action individuals behavior useful various appli...\"],[\"recent efforts combining deep models probabilistic graphical models promising providing flexible mod...\"],[\"lactate threshold considered essential parameter assessing performance elite recreational runners pr...\"],[\"propose novel class network models temporal dyadic interaction data goal capture number important fe...\"],[\"framework supervised learning real function defined space called kriging method stands real gaussian...\"],[\"bayesian nonnegative matrix factorization nmf promising approach understanding uncertainty structure...\"],[\"paper investigate capability universal kriging model singleobjective global optimization applied wit...\"],[\"data science determining proximity observations critical many downstream analyses clustering informa...\"],[\"spectral features empirical moment matrix constitute resourceful tool unveiling properties cloud poi...\"],[\"neuroscientists enjoyed much success understanding brain functions constructing brain connectivity n...\"],[\"paper presents simulation free framework solving reliability analysis problems method proposed roote...\"],[\"selection validation basis full dataset often required industrial use supervised machine learning al...\"],[\"connection bayesian neural networks gaussian processes gained lot attention last years flagship resu...\"],[\"vector quantizedvariational autoencoders vqvae generative models based discrete latent representatio...\"],[\"mapping nearfield pollutant concentration essential track accidental toxic plume dispersion urban ar...\"],[\"predicting individual risk clinical event using complete patient history still major challenge perso...\"],[\"present application conformal prediction form uncertainty quantification guarantees detection railwa...\"],[\"paper introduce conformal prediction method construct prediction sets oneshot federated learning set...\"],[\"semisupervised learning powerful technique leveraging unlabeled data improve machine learning models...\"],[\"tensor data multidimension arrays lowrank decompositionbased regression methods tensor predictors ex...\"],[\"paper introduces general framework iterative optimization algorithms establishes general assumptions...\"],[\"context large samples small number individuals might spoil basic statistical indicators like mean di...\"],[\"consider problem minimizing convex function closed convex set projected gradient descent pgd propose...\"],[\"past decade techniques topological data analysis tda grown prominence describe shape data recent yea...\"],[\"discrete latent space models recently achieved performance par continuous counterparts deep variatio...\"],[\"classification problems datasets usually imbalanced noisy complex sampling algorithms make improveme...\"],[\"sequential neural architectures become deeper complex uncertainty estimation challenging efforts qua...\"],[\"arrival digital platforms revolutionized occupational health giving possibility occupational health ...\"],[\"causal random forests provide efficient estimates heterogeneous treatment effects however forest alg...\"],[\"inverse problems arise anywhere indirect measurement general illposed obtain satisfactory solutions ...\"],[\"brain segmentation neonatal mri images challenging task due large changes shape cerebral structures ...\"],[\"mapping forest resources carbon important improving forest management meeting objectives storing car...\"],[\"optimal transport based distances powerful tools machine learning compare probability measures manip...\"]],\"hovertemplate\":\"x1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\",\"line\":{\"color\":\"white\",\"width\":0.5}},\"mode\":\"markers\",\"name\":\"\",\"showlegend\":false,\"x\":[-0.6570604,-1.5186067,-3.1456141,-3.2035108,-4.5465264,-2.27789,-0.37560064,-3.6229222,-4.468881,-2.7423446,-2.8424616,-0.39746752,-1.913516,-1.7506086,-4.5472064,-2.1438022,-4.5524273,0.51646495,-1.4145582,-2.2452781,-2.231826,-1.8598846,-0.7178993,-2.253539,-3.31594,-2.9342616,-3.7018342,-1.2575328,-4.2679462,-1.4815822,-2.50474,0.44379416,-0.42922145,-3.1454298,-1.843473,-2.2459722,-2.4850407,-2.7652347,-1.4336843,-0.6097802,-1.8650696,1.6730087,-1.0335921,-2.574306,-1.320524,-0.41019621,-0.42705962,-0.72540843,-4.047861,-4.8509445,-2.0634894,-2.0082052,-1.817901,-3.3259501,-1.3497646,-1.356004,-3.4549992,-2.2802904,-2.1887825,-0.30175686,1.7077923,-3.267476,-4.3690596,-1.4982964,-4.892758,-1.8163476,-0.5596965,1.6974531,1.6901164,0.21168412,-3.2396302,-2.044988,-3.25035,-0.6051509,0.4195511,-3.2620735,-1.4489741,-2.7639647,-4.848768,-1.8561294,-1.3066537,0.25382748,-1.3454894,-4.8305655,-3.5088158,-0.89441514,-2.2736106,-2.3253303,-3.2809505,-1.9472476,-1.4350325,-1.4731208,-4.739567,-1.9083558,-2.4834998,-3.1462982,-3.4012756,-2.8608527,-1.0964985,-1.7108372,-1.1748328,-1.5745463,-0.3699215,-4.6823997,-0.853024,-0.3281795,-1.4718589,-0.49780264,-0.14869796,-2.8302996,-1.8937366,-0.617106,-1.6017945,-1.7278247,-0.51784766,-0.8109651,-2.446809,-1.2657253,-0.49521765,-0.4519379,-1.2803552,-1.392633,-2.0921075,0.44477352,-4.965491,-1.9336882,-0.59823626,-2.6170413,-2.6321115,-3.003081,1.7422812,-2.9692078,-0.39892435,-0.044589765,-2.9020936,-2.191423,-0.8261026,-1.7774644,-2.0746005,-3.5028315,-0.42725563,-4.6498404,0.09651743,-0.8303255,-1.7200811,1.6938928,-2.2193966,-1.0918784,-4.0227017,1.6832055,0.96161115,-0.9490922,-0.23348095,-0.1539833,-3.175972,-2.1083004,-4.502121,-0.44473228,1.7590528,-1.3464279,-4.003938,-2.0410166,-0.46015668,-1.0719587,-4.5181046,-1.5282207,-3.177786,-1.5829837,-2.602396,-2.9489932,-0.39116734,-0.5659205,-0.4790767,-0.5642318,-0.011837899,-3.1485567,-4.001474,0.38944635,-1.6189708,-1.5332891,-1.29657,-3.313183,-0.44314748,-2.2853794,-3.0425851,-1.1978313,-1.9560499,-1.7387886,-1.4466333,-1.4989558,-3.4053137,-3.1928988,-3.2699533,-2.976243,-4.2957177,-3.6956663,-3.1740615,-0.45336333,-3.1788564,-0.4417019,-3.232972,-3.2032883,-2.5277135,-1.8331395,0.058173947,-2.9687026,-3.190394,-1.3842257,-1.8904458,-1.6657912,-1.0900395,-0.3917391,1.7117504,-1.5803996,-1.8617164,0.34214768,-1.7720388,-3.6431751,-3.792328,-2.4390683,-0.7869349,-3.766566,-1.1451545,-3.4124413,-4.3921227,-3.7605255,1.0164591,-0.37596092,1.6519492,-4.851157,-2.1763678,-2.686362,-1.4398987,-2.578905,-0.5245872,-3.7461026,-5.0092645,0.9541085,-0.84975564,-3.0794926,-4.6419353,-0.6154342,-1.9062248,-2.5614207,-4.92403,0.17093666,-2.6843863,-0.10440927,-2.5799448,-4.4453034,-4.404427,-2.3825455,-2.5449202,-1.0692348,-3.7785678,-1.0633777,-0.010749481,-0.050655544,-4.619837,-3.1601317,-0.9698395,-1.6240088,-3.144834,-3.7675135,-4.836074,-0.09526228,-1.8197592,-0.66412824,-1.1742797,-2.756955,-3.0598352,-0.40520757,-3.1797934,-2.4355795,-3.4900453,-0.29779375,-0.7119797,-1.5382464,-4.5567994,-3.2296658,-3.6165698,1.727449,-0.6118934,-0.37708962,-1.9361099,-2.4320536,-2.221149,0.07558126,0.015409523,-0.7346298,-3.1347754,-1.6117102,-3.8836586,-4.9170284,-2.7306492,0.020673929,-0.037997976,-2.2943087,0.48071918,-2.8280694,-2.6649208,1.702107,-4.3068385,-1.5569993,-0.91643924,-1.4634997,-1.4971234,-2.7029212,-0.99666053,-0.15760136,-3.0573063,-3.5388947,0.08031709,-2.8029714,-3.0636506,-3.194493,-0.9559551,-1.4013652,-0.44296348,-0.951918,-3.71552,-1.0796269,0.6892371,-0.2406963,-1.0116357,-1.3159606,-3.1796865,0.657897,-3.8524413,-0.6716992,-2.172152,1.7443285,-0.70024765,-3.2204025,-2.970507,-2.157212,0.07495095,-4.640256,-1.1769314,-1.0384207,-0.36768365,-1.0190572,-2.6593974,0.95858765,-5.0281954,-1.9863021,-1.9710952,-0.5678302,0.8845603,-3.6721563,1.7118565,-3.1506634,-2.3007662,-2.642482,0.8505658,-3.5048263,-1.4375767,-2.1345263,-2.7212536,-2.7938783,-1.8533813,-1.5352124,-2.314319,-1.9955832,-2.791284,-0.2510453,1.7397912,-0.19829527,-3.1346276,-1.4432315,-4.7930584,-2.711194,-2.5986233,-3.9002166,-2.017372,-1.6433982,-0.76713574,-1.6620679,-4.294873,-2.3709083,-3.2931056,-3.3026223,-2.8954833,-0.6098339,-1.405359,-0.7006311,-3.8601468,-3.6161788,-2.4063523,-2.8649435,-2.045274,-2.8209746,-2.2412658,-2.877763,1.7029724,-5.0171113,-0.44893616,-1.3960925,-4.398933,-3.6670156,-1.7361822,-2.2478251,-3.722623,-1.5427498,1.6376083,-0.8250334,-2.137068,-0.18120764,-4.5835123,-2.2214851,-0.8585605,-4.646098,-0.5102036,-0.7046419,-0.6210734,-0.562969,-0.8080239,-0.46923593,-2.4678485,-2.1900046,-1.9048666,-1.844632,-2.9510741,-1.2274286,-0.45455924,0.26312694,-3.1754675,-3.7112954,-3.8682973,-1.7907821,-2.061399,-1.0500782,-2.0539882,-2.0840707,-1.1597736,-4.4184704,1.6625074,-0.95262545,-1.2004771,-3.31177,-3.249869,-2.0310206,-0.2665633,-0.7773596,-1.8967184,-2.0663652,-1.5743735,-1.2668315,0.57735586,-1.5120819,-0.40284237,-1.9877077,-3.2418153,-3.227597,-3.292438,-1.6231241,-2.6827621,-2.219508,-0.8133126,0.86901945,-3.1307065,-3.9791985,-0.47259548,-0.5481883,-2.2632847,0.20160834,-4.192479,-1.1066357,-1.4672353,-1.4298096,-4.072594,-1.5191137,-2.132468,-3.1630266,0.22549365,-4.6337924,-2.0182776,-4.5138535,-1.4163969,-3.6825202,-2.4367008,1.6505511,1.7033259,-5.0091095,0.7629003,-0.6638328,0.16091803,-3.6625395,-0.9739248,-1.8452597,-1.4259751,-1.8346161,-0.46206892,-3.7609582,-3.5406055,-2.3877952,-4.0543017,-0.24366222,0.1820924,0.21044135,-3.6247678,0.81319046,0.3821629,-0.36846727,-0.40298772,-2.3994331,-0.95747423,-4.172413,-4.5242386,-3.8631852,-4.3516545,1.8103795,-5.004359,-4.276453,-3.8297675,-3.745818,-2.1107564,1.6563958,-2.5264833,-3.3362093,0.8725567,0.54271954,-2.721368,-0.42361084,-4.4655213,-5.1362348,-1.5999827,-2.9933803,-2.0721514,0.033625968,-5.38496,-1.3377221,-1.1435536,-0.47140726,-1.8638873,-1.3669302,-1.5894507,-1.5137957,-1.7439045,-0.8671184,-2.479923,-0.4268808,0.4856127,-1.9125749,-0.31820595,-0.3803746,-4.584308,-3.3984187,-1.4547814,-2.2162743,-0.61653084,-2.1179347,-0.4379876,-2.0004585,-3.1811314,-2.1848936,-1.1556414,-0.01014379,-3.485234,-3.3574438,0.9817193,-1.4204086,-1.8945076,-3.9468231,-2.7631838,-3.190295,-0.34237152,-0.9636413,-1.2715232,-0.960306,-3.669648,-3.7156088,-4.5715895,-3.9289315,-0.768926,-0.694299,-4.20611,-0.96772027,-0.7781052,-1.1771368,-5.0630236,-3.575995,-3.4987214,-1.7853962,-2.1532838,-0.5084294,0.6937842,-2.1526327,-3.4736736,-1.0020245,-3.9101741,-2.146248,-2.8104792,-3.7512057,-2.2927334,0.97610754,0.03354705,-1.8821738,-2.4192102,-2.3909256,-3.882476,-4.2007027,-4.579681,-1.0965077,-3.0800002,-3.7211456,-4.7807913,-3.1346407,1.1262472,-0.85277563,-2.9786656,-0.8512804,-2.5235362,-1.906853,-5.120061,-5.267962,-3.7803254,-4.2834444,-0.52013993,-4.640523,-4.686613,-2.4127812,-3.2938578,-0.4228859,-0.6876769,-4.7454576,-3.828849,-2.5823605,-4.541907,-4.679464,-5.098538,-2.3514128,-1.0276783,-0.9135548,-3.6733825,-4.863253,-2.678738,-3.047069,-0.59453255,-2.1847382,0.24328884,-4.5905724,-0.13733886,-3.313594,-2.6911166,-4.6246724,-0.41686964,-0.45723674,-1.9890003,-4.080101,-3.6629972,-3.059298,-3.769988,-3.3069983,-2.2652447,-0.34864464,-2.5536323,-0.023332499,-0.5950173,-2.119385,-1.1076729,-0.37386477,-5.0267186,-4.03639,-5.0123277,-3.8745966,-0.84304285,-4.466633,-1.7646391,-4.4995413,-1.9197454,-2.0205758,-4.204885,-1.5161388,-3.3117416,-3.8661478,-3.3642802,-3.1273143,-1.787254,-3.485902,-3.2733192,1.7540543,-1.4930848,-4.714961,-1.1943736,-3.6457312,-3.6759117,-3.7017086,-1.2848067,-2.0687935,-5.018129,-0.64861846,-1.6101717,-2.6822069,-1.5373726,-3.813869,-4.998894,-0.44389167,-2.9743612,-3.184772,0.3616613,-1.8572977,-4.126127,-4.613095,-2.9096384,-2.7785008,-4.5282636,-4.007346,-2.6378188,-2.967152,-3.7622483,-3.151324,-1.5434785,-3.2773888,-1.5293071,-1.1079881,-2.8360066,-4.7095866,-4.5000815,-3.0551538,-3.216796,-1.4864695,-3.3048851,-3.425945,-3.0503445,-4.0375605,-0.44722998,-3.8067355,-3.1690185,-2.514449,-1.8923415,-0.94518507,-1.4589479,-0.50605416,-1.1163738,-0.8453384,1.671506,-5.082569,-3.9418962,-4.47589,-1.6082618,-4.564432,-0.5567782,-3.272894,-3.585753,-3.6088223,1.0317276,-3.306656,-1.3178514,-4.671588,-3.9898815,-3.480131,-4.678787,-3.4514928,-3.47525,-0.7314971,-3.6450546,0.2912725,-3.338172,-4.9019365,-4.9128723,1.0160652,-4.5202723,-4.8274455,-4.822992,-0.4350513,-2.7908049,-4.724087,-4.7670846,1.6594498,0.883023,-0.62591636,-4.5873065,-3.187296,-4.844451,-4.6772814,-4.291004,-1.7493237,0.96237284,-0.44362748,-2.0178604,0.46406657,-1.9442061,-3.1946678,-1.0472454,0.26519006,-0.96720326,-0.09665397,-2.2390888,-2.8167117,-4.8301845,-3.3056169,-1.0686764,-3.3522937,-0.7529918,-2.3866386,-3.5949242,-2.2725585,-4.6916203,-1.1939298,-2.754627,-0.8256898,-0.8478418,-3.5512638,-4.7267756,-2.1150045,-0.9160311,-4.8500285,-1.8532178,-4.5973725,-4.8467965,-3.3362277,-4.1478724,-3.5016794,-2.1750445,0.08949758,-5.132272,-4.9176435,-2.3220563,-5.194834,-1.1819501,-1.9538618,-0.57434505,-2.24289,-3.3378017,-3.106751,-4.710059,-2.952407,-5.1844077,-4.2528634,-1.3080355,0.037344296,-1.3934047,-3.21343,-1.0368915,-2.3387764,-0.7389546,-5.2417393,-2.0786126,-3.0060627,-2.5269022,-2.3423383,-0.8337203,-4.6937633,-0.49732104,-4.462402,-0.76380086,-1.6489946,-0.71893454,-3.271831,-4.3738403,-1.4030954,-2.944211,-3.2734873,-0.6434444,-2.1655397,-3.025966,-1.4887646,-3.325029,0.9762178,-3.3035388,-0.22086641,-1.8224169,0.055605147,-3.345442,-0.6723621,-2.0069342,-2.5037324,0.38111955,1.05131,-2.287089,-3.0959458,-2.9078217,-1.510724,0.017798211,-5.2687573,-1.9875699,-2.11041,-0.6583971,-1.9862812,-0.34595865,-3.4084833,-0.4982907,-4.1331825,-0.38826767,-5.0332046,0.8302833,0.77444345,0.4920255,-1.960377,-1.0049334,-1.8759931,-1.0401753,-2.9304345,-4.3685894,-3.0444202,-3.450907,-3.8263054,-2.7278943,-1.9700859,-3.6253996,0.9472132,-0.92891794,-0.42300588,-0.4344552,-2.9636884,0.95140326,-0.41988137,-1.5787073,-1.8601272,-3.9869268,-4.0227275,-4.615687,-4.717145,-5.117319,-0.125329,-2.2301764,-1.4425603,-0.41284174,-3.0101588,0.10179719,-4.1381335,-2.6438723,-2.598003,-2.9464304,-3.6435237,-1.0250583,-0.4907344,-4.7741776,1.5286796,1.3132728,-3.7787902,-2.7385705,-3.2748961,-3.092816,-3.524066,-2.135756,-2.171443,-1.53526,0.71447736,0.14870234,-2.846999,-2.2118979,-4.361565,-0.5787897,0.36623213,-2.8197186,-1.4330442,-4.9204235,0.06023435,-0.5146677,-3.3899047,-2.8046005,-3.7539425,0.16509093,-3.3315544,-2.9655778,-2.8462918,-4.4816985,-3.8251529,-1.8915745,-0.81439424,-3.7270253,-3.6371171,0.85270256,-4.296583,-4.7519946,-1.3700243,0.5817907,-1.0785897,-2.2763648,-1.8873211,-5.032144,-2.6140983,0.28967595,-2.2632601,-3.3852377,-0.60394615,-2.5537362,-5.2047067,-2.0873513,-5.4118147,-5.0517244,0.88361067,-4.633618,0.46247393,-2.8947291,-1.2285984,-0.8677555,-3.726896,-1.9626892,-0.8041384,-3.3516836,-2.2828372,-3.3706272,-1.2956127,0.25508463,-1.8720716,-4.7162595,-0.36629036,-3.6808827,0.249886,-3.0471957,-4.2946653,0.10223172,-3.7228491,-3.3010466,-2.1519141,-3.6222842,-3.271979,-1.5085058,-3.6512756,-0.37053776,-4.6557198,-4.861895,0.23676997,-3.615201,0.09253106,0.65479314,0.5435022,1.7129822,-1.9922713,-4.7489977,-1.0061382,-0.11612301,-0.0051930463,-3.138336,-1.6256753,-3.548184,0.48397812,-1.7587502,-2.2805703,0.45316124,-3.8357491,-0.66784483,-0.45039806,-3.4176633,-0.3536011,-2.9650028,-3.2576284,-4.5591736,0.43067774,-0.3712067,-3.7967784,0.5502301,-1.146481,-5.1668477,-1.6809103,-4.244778,-1.0422505,-4.868392,-0.5614945,-0.75900686,-4.8342857,-5.0775137,-3.1739388,-2.6372535,1.7303814,-1.1106535,-2.336252,-3.7108037,-2.3014317,-1.8151115,-0.0803722,0.4557609,-0.7782323,-3.6999073,1.0821487,-2.2035131,-2.7519407,-1.1831481,-1.9055741,-0.6324212,-0.67001593,-1.2528958,-3.817254,-0.37688565,0.39765292,-1.2205524,-3.3615613,-0.7379032,-2.079005,-3.7357066,-1.0914998,-0.13095436,-1.8654969,-0.92445314,-3.0909417,-3.9593318,-3.223461,-2.9100738,-3.2658918,-3.583705,-0.36938608,-3.5427575,0.51604676,-3.437201,-1.1113774,-3.6685143,-3.0410047,-1.1796168,-4.645184,-1.7216336,-1.4918232,-0.8829937,-0.5916563,-0.9008841,-2.8263178,-1.850578,-5.0169225,-0.9845758,0.20018137,-2.4723082,-1.5212265,0.7866099,-3.7030876,-2.6277616,-1.5638981,-3.4154058,-5.2291155,-3.6971104,-4.8805566,-2.9705417,-3.0596843,-3.5244243,-0.267722,-5.2928123,-4.3768396,-3.3885155,-1.6784405,-2.3223498,-3.6079297,-4.4844265,-3.0718124,1.0194045,-3.6482322,-3.2788465,-3.2002327,-4.1578226,-3.3567882,-1.2505399,-1.5990658,-3.0610003,-3.3474586,-3.3326623,-1.6607352,-2.1090617,-3.5178137,-2.0820107,-2.8570032,-3.4200892,0.31531003,-0.78534335,-1.9667016,-3.1830826,-3.2182384,-1.504982,0.3481981,0.7168645,-0.85509163,-3.0904162,-1.3089843,-1.0411397,-3.2164502,-3.1875005,0.8120334,-1.0497836,0.028137106,-3.7122982,-0.27840257,-3.6716263,1.7202585,-3.228724,0.5243063,-0.50343955,-0.3340033,-0.4543311,-0.39897552,-0.22319944,-2.1009674,0.44551104,-5.237878,-3.55908,0.61671793,-0.79711974,-3.5313804,-0.65916723,-2.5362294,-4.9042687,-5.141709,-1.593273,-4.065156,-3.0999336,-4.9321012,-0.99555135,-2.7550058,-3.6529043,0.19871223,-0.81566644,-4.8784056,-0.38621053,-2.7244964,-3.6331186,-2.2706645,-1.6316937,-1.9747387,-1.5397576,-2.1029012,0.3624843,-0.43418926,-0.3098737,-4.9169025,-2.2068844,-2.9473937,-3.1402435,-0.43828502,-1.3032014,-4.025748,-3.8523436,-3.2676175,-3.8027012,-1.5429935,-2.5436172,-4.368417,-1.2065701,-4.730967,-3.1442652,-1.2996323,-2.6277633,-0.8893264,0.21278863,-3.352255,-3.1960812,-5.382616,0.1549529,-1.3444498,-5.4267516,-4.583617,-4.4795775,0.87967175,-2.707142,-2.1927888,-3.1816165,-4.0813923,-3.4067104,-4.606694,-5.4484415,-3.6655712,-4.645189,-5.0453506,1.4571632,-4.6009727,-3.7427104,-2.0668597,-3.8648055,-2.6744874,-2.1855404,-4.741699,-2.6967328,-0.79246795,-5.420282,-3.8267372,-1.3043703,-1.690995,-1.6875831,-1.2995661,-3.115681,0.3330466,-4.1863594,-3.7324526,-3.5065224,-4.7099814,-2.750895,-1.5749214,-0.43070737,-3.97484,-0.5517716,-3.0803335,-0.30647442,-3.804571,0.41353357,-5.3679056,-1.7806497,-2.2421637,-4.461657,-3.0883605,-1.5859487,1.0692271,-0.44201994,-4.45473,-1.7240181,-4.7792172,-0.90917176,0.50010145,-0.012599041,-3.1053867,-2.846414,-1.3862618,-4.4893627,-2.6371005,-0.48612446,-2.4043505,-1.0313302,-3.136612,-0.72912586,-3.6726437,0.9196687,-3.0997708,-2.114535,-1.7593955,-4.0615425,-3.850656,-3.5374982,-5.1884227,-2.214139,0.4619303,0.3182152,-3.6690066,-0.3678241,-3.5896938,-3.7399569,-0.7948734,-1.7594116,-2.0416052,0.14319775,-0.23650356,0.26389366,0.54446393,-5.1619763,-1.081701,-0.42386895,-3.3805265,-0.46894935,-0.6054285,-2.658997,0.71795255,-4.4506907,-3.524572,-1.2715396,-2.176533,0.17650479,-1.6345835,-0.5598543,-0.64476347,-1.313976,1.4962682,-2.1759093,-5.40014,-4.714113,0.6103441,-4.328811,-1.0742224,-4.797403,0.06139763,-0.9494203,0.41954467,0.046130214,-3.0529892,-3.9043903,1.6026502,-5.313054,-1.9591726,-3.7343614,-1.4925262,-1.5720903,-2.2316916,-1.2113941,-3.021354,-4.123317,1.6701336,-3.4262645,-3.1560037,-4.0137897,0.16311787,-0.442083,-0.51696104,-3.0588982,-1.8985509,-1.8284057,-2.0563297,-3.748896,-4.528135,-1.9260222,-3.1107965,-0.41693527,-1.0539194,-0.9639398,-0.83492744,-4.8404717,-2.9490402,0.88420564,0.26005822,-3.5688334,-4.0842876,-0.74981034,1.0059426,-3.1811442,-0.9074158,-3.0731494,-1.0089213,-3.1906273,-4.8577523,-0.2629284,-1.1201352,-0.80682576,-0.5599073,-4.4653974,-0.4956829,-2.674831,-2.7441535,0.039403915,-1.8946006,-0.5285686,-3.3955708,-0.9628816,-3.6783843,-5.440878,-5.3801765,1.3277476,-1.1593733,-3.6127274,-4.0466447,0.60234225,-0.58890975,1.1071497,1.2017761,-2.8785183,-4.72095,-4.3694944,-0.57883364,-4.6609707,-3.4321396,-3.7779949,-5.037217,-3.6741838,-2.8032575,-3.0160725,-2.588869,-3.0376627,-1.5224653,-4.700896,-4.793885,0.33685857,-4.305723,-3.3920317,-1.7011948,-0.35992277,-0.33312905,1.6502935,-3.6886656,-1.346842,0.86685246,-1.578009,-2.4471807,1.7136192,-2.3424811,-0.41631237,-0.37600458,0.77251583,0.5056951,0.34426236,0.22132848,-4.871745,-3.1664045,-5.0217285,-4.5602117,-0.35190392,-4.7915893,-4.00138,0.4428626,-4.8819175,-0.84445226,-2.8292,-1.7680329,-3.751347,-0.83811533,-4.0225687,-4.434399,-0.56739706,-0.9597898,-3.1861978,-5.1944847,-1.3174944,-3.1015136,-1.0117639,-5.2588787,-2.8064044,-4.082393,-3.038682,-1.7160354,-3.5229626,0.1487275,-5.217553,-4.125158,-0.43048522,-4.658052,0.27942744,-3.3651123,-1.2958925,-1.8993558,-4.707337,-4.7193756,-0.82370657,-1.2738378,-1.1304592,-3.191859,-2.9235654,-3.94517,1.6691602,-3.5885727,-1.517918,-4.8479166,0.70162004,0.9129256,0.87350863,0.1275869,-2.3777692,-3.4815774,-1.3456473,-4.64602,-2.2324507,-0.7800031,-4.1372905,-4.1157947,-3.025468,-3.007885,0.021885956,-3.6786451,-3.683445,0.24470413,0.2367932,1.5487859,-5.0914297,1.4757179,-5.397319,-2.7846935,-4.6972213,-0.738098,-2.8471324,-4.424373,-3.8067741,-1.2235893,-4.8649874,1.3070706,-3.122148,-3.4841104,-3.9367256,-1.6042998,-1.7931596,-3.8744173,-1.9915563,-1.4270141,-4.857499,-2.7023606,-3.4426427,-2.2525144,-3.688236,-1.8769423,-2.172566,-4.6301126,-5.0337977,-3.9991615,-3.7013402,-4.1558957,-1.801227,1.7482445,-3.431801,-5.283809,-0.32564712,-5.231437,0.23280354,-2.515318,-3.1488564,-4.5914226,-3.209887,-1.2814597,-1.4563504,1.0522017,-4.5849366,-1.7595611,-4.540215,-5.327433,-3.0457075,0.29380298,-2.5029538,-2.0178792,-1.7858448,-0.12010407,-1.7530745,-1.6820567,-1.948268,-0.6537912,-5.3094444,-1.1352518,-4.4313397,-0.6728833,-0.28631628,-4.5649753,0.96324176,-0.56967556,-2.340494],\"xaxis\":\"x\",\"y\":[4.7790065,3.9584322,6.410346,6.571202,5.217755,4.325052,3.8113635,2.5990431,6.1627216,3.3287795,3.7099059,2.9050186,3.439399,5.795802,6.4310994,4.2170734,3.784682,4.2514253,6.0724535,6.155852,6.099156,3.882961,5.3861375,6.1263313,6.933715,7.9765706,5.2445326,3.1562514,5.352194,5.9721017,4.4812827,4.227561,2.820288,7.9602985,3.2708445,5.533698,8.077706,3.392615,5.5383945,2.9883997,7.4309473,2.011017,4.784302,8.001013,3.160439,2.860929,2.8550944,3.985563,6.3551874,6.365045,7.994954,3.5833774,3.2249935,6.929758,3.7790215,6.425888,2.6061604,7.668131,4.4046416,5.6335874,1.9791759,6.792601,5.9239144,3.522115,5.0937266,3.9894133,4.9881406,1.9998411,1.9952638,5.922965,4.089605,4.15033,5.1083045,5.521571,4.484156,4.1752768,3.5166368,2.7626135,3.91236,7.384709,6.259471,5.7947345,3.4204948,6.348945,2.4484615,5.0178046,5.822498,6.10382,6.872608,7.0335684,3.5667608,6.09414,3.5630627,7.572698,6.0073376,7.8851957,7.0016227,5.4868336,5.2889867,3.9694755,3.5227,3.9656448,3.349528,3.4890065,4.700833,5.7935734,3.5363684,5.927297,5.6336217,4.365752,4.2709684,5.224401,3.1773205,4.3128276,5.085552,6.186572,4.5485487,3.9065359,5.22158,3.7077703,3.9379609,3.932599,5.418204,4.24445,5.1367292,7.779878,4.5549636,4.4967737,4.4696646,3.5562313,1.9692141,7.956482,6.305879,4.422631,3.834871,3.2492135,5.118199,4.511357,4.445591,3.9519541,5.125546,6.1358395,6.00777,5.1693993,4.5397053,2.005433,6.0855184,4.7771688,4.126493,1.9665153,5.2787538,5.392875,5.9234905,3.9576373,2.0650368,3.5848186,4.979817,5.1391935,2.0514722,6.459284,6.4736857,4.3386455,6.5884643,5.819627,5.881026,6.386157,2.0627925,3.016317,4.106552,8.111632,5.6346498,5.533127,5.538063,2.8620162,4.2799187,4.930992,4.599362,4.921901,4.152894,3.9778624,6.1533628,4.1679087,4.9173784,6.12905,2.2720315,5.035981,3.595426,4.5693603,5.1502957,3.4552772,4.062762,6.781454,4.3071327,6.7575254,4.541516,4.7088127,7.930139,2.8150368,2.067172,5.6061387,6.8734813,6.816629,7.95993,7.4113307,6.493053,3.8170125,6.879123,5.2759786,4.270981,4.565982,3.6850412,5.6635313,1.9764657,3.9226217,4.014289,4.617593,3.365892,5.4396033,6.321623,7.9363103,6.314533,5.7791076,6.3875823,6.6028585,5.3718033,4.4906464,5.230591,5.8994646,2.098296,6.3298945,6.1610274,8.000705,3.2191663,8.244081,5.920588,3.964588,5.1444836,5.141682,6.3729234,8.040609,4.852147,5.152883,7.8999233,7.933619,3.887568,4.348939,4.709834,5.6807923,7.982851,4.746954,5.9714613,3.6839125,7.9616756,4.452613,6.221025,3.5662875,4.176454,5.4410315,6.54437,7.970493,4.8940597,6.168184,7.793057,4.5139556,6.380164,5.6539264,3.4527407,4.919653,4.654043,3.838157,7.7018347,2.7811654,7.958869,8.098812,3.9987307,4.4304314,2.8712122,3.233465,4.836305,6.943989,4.4475594,2.0299168,5.3532124,6.688616,4.0750575,5.4180145,7.7532773,4.710498,5.1699533,6.022026,4.941223,3.07843,4.4038415,5.1367526,7.9791684,4.728,4.5846205,6.126215,4.1824617,4.802451,5.533588,1.9984746,5.3749313,4.58421,6.5035753,5.805386,5.0957594,4.4309893,3.1499238,6.1403403,3.5051918,4.427241,4.735651,8.259179,8.156455,2.0695796,5.31275,3.8765328,5.191617,3.3074574,5.768991,6.6538415,5.3904786,4.3650274,6.2190704,6.4146876,8.017194,5.551144,4.410197,6.6465745,7.3356643,2.054021,3.012419,6.899728,8.093487,3.9098437,4.7058067,3.529377,3.5110343,3.3769257,6.191772,6.2837257,5.5735416,5.1747165,5.115073,4.1772223,7.4078608,4.730976,5.153241,3.581389,1.9991562,6.4960194,4.7675743,4.453214,5.36673,4.2097607,5.2310953,8.029693,6.2237177,8.264925,7.1319985,5.0076675,6.8502126,7.283765,8.03436,6.678695,2.015114,6.20966,8.010961,5.213279,6.3425517,8.055647,8.140909,4.6962147,7.7787747,6.3857236,5.503532,3.4696813,4.775813,8.196713,4.284029,6.948088,5.8320436,4.67207,5.1740417,6.5786195,5.1435165,5.167285,4.759347,3.8307881,7.164869,7.9511843,7.789051,4.4676766,1.9813529,4.7956586,5.176158,3.814436,4.869188,6.3712816,3.9558997,7.1676335,4.3032956,4.386388,2.0186944,5.888487,4.169747,6.089527,3.3358426,3.615982,6.2127914,5.1045117,5.0055027,5.8639517,5.4610043,6.517606,6.3960733,2.8672805,4.2566614,7.595876,7.8463387,7.1114745,5.0139465,5.8991957,4.875088,3.4422886,6.5385857,4.0674224,4.9085402,7.2219667,4.847738,5.408719,3.5701668,4.285291,5.3733754,5.6050897,1.9805181,3.5074382,5.656471,7.0995502,4.5909805,4.228007,6.7157536,6.5430636,7.357219,3.392935,4.8592553,5.423055,5.5812316,3.0112152,6.487677,5.592791,7.898294,7.867128,6.9054174,4.833707,4.9721875,7.747213,5.888586,5.1968446,8.025952,4.5896435,2.8667932,6.686473,2.9660752,6.0032535,4.9936132,5.699516,3.2472572,3.5273442,6.4561954,6.3996525,4.2081532,4.2044315,5.979162,4.6503606,7.2243314,6.0364513,6.3386846,2.5565333,8.159083,1.9989659,1.9579687,5.055434,5.3177395,5.5966387,5.7037024,3.6358013,6.819542,7.098796,5.4694467,3.7466736,4.777797,2.6327226,2.5687122,6.0655055,4.7774973,3.7854908,3.606548,3.5779498,2.7343647,5.36031,4.1507683,6.6760483,5.581645,4.5667305,6.8707485,5.037023,4.758365,3.9660864,5.8777695,2.0848515,5.0981917,4.84167,4.2214446,5.9719286,5.243776,2.0148787,4.8587294,5.0234566,5.185877,5.3402815,8.199703,5.16488,5.979855,4.6057043,4.26863,8.239039,5.5372314,6.534995,4.1771636,6.284492,5.5808277,5.0976295,7.3946424,4.5629334,6.5429916,4.350291,5.9408765,3.192162,8.348546,3.923581,4.996674,3.6728888,5.7879,3.1006143,6.0764093,2.6487606,4.482223,7.39142,5.9375725,4.258492,2.8153918,3.757174,7.935474,7.3424797,6.29075,6.6021175,6.407974,4.3759108,5.201199,4.9737387,7.3756986,4.1821017,8.203469,3.6110363,3.673526,3.5005348,6.2204747,6.8753214,2.513852,2.769301,3.661884,2.9075787,6.494289,6.1446276,5.368363,5.3843412,6.4951615,5.7753587,4.959614,4.4448237,4.34826,7.451816,3.566767,5.412739,5.3317385,3.560738,4.1707397,6.46672,4.912106,7.7465935,7.9355555,5.868148,7.483806,5.2777505,4.6267567,7.3661156,7.8874936,7.766265,4.545317,5.782832,5.202838,3.353778,8.044166,2.6526978,6.294092,7.9025364,5.303769,5.9159274,5.5711803,6.4679685,3.8060925,5.69428,4.89987,4.3981433,2.731337,4.8224573,2.754764,4.6280823,3.6445205,4.431068,3.9523282,2.881463,6.24095,4.715852,3.1497827,8.161142,4.8445177,3.472128,4.937002,6.0848703,5.005256,6.495231,4.2697525,5.077091,8.328424,8.18736,6.588663,4.211515,5.879866,6.1270337,6.6869955,6.9028654,5.595399,5.291449,3.769124,4.4109135,4.1536474,5.6830893,4.1482735,4.426658,4.8843455,4.2253385,4.2622237,6.0523453,8.09675,3.2009957,6.8500605,8.101868,5.4308095,6.6380143,4.90821,4.312264,5.375879,3.179023,6.4442105,5.986854,2.809608,4.794649,4.6794553,4.8337545,4.9852133,3.8174143,6.877314,4.239305,4.6463833,6.468758,4.8399568,3.7829669,4.5440903,2.0516837,5.198207,3.6158457,3.3783479,2.5951147,3.9939098,4.9534655,3.5879736,4.8202667,5.0260153,6.491489,4.879467,8.320209,4.2444053,3.9563746,4.752085,4.606606,3.468213,2.0600877,4.5579596,7.2246284,6.285338,3.4321918,4.948025,4.0049844,4.858388,4.8817883,8.253193,8.029053,6.0771966,2.1062095,2.9924417,3.6779175,6.655086,4.4391813,3.7501502,6.092036,4.556103,7.462695,2.0786533,4.716925,6.813815,2.3497934,2.4189634,6.2152395,2.833422,4.0175776,7.8377566,6.465777,7.755684,5.338569,6.1199827,4.4762363,3.1567276,5.1746793,1.9827604,4.9225645,4.196496,3.8240175,3.6472971,4.6225247,5.0983033,4.092563,6.228922,2.4523659,5.1884885,5.1319904,5.442344,5.9739246,4.218537,5.1299553,5.900509,6.8337746,6.6138096,5.743823,2.492537,4.4194427,6.881072,5.7156196,5.67519,5.1984773,5.9802675,6.34788,6.3209844,2.882745,8.21306,5.284386,3.6100829,1.9917909,5.461263,4.7212734,4.3457093,6.9334316,4.0587516,3.7675495,6.149956,7.101766,5.2497063,5.5330434,5.6070876,4.2234993,5.505701,2.0718343,3.6205266,3.506697,4.9062943,6.0878043,6.0648355,4.5776534,6.313799,6.851689,3.6612444,5.0596123,4.1000586,4.4334917,6.3856883,4.274749,4.6715946,6.328905,4.8938913,3.4274948,3.4499843,4.391738,4.889849,4.541197,2.8371584,6.328179,3.5715952,5.2722044,4.4266696,6.9918294,3.3683612,2.3937132,4.2029796,6.055879,4.916905,4.579901,4.290458,4.7093077,4.154916,7.2213826,5.947574,6.1404853,2.7647717,3.6909094,4.6183414,7.997521,4.430528,5.5381026,3.8058758,2.9187784,3.0473628,6.6753354,2.6652405,7.648813,6.476491,4.4343944,4.858548,2.4736197,8.060886,6.0829864,2.9660764,4.569614,6.269998,5.207685,6.4612517,4.9851556,6.609017,4.567742,6.000498,5.839199,2.624175,6.72768,6.0140333,5.9705477,3.8013654,3.3228083,6.8289003,2.6243067,7.1162486,3.1670146,4.1749644,5.989297,6.7423816,6.5854697,8.02222,5.2036023,2.9734707,5.182675,8.170484,8.029419,3.3601646,2.9703655,5.942461,4.480314,7.878175,4.293578,5.726893,5.223511,5.8082547,3.992021,4.974324,4.6862698,6.1885138,5.1818476,5.25417,4.8223267,4.443537,7.2402883,5.6723833,7.1605577,3.5380461,6.6311164,4.74278,4.785936,5.9595327,3.9405587,4.4520683,7.651045,4.363108,5.224941,3.0547693,2.8453596,2.7739213,3.400456,5.2315993,5.549939,3.4982142,3.496264,6.133279,5.949182,6.1768517,6.2233005,4.923545,4.239519,8.155156,5.7606688,2.843299,8.298372,3.3309753,4.724029,8.008256,8.27684,4.3019643,3.4783642,5.0027194,2.8027816,4.7232757,2.1034458,2.1480558,6.064249,4.8151174,2.2945156,7.7709165,3.9855185,3.5563133,4.898474,4.0832534,5.32461,3.3818142,8.020139,4.487332,4.8413033,5.978618,4.16319,5.162198,3.335847,5.2684774,3.8100069,6.481735,6.8281097,3.5344794,3.7671432,3.5816035,6.990734,4.362655,6.5346427,4.7873034,3.9646559,7.645953,5.686478,4.439118,2.4648843,5.273362,4.8874598,4.688014,6.44993,4.4764347,4.999812,4.0998373,3.3819149,4.961038,3.7057145,3.5705304,7.552903,4.073939,6.5591316,4.582776,4.912532,4.1012225,4.1718745,4.9738617,5.362281,4.523331,4.1991887,6.734492,5.6694202,6.408001,4.1134453,3.564814,6.4110055,4.127769,4.3720303,6.992254,4.523313,5.9405036,2.6609693,4.5586605,5.7414317,3.9427555,5.9584713,3.4216993,5.881867,3.6122847,4.233063,3.7819488,8.10335,3.4481025,4.2933006,4.9597583,4.07082,5.8613663,5.510573,6.3358984,5.1073475,5.319327,4.7012486,5.455452,3.0303588,2.0205479,4.2595615,6.259016,5.8361106,3.674097,6.609665,8.080501,4.7583685,2.3936763,3.1227758,4.594625,6.1263003,4.2634106,4.7022843,4.9934754,4.962573,4.288104,2.8601835,3.5861819,2.3889408,5.9359674,3.091684,2.4343612,4.7618637,5.4459925,5.7222342,4.8352985,2.8401597,5.0227685,4.6749897,6.395584,2.7547696,5.3047633,6.3477287,5.0699854,7.9699078,8.152744,1.9718028,4.4195633,7.866785,2.5056005,6.135902,7.274526,6.154131,4.289446,6.3966866,3.4785244,5.2270966,7.747591,3.8468032,5.799649,4.081808,2.9295275,5.7292666,4.3494954,3.991962,2.8623393,2.9215744,3.2968748,6.9958553,6.0651746,4.558415,6.1211343,4.7139163,3.6797812,4.7714963,4.731393,3.226051,4.868128,6.5524464,3.559533,6.7189817,3.3672142,2.952021,3.6275063,4.230723,4.3274083,5.1351457,3.6823099,5.0944076,5.2599187,3.434052,4.3178153,5.3228664,6.322106,6.2158537,5.4639378,6.454235,4.404557,4.067253,4.7573795,5.413771,7.972736,5.272562,5.520625,4.274553,4.326355,4.230255,4.005173,5.0107255,5.214902,6.3593645,3.4326072,3.4290228,3.0348618,3.4649892,4.3657594,4.3044677,2.7205026,6.011359,4.806095,2.4665992,5.797834,6.435634,5.269698,2.4832864,3.7598038,3.2680051,6.0425525,6.8518925,4.8148746,3.043896,7.904558,2.2146885,6.8284445,3.8762403,4.259006,4.011689,4.6507545,7.760511,6.347525,3.4506235,5.20353,3.935629,2.0667531,2.0905237,3.3440785,3.4442894,5.301665,3.270665,3.4991052,6.2722616,4.8344016,7.988838,2.102937,5.1262403,4.4931307,3.2328107,2.7409701,6.338294,2.746746,2.0140145,6.8366737,4.2618046,5.365086,6.0770054,4.8169966,2.832534,6.7000794,4.268266,5.500294,4.3968506,2.363881,5.5286193,6.509166,6.4312463,5.117375,8.190965,4.4913855,4.876548,3.175812,4.082427,8.122873,4.541489,2.647052,5.074386,4.8371468,3.5333884,2.93609,6.3527145,5.5044975,8.2222,3.5990462,3.763592,2.8298435,7.9797773,6.5353546,4.4971046,3.4681365,5.214114,6.7009115,3.904998,2.914224,6.544976,6.6283245,4.926321,6.1232862,6.1484194,3.9490306,4.6134353,4.002578,4.800584,4.4998612,5.6130867,2.9245458,3.5770051,6.544332,3.391788,3.5976508,6.4959574,3.951492,6.8667784,2.193999,4.227072,5.9819283,5.4050655,4.166451,3.7694254,3.3537621,5.171138,8.194084,8.141195,2.0556276,3.931278,4.3702106,4.6939325,4.147316,3.5308044,3.4722662,4.314138,2.0903401,3.7481458,3.9290316,8.117608,3.8682518,8.018842,3.1566596,4.0959845,3.6782486,5.716067,4.1990714,3.038609,5.044048,5.6683817,4.7520175,6.035962,3.292446,5.102835,2.959718,2.5773625,4.571811,4.736192,8.36074,3.556836,2.755262,6.1090565,2.8818974,3.2273197,3.5418942,4.3864293,5.673233,4.3054056,3.0619705,6.1109567,3.8476453,3.2182748,6.8719316,5.2807474,5.7037973,3.9369094,5.2595196,6.285536,4.731924,3.076089,3.7166002,6.445536,5.234199,6.4096494,3.641463,8.392423,5.1959496,5.1249595,5.0276346,6.7415094,3.5595593,2.5458395,4.9918604,3.2310927,3.5698788,3.0029256,3.8853443,5.3249373,3.5679297,4.421923,7.299582,3.6033418,3.4332485,6.3805647,5.728373,3.3566787,6.238581,4.003666,2.8486757,4.9300756,3.5191762,6.706338,3.5148826,3.7760017,4.789668,4.7530236,2.793356,4.2310295,6.498905,5.969714,6.1632113,4.863553,5.8416734,2.4667706,5.545339,7.306607,3.4796212,5.179659,5.0959,4.9994726,4.509593,1.9785588,3.561715,4.1736884,4.516407,4.4604945,3.8135595,5.9567657,4.738056,4.468882,5.336904,2.948562,4.760237,2.245017,5.915909,2.037253,4.3667574,7.8644857,6.039782,3.9923875,3.9303985,7.407275,5.9610653,3.1903903,5.1528325,2.0131156,6.9359574,3.5284083,5.669913,4.3060594,5.9460793,3.7268248,3.344427,5.649309,3.5032935,4.0916505,4.056319,3.3355842,3.7189388,3.2244122,3.609156,2.636509,2.627927,2.7254786,6.339903,3.7424214,5.1811323,5.931636,3.166681,3.3337865,6.111477,5.2022076,2.0647857,3.2937875,3.2986355,5.198567,2.072162,4.4419484,6.68411,5.7291107,3.6424856,5.834972,4.8406305,2.7279396,5.9887576,5.5098987,4.6336713,5.1609707,2.8777962,3.5500813,3.6226144,3.6349146,4.1663103,4.2662683,2.2774992,3.8962216,2.4305933,3.8560476,4.367011,5.0633235,5.2173276,5.1740227,4.782193,4.227444,4.121164,2.9350562,3.7745903,6.777309,2.7421021,4.6249804,4.672353,8.278723,4.8100224,8.046795,3.845756,2.9525719,3.5220637,6.3926573,3.7040725,4.851988,6.6482463,3.3554306,2.4513726,2.4211905,2.012498,6.167026,5.3145356,2.7288992,3.6622257,3.4430695,1.9835913,5.159774,2.4302554,2.4419918,5.281054,3.086663,3.4320536,5.9815116,4.5355597,3.289125,4.5922494,3.3680177,2.8636937,4.6557755,5.5945306,4.2210393,6.3705034,2.973114,7.8406954,4.7625394,2.6225264,6.4808707,4.909702,3.8597095,5.951145,4.7113647,7.8201275,4.821308,3.6539485,3.2407131,4.4097786,4.9697533,4.5635448,3.8944242,3.356036,3.245161,3.7824485,3.8277256,4.813399,3.3463852,2.8190408,4.370377,5.909501,6.6380167,3.500356,5.1775513,6.2084174,6.2252526,2.9339795,6.209831,3.5336797,2.0522835,4.852584,3.8656113,1.9711177,3.9108496,2.9904654,6.3222303,4.5714827,5.393988,5.452523,6.0054626,7.879265,4.052293,4.015991,3.4013915,3.8011713,4.8826957,3.3075492,4.510737,4.8327336,8.0552845,6.4780765,2.5590012,2.52929,6.0051136,6.0079417,2.0970645,4.750477,2.1791384,4.1419773,7.98219,3.6970024,6.3426075,4.830397,3.7983096,3.900202,4.8429103,3.9725983,2.1137726,2.3383873,4.1678395,4.8683257,6.1218333,2.8450694,3.9655495,3.012719,6.7058544,6.3553534,5.526844,3.6919432,6.1835117,2.566211,5.0435147,4.0793324,3.7138605,5.0224304,5.222696,3.6596768,3.3513763,2.8775518,1.9987242,4.1937733,4.988485,2.879592,4.6088834,3.5458605,8.356715,4.0853148,5.47718,2.680951,3.8379824,6.6279907,5.236548,3.5949278,3.4144676,3.789591,4.3695555,3.8813653,3.376126,3.3051336,3.5780838,3.2023325,6.5955544,5.23333,4.4710426,5.255082,4.3516383,4.384941,2.9509842,3.811891,2.945316,2.782317,4.2345424,5.1521707,2.8986082,5.2008452],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x1\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x2\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Abstracts\",\"font\":{\"size\":20}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5393b769-10f5-4a26-88fb-a8883d073648');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 88
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTopic with default parameters/models (e.g. all-MiniLM-L6-v2, UMAP, HDBSCAN)\n",
        "topic_model = BERTopic()\n",
        "\n",
        "# fit_transform runs faster since embeddings were already processed prior\n",
        "topics, probs = topic_model.fit_transform(abstracts, embeddings)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:42.827515Z",
          "iopub.execute_input": "2025-05-13T21:01:42.828345Z",
          "iopub.status.idle": "2025-05-13T21:01:49.141115Z",
          "shell.execute_reply.started": "2025-05-13T21:01:42.82831Z",
          "shell.execute_reply": "2025-05-13T21:01:49.140371Z"
        },
        "id": "6MOlUCDRcYLr"
      },
      "outputs": [],
      "execution_count": 89
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:49.141834Z",
          "iopub.execute_input": "2025-05-13T21:01:49.142115Z",
          "iopub.status.idle": "2025-05-13T21:01:49.172645Z",
          "shell.execute_reply.started": "2025-05-13T21:01:49.142073Z",
          "shell.execute_reply": "2025-05-13T21:01:49.171725Z"
        },
        "id": "giEJM-gkcYLr",
        "outputId": "f096c252-7e33-4a74-d353-6f8e8cdf534a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Topic  Count                                           Name  \\\n",
              "0      -1    536               -1_data_model_learning_algorithm   \n",
              "1       0    163               0_graph_graphs_network_graphical   \n",
              "2       1     78             1_gaussian_process_processes_model   \n",
              "3       2     69                2_lasso_sparse_dictionary_group   \n",
              "4       3     57               3_forest_trees_ensembles_forests   \n",
              "5       4     56             4_clustering_cluster_clusters_data   \n",
              "6       5     55      5_variational_inference_models_generative   \n",
              "7       6     52           6_kernel_kernels_reproducing_hilbert   \n",
              "8       7     50             7_topic_dirichlet_inference_models   \n",
              "9       8     49             8_matrix_lowrank_tensor_completion   \n",
              "10      9     41          9_causal_variables_data_observational   \n",
              "11     10     41       10_brain_imaging_neuroimaging_functional   \n",
              "12     11     35                11_deep_neural_networks_dropout   \n",
              "13     12     33                   12_carlo_sampling_monte_mcmc   \n",
              "14     13     33  13_optimization_function_bayesian_evaluations   \n",
              "15     14     28           14_regression_parameters_models_risk   \n",
              "16     15     27               15_dwd_losses_svm_classification   \n",
              "17     16     26        16_disease_health_covariates_prediction   \n",
              "18     17     23             17_forecasting_series_time_weather   \n",
              "19     18     22             18_policy_regret_bandit_multiarmed   \n",
              "20     19     21                 19_manifold_manifolds_lle_data   \n",
              "21     20     19                    20_data_cancer_cell_methods   \n",
              "22     21     19           21_ica_fastica_component_independent   \n",
              "23     22     16              22_subspace_subspaces_points_data   \n",
              "24     23     16  23_regularization_problems_convex_convergence   \n",
              "25     24     13               24_metric_distance_metrics_borel   \n",
              "26     25     12   25_mixture_clustering_bayesian_smallvariance   \n",
              "27     26     11        26_dynamical_neural_recordings_dynamics   \n",
              "\n",
              "                                       Representation  \\\n",
              "0   [data, model, learning, algorithm, method, usi...   \n",
              "1   [graph, graphs, network, graphical, model, net...   \n",
              "2   [gaussian, process, processes, model, gps, var...   \n",
              "3   [lasso, sparse, dictionary, group, sparsity, r...   \n",
              "4   [forest, trees, ensembles, forests, ensemble, ...   \n",
              "5   [clustering, cluster, clusters, data, density,...   \n",
              "6   [variational, inference, models, generative, l...   \n",
              "7   [kernel, kernels, reproducing, hilbert, test, ...   \n",
              "8   [topic, dirichlet, inference, models, topics, ...   \n",
              "9   [matrix, lowrank, tensor, completion, rank, al...   \n",
              "10  [causal, variables, data, observational, infer...   \n",
              "11  [brain, imaging, neuroimaging, functional, ima...   \n",
              "12  [deep, neural, networks, dropout, bayesian, un...   \n",
              "13  [carlo, sampling, monte, mcmc, sgmcmc, gradien...   \n",
              "14  [optimization, function, bayesian, evaluations...   \n",
              "15  [regression, parameters, models, risk, additiv...   \n",
              "16  [dwd, losses, svm, classification, binary, cla...   \n",
              "17  [disease, health, covariates, prediction, pati...   \n",
              "18  [forecasting, series, time, weather, energy, m...   \n",
              "19  [policy, regret, bandit, multiarmed, bound, ar...   \n",
              "20  [manifold, manifolds, lle, data, embedding, di...   \n",
              "21  [data, cancer, cell, methods, populations, ana...   \n",
              "22  [ica, fastica, component, independent, analysi...   \n",
              "23  [subspace, subspaces, points, data, ssc, ideal...   \n",
              "24  [regularization, problems, convex, convergence...   \n",
              "25  [metric, distance, metrics, borel, pvm, protot...   \n",
              "26  [mixture, clustering, bayesian, smallvariance,...   \n",
              "27  [dynamical, neural, recordings, dynamics, psrn...   \n",
              "\n",
              "                                  Representative_Docs  \n",
              "0   [hierarchical probabilistic models gaussian mi...  \n",
              "1   [latent space model family random graphs assig...  \n",
              "2   [study gaussian process regression model conte...  \n",
              "3   [study problem learning sparse linear regressi...  \n",
              "4   [tree ensembles random forest boosted trees re...  \n",
              "5   [goal data clustering partition data points gr...  \n",
              "6   [stochastic variational inference relatively w...  \n",
              "7   [paper propose family tractable kernels dense ...  \n",
              "8   [paper proposes novel dynamic hierarchical dir...  \n",
              "9   [consider problem noisy bit matrix completion ...  \n",
              "10  [one fundamental problems causal inference est...  \n",
              "11  [brain decoding involves determination subject...  \n",
              "12  [deep gaussian processes dgps multilayer hiera...  \n",
              "13  [propose kernel hamiltonian monte carlo kmc gr...  \n",
              "14  [bayesian optimization methods useful optimizi...  \n",
              "15  [propose robust inferential procedure assessin...  \n",
              "16  [distance weighted discrimination dwd marginba...  \n",
              "17  [accurate reliable predictions infectious dise...  \n",
              "18  [renewable distributed energy resources ders p...  \n",
              "19  [consider explorationexploitation tradeoff lin...  \n",
              "20  [spectral dimensionality reduction frequently ...  \n",
              "21  [flow cytometry highthroughput technology used...  \n",
              "22  [independent component analysis ica powerful m...  \n",
              "23  [sparse subspace clustering ssc elegant approa...  \n",
              "24  [many statistical learning problems posed mini...  \n",
              "25  [propose new class metrics sets vectors functi...  \n",
              "26  [paper generalized multivariate studentt mixtu...  \n",
              "27  [powerful approach understanding neural popula...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b9ec9a78-780b-4465-96a5-3b65906b6510\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic</th>\n",
              "      <th>Count</th>\n",
              "      <th>Name</th>\n",
              "      <th>Representation</th>\n",
              "      <th>Representative_Docs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>536</td>\n",
              "      <td>-1_data_model_learning_algorithm</td>\n",
              "      <td>[data, model, learning, algorithm, method, usi...</td>\n",
              "      <td>[hierarchical probabilistic models gaussian mi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>163</td>\n",
              "      <td>0_graph_graphs_network_graphical</td>\n",
              "      <td>[graph, graphs, network, graphical, model, net...</td>\n",
              "      <td>[latent space model family random graphs assig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>78</td>\n",
              "      <td>1_gaussian_process_processes_model</td>\n",
              "      <td>[gaussian, process, processes, model, gps, var...</td>\n",
              "      <td>[study gaussian process regression model conte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>69</td>\n",
              "      <td>2_lasso_sparse_dictionary_group</td>\n",
              "      <td>[lasso, sparse, dictionary, group, sparsity, r...</td>\n",
              "      <td>[study problem learning sparse linear regressi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>57</td>\n",
              "      <td>3_forest_trees_ensembles_forests</td>\n",
              "      <td>[forest, trees, ensembles, forests, ensemble, ...</td>\n",
              "      <td>[tree ensembles random forest boosted trees re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4</td>\n",
              "      <td>56</td>\n",
              "      <td>4_clustering_cluster_clusters_data</td>\n",
              "      <td>[clustering, cluster, clusters, data, density,...</td>\n",
              "      <td>[goal data clustering partition data points gr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>55</td>\n",
              "      <td>5_variational_inference_models_generative</td>\n",
              "      <td>[variational, inference, models, generative, l...</td>\n",
              "      <td>[stochastic variational inference relatively w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6</td>\n",
              "      <td>52</td>\n",
              "      <td>6_kernel_kernels_reproducing_hilbert</td>\n",
              "      <td>[kernel, kernels, reproducing, hilbert, test, ...</td>\n",
              "      <td>[paper propose family tractable kernels dense ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7</td>\n",
              "      <td>50</td>\n",
              "      <td>7_topic_dirichlet_inference_models</td>\n",
              "      <td>[topic, dirichlet, inference, models, topics, ...</td>\n",
              "      <td>[paper proposes novel dynamic hierarchical dir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8</td>\n",
              "      <td>49</td>\n",
              "      <td>8_matrix_lowrank_tensor_completion</td>\n",
              "      <td>[matrix, lowrank, tensor, completion, rank, al...</td>\n",
              "      <td>[consider problem noisy bit matrix completion ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>9</td>\n",
              "      <td>41</td>\n",
              "      <td>9_causal_variables_data_observational</td>\n",
              "      <td>[causal, variables, data, observational, infer...</td>\n",
              "      <td>[one fundamental problems causal inference est...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>10</td>\n",
              "      <td>41</td>\n",
              "      <td>10_brain_imaging_neuroimaging_functional</td>\n",
              "      <td>[brain, imaging, neuroimaging, functional, ima...</td>\n",
              "      <td>[brain decoding involves determination subject...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>11</td>\n",
              "      <td>35</td>\n",
              "      <td>11_deep_neural_networks_dropout</td>\n",
              "      <td>[deep, neural, networks, dropout, bayesian, un...</td>\n",
              "      <td>[deep gaussian processes dgps multilayer hiera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>12</td>\n",
              "      <td>33</td>\n",
              "      <td>12_carlo_sampling_monte_mcmc</td>\n",
              "      <td>[carlo, sampling, monte, mcmc, sgmcmc, gradien...</td>\n",
              "      <td>[propose kernel hamiltonian monte carlo kmc gr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>13</td>\n",
              "      <td>33</td>\n",
              "      <td>13_optimization_function_bayesian_evaluations</td>\n",
              "      <td>[optimization, function, bayesian, evaluations...</td>\n",
              "      <td>[bayesian optimization methods useful optimizi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>14</td>\n",
              "      <td>28</td>\n",
              "      <td>14_regression_parameters_models_risk</td>\n",
              "      <td>[regression, parameters, models, risk, additiv...</td>\n",
              "      <td>[propose robust inferential procedure assessin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>15</td>\n",
              "      <td>27</td>\n",
              "      <td>15_dwd_losses_svm_classification</td>\n",
              "      <td>[dwd, losses, svm, classification, binary, cla...</td>\n",
              "      <td>[distance weighted discrimination dwd marginba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>16</td>\n",
              "      <td>26</td>\n",
              "      <td>16_disease_health_covariates_prediction</td>\n",
              "      <td>[disease, health, covariates, prediction, pati...</td>\n",
              "      <td>[accurate reliable predictions infectious dise...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>17</td>\n",
              "      <td>23</td>\n",
              "      <td>17_forecasting_series_time_weather</td>\n",
              "      <td>[forecasting, series, time, weather, energy, m...</td>\n",
              "      <td>[renewable distributed energy resources ders p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>18</td>\n",
              "      <td>22</td>\n",
              "      <td>18_policy_regret_bandit_multiarmed</td>\n",
              "      <td>[policy, regret, bandit, multiarmed, bound, ar...</td>\n",
              "      <td>[consider explorationexploitation tradeoff lin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>19</td>\n",
              "      <td>21</td>\n",
              "      <td>19_manifold_manifolds_lle_data</td>\n",
              "      <td>[manifold, manifolds, lle, data, embedding, di...</td>\n",
              "      <td>[spectral dimensionality reduction frequently ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>20</td>\n",
              "      <td>19</td>\n",
              "      <td>20_data_cancer_cell_methods</td>\n",
              "      <td>[data, cancer, cell, methods, populations, ana...</td>\n",
              "      <td>[flow cytometry highthroughput technology used...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>21</td>\n",
              "      <td>19</td>\n",
              "      <td>21_ica_fastica_component_independent</td>\n",
              "      <td>[ica, fastica, component, independent, analysi...</td>\n",
              "      <td>[independent component analysis ica powerful m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>22</td>\n",
              "      <td>16</td>\n",
              "      <td>22_subspace_subspaces_points_data</td>\n",
              "      <td>[subspace, subspaces, points, data, ssc, ideal...</td>\n",
              "      <td>[sparse subspace clustering ssc elegant approa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>23</td>\n",
              "      <td>16</td>\n",
              "      <td>23_regularization_problems_convex_convergence</td>\n",
              "      <td>[regularization, problems, convex, convergence...</td>\n",
              "      <td>[many statistical learning problems posed mini...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>24</td>\n",
              "      <td>13</td>\n",
              "      <td>24_metric_distance_metrics_borel</td>\n",
              "      <td>[metric, distance, metrics, borel, pvm, protot...</td>\n",
              "      <td>[propose new class metrics sets vectors functi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>25</td>\n",
              "      <td>12</td>\n",
              "      <td>25_mixture_clustering_bayesian_smallvariance</td>\n",
              "      <td>[mixture, clustering, bayesian, smallvariance,...</td>\n",
              "      <td>[paper generalized multivariate studentt mixtu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>26</td>\n",
              "      <td>11</td>\n",
              "      <td>26_dynamical_neural_recordings_dynamics</td>\n",
              "      <td>[dynamical, neural, recordings, dynamics, psrn...</td>\n",
              "      <td>[powerful approach understanding neural popula...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b9ec9a78-780b-4465-96a5-3b65906b6510')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b9ec9a78-780b-4465-96a5-3b65906b6510 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b9ec9a78-780b-4465-96a5-3b65906b6510');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-fffb4567-72cf-4dfd-aae8-4f7a0ab2acb1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fffb4567-72cf-4dfd-aae8-4f7a0ab2acb1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-fffb4567-72cf-4dfd-aae8-4f7a0ab2acb1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"topic_model\",\n  \"rows\": 28,\n  \"fields\": [\n    {\n      \"column\": \"Topic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": -1,\n        \"max\": 26,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          8,\n          24,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 98,\n        \"min\": 11,\n        \"max\": 536,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          50,\n          23,\n          536\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 28,\n        \"samples\": [\n          \"8_matrix_lowrank_tensor_completion\",\n          \"24_metric_distance_metrics_borel\",\n          \"7_topic_dirichlet_inference_models\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representation\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representative_Docs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "execution_count": 90
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example output from cs.AI category:\n",
        "| Topic | Count | Name                         | Representation                                               | Representative Docs                                     |\n",
        "|-------|-------|------------------------------|---------------------------------------------------------------|----------------------------------------------------------|\n",
        "| -1    | 4525  | -1_the_of_and_to             | [the, of, and, to, in, we, that, for, is, this]               | [ In this work we propose a planning and acti...         |\n",
        "| 0     | 665   | 0_belief_of_theory_the       | [belief, of, theory, the, is, probability, in, evidence, a, that] | [ The paper presents a novel view of the Demp...         |\n",
        "| 1     | 452   | 1_game_games_the_of          | [game, games, the, of, player, to, in, and, we, strategy]     | [ In many board games and other abstract game...         |\n",
        "| 2     | 434   | 2_networks_bayesian_network_the | [networks, bayesian, network, the, inference, structure, in, of, learning, probabilistic] | [ Structure and parameters in a Bayesian netw... |\n",
        "| 3     | 399   | 3_intelligence_of_the_and    | [intelligence, of, the, and, cognitive, artificial, agents, in, is, systems] | [ The overarching problem in artificial intel... |\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "## Error Output with high concentration on Topic -1:\n",
        "| Topic | Count | Name                                    | Representation                                      | Representative\\_Docs                                |   |\n",
        "| ----- | ----- | --------------------------------------- | --------------------------------------------------- | --------------------------------------------------- | - |\n",
        "| -1    | 10318 | -1\\_model\\_paper\\_models\\_learning      | \\[model, paper, models, learning, data, problem...] | \\[markov decision processes mdps well studied f...] |   |\n",
        "| 0     | 1726  | 0\\_learning\\_models\\_model\\_data        | \\[learning, models, model, data, knowledge, pap...] | \\[artificial intelligence techniques used class...] |   |\n",
        "| 1     | 136   | 1\\_problem\\_search\\_algorithm\\_problems | \\[problem, search, algorithm, problems, algorit...] | \\[constraint satisfaction problem csp framework...] |   |\n",
        "\n"
      ],
      "metadata": {
        "id": "QcZ2aNGMcYLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune BERTopic with alternative modeling techniques\n",
        "\n",
        "### Swap or remove any of the following:\n",
        "1) Embedding model <br>\n",
        "    * Note: The following embedding models kept grouping majority (12134) into group 0 with filler words\n",
        "        * word2vec, Universal Sentence Encoder (USE), hugging face transformer (distilbert-base) (40+ min) <br>\n",
        "    \n",
        "2) Reducing dimensionality of embeddings (default=UMAP, PCA, t-SVD, cuML UMAP, or remove this layer)<br>\n",
        "3) Clustering into topics (default=HDBSCAN, k-Means, sklearn.cluster, cuML HDBSCAN) <br>\n",
        "4) Tokenization of topics <br>\n",
        "5) Weight tokens\n",
        "6) Represent topics with one or multiple representations <br>\n",
        "\n",
        "(Gathered from here: https://maartengr.github.io/BERTopic/index.html#modularity)"
      ],
      "metadata": {
        "id": "P2e7xySwcYLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding model ([more models here](https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html#sentence-transformers))"
      ],
      "metadata": {
        "id": "pvAuFdQbcYLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### (Default): all-MiniLM-L6-v2\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "### (alternative): bge-base\n",
        "# embedding_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:49.173576Z",
          "iopub.execute_input": "2025-05-13T21:01:49.173892Z",
          "iopub.status.idle": "2025-05-13T21:01:49.828522Z",
          "shell.execute_reply.started": "2025-05-13T21:01:49.17387Z",
          "shell.execute_reply": "2025-05-13T21:01:49.827778Z"
        },
        "id": "vsgmZar8cYLr"
      },
      "outputs": [],
      "execution_count": 91
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Utilize GPUs if available"
      ],
      "metadata": {
        "id": "QCA-bDpNcYLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if torch.cuda.device_count() > 1:\n",
        "  print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
        "  embedding_model = torch.nn.DataParallel(embedding_model)\n",
        "else:\n",
        "  print(\"Using a single GPU or CPU\")\n",
        "  device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "  embedding_model = embedding_model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:49.829489Z",
          "iopub.execute_input": "2025-05-13T21:01:49.82978Z",
          "iopub.status.idle": "2025-05-13T21:01:49.83981Z",
          "shell.execute_reply.started": "2025-05-13T21:01:49.829753Z",
          "shell.execute_reply": "2025-05-13T21:01:49.838963Z"
        },
        "id": "mSTu6ot_cYLs",
        "outputId": "b71169c8-a878-4c78-b6b5-0375cb79f3a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using a single GPU or CPU\n"
          ]
        }
      ],
      "execution_count": 92
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dimensionality Reduction ([more models here](https://maartengr.github.io/BERTopic/getting_started/dim_reduction/dim_reduction.html))"
      ],
      "metadata": {
        "id": "fLxzMAa0cYLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### (Default): UMAP\n",
        "from umap import UMAP\n",
        "dim_model = UMAP(n_components=2, n_neighbors=10, min_dist=0.0, metric='cosine')\n",
        "\n",
        "### (alternative): cuML UMAP (Note: ideal with GPU, error with cuda version if P100 is used)\n",
        "# from cuml.manifold import UMAP\n",
        "# dim_model = UMAP(n_components=2, n_neighbors=5, min_dist=0.0, metric='cosine')\n",
        "\n",
        "### (alternative): PCA\n",
        "# from sklearn.decomposition import PCA\n",
        "# dim_model = PCA(n_components=2)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:49.84091Z",
          "iopub.execute_input": "2025-05-13T21:01:49.841563Z",
          "iopub.status.idle": "2025-05-13T21:01:49.857544Z",
          "shell.execute_reply.started": "2025-05-13T21:01:49.84154Z",
          "shell.execute_reply": "2025-05-13T21:01:49.856383Z"
        },
        "id": "LHpN0UtbcYLs"
      },
      "outputs": [],
      "execution_count": 93
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering ([more models here](https://maartengr.github.io/BERTopic/getting_started/clustering/clustering.html))"
      ],
      "metadata": {
        "id": "H0ZEL-NNcYLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### (Default): HDSCAN\n",
        "# from hdbscan import HDBSCAN\n",
        "# cluster_model = HDBSCAN(min_cluster_size=20, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
        "\n",
        "### (alternative): cuML HDSCAN (ideal with GPU)\n",
        "# from cuml.cluster import HDBSCAN\n",
        "# cluster_model = HDBSCAN(min_samples=50, gen_min_span_tree=True, prediction_data=True)\n",
        "\n",
        "### (alternative): k-Means\n",
        "from sklearn.cluster import KMeans\n",
        "cluster_model = KMeans(n_clusters=20)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:49.858643Z",
          "iopub.execute_input": "2025-05-13T21:01:49.858965Z",
          "iopub.status.idle": "2025-05-13T21:01:49.879296Z",
          "shell.execute_reply.started": "2025-05-13T21:01:49.858937Z",
          "shell.execute_reply": "2025-05-13T21:01:49.878304Z"
        },
        "id": "RblmFePGcYLs"
      },
      "outputs": [],
      "execution_count": 94
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Representation Model ([more models here](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html))"
      ],
      "metadata": {
        "id": "484aH7O_cYLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
        "\n",
        "# KeyBERT\n",
        "keybert = KeyBERTInspired()\n",
        "\n",
        "# MMR\n",
        "mmr = MaximalMarginalRelevance(diversity=0.3)\n",
        "\n",
        "# All representation models\n",
        "representation_model = {\n",
        "    \"KeyBERT\": keybert,\n",
        "    \"MMR\": mmr,\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:49.880226Z",
          "iopub.execute_input": "2025-05-13T21:01:49.880544Z",
          "iopub.status.idle": "2025-05-13T21:01:49.899085Z",
          "shell.execute_reply.started": "2025-05-13T21:01:49.880517Z",
          "shell.execute_reply": "2025-05-13T21:01:49.898162Z"
        },
        "id": "wvS1PVbecYLs"
      },
      "outputs": [],
      "execution_count": 95
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom BERTopic model and fitting"
      ],
      "metadata": {
        "id": "qB_6M8e-cYLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the BERTopic model with modular/custom sub-models\n",
        "topic_model = BERTopic(verbose=True,\n",
        "\n",
        "           # Sub-models\n",
        "           embedding_model=embedding_model,\n",
        "           umap_model=dim_model,\n",
        "           hdbscan_model=cluster_model,\n",
        "           representation_model=representation_model,\n",
        "\n",
        "           #Hyperparameters\n",
        "           min_topic_size=25\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:49.900244Z",
          "iopub.execute_input": "2025-05-13T21:01:49.900536Z",
          "iopub.status.idle": "2025-05-13T21:01:49.919619Z",
          "shell.execute_reply.started": "2025-05-13T21:01:49.900514Z",
          "shell.execute_reply": "2025-05-13T21:01:49.918561Z"
        },
        "id": "-cl_hjBrcYLt"
      },
      "outputs": [],
      "execution_count": 96
    },
    {
      "cell_type": "code",
      "source": [
        "### Fit the model on your list of abstracts, but no pre-processed embeddings\n",
        "# topics, probs = topic_model.fit_transform(abstracts)\n",
        "\n",
        "### Faster fitting of the model with abstracts pre-processed embeddings\n",
        "topics, probs = topic_model.fit_transform(abstracts, embeddings)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:49.920637Z",
          "iopub.execute_input": "2025-05-13T21:01:49.920948Z",
          "iopub.status.idle": "2025-05-13T21:01:49.985651Z",
          "shell.execute_reply.started": "2025-05-13T21:01:49.920927Z",
          "shell.execute_reply": "2025-05-13T21:01:49.984112Z"
        },
        "id": "JDUvz8d4cYLx",
        "outputId": "8ae25540-9c7c-476a-b0d9-2887856ab803",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-14 16:08:14,713 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2025-05-14 16:08:21,922 - BERTopic - Dimensionality - Completed ✓\n",
            "2025-05-14 16:08:21,925 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2025-05-14 16:08:21,969 - BERTopic - Cluster - Completed ✓\n",
            "2025-05-14 16:08:21,975 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
            "2025-05-14 16:08:24,152 - BERTopic - Representation - Completed ✓\n"
          ]
        }
      ],
      "execution_count": 97
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outputs"
      ],
      "metadata": {
        "id": "2WA3K5IWcYLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a summary of all topics\n",
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:49.986598Z",
          "iopub.status.idle": "2025-05-13T21:01:49.98695Z",
          "shell.execute_reply.started": "2025-05-13T21:01:49.986783Z",
          "shell.execute_reply": "2025-05-13T21:01:49.986799Z"
        },
        "id": "a2bkYvn7cYLx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b01a1d3b-5546-4cc3-c459-7b3b002f69d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Topic  Count                                         Name  \\\n",
              "0       0    150           0_gaussian_process_model_processes   \n",
              "1       1    130           1_matrix_dictionary_algorithm_data   \n",
              "2       2    115              2_lasso_regression_sparse_group   \n",
              "3       3    110               3_kernel_learning_kernels_test   \n",
              "4       4     93                4_graph_graphs_network_vertex   \n",
              "5       5     92                   5_brain_data_kernel_method   \n",
              "6       6     85  6_algorithms_problems_algorithm_statistical   \n",
              "7       7     81             7_sampling_carlo_monte_algorithm   \n",
              "8       8     81    8_variational_inference_models_generative   \n",
              "9       9     76                9_forest_trees_data_ensembles   \n",
              "10     10     74               10_classification_data_dwd_svm   \n",
              "11     11     71          11_clustering_clusters_data_cluster   \n",
              "12     12     70            12_graphical_models_model_network   \n",
              "13     13     69                13_manifold_analysis_ica_data   \n",
              "14     14     67   14_optimization_bayesian_function_problems   \n",
              "15     15     61             15_series_time_forecasting_model   \n",
              "16     16     55          16_topic_dirichlet_inference_models   \n",
              "17     17     45       17_causal_variables_data_observational   \n",
              "18     18     44          18_treatment_disease_patient_health   \n",
              "19     19     32           19_mixture_model_models_clustering   \n",
              "\n",
              "                                       Representation  \\\n",
              "0   [gaussian, process, model, processes, models, ...   \n",
              "1   [matrix, dictionary, algorithm, data, sparse, ...   \n",
              "2   [lasso, regression, sparse, group, regularizat...   \n",
              "3   [kernel, learning, kernels, test, reproducing,...   \n",
              "4   [graph, graphs, network, vertex, clustering, s...   \n",
              "5   [brain, data, kernel, method, analysis, model,...   \n",
              "6   [algorithms, problems, algorithm, statistical,...   \n",
              "7   [sampling, carlo, monte, algorithm, markov, st...   \n",
              "8   [variational, inference, models, generative, d...   \n",
              "9   [forest, trees, data, ensembles, forests, ense...   \n",
              "10  [classification, data, dwd, svm, classifier, c...   \n",
              "11  [clustering, clusters, data, cluster, density,...   \n",
              "12  [graphical, models, model, network, variables,...   \n",
              "13  [manifold, analysis, ica, data, component, mat...   \n",
              "14  [optimization, bayesian, function, problems, r...   \n",
              "15  [series, time, forecasting, model, data, weath...   \n",
              "16  [topic, dirichlet, inference, models, model, t...   \n",
              "17  [causal, variables, data, observational, infer...   \n",
              "18  [treatment, disease, patient, health, patients...   \n",
              "19  [mixture, model, models, clustering, latent, t...   \n",
              "\n",
              "                                              KeyBERT  \\\n",
              "0   [gaussian, models, prediction, modelling, lear...   \n",
              "1   [algorithms, lowrank, algorithm, matrix, matri...   \n",
              "2   [lasso, regularization, sparse, optimization, ...   \n",
              "3   [kernels, kernel, classification, supervised, ...   \n",
              "4   [graphs, nodes, adjacency, clustering, cluster...   \n",
              "5   [fmri, neuroimaging, classification, imaging, ...   \n",
              "6   [algorithms, optimization, algorithm, regulari...   \n",
              "7   [sgmcmc, mcmc, sampling, bayesian, stochastic,...   \n",
              "8   [variational, stochastic, probabilistic, bayes...   \n",
              "9   [ensembles, classification, forests, forest, e...   \n",
              "10  [classifiers, svm, classifier, svms, classific...   \n",
              "11  [clustering, cluster, clusters, datasets, algo...   \n",
              "12  [graphical, graphs, models, modeling, bayesian...   \n",
              "13  [pca, lowdimensional, components, algorithms, ...   \n",
              "14  [optimisation, optimization, algorithms, algor...   \n",
              "15  [forecasting, forecasted, prediction, autoregr...   \n",
              "16  [topics, topic, lda, dirichlet, hierarchical, ...   \n",
              "17  [causal, causality, confounders, inference, es...   \n",
              "18  [clinical, patients, outcomes, healthcare, cla...   \n",
              "19  [bayesian, clustering, probabilistic, posterio...   \n",
              "\n",
              "                                                  MMR  \\\n",
              "0   [gaussian, processes, models, variational, lea...   \n",
              "1   [matrix, sparse, tensor, decomposition, lowran...   \n",
              "2   [lasso, regression, sparse, group, regularizat...   \n",
              "3   [kernel, learning, kernels, hilbert, distribut...   \n",
              "4   [graphs, spectral, nodes, networks, vertices, ...   \n",
              "5   [brain, data, kernel, imaging, cca, functional...   \n",
              "6   [algorithms, convex, optimization, convergence...   \n",
              "7   [sampling, monte, stochastic, algorithms, mcmc...   \n",
              "8   [variational, models, generative, distribution...   \n",
              "9   [forest, ensembles, forests, ensemble, boostin...   \n",
              "10  [classification, dwd, svm, classifier, crossva...   \n",
              "11  [clustering, clusters, cluster, kmeans, estima...   \n",
              "12  [graphical, models, networks, graph, dependenc...   \n",
              "13  [ica, matrix, covariance, sparse, pca, fastica...   \n",
              "14  [optimization, bayesian, regret, evaluations, ...   \n",
              "15  [series, forecasting, weather, models, gaussia...   \n",
              "16  [dirichlet, topics, gibbs, lda, posterior, nmf...   \n",
              "17  [causal, observational, inference, causality, ...   \n",
              "18  [health, patients, covariates, survival, mood,...   \n",
              "19  [mixture, models, clustering, bayesian, kmeans...   \n",
              "\n",
              "                                  Representative_Docs  \n",
              "0   [multioutput gaussian processes received incre...  \n",
              "1   [consider generalization lowrank matrix comple...  \n",
              "2   [group lasso penalized regression method used ...  \n",
              "3   [study strictly proper scoring rules reproduci...  \n",
              "4   [partitioning graph groups vertices within gro...  \n",
              "5   [imaging genetic research essentially focused ...  \n",
              "6   [consider statistical algorithmic aspects solv...  \n",
              "7   [hamiltonian monte carlo hmc popular markov ch...  \n",
              "8   [stochastic variational inference svi paradigm...  \n",
              "9   [missing data expected issue large amounts dat...  \n",
              "10  [paper concerned problems interaction screenin...  \n",
              "11  [goal data clustering partition data points gr...  \n",
              "12  [paper propose semiparametric approach named n...  \n",
              "13  [statistical dependencies independent componen...  \n",
              "14  [bayesian optimization methods useful optimizi...  \n",
              "15  [paper deals inference prediction multiple cor...  \n",
              "16  [paper proposes novel dynamic hierarchical dir...  \n",
              "17  [important topic systems biology developing st...  \n",
              "18  [devising course treatment patient doctors oft...  \n",
              "19  [hierarchical learning models mixture models b...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2f22ca10-d76d-4377-92dc-65d7bf8f877a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic</th>\n",
              "      <th>Count</th>\n",
              "      <th>Name</th>\n",
              "      <th>Representation</th>\n",
              "      <th>KeyBERT</th>\n",
              "      <th>MMR</th>\n",
              "      <th>Representative_Docs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0_gaussian_process_model_processes</td>\n",
              "      <td>[gaussian, process, model, processes, models, ...</td>\n",
              "      <td>[gaussian, models, prediction, modelling, lear...</td>\n",
              "      <td>[gaussian, processes, models, variational, lea...</td>\n",
              "      <td>[multioutput gaussian processes received incre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>1_matrix_dictionary_algorithm_data</td>\n",
              "      <td>[matrix, dictionary, algorithm, data, sparse, ...</td>\n",
              "      <td>[algorithms, lowrank, algorithm, matrix, matri...</td>\n",
              "      <td>[matrix, sparse, tensor, decomposition, lowran...</td>\n",
              "      <td>[consider generalization lowrank matrix comple...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>115</td>\n",
              "      <td>2_lasso_regression_sparse_group</td>\n",
              "      <td>[lasso, regression, sparse, group, regularizat...</td>\n",
              "      <td>[lasso, regularization, sparse, optimization, ...</td>\n",
              "      <td>[lasso, regression, sparse, group, regularizat...</td>\n",
              "      <td>[group lasso penalized regression method used ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>110</td>\n",
              "      <td>3_kernel_learning_kernels_test</td>\n",
              "      <td>[kernel, learning, kernels, test, reproducing,...</td>\n",
              "      <td>[kernels, kernel, classification, supervised, ...</td>\n",
              "      <td>[kernel, learning, kernels, hilbert, distribut...</td>\n",
              "      <td>[study strictly proper scoring rules reproduci...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>4_graph_graphs_network_vertex</td>\n",
              "      <td>[graph, graphs, network, vertex, clustering, s...</td>\n",
              "      <td>[graphs, nodes, adjacency, clustering, cluster...</td>\n",
              "      <td>[graphs, spectral, nodes, networks, vertices, ...</td>\n",
              "      <td>[partitioning graph groups vertices within gro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>92</td>\n",
              "      <td>5_brain_data_kernel_method</td>\n",
              "      <td>[brain, data, kernel, method, analysis, model,...</td>\n",
              "      <td>[fmri, neuroimaging, classification, imaging, ...</td>\n",
              "      <td>[brain, data, kernel, imaging, cca, functional...</td>\n",
              "      <td>[imaging genetic research essentially focused ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>85</td>\n",
              "      <td>6_algorithms_problems_algorithm_statistical</td>\n",
              "      <td>[algorithms, problems, algorithm, statistical,...</td>\n",
              "      <td>[algorithms, optimization, algorithm, regulari...</td>\n",
              "      <td>[algorithms, convex, optimization, convergence...</td>\n",
              "      <td>[consider statistical algorithmic aspects solv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>81</td>\n",
              "      <td>7_sampling_carlo_monte_algorithm</td>\n",
              "      <td>[sampling, carlo, monte, algorithm, markov, st...</td>\n",
              "      <td>[sgmcmc, mcmc, sampling, bayesian, stochastic,...</td>\n",
              "      <td>[sampling, monte, stochastic, algorithms, mcmc...</td>\n",
              "      <td>[hamiltonian monte carlo hmc popular markov ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>81</td>\n",
              "      <td>8_variational_inference_models_generative</td>\n",
              "      <td>[variational, inference, models, generative, d...</td>\n",
              "      <td>[variational, stochastic, probabilistic, bayes...</td>\n",
              "      <td>[variational, models, generative, distribution...</td>\n",
              "      <td>[stochastic variational inference svi paradigm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>76</td>\n",
              "      <td>9_forest_trees_data_ensembles</td>\n",
              "      <td>[forest, trees, data, ensembles, forests, ense...</td>\n",
              "      <td>[ensembles, classification, forests, forest, e...</td>\n",
              "      <td>[forest, ensembles, forests, ensemble, boostin...</td>\n",
              "      <td>[missing data expected issue large amounts dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>74</td>\n",
              "      <td>10_classification_data_dwd_svm</td>\n",
              "      <td>[classification, data, dwd, svm, classifier, c...</td>\n",
              "      <td>[classifiers, svm, classifier, svms, classific...</td>\n",
              "      <td>[classification, dwd, svm, classifier, crossva...</td>\n",
              "      <td>[paper concerned problems interaction screenin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>71</td>\n",
              "      <td>11_clustering_clusters_data_cluster</td>\n",
              "      <td>[clustering, clusters, data, cluster, density,...</td>\n",
              "      <td>[clustering, cluster, clusters, datasets, algo...</td>\n",
              "      <td>[clustering, clusters, cluster, kmeans, estima...</td>\n",
              "      <td>[goal data clustering partition data points gr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>70</td>\n",
              "      <td>12_graphical_models_model_network</td>\n",
              "      <td>[graphical, models, model, network, variables,...</td>\n",
              "      <td>[graphical, graphs, models, modeling, bayesian...</td>\n",
              "      <td>[graphical, models, networks, graph, dependenc...</td>\n",
              "      <td>[paper propose semiparametric approach named n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>69</td>\n",
              "      <td>13_manifold_analysis_ica_data</td>\n",
              "      <td>[manifold, analysis, ica, data, component, mat...</td>\n",
              "      <td>[pca, lowdimensional, components, algorithms, ...</td>\n",
              "      <td>[ica, matrix, covariance, sparse, pca, fastica...</td>\n",
              "      <td>[statistical dependencies independent componen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>67</td>\n",
              "      <td>14_optimization_bayesian_function_problems</td>\n",
              "      <td>[optimization, bayesian, function, problems, r...</td>\n",
              "      <td>[optimisation, optimization, algorithms, algor...</td>\n",
              "      <td>[optimization, bayesian, regret, evaluations, ...</td>\n",
              "      <td>[bayesian optimization methods useful optimizi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>61</td>\n",
              "      <td>15_series_time_forecasting_model</td>\n",
              "      <td>[series, time, forecasting, model, data, weath...</td>\n",
              "      <td>[forecasting, forecasted, prediction, autoregr...</td>\n",
              "      <td>[series, forecasting, weather, models, gaussia...</td>\n",
              "      <td>[paper deals inference prediction multiple cor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>55</td>\n",
              "      <td>16_topic_dirichlet_inference_models</td>\n",
              "      <td>[topic, dirichlet, inference, models, model, t...</td>\n",
              "      <td>[topics, topic, lda, dirichlet, hierarchical, ...</td>\n",
              "      <td>[dirichlet, topics, gibbs, lda, posterior, nmf...</td>\n",
              "      <td>[paper proposes novel dynamic hierarchical dir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>45</td>\n",
              "      <td>17_causal_variables_data_observational</td>\n",
              "      <td>[causal, variables, data, observational, infer...</td>\n",
              "      <td>[causal, causality, confounders, inference, es...</td>\n",
              "      <td>[causal, observational, inference, causality, ...</td>\n",
              "      <td>[important topic systems biology developing st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>44</td>\n",
              "      <td>18_treatment_disease_patient_health</td>\n",
              "      <td>[treatment, disease, patient, health, patients...</td>\n",
              "      <td>[clinical, patients, outcomes, healthcare, cla...</td>\n",
              "      <td>[health, patients, covariates, survival, mood,...</td>\n",
              "      <td>[devising course treatment patient doctors oft...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>32</td>\n",
              "      <td>19_mixture_model_models_clustering</td>\n",
              "      <td>[mixture, model, models, clustering, latent, t...</td>\n",
              "      <td>[bayesian, clustering, probabilistic, posterio...</td>\n",
              "      <td>[mixture, models, clustering, bayesian, kmeans...</td>\n",
              "      <td>[hierarchical learning models mixture models b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f22ca10-d76d-4377-92dc-65d7bf8f877a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2f22ca10-d76d-4377-92dc-65d7bf8f877a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2f22ca10-d76d-4377-92dc-65d7bf8f877a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a47ed201-7680-44a8-8cc0-7c07a363c0db\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a47ed201-7680-44a8-8cc0-7c07a363c0db')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a47ed201-7680-44a8-8cc0-7c07a363c0db button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"topic_model\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"Topic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 0,\n        \"max\": 19,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0,\n          17,\n          15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 32,\n        \"max\": 150,\n        \"num_unique_values\": 19,\n        \"samples\": [\n          150,\n          92,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"0_gaussian_process_model_processes\",\n          \"17_causal_variables_data_observational\",\n          \"15_series_time_forecasting_model\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representation\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"KeyBERT\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MMR\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representative_Docs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "execution_count": 98
    },
    {
      "cell_type": "code",
      "source": [
        "# Show top keywords for specific Topic\n",
        "topic_model.get_topic(1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:49.9879Z",
          "iopub.status.idle": "2025-05-13T21:01:49.988205Z",
          "shell.execute_reply.started": "2025-05-13T21:01:49.98804Z",
          "shell.execute_reply": "2025-05-13T21:01:49.988052Z"
        },
        "id": "A-zdNzOucYLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342a1113-d865-47f1-dcc2-9864c531beac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('matrix', np.float64(0.04432528328039532)),\n",
              " ('dictionary', np.float64(0.027532395201076802)),\n",
              " ('algorithm', np.float64(0.026082073785749598)),\n",
              " ('data', np.float64(0.025180198812647936)),\n",
              " ('sparse', np.float64(0.02387081271662058)),\n",
              " ('tensor', np.float64(0.022564080474555014)),\n",
              " ('subspace', np.float64(0.020480526917789912)),\n",
              " ('problem', np.float64(0.019544534767337578)),\n",
              " ('decomposition', np.float64(0.018440210165769556)),\n",
              " ('lowrank', np.float64(0.017763061428544016))]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "execution_count": 99
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizations"
      ],
      "metadata": {
        "id": "zYY4x3vAcYLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# note: outputs may differ and not save\n",
        "try:\n",
        "  topic_model.visualize_topics()\n",
        "except Exception as error:\n",
        "  print(\"Unable to display visualization. The count for the first topic is likely most of the documents. Please try fitting the model again.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:49.9895Z",
          "iopub.status.idle": "2025-05-13T21:01:49.989799Z",
          "shell.execute_reply.started": "2025-05-13T21:01:49.989639Z",
          "shell.execute_reply": "2025-05-13T21:01:49.98965Z"
        },
        "id": "sWALQgTWcYLy"
      },
      "outputs": [],
      "execution_count": 100
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_documents(abstracts)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:49.990995Z",
          "iopub.status.idle": "2025-05-13T21:01:49.991327Z",
          "shell.execute_reply.started": "2025-05-13T21:01:49.991156Z",
          "shell.execute_reply": "2025-05-13T21:01:49.991168Z"
        },
        "id": "f0IOHeLRcYLy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "outputId": "f6b4a832-72a7-4f8e-84da-d163b07370d5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"868988f9-29f2-458f-b420-a49f78c12e59\" class=\"plotly-graph-div\" style=\"height:750px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"868988f9-29f2-458f-b420-a49f78c12e59\")) {                    Plotly.newPlot(                        \"868988f9-29f2-458f-b420-a49f78c12e59\",                        [{\"hoverinfo\":\"text\",\"hovertext\":[null],\"marker\":{\"color\":\"#CFD8DC\",\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"other\",\"showlegend\":false,\"x\":[null],\"y\":[null],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"deep gaussian processes dgps multilayer hierarchical generalisations gaussian processes gps formally equivalent neural networks multiple infinitely wide hidden layers dgps probabilistic nonparametric arguably flexible greater capacity generalise provide better calibrated uncertainty estimates alternative deep models focus paper scalable approximate bayesian learning networks paper develops novel efficient extension probabilistic backpropagation stateoftheart method training bayesian neural networks used train dgps new method leverages recently proposed method scaling expectation propagation called stochastic expectation propagation method able automatically discover useful input warping expansion compression therefore flexible form bayesian kernel design demonstrate success new method supervised learning several realworld datasets showing typically outperforms regression never much worse\",\"multivariate categorical data occur many applications machine learning one main difficulties vectors categorical variables sparsity number possible observations grows exponentially vector length dataset diversity might poor comparison recent models gained significant improvement supervised tasks data models embed observations continuous space capture similarities building ideas propose bayesian model unsupervised task distribution estimation multivariate categorical data model vectors categorical variables generated nonlinear transformation continuous latent space nonlinearity captures multimodality distribution continuous representation addresses sparsity model ties together many existing models linking linear categorical latent gaussian model gaussian process latent variable model gaussian process classification derive inference model based recent developments sampling based variational inference show empirically model outperforms linear discrete counterparts imputation tasks sparse data\",\"model criticism usually carried assessing replicated data generated fitted model looks similar observed data see gelman carlin stern rubin paper presents method latent variable models pulling back data space latent variables carrying model criticism space making use models structure enables direct assessment assumptions made prior likelihood demonstrate method examples model criticism latent space applied factor analysis linear dynamical systems gaussian processes\",\"conventional ode modelling coefficients equation driving system state forward time estimated however many complex systems practically impossible determine equations interactions governing underlying dynamics settings parametric ode model cannot formulated overcome issue introducing novel paradigm nonparametric ode modelling learn underlying dynamics arbitrary continuoustime systems without prior knowledge propose learn nonlinear unknown differential functions state observations using gaussian process vector fields within exact ode formalism demonstrate models capabilities infer dynamics sparse data simulate system forward future\",\"method large scale gaussian process classification recently proposed based expectation propagation method allows gaussian process classifiers trained large datasets reach previous deployments shown competitive related techniques based stochastic variational inference nevertheless memory resources required scale linearly dataset size unlike variational methods severe limitation number instances large show problem avoided stochastic used train model\",\"dropoutbased regularization methods regarded injecting random noise predefined magnitude different parts neural network training recently shown bayesian dropout procedure improves generalization also leads extremely sparse neural architectures automatically setting individual noise magnitude per weight however sparsity hardly used acceleration since unstructured paper propose new bayesian model takes account computational structure neural networks provides structured sparsity removes neurons andor convolutional channels cnns inject noise neurons outputs keeping weights unregularized establish probabilistic model proper truncated loguniform prior noise truncated lognormal variational approximation ensures klterm evidence lower bound computed closedform model leads structured sparsity removing elements low snr computation graph provides significant acceleration number deep neural architectures model easy implement formulated separate dropoutlike layer\",\"dropout recently emerged powerful simple method training neural networks preventing coadaptation stochastically omitting neurons dropout currently grounded explicit modelling assumptions far precluded adoption bayesian modelling using bayesian entropic reasoning show dropout interpreted optimal inference constraints demonstrate analytically tractable regression model providing bayesian interpretation mechanism regularizing preventing coadaptation well connection bayesian techniques also discuss two general approximate techniques applying bayesian dropout general models one based analytical approximation stochastic variational techniques techniques applied baysian logistic regression problem shown improve performance model become misspecified framework roots dropout theoretically justified practical tool statistical modelling allowing bayesians tap benefits dropout training\",\"paper develop method learning nonlinear systems multiple outputs inputs begin modelling errors nominal predictor system using latent variable framework using maximum likelihood principle derive criterion learning model resulting optimization problem tackled using majorizationminimization approach finally develop convex majorization technique show enables recursive identification method method learns parsimonious predictive models tested synthetic real nonlinear systems\",\"zeroinflated datasets excess zero outputs commonly encountered problems climate rare event modelling conventional machine learning approaches tend overestimate nonzeros leading poor performance propose novel model family zeroinflated gaussian processes zigp zeroinflated datasets produced sparse kernels learning latent probit gaussian process zero kernel rows columns whenever signal absent zigps particularly useful making powerful gaussian process networks interpretable introduce sparse networks variableorder latent modelling achieved sparse mixing signals derive nontrivial stochastic variational inference tractably scalable learning sparse kernels models novel outputsparse approach improves prediction zeroinflated data interpretability latent mixing models\",\"many modern data sets sampled error complex highdimensional surfaces methods tensor product splines gaussian processes effectivewell suited characterizing surface two three dimensions may suffer difficulties representing higher dimensional surfaces motivated high throughput toxicity testing observed doseresponse curves cross sections surface defined chemicals structural properties model developed characterize surface predict untested chemicals doseresponses manuscript proposes novel approach models multidimensional surface sum learned basis functions formed tensor product lower dimensional functions representable basis expansion learned data model described gibbs sampling algorithm proposed investigated simulation study well data taken epas toxcast high throughput toxicity testing platform\",\"consider probabilistic multinomial probit classification using gaussian process priors challenges multiclass classification integration nongaussian posterior distribution increase number unknown latent variables number target classes grows expectation propagation proven accurate method approximate inference existing approaches multinomial probit classification rely numerical quadratures independence assumptions latent values different classes facilitate computations paper propose novel nested approach require numerical quadratures approximates accurately betweenclass posterior dependencies latent values still scales linearly number classes predictive accuracy nested approach compared laplace variational bayes markov chain monte carlo mcmc approximations various benchmark data sets experiments nested consistent method respect mcmc sampling differences compared methods small classification accuracy concerned\",\"ordinary stochastic neural networks mostly rely expected values weights make predictions whereas induced noise mostly used capture uncertainty prevent overfitting slightly boost performance testtime averaging paper introduce variance layers different kind stochastic layers weight variance layer follows zeromean distribution parameterized variance show layers learn surprisingly well serve efficient exploration tool reinforcement learning tasks provide decent defense adversarial attacks also show number conventional bayesian neural networks naturally converge zeromean posteriors observe cases zeromean parameterization leads much better training objective conventional parameterizations mean learned\",\"significant success reported recently using deep neural networks classification large networks computationally intensive even training implementing trained networks hardware chips limited precision synaptic weights may improve speed energy efficiency several orders magnitude thus enabling integration small lowpower electronic devices motivation develop computationally efficient learning algorithm multilayer neural networks binary weights assuming hidden neurons fanout one algorithm derived within bayesian probabilistic online setting shown work well synthetic realworld problems performing comparably algorithms realvalued weights retaining computational tractability\",\"scale gaussian processes gps large data sets introduce robust bayesian committee machine rbcm practical scalable productofexperts model largescale distributed regression unlike stateoftheart sparse approximations rbcm conceptually simple rely inducing variational parameters key idea recursively distribute computations independent computational units subsequently recombine form overall result efficient closedform inference allows straightforward parallelisation distributed computations small memory footprint rbcm independent computational graph used heterogeneous computing infrastructures ranging laptops clusters sufficient computing resources distributed model handle arbitrarily large data sets\",\"recently increasing interest methods deal multiple outputs motivated partly frameworks like multitask learning multisensor networks structured output data gaussian processes perspective problem reduces specifying appropriate covariance function whilst positive semidefinite captures dependencies data points across outputs one approach account nontrivial correlations outputs employs convolution processes latent function interpretation convolution transform establish dependencies output variables main drawbacks approach associated computational storage demands paper address issues present different sparse approximations dependent output gaussian processes constructed convolution formalism exploit conditional independencies present naturally model leads form covariance similar spirit called pitc fitc approximations single output show experimental results synthetic real data particular show results pollution prediction school exams score prediction gene expression data\",\"despite fundamental nature inhomogeneous poisson process theory application stochastic processes attractive generalizations cox process tractable nonparametric modeling approaches intensity functions exist especially observed points lie highdimensional space paper develop new computationally tractable reproducing kernel hilbert space rkhs formulation inhomogeneous poisson process model square root intensity rkhs function whereas rkhs models used supervised learning rely socalled representer theorem form inhomogeneous poisson process likelihood means representer theorem apply however prove representer theorem hold appropriately transformed rkhs guaranteeing optimization penalized likelihood cast tractable finitedimensional problem resulting approach simple implement readily scales high dimensions largescale datasets\",\"use statistical learning methods construct adaptive state estimator nonlinear stochastic systems optimal state estimation form kalman filter requires knowledge systems process measurement uncertainty propose uncertainties estimated conditioned past observed data without making assumptions systems prior distribution systems prior distribution time step constructed ensemble leastsquares estimates subsampled sets data via jackknife sampling new data acquired state estimates process uncertainty measurement uncertainty updated accordingly described manuscript\",\"neural network based generative models discriminative components powerful approach semisupervised learning however techniques cannot account model uncertainty estimation models discriminative component lack flexibility capture complex stochastic patterns label generation process avoid problems first propose use discriminative component stochastic inputs increased noise flexibility show efficient gibbs sampling procedure marginalize stochastic inputs inferring missing labels model following extend discriminative component fully bayesian produce estimates uncertainty parameter values opens door semisupervised bayesian active learning\",\"latent force models lfm principled approaches incorporating solutions differential equations within nonparametric inference methods unfortunately development application lfms inhibited computational cost especially closedform solutions lfm unavailable case many real world problems latent forces exhibit periodic behaviour given develop new sparse representation lfms considerably improves computational efficiency well broadening applicability principled way domains periodic near periodic latent forces approach uses linear basis model approximate one generative model periodic force assume latent forces generated gaussian process priors develop linear basis model fully expresses priors apply approach model thermal dynamics domestic buildings show effective predicting dayahead temperatures within homes also apply approach within queueing theory quasiperiodic arrival rates modelled latent forces cases demonstrate approach implemented efficiently using statespace methods encode linear dynamic systems via lfms show state estimates obtained using periodic latent force models reduce root mean squared error nonperiodic models nearest rival approach resonator model\",\"present causal gaussian process convolution model cgpcm doubly nonparametric model causal spectrally complex dynamical phenomena cgpcm generative model white noise passed causal nonparametricwindow movingaverage filter construction show equivalent gaussian process nonparametric kernel biased towards causallygenerated signals develop enhanced variational inference learning schemes cgpcm previous acausal variant gpcm tobar significantly improve statistical accuracy modelling inferential contributions demonstrated range synthetic realworld signals\",\"paper presents simulation free framework solving reliability analysis problems method proposed rooted recently developed deep learning approach referred physicsinformed neural network primary idea learn neural network parameters directly physics problem need running simulation generating data completely eliminated additionally proposed approach also satisfies physical laws invariance properties conservation laws associated problem proposed approach used solving three benchmark reliability analysis problems results obtained illustrates proposed approach highly accurate moreover primary bottleneck solving reliability analysis problems running expensive simulations generate data eliminated method\",\"large amount observational data accumulated various fields recent times growing need estimate generating processes data linear nongaussian acyclic model lingam based nongaussianity external influences proposed estimate datagenerating processes variables however results estimation biased latent classes paper first review lingam extended model well estimation procedure lingam bayesian framework propose new bayesian estimation procedure solves problem\",\"exact inference linear regression model spike slab priors often intractable expectation propagation used approximate inference however regular sequential form rep may fail converge model size training set small alternative propose provably convergent algorithm pcep pcep proved minimize energy function constraints bounded whose stationary points coincide solution rep experiments synthetic data indicate rep converge approximation generated pcep often better contrast rep converges methods perform similarly\",\"paper propose first nonparametric bayesian model using gaussian processes make inference poisson point processes without resorting gridding domain introducing latent thinning points unlike competing models scale cubically squared memory requirement number data points model linear complexity memory requirement propose mcmc sampler show model faster accurate generates less correlated samples competing models synthetic reallife data finally show model easily handles data sizes considered thus far alternate approaches\",\"multioutput gaussian processes received increasing attention last years natural mechanism extend powerful flexibility gaussian processes setup multiple output variables key point ability design kernel functions allow exploiting correlations outputs fulfilling positive definiteness requisite covariance function alternatives construct covariance functions linear model coregionalization process convolutions methods demand specification number latent gaussian process used build covariance function outputs propose paper use indian buffet process way perform model selection number latent gaussian processes type model particularly important context latent force models latent forces associated physical quantities like protein profiles latent forces mechanical systems use variational inference estimate posterior distributions variables involved show examples model performance artificial data motion capture dataset gene expression dataset\",\"noise injection efficient technique mitigate overfitting neural networks nns bernoulli procedure implemented dropout shakeout connections regularization model parameters propose whiteout family regularization techniques nirt injecting adaptive gaussian noises training nns whiteout first nirt imposes broad range lgamma sparsity regularization gammain without involving regularization whiteout also extended offer regularizations similar adaptive lasso group lasso establish regularization effect whiteout framework generalized linear models closedform penalty terms show whiteout stabilizes training nns decreased sensitivity small perturbations input establish noiseperturbed empirical loss function pelf whiteout converges almost surely ideal loss function ilf minimizer pelf consistent minimizer ilf derive tail bound pelf establish practical feasibility minimization superiority whiteout bernoulli nirts dropout shakeout learning nns relatively smallsized training sets noninferiority largesized training sets demonstrated simulated reallife data sets work represents first indepth theoretical methodological practical examination regularization effects additive multiplicative gaussian deep nns\",\"multiresolution gaussian process gained increasing attention viable approach towards improving quality approximations gps scale well largescale data current constructions assume full independence across resolutions assumption simplifies inference underestimates uncertainties transitioning one resolution another turn results models prone overfitting sense excessive sensitivity chosen resolution predictions nonsmooth boundaries contribution new construction instead assumes conditional independence among gps across resolutions show relaxing full independence assumption enables robustness overfitting delivers predictions smooth boundaries new model compared current state art synthetic realworld datasets cases new conditionally independent construction performed favorably compared models based full independence assumption particular exhibits little signs overfitting\",\"variational methods recently considered scaling training process gaussian process classifiers large datasets alternative describe train classifiers efficiently using expectation propagation proposed method allows handling datasets millions data instances precisely used training distributed fashion data instances sent different nodes required computations carried maximizing estimate marginal likelihood using stochastic approximation gradient several experiments indicate method described competitive variational approach\",\"consider inverse problem reconstructing posterior measure trajec tories diffusion process discrete time observations continuous time constraints cast problem bayesian framework derive approximations posterior distributions single time marginals using variational approximate inference show approximation extended wide class discretestate markov jump pro cesses making use chemical langevin equation empirical results show proposed method computationally efficient provides good approximations classes inverse problems\",\"interested solving multiple measurement vector mmv problem instances underlying sparsity pattern exhibit spatiotemporal structure motivated electroencephalogram eeg source localization problem propose probabilistic model takes structure account generalizing structured spike slab prior associated expectation propagation inference scheme based numerical experiments demonstrate viability model approximate inference scheme\",\"paper new bayesian model sparse linear regression spatiotemporal structure proposed incorporates structural assumptions based hierarchical gaussian process prior spike slab coefficients design inference algorithm based expectation propagation evaluate model real data\",\"gaussian process models form core part probabilistic machine learning considerable research effort made attacking three issues models compute efficiently number data large approximate posterior likelihood gaussian estimate covariance function parameter posteriors paper simultaneously addresses using variational approximation posterior sparse support function otherwise freeform result hybrid montecarlo sampling scheme allows nongaussian approximation function values covariance parameters simultaneously efficient computations based inducingpoint sparse gps code replicate experiment paper available shortly\",\"paper introduces kernelbased information criterion kic model selection regression analysis novel kernelbased complexity measure kic efficiently computes interdependency parameters model using variablewise variance yields selection better robust regressors experimental results show superior performance simulated real data sets compared leaveoneout crossvalidation loocv kernelbased information complexity icomp maximum log marginal likelihood gaussian process regression gpr\",\"work brings together two powerful concepts gaussian processes variational approach sparse approximation spectral representation gaussian processes gives rise approximation inherits benefits variational approach representational power computational scalability spectral representations work hinges key result exist spectral features related finite domain gaussian process exhibit almostindependent covariances derive expressions matern kernels one dimension generalize dimensions using kernels specific structures assumption additive gaussian noise method requires single pass dataset making fast accurate computation fit model million training points minutes standard laptop nonconjugate likelihoods mcmc scheme reduces cost computation onm sparse gaussian process onm per iteration number data number features\",\"present novel approach fully nonstationary gaussian process regression gpr three key parameters noise variance signal variance lengthscale simultaneously inputdependent develop gradientbased inference methods learn unknown function nonstationary model parameters without requiring model approximations propose infer full parameter posterior hamiltonian monte carlo hmc conveniently extends analytical gradientbased gpr learning guiding sampling model gradients also learn map solution posterior gradient ascent experiments several synthetic datasets modelling temporal gene expression nonstationary gpr shown necessary modeling realistic inputdependent dynamics performs comparably conventional stationary previous nonstationary gpr models otherwise\",\"basis adaptation homogeneous chaos spaces rely suitable rotation underlying gaussian germ several rotations proposed literature resulting adaptations different convergence properties paper present new adaptation mechanism builds compressive sensing algorithms resulting reduced polynomial chaos approximation optimal sparsity developed adaptation algorithm consists twostep optimization procedure computes optimal coefficients input projection matrix low dimensional chaos expansion respect optimally rotated basis demonstrate attractive features algorithm several numerical examples including application largeeddy simulation les calculations turbulent combustion hifire scramjet engine\",\"gaussian processes gps powerful nonparametric function estimators however applications largely limited expensive computational cost inference procedures existing stochastic distributed synchronous variational inferences although alleviated issue scaling gps millions samples still far satisfactory realworld large applications data sizes often orders magnitudes larger say billions solve problem propose advgp first asynchronous distributed variational gaussian process inference regression recent largescale machine learning platform parameterserver advgp uses novel flexible variational framework based weight space augmentation implements highly efficient asynchronous proximal gradient optimization maintaining comparable better predictive performance advgp greatly improves upon efficiency existing variational methods advgp effortlessly scale regression realworld application billions samples demonstrate excellent superior prediction accuracy popular linear models\",\"gaussian process classification popular method number appealing properties show scale model within variational inducing point framework outperforming state art benchmark datasets importantly variational formulation exploited allow classification problems millions data points demonstrate experiments\",\"gaussian process models also called kriging models often used mathematical approximations expensive experiments however number observation required building emulator becomes unrealistic using classical covariance kernels dimension input increases oder get round curse dimensionality popular approach consider simplified models additive models ambition present work give insight covariance kernels well suited building additive kriging models describe properties resulting models\",\"gradient matching promising tool learning parameters state dynamics ordinary differential equations grid free inference approach fully observable systems times competitive numerical integration however many realworld applications sparse observations available even unobserved variables included model description cases gradient matching methods difficult apply simply provide satisfactory results despite high computational cost numerical integration still gold standard many applications using existing gradient matching approach propose scalable variational inference framework infer states parameters simultaneously offers computational speedups improved accuracy works well even model misspecifications partially observable system\",\"deep learning applies hierarchical layers hidden variables construct nonlinear high dimensional predictors goal develop train deep learning architectures spatiotemporal modeling training deep architecture achieved stochastic gradient descent sgd dropout parameter regularization goal minimizing outofsample predictive mean squared error illustrate methodology predict sharp discontinuities traffic flow data secondly develop classification rule predict shortterm futures market prices function order book depth finally conclude directions future research\",\"study largescale spatial systems contain exogenous variables environmental factors significant predictors spatial processes building predictive models processes challenging large numbers observations present makes inefficient apply full kriging order reduce computational complexity paper proposes sparse pseudoinput local kriging splk utilizes hyperplanes partition domain smaller subdomains applies sparse approximation full kriging subdomain also develop optimization procedure find desired hyperplanes alleviate problem discontinuity global predictor impose continuity constraints boundaries neighboring subdomains furthermore partitioning domain smaller subdomains makes possible use different parameter values covariance function region therefore heterogeneity data structure effectively captured numerical experiments demonstrate splk outperforms comparable algorithms commonly applied spatial datasets\",\"exact gaussian process regression runtime data size making intractable large many algorithms improving scaling approximate covariance lower rank matrices work exploited structure inherent particular covariance functions including gps implied markov structure equispaced inputs enable runtime however advances extended multidimensional input setting despite preponderance multidimensional applications paper introduces tests novel extensions structured gps multidimensional inputs present new methods additive gps showing novel connection classic backfitting method bayesian framework achieve optimal accuracycomplexity tradeoff extend model novel variant projection pursuit regression primary result projection pursuit gaussian process regression shows orders magnitude speedup preserving high accuracy natural second third steps include nongaussian observations higher dimensional equispaced grid methods introduce novel techniques address necessary directions thoroughly illustrate power three advances several datasets achieving close performance naive full orders magnitude less cost\",\"present first framework gaussianprocessmodulated poisson processes temporal data appear form panel counts panel count data frequently arise experimental subjects observed discrete time points numbers occurrences events subsequent observation times available exact occurrence timestamps events unknown method conducting efficient variational inference presented based assumption gaussianprocessmodulated intensity function derive tractable lower bound alleviate problems intractable evidence lower bound inherent variational inference framework algorithm outperforms classical methods synthetic three real panel count sets\",\"propose parallelizable sparse inverse formulation gaussian process spingp temporal models uses sparse precision formulation sparse matrix routines speed computations due statespace formulation used algorithm time complexity basic spingp linear computations parallelizable parallel form algorithm sublinear number data points provide example algorithms implement sparse matrix routines experimentally test method using simulated real data\",\"statespace models ssms highly expressive model class learning patterns time series data system identification deterministic versions ssms lstms proved extremely successful modeling complex time series data fully probabilistic ssms however often found hard train even smaller problems overcome limitation propose novel model formulation scalable training algorithm based doubly stochastic variational inference gaussian processes contrast existing work proposed variational approximation allows one fully capture latent state temporal correlations correlations key robust training effectiveness proposed prssm evaluated set realworld benchmark datasets comparison stateoftheart probabilistic model learning methods scalability robustness demonstrated high dimensional problem\",\"paper introduce novel framework making exact nonparametric bayesian inference latent functions particularly suitable big data tasks firstly introduce class stochastic processes refer string gaussian processes string gps mistaken gaussian processes operating text construct string gps finitedimensional marginals exhibit suitable local conditional independence structures allow scalable distributed flexible nonparametric bayesian inference without resorting approximations ensuring mild global regularity constraints furthermore string priors naturally cope heterogeneous input data gradient learned latent function readily available explanatory analysis secondly provide theoretical results relating approach standard paradigm particular prove string gps gaussian processes provides complementary global perspective framework finally derive scalable distributed mcmc scheme supervised learning tasks string priors proposed mcmc scheme computational time complexity mathcalon memory requirement mathcalodn data size dimension input space illustrate efficacy proposed approach several synthetic realworld datasets including dataset millions input points attributes\",\"deep neural networks dnns excellent representative power state art classifiers many tasks however often capture uncertainties well making less robust real world overconfidently extrapolate notice domain shift gaussian processes gps rbf kernels hand better calibrated uncertainties overconfidently extrapolate far data training set however gps poor representational power perform well dnns complex domains paper show hybrid deep networks gpdnns gps top dnns trained endtoend inherit nice properties gps dnns much robust adversarial examples extrapolating adversarial examples testing domain shift settings gpdnns frequently output high entropy class probabilities corresponding essentially dont know gpdnns therefore promising deep architectures know dont know\",\"paper propose outlierrobust regularized kernelbased method linear system identification unknown impulse response modeled zeromean gaussian process whose covariance kernel given recently proposed stable spline kernel encodes information regularity exponential stability build robustness outliers model measurement noise realizations independent laplacian random variables identification problem cast bayesian framework solved new markov chain monte carlo mcmc scheme particular exploiting representation laplacian random variables scale mixtures gaussians design gibbs sampler quickly converges target distribution numerical simulations show substantial improvement accuracy estimates stateoftheart kernelbased methods\",\"study introduce new technique symbolic regression guarantees global optimality achieved formulating mixed integer nonlinear program minlp whose solution symbolic mathematical expression minimum complexity explains observations demonstrate approach rediscovering keplers law planetary motion using exoplanet data galileos pendulum periodicity equation using experimental data\",\"paper develop statistical theory implementation deep learning models show elegant variable splitting scheme alternating direction method multipliers optimises deep learning objective allow nonsmooth nonconvex regularisation penalties induce sparsity parameter weights provide link traditional shallow layer statistical models principal component sliced inverse regression deep layer models also define degrees freedom deep learning predictor predictive mse criteria perform model selection comparing architecture designs focus deep multiclass logistic learning although methods apply generally results suggest interesting previously underexploited relationship deep learning proximal splitting techniques illustrate methodology provide multiclass logit classification analysis fishers iris data illustrate convergence algorithm finally conclude directions future research\",\"investigate capabilities limitations gaussian process models jointly exploring three complementary directions scalable statistically efficient inference flexible kernels iii objective functions hyperparameter learning alternative marginal likelihood approach outperforms previously reported methods standard mnist dataset performs comparatively previous kernelbased methods using rectanglesimage dataset breaks errorrate barrier models using mnistm dataset showing along way scalability method unprecedented scale models million observations classification problems overall approach represents significant breakthrough kernel methods models bridging gap deep learning approaches kernel machines\",\"training gaussian processbased models typically involves computational bottleneck due inverting covariance matrix popular methods overcoming matrix inversion problem cannot adequately model types latent functions often parallelizable however judicious choice model structure ameliorate problem mixtureofexperts model uses mixture gaussian processes offers modeling flexibility opportunities scalable inference embarrassingly parallel algorithm combines lowdimensional matrix inversions importance sampling yield flexible scalable mixtureofexperts model offers comparable performance gaussian process regression much lower computational cost\",\"study statistical calibration adjusting features computational model observable controllable associated physical system focus functional calibration arises many manufacturing processes unobservable features called calibration variables function input variables major challenge many applications computational models expensive evaluated limited number times furthermore without making strong assumptions calibration variables identifiable propose bayesian nonisometric matching calibration bnmc allows calibration expensive computational models limited number samples taken computational model associated physical system bnmc replaces computational model dynamic gaussian process whose parameters trained calibration procedure resolve identifiability issue present calibration problem geometric perspective nonisometric curve surface matching enables take advantage combinatorial optimization techniques extract necessary information constructing prior distributions numerical experiments demonstrate terms prediction accuracy bnmc outperforms comparable existing calibration frameworks\",\"unscented transformation efficient method solve state estimation problem nonlinear dynamic system utilizing derivativefree higherorder approximation approximating gaussian distribution rather approximating nonlinear function applying kalman filter type estimator leads wellknown unscented kalman filter ukf although ukf works well gaussian noises performance may deteriorate significantly noises nongaussian especially system disturbed heavytailed impulsive noises improve robustness ukf impulsive noises new filter nonlinear systems proposed work namely maximum correntropy unscented filter mcuf mcuf applied obtain prior estimates state covariance matrix robust statistical linearization regression based maximum correntropy criterion mcc used obtain posterior estimates state covariance satisfying performance new algorithm confirmed two illustrative examples\",\"hyperparameters gaussian process regression gpr model specified kernel often estimated data via maximum marginal likelihood due nonconvexity marginal likelihood respect hyperparameters optimization may converge global maxima common approach tackle issue use multiple starting points randomly selected specific prior distribution result choice prior distribution may play vital role predictability approach however exists little research literature study impact prior distributions hyperparameter estimation performance gpr paper provide first empirical study problem using simulated real data experiments consider different types priors initial values hyperparameters commonly used kernels investigate influence priors predictability gpr models results reveal kernel chosen different priors initial hyperparameters significant impact performance gpr prediction despite estimates hyperparameters different true values cases\",\"gradient matching gaussian processes promising tool learning parameters ordinary differential equations odes essence gradient matching model prior state variables gaussian process implies joint distribution given odes kernels also gaussian distributed statederivatives integrated analytically since modelled latent variables however state variables also latent variables contaminated noise previous work sampled state variables since integrating textitnot analytically tractable paper use meanfield approximation establish tight variational lower bounds decouple state variables therefore contrast integral state variables analytically tractable even concave restricted family odes including nonlinear periodic odes variational lower bounds facilitate hill climbing determine maximum posteriori estimate ode parameters additional advantage approach sampling methods determination proxy intractable posterior distribution state variables given observations odes\",\"kalman filter used variety applications computing posterior distribution latent states state space model model requires linear relationship states observations extensions kalman filter proposed incorporate linear approximations nonlinear models extended kalman filter ekf unscented kalman filter ukf however argue cases dimensionality observed variables greatly exceeds dimensionality state variables model ptextstatetextobservation proves easier learn accurate latent space estimation derive validate call discriminative kalman filter dkf closedform discriminative version bayesian filtering readily incorporates offtheshelf discriminative learning techniques demonstrate given mild assumptions highly nonlinear models ptextstatetextobservation specified motivate validate synthetic datasets neural decoding nonhuman primates showing substantial increases decoding performance versus standard kalman filter\",\"gaussian process models provide powerful tool prediction computationally prohibitive using large data sets scenarios one resort approximate methods derive approximation based composite likelihood approach using general belief updating framework leads recursive computation predictor well learning hyperparameters provide analysis derived composite model predictive informationtheoretic terms finally evaluate approximation synthetic data realworld application\",\"dynamic mode decomposition dmd emerged powerful tool analyzing dynamics nonlinear systems experimental datasets recently several attempts extended dmd context lowrank approximations extension particular interest reducedorder modeling various applicative domains climate prediction study molecular dynamics microelectromechanical devices lowrank extension takes form nonconvex optimization problem best knowledge suboptimal algorithms proposed literature compute solution problem paper prove exists closedform optimal solution problem design effective algorithm compute based singular value decomposition svd toyexample illustrates gain performance proposed algorithm compared stateoftheart techniques\",\"theoretically discuss deep neural networks dnns performs better models cases investigating statistical properties dnns nonsmooth functions dnns empirically shown higher performance standard methods understanding mechanism still challenging problem aspect statistical theory known many standard methods attain optimal rate generalization errors smooth functions large sample asymptotics thus straightforward find theoretical advantages dnns paper fills gap considering learning certain class nonsmooth functions covered previous theory derive generalization error estimators dnns relu activation show convergence rates generalization dnns almost optimal estimate nonsmooth functions popular models attain optimal rate addition theoretical result provides guidelines selecting appropriate number layers edges dnns provide numerical experiments support theoretical results\",\"since learning typically slow boltzmann machines need restrict connections within hidden layers however resulting states hidden units exhibit statistical dependencies based observation propose using regularization upon activation possibilities hidden units restricted boltzmann machines capture loacal dependencies among hidden units regularization encourages hidden units many groups inactive given observed data also makes hidden units within group compete modeling observed data thus regularization rbms yields sparsity group hidden unit levels call rbms trained regularizer emphsparse group rbms proposed sparse group rbms applied three tasks modeling patches natural images modeling handwritten digits pretaining deep networks classification task furthermore illustrate regularizer also applied deep boltzmann machines lead sparse group deep boltzmann machines adapted mnist data set twolayer sparse group boltzmann machine achieves error rate knowledge best published result permutationinvariant version mnist task\",\"present first fully variational bayesian inference scheme continuous gaussianprocessmodulated poisson processes point processes used variety domains including neuroscience geostatistics astronomy use hindered computational cost existing inference schemes scheme requires discretisation domain scales linearly number observed events many orders magnitude faster previous sampling based approaches resulting algorithm shown outperform standard methods synthetic examples coal mining disaster data prediction malaria incidences kenya\",\"effective training deep neural networks suffers two main issues first parameter spaces models exhibit pathological curvature recent methods address problem using adaptive preconditioning stochastic gradient descent sgd methods improve convergence adapting local geometry parameter space second issue overfitting typically addressed early stopping however recent work demonstrated bayesian model averaging mitigates problem posterior sampled using stochastic gradient langevin dynamics sgld however rapidly changing curvature renders default sgld methods inefficient propose combining adaptive preconditioners sgld support idea give theoretical properties asymptotic convergence predictive risk also provide empirical results logistic regression feedforward neural nets convolutional neural nets demonstrating preconditioned sgld method gives stateoftheart performance models\",\"mapping nearfield pollutant concentration essential track accidental toxic plume dispersion urban areas solving large part turbulence spectrum largeeddy simulations les potential accurately represent pollutant concentration spatial variability finding way synthesize large amount information improve accuracy lowerfidelity operational models providing better turbulence closure terms particularly appealing challenge multiquery contexts les become prohibitively costly deploy understand plume flow tracer dispersion change various atmospheric source parameters overcome issue propose nonintrusive reducedorder model combining proper orthogonal decomposition pod gaussian process regression gpr predict les field statistics interest associated tracer concentrations gpr hyperpararameters optimized componentbycomponent maximum posteriori map procedure informed pod provide detailed analysis reducedorder model performance twodimensional case study corresponding turbulent atmospheric boundarylayer flow surfacemounted obstacle show nearsource concentration heterogeneities upstream obstacle require large number pod modes well captured also show componentbycomponent optimization allows capture range spatial scales pod modes especially shorter concentration patterns highorder modes reducedorder model predictions remain acceptable learning database made least fifty hundred les snapshot providing first estimation required budget move towards realistic atmospheric dispersion applications\",\"present efficient blockdiagonal proximation gaussnewton matrix feedforward neural networks result ing algorithm competitive state oftheart first order optimisation methods sometimes significant improvement optimisation performance unlike firstorder methods hyperparameter tuning optimisation parameters often labo rious process approach provide good performance even used default set tings side result work piecewise linear transfer functions net work objective function differ entiable local maxima may partially explain transfer functions facilitate effective optimisation\",\"provide comprehensive overview tooling modeling nongaussian likelihoods using state space methods state space formulation allows solving onedimensional models mathcalon time memory complexity existing literature focused connection regression state space methods computational primitives allowing inference using general likelihoods combination laplace approximation variational bayes assumed density filtering adf aka singlesweep expectation propagation schemes largely overlooked present means combining efficient mathcalon state space methodology existing inference methods extend existing methods provide unifying code implementing approaches\",\"framework supervised learning real function defined space called kriging method stands real gaussian field defined euclidean case well known widely studied paper explore less classical case non commutative finite group permutations setting propose study harmonic analysis covariance operators enables consider gaussian processes models forecasting issues theory motivated statistical ranking problems\",\"consider modification covariance function gaussian processes correctly account known linear constraints modelling target function transformation underlying function constraints explicitly incorporated model guaranteed fulfilled sample drawn prediction made also propose constructive procedure designing transformation operator illustrate result simulated realdata examples\",\"tutorial explain inference procedures developed sparse gaussian process regression gaussian process latent variable model gplvm due page limit derivation given titsias titsias lawrence brief hence getting full picture requires collecting results several different sources substantial amount algebra fillin gaps main goal thus collect results full derivations one place help speed understanding work present reparametrisation inference allows carried parallel secondary goal document therefore accompany paper opensource implementation parallel inference scheme models hope document bridge gap equations implemented code published original papers order make easier extend existing work assume prior knowledge gaussian processes variational inference also include references reading appropriate\",\"background statistical mechanics results dauphin choromanska suggest local minima high error exponentially rare high dimensions however prove low error guarantees multilayer neural networks mnns previous works far required either heavily modified mnn model training method strong assumptions labels near linear separability unrealistic hidden layer omegaleftnright units results examine mnn one hidden layer piecewise linear units single output quadratic loss prove high probability limit nrightarrowinfty datapoints volume differentiable regions empiric loss containing suboptimal differentiable local minima exponentially vanishing comparison volume global minima given standard normal input dimension dtildeomegaleftsqrtnright realistic number dtildeomegaleftndright hidden units demonstrate results numerically example binary classification training error cifar ndapprox hidden neurons\",\"spatiotemporal point process models play central role analysis spatially distributed systems several disciplines yet scalable inference remains computa tionally challenging due high resolution modelling generally required analytically intractable likelihood function exploit sparsity structure typical spatially discretised loggaussian cox process models using approximate messagepassing algorithms proposed algorithms scale well state dimension length temporal horizon moderate loss distributional accuracy hence provide flexible faster alternative nonlinear filteringsmoothing type algorithms approaches implement laplace method expectation propagation block sparse latent gaussian models infer parameters latent gaussian model using structured variational bayes approach demonstrate proposed framework simulation studies gaussian pointprocess observations use reconstruct conflict intensity dynamics afghanistan wikileaks afghan war diary\",\"sequential neural architectures become deeper complex uncertainty estimation challenging efforts quantifying uncertainty often rely specific training procedures bear additional computational costs due dimensionality models paper propose decompose classification regression task two steps representation learning stage learn lowdimensional states state space model uncertainty estimation approach allows separate representation learning design generative models demonstrate predictive distributions estimated top existing trained neural network adding state spacebased last layer whose parameters estimated sequential monte carlo methods apply proposed methodology hourly estimation electricity transformer oil temperature publicly benchmarked dataset model accounts noisy data structure due unknown unavailable variables able provide confidence intervals predictions\",\"study gaussian process regression model context training data noise input output presence two sources noise makes task learning accurate predictive models extremely challenging however instances additional constraints may available reduce uncertainty resulting predictive models particular consider case monotonically ordered latent input occurs many application domains deal temporal data present novel inference learning approach based nonparametric gaussian variational approximation learn model taking account new constraints resulting strategy allows one gain access posterior estimates input output results improved predictive performance compare proposed models stateoftheart noisy input gaussian process nigp competing approaches synthetic real sealevel rise data experimental results suggest proposed approach consistently outperforms selected methods time reducing computational costs learning inference\",\"gaussian processes gps good choice function approximation flexible robust overfitting provide wellcalibrated predictive uncertainty deep gaussian processes dgps multilayer generalisations gps inference models proved challenging existing approaches inference dgp models assume approximate posteriors force independence layers work well practice present doubly stochastic variational inference algorithm force independence layers method inference demonstrate dgp model used effectively data ranging size hundreds billion points provide strong empirical evidence inference scheme dgps works well practice classification regression\",\"propose novel approach nonlinear regression using twolayer neural network model structure sparsityfavoring hierarchical priors network weights present expectation propagation approach approximate integration posterior distribution weights hierarchical scale parameters priors residual scale using factorized posterior approximation derive computationally efficient algorithm whose complexity scales similarly ensemble independent sparse linear models approach enables flexible definition weight priors different sparseness properties independent laplace priors common scale parameter gaussian automatic relevance determination ard priors different relevance parameters inputs approach extended beyond standard activation functions model structures form flexible nonlinear predictors multiple sparse linear models effects hierarchical priors predictive performance algorithm assessed using simulated realworld data comparisons made two alternative models ard priors gaussian process covariance function marginal maximum posteriori estimates relevance parameters markov chain monte carlo integration unknown model parameters\",\"compression neural networks become highly studied topic recent years main reason demand industrial scale usage nns deploying mobile devices storing efficiently transmitting via bandlimited channels importantly inference scale work propose join softweight sharing variational dropout approaches show strong results define new stateoftheart terms model compression\",\"sparse pseudopoint approximations gaussian process models provide suite methods support deployment gps large data regime enable analytic intractabilities sidestepped however field lacks principled method handle streaming data posterior distribution function values hyperparameter estimates updated online fashion small number existing approaches either use suboptimal handcrafted heuristics hyperparameter learning suffer catastrophic forgetting slow updating new data arrive paper develops new principled framework deploying gaussian process probabilistic models streaming setting providing methods learning hyperparameters optimising pseudoinput locations proposed framework assessed using synthetic realworld datasets\",\"modeling sequential data become important practice applications autonomous driving virtual sensors weather forecasting model systems called recurrent models used article introduce two new deep recurrent gaussian process drgp models based sparse spectrum gaussian process ssgp improved variational version called variational sparse spectrum gaussian process vssgp follow recurrent structure given existing drgp based specific sparse nystrom approximation therefore also variationally integrate inputspace hence propagate uncertainty layers show resulting lower bound optimal variational distribution exists training realized optimizing variational lower bound using distributed variational inference dvi reduce computational complexity improve current state art methods prediction accuracy experimental datasets used evaluation introduce new dataset engine control named emission furthermore method easily adapted unsupervised learning latent variable model deep version\",\"standard sparse pseudoinput approximations gaussian process cannot handle complex functions well sparse spectrum alternatives attempt answer known overfit suggest use variational inference sparse spectrum approximation avoid issues model covariance function finite fourier series approximation treat random variable random covariance function posterior variational distribution placed variational distribution transforms random covariance function fit data study properties approximate inference compare alternative ones extend distributed stochastic domains approximation captures complex functions better standard approaches avoids overfitting\",\"paper presents novel approach approximate integration uncertainty noise signal variances gaussian process regression efficient straightforward approach also applied integration input dependent noise variance heteroscedasticity input dependent signal variance nonstationarity setting independent priors noise signal variances use expectation propagation inference compare results markov chain monte carlo two simulated data sets three empirical examples results show produces comparable results less computational burden\",\"good sparse approximations essential practical inference gaussian processes computational cost exact methods prohibitive large datasets fully independent training conditional fitc variational free energy vfe approximations two recent popular methods despite superficial similarities approximations surprisingly different theoretical properties behave differently practice thoroughly investigate two methods regression analytically illustrative examples draw conclusions guide practical application\",\"connection bayesian neural networks gaussian processes gained lot attention last years flagship result hidden units converge gaussian process limit layers width tends infinity underpinning result fact hidden units become independent infinitewidth limit aim shed light hidden units dependence properties practical finitewidth bayesian neural networks addition theoretical results assess empirically depth width impacts hidden units dependence properties\",\"inverse problems arise anywhere indirect measurement general illposed obtain satisfactory solutions needs prior knowledge classically different regularization methods bayesian inference based methods proposed methods need great number forward backward computations become costly computation particular forward generative models complex evaluation likelihood becomes costly using deep neural network surrogate models approximate computation become helpful however accounting uncertainties need first understand bayesian deep learning see use inverse problems work focus specifically bayesian particularly adapted inverse problems first give details bayesian approximate computations exponential families see use inverse problems consider two cases first case forward operator known used physics constraint second general data driven methods keyword neural network variational bayesian inference bayesian deep learning inverse problems physics based\",\"gaussian processes powerful yet analytically tractable models supervised learning gaussian process characterized mean function covariance function kernel determined model selection criterion functions compared differ parametrization fundamental structure often clear function structure choose instance decide squared exponential rational quadratic kernel based principle approximation set coding develop framework model selection rank kernels gaussian process regression experiments approximation set coding shows promise become model selection criterion competitive maximum evidence also called marginal likelihood leaveoneout crossvalidation\",\"paper proposes novel scheme reducedrank gaussian process regression method based approximate series expansion covariance function terms eigenfunction expansion laplace operator compact subset mathbbrd approximate eigenbasis eigenvalues covariance function expressed simple functions spectral density gaussian process allows inference solved computational cost scaling mathcalonm initial mathcalom hyperparameter learning basis functions data points furthermore basis functions independent parameters covariance function allows fast hyperparameter learning approach also allows rigorous error analysis hilbert space theory show approximation becomes exact size compact subset number eigenfunctions infinity also show convergence rate truncation error independent input dimensionality provided differentiability order covariance function increases appropriately squared exponential covariance function always bounded simm regardless input dimensionality expansion generalizes hilbert spaces inner product defined integral specified input density method compared previously proposed methods theoretically empirical tests simulated real data\",\"reduced modeling computationally demanding dynamical system aims approximating trajectories optimizing tradeoff accuracy computational complexity work propose achieve approximation first embedding trajectories reproducing kernel hilbert space rkhs exhibits appealing approximation computational capabilities solving associated reduced model problem specifically propose new efficient algorithm datadriven reduced modeling nonlinear dynamics based linear approximations rkhs algorithm takes advantage closedform solution lowrank constraint optimization problem exploiting advantageously kernelbased computations reduced modeling algorithm reveals gain approximation accuracy shown numerical simulations complexity respect existing approaches\",\"present application conformal prediction form uncertainty quantification guarantees detection railway signals stateoftheart architectures tested promising one undergoes process conformalization correction applied predicted bounding boxes height width comply predefined probability success work novel exploratory dataset images taken perspective train operator first step build validate future trustworthy machine learning models detection railway signals\",\"online passiveaggressive learning class online marginbased algorithms suitable wide range realtime prediction tasks including classification regression algorithms formulated terms deterministic pointestimation problems governed set userdefined hyperparameters approach fails capture modelprediction uncertainty makes performance highly sensitive hyperparameter configurations paper introduce novel learning framework regression overcomes limitations contribute bayesian statespace interpretation regression along novel online variational inference scheme produces probabilistic predictions also offers benefit automatic hyperparameter tuning experiments various realworld data sets show approach performs significantly better standard linear gaussian statespace model\",\"show neural network arbitrary depth nonlinearities dropout applied every weight layer mathematically equivalent approximation well known bayesian model interpretation might offer explanation dropouts key properties robustness overfitting interpretation allows reason uncertainty deep learning allows introduction bayesian machinery existing deep learning frameworks principled way document appendix main paper dropout bayesian approximation representing model uncertainty deep learning gal ghahramani\",\"runtime kernel partial least squares kpls compute fit quadratic number examples however necessity obtaining sensitivity measures degrees freedom model selection confidence intervals detailed analysis requires cubic runtime thus constitutes computational bottleneck realworld data analysis propose novel algorithm kpls computes fit also approximate degrees freedom error bars quadratic runtime algorithm exploits close connection kernel pls lanczos algorithm approximating eigenvalues symmetric matrices uses approximation compute trace powers kernel matrix quadratic runtime\",\"present blitzkriging new approach fast inference gaussian processes applicable regression optimisation classification stateoftheart stochastic inference gaussian processes large datasets scales cubically number inducing inputs variables introduced factorise model blitzkriging shares stateoftheart scaling data reduces scaling number inducing points approximately linear contrast methods blitzkriging force data conform particular structure including gridlike reduces reliance errorprone optimisation inducing point locations able learn rich covariance structure data demonstrate benefits approach real data regression timeseries prediction signalinterpolation experiments\",\"exponential family distributions highly useful machine learning since calculation performed efficiently natural parameters exponential family recently extended texponential family contains studentt distributions family members thus allows handle noisy data well however since texponential family denied deformed exponential cannot derive efficient learning algorithm texponential family expectation propagation paper borrow mathematical tools qalgebra statistical physics show pseudo additivity distributions allows perform calculation texponential family distributions natural parameters develop expectation propagation algorithm texponential family provides deterministic approximation posterior predictive distribution simple moment matching finally apply proposed algorithm bayes point machine studentt process classication demonstrate performance numerically\",\"gaussian process promising novel technology applied regression problem classification problem regression problem yields simple exact solutions case classification problem encounter intractable integrals paper develop new derivation transforms problem evaluating ratio multivariate gaussian orthant integrals moreover develop new monte carlo procedure evaluates integrals based aspects bootstrap sampling acceptancerejection proposed approach beneficial properties compared existing markov chain monte carlo approach simplicity reliability speed\",\"paper proposes novel gaussian process approach fault removal timeseries data fault removal delete faulty signal data instead massages fault data assume one fault occurs one time model signal two separate nonparametric gaussian process models physical phenomenon fault order facilitate fault removal introduce markov region link kernel handling nonstationary gaussian processes kernel piecewise stationary guarantees functions generated derivatives required everywhere continuous apply kernel removal drift bias errors faulty sensor data also recovery eog artifact corrupted eeg signals\",\"present first treatment arc length gaussian process single output dimension gps commonly used tasks trajectory modelling path length crucial quantity interest previously paths one dimension considered theoretical consideration higher dimensional problems fill gap existing literature deriving moments arc length stationary multiple output dimensions new method used derive mean onedimensional finite interval considering distribution arc length integrand technique used derive approximate distribution arc length vector valued mathbbrn moment matching distribution numerical simulations confirm theoretical derivations\",\"gaussian process latent variable model gplvm nonlinear probabilistic method embedding high dimensional dataset terms low dimensional latent variables paper illustrate maximum posteriori map estimation latent variables hyperparameters used model selection hence determine optimal number latent variables appropriate model alternative variational approaches developed recently may useful want use nongaussian prior kernel functions dont automatic relevance determination ard parameters using second order expansion latent variable posterior marginalise latent variables obtain estimate hyperparameter posterior secondly use gplvm integrate multiple data sources simultaneously embedding terms common latent variables present results synthetic data illustrate successful detection retrieval low dimensional structure high dimensional data demonstrate integration multiple data sources leads robust performance finally show data used binary classification tasks attain significant gain prediction accuracy low dimensional representation used\",\"deep gaussian processes dgp hierarchical generalizations gaussian processes proven work effectively multiple supervised regression tasks combine well calibrated uncertainty estimates gps great flexibility multilayer models dgps given inputs outputs layers gaussian distributions parameterized means covariances layers realized sparse gps training data approximated using small set pseudo points work show computational cost dgps reduced loss performance using separate smaller set pseudo points calculating layerwise variance using larger set pseudo points calculating layerwise mean enabled train larger models lower cost better predictive performance\",\"reliable uncertainty estimation time series prediction critical many fields including physics biology manufacturing uber probabilistic time series forecasting used robust prediction number trips special events driver incentive allocation well realtime anomaly detection across millions metrics classical time series models often used conjunction probabilistic formulation uncertainty estimation however models hard tune scale add exogenous variables motivated recent resurgence long short term memory networks propose novel endtoend bayesian deep model provides time series prediction along uncertainty estimation provide detailed experiments proposed solution completed trips data successfully apply largescale time series anomaly detection uber\",\"gpflow gaussian process library uses tensorflow core computations python front end distinguishing features gpflow uses variational inference primary approximation method provides concise code use automatic differentiation engineered particular emphasis software testing able exploit gpu hardware\",\"derive novel sensitivity analysis input variables predictive epistemic aleatoric uncertainty use bayesian neural networks latent variables model class illustrate usefulness sensitivity analysis realworld datasets method increases interpretability complex blackbox probabilistic models\",\"work falls within context predicting value real function input locations given limited number observations function kriging interpolation technique gaussian process regression often considered tackle problem method suffers computational burden number observation points large introduce article nested kriging predictors constructed aggregating submodels based subsets observation points approach proven better theoretical properties aggregation methods found literature contrarily methods shown proposed aggregation method consistent finally practical interest proposed method illustrated simulated datasets industrial test case observations dimensional space\",\"gaussian multiplicative noise commonly used stochastic regularisation technique training deterministic neural networks recent paper reinterpreted technique specific algorithm approximate inference bayesian neural networks several extensions ensued show loguniform prior used publications generally induce proper posterior thus bayesian inference models illposed independent loguniform prior correlated weight noise approximation issues leading either infinite objective high risk overfitting implies reported sparsity obtained solutions cannot explained bayesian related minimum description length arguments thus study objective nonbayesian perspective provide previously unknown analytical form allows exact gradient evaluation show later proposed additive reparametrisation introduces minima present original multiplicative parametrisation implications future research directions discussed\",\"exploiting fact arrival processes exhibit cyclic behaviour propose simple procedure estimating intensity nonhomogeneous poisson process estimator superresolution analogue shao shao lii sum sinusoids frequency amplitude phase wave known need estimated results interpretable yet flexible specification suitable use modelling well high resolution simulations estimation procedure sits classic periodogram methods atomictotal variation norm thresholding novel use window functions point process domain approach attains superresolution without semidefinite programming suitable conditions finite sample guarantees derived procedure resolve open questions expand existing results spectral estimation literature\",\"paper compares classical parametric methods recently developed bayesian methods system identification full bayes solution considered together one standard approximations based empirical bayes paradigm results regarding point estimators impulse response well confidence regions reported\",\"gaussian process priors commonly used aerospace design performing bayesian optimization nonetheless gaussian processes suffer two significant drawbacks outliers priori assumed unlikely posterior variance conditioned observed data depends locations data associated sample values studentst processes generalization gaussian processes founded studentst distribution instead gaussian distribution studentst processes maintain primary advantages gaussian processes kernel function analytic update rule additional benefits beyond gaussian processes studentst distribution higher kurtosis gaussian distribution outliers much likely posterior variance increases decreases depending variance observed data sample values describe studentst processes discuss advantages context aerospace optimization show construct studentst process using kernel function update process given new samples provide clear derivation optimizationrelevant quantities expected improvement contrast related computations gaussian processes finally compare performance studentst processes gaussian process canonical test problems bayesian optimization apply studentst process optimization aerostructural design problem\",\"show training deep network using batch normalization equivalent approximate inference bayesian models demonstrate finding allows make meaningful estimates model uncertainty using conventional architectures without modifications network training procedure approach thoroughly validated measuring quality uncertainty series empirical experiments different tasks outperforms baselines strong statistical significance displays competitive performance recent bayesian approaches\",\"propose family multivariate gaussian process models correlated outputs based assuming likelihood function takes generic form multivariate exponential family distribution efd denote model multivariate generalized gaussian process model derive taylor laplace algorithms approximate inference generic model instantiating efd specific parameter functions obtain two novel models corresponding inference algorithms correlated outputs vonmises angle regression dirichlet regressing multinomial simplex\",\"adaptive filtering algorithms operating reproducing kernel hilbert spaces demonstrated superiority linear counterpart nonlinear system identification unfortunately undesirable characteristic methods order filters grows linearly number input data dramatically increases computational burden memory requirement variety strategies based dictionary learning proposed overcome severe drawback works analyze problem updating dictionary timevarying environment paper present analytical study convergence behavior gaussian leastmeansquare algorithm case statistics dictionary elements partially match statistics input data allows emphasize need updating dictionary online way discarding obsolete elements adding appropriate ones introduce kernel leastmeansquare algorithm lnorm regularization automatically perform task stability mean method analyzed performance tested experiments\",\"many realworld problems encountered several disciplines deal modeling timeseries containing different underlying dynamical regimes probabilistic approaches often employed paper describe several approaches common framework graphical models give unified overview models previously introduced literature simpler comprehensive previous descriptions enables highlight commonalities differences among models observed past addition present several new models inference routines naturally derived within unified viewpoint\",\"incrementalonline state dynamic learning method proposed identification nonlinear gaussian state space models method embeds stochastic variational sparse gaussian process probabilistic state dynamic model inside particle filter framework model updating done measurement sample rate using stochastic gradient descent based optimization implemented state estimation filtering loop performance proposed method compared stateoftheart gaussian process based batch learning methods finally shown state estimation performance significantly improves due online learning state dynamics\",\"corrupting input hidden layers deep neural networks dnns multiplicative noise often drawn bernoulli distribution dropout provides regularization significantly contributed deep learnings success however understanding multiplicative corruptions prevent overfitting difficult due complexity dnns functional form paper show gaussian prior placed dnns weights applying multiplicative noise induces gaussian scale mixture reparameterized circumvent problematic likelihood function analysis proceed using typeii maximum likelihood procedure derive closedform expression revealing regularization evolves function networks weights results show multiplicative noise forces weights become either sparse invariant rescaling find analysis implications model compression naturally reveals weight pruning rule starkly contrasts commonly used signaltonoise ratio snr snr prunes weights large variances seeing noisy approach recognizes robustness retains empirically demonstrate approach strong advantage snr heuristic competitive retraining soft targets produced teacher model\",\"paper introduces linear statespace model timevarying dynamics time dependency obtained forming state dynamics matrix timevarying linear combination set matrices time dependency weights linear combination modelled another linear gaussian dynamical model allowing model learn dynamics process changes previous approaches used switching models small set possible state dynamics matrices model selects one matrices time thus jumping model forms dynamics linear combination changes smooth continuous model motivated physical processes described linear partial differential equations whose parameters vary time example process could temperature field whose evolution driven varying wind direction posterior inference performed using variational bayesian approximation experiments stochastic advectiondiffusion processes realworld weather processes show model timevarying dynamics outperform previously introduced approaches\",\"nonparametric regression massive numbers samples features increasingly important problem big settings common strategy partition feature space separately apply simple models partition set propose alternative approach avoids partitioning associated sensitivity neighborhood choice distance metrics using random compression combined gaussian process regression proposed approach particularly motivated setting response conditionally independent features given projection low dimensional manifold conditionally random compression matrix smoothness parameter posterior distribution regression surface posterior predictive distributions available analytically running analysis parallel many random compression matrices smoothness parameters model averaging used combine results algorithm implemented rapidly even big problems strong theoretical justification found yield state art predictive performance\",\"propose simple method combines neural networks gaussian processes proposed method estimate uncertainty outputs flexibly adjust target functions training data exist advantages gaussian processes proposed method also achieve high generalization performance unseen input configurations advantage neural networks proposed method neural networks used mean functions gaussian processes present scalable stochastic inference procedure sparse gaussian processes inferred stochastic variational inference parameters neural networks kernels estimated stochastic gradient descent methods simultaneously use two realworld spatiotemporal data sets demonstrate experimentally proposed method achieves better uncertainty estimation generalization performance neural networks gaussian processes\",\"paper presents bayesian generative model dependent cox point processes alongside efficient inference scheme scales point processes modelled independently handle missing data naturally infer latent structure cope large numbers observed processes novel contribution enables model work effectively higher dimensional spaces using method achieve vastly improved predictive performance real data validating structured approach\",\"multioutput gaussian processes mogp probability distributions vectorvalued functions previously used multioutput regression multiclass classification less explored facet multioutput gaussian process used generative model vectorvalued random fields context pattern recognition generative model multioutput able handle vectorvalued functions continuous inputs opposed example hidden markov models also offers ability model multivariate random functions high dimensional inputs report use discriminative training criteria known minimum classification error fit parameters multioutput gaussian process compare performance generative training discriminative training mogp emotion recognition activity recognition face recognition also compare proposed methodology hidden markov models trained generative discriminative way\",\"informally call stochastic process learnable admits generalization error approaching zero probability concept class finite vcdimension iid processes simplest example mixture learnable processes need learnable certainly generalization error need decay rate paper argue natural predictive pac condition past observations mixture component sample path definition matches realistic learner might demand also allows sidestep several otherwise grave problems learning dependent data particular give novel pac generalization bound mixtures learnable processes generalization error worse mixture component also provide characterization mixtures absolutely regular betamixing processes independent probabilitytheoretic interest\",\"gaussian probability densities omnipresent applied mathematics gaussian cumulative probabilities hard calculate univariate case study utility expectation propagation approximate integration method problem rectangular integration regions approximation highly accurate also extend derivations general case polyhedral integration regions however find polyhedral case eps answer though often accurate almost arbitrarily wrong consider unexpected results empirically theoretically problem gaussian probabilities generally results elucidate interesting nonobvious feature yet studied detail\",\"bayesian neural networks bnns recently received increasing attention ability provide wellcalibrated posterior uncertainties however model selectioneven choosing number nodesremains open question work apply horseshoe prior node preactivations bayesian neural network effectively turns nodes help explain data demonstrate prior prevents bnn underfitting even number nodes required grossly overestimated moreover model selection number nodes doesnt come expense predictive computational performance fact learn smaller networks comparable predictive performance current approaches\",\"gaussian processes gps proven powerful tools various areas machine learning however applications gps scenario multiview learning paper present new model multiview learning unlike existing methods combines multiple views regularizing marginal likelihood consistency among posterior distributions latent functions different views moreover give general point selection scheme multiview learning improve proposed model criterion experimental results multiple real world data sets verified effectiveness proposed model witnessed performance improvement employing novel point selection scheme\",\"paper presents novel formulation solution orbit determination finite time horizons learning problem present approach orbit determination broad conditions satisfied nbody problems weak conditions allow perform orbit determination noisy highly nonlinear observations presented rangerate doppler observations show domain generalization distribution regression techniques learn estimate orbits group satellites identify individual satellites especially prior understanding correlations orbits provide asymptotic convergence conditions approach presented requires visibility observability underlying state observations particularly useful autonomous spacecraft operations using lowcost ground stations sensors validate orbit determination approach using observations two spacecraft grifex mcubed along synthetic datasets multiple spacecraft deployments lunar orbits also provide comparison standard techniques ekf highly noisy conditions\",\"parametric point process model developed modeling based assumption sequential observations often share latent phenomena also possessing idiosyncratic effects alternating optimization method proposed learn registered point process accounts shared structure well warping functions characterize idiosyncratic aspects observed sequence reasonable constraints iteration update samplespecific warping functions solving set constrained nonlinear programming problems parallel update model maximum likelihood estimation justifiability complexity robustness proposed method investigated detail influence sequence stitching learning results examined empirically experiments synthetic realworld data demonstrate method yields explainable point process models achieving encouraging results compared stateoftheart methods\",\"recurrent neural networks rnns stand forefront many recent developments deep learning yet major difficulty models tendency overfit dropout shown fail applied recurrent layers recent results intersection bayesian modelling deep learning offer bayesian interpretation common deep learning techniques dropout grounding dropout approximate bayesian inference suggests extension theoretical results offering insights use dropout rnn models apply new variational inference based dropout technique lstm gru models assessing language modelling sentiment analysis tasks new approach outperforms existing techniques best knowledge improves single model stateoftheart language modelling penn treebank test perplexity extends arsenal variational tools deep learning\",\"quantitative modeling posttranscriptional regulation process challenging problem systems biology mechanical model regulatory process needs able describe available spatiotemporal protein concentration mrna expression data recover continuous spatiotemporal fields rigorous methods required identify model parameters promising approach deal difficulties proposed using gaussian process prior distribution latent function protein concentration mrna expression study consider partial differential equation mechanical model differential operators latent function since operators stake linear information physical model encoded kernel function hybrid monte carlo methods employed carry bayesian inference partial differential equation parameters gaussian process kernel parameters spatiotemporal field protein concentration mrna expression reconstructed without explicitly solving partial differential equation\",\"gaussian process models often used mathematical approximations computationally expensive experiments provided kernel suitably chosen enough data available obtain reasonable fit simulator model beneficially used tasks prediction optimization montecarlobased quantification uncertainty however former conditions become unrealistic using classical gps dimension input increases one popular alternative turn generalized additive models gams relying assumption simulators response approximately decomposed sum univariate functions approach successfully applied approximation nevertheless completely compatible framework versatile applications ambition present work give insight use gps additive models integrating additivity within kernel proposing parsimonious numerical method datadriven parameter estimation first part article deals kernels naturally associated additive processes properties models based kernels second part dedicated numerical procedure based relaxation additive kernel parameter estimation finally efficiency proposed method illustrated compared approaches sobols gfunction\",\"multioutput regression models must exploit dependencies outputs maximise predictive performance application gaussian processes gps setting typically yields models computationally demanding limited representational power present gaussian process autoregressive regression gpar model scalable multioutput model able capture nonlinear possibly inputvarying dependencies outputs simple tractable way product rule used decompose joint distribution outputs set conditionals modelled standard gpars efficacy demonstrated variety synthetic realworld problems outperforming existing models achieving stateoftheart performance established benchmarks\",\"gaussian processes gps distributions arbitrary functions continuous domain generalized multioutput case linear model coregionalization lmc one approach lmcs estimate exploit correlations across multiple outputs model estimation performed efficiently singleoutput gps assume stationarity multioutput case crosscovariance interaction stationary propose large linear llgp circumvents need stationarity inducing structure lmc kernel common grid inputs shared outputs enabling optimization hyperparameters multidimensional outputs lowdimensional inputs applied synthetic twodimensional real time series data find theoretical improvement relative current solutions multioutput gps realized llgp reducing training time improving maintaining predictive mean accuracy moreover using direct likelihood approximation rather variational one model confidence estimates significantly improved\",\"variational framework learning inducing variables titsias large impact gaussian process literature framework may interpreted minimizing rigorously defined kullbackleibler divergence approximating posterior processes knowledge connection thus far gone unremarked literature paper give substantial generalization literature topic give new proof result infinite index sets allows inducing points data points likelihoods depend function values discuss augmented index sets show contrary previous works marginal consistency augmentation enough guarantee consistency variational inference original model characterize extra condition guarantee obtainable finally show framework sheds light interdomain sparse approximations sparse approximations cox processes\",\"develop automated variational method inference models gaussian process priors general likelihoods method supports multiple outputs multiple latent functions require detailed knowledge conditional likelihood needing evaluation blackbox function using mixture gaussians variational distribution show evidence lower bound gradients estimated efficiently using samples univariate gaussian distributions furthermore method scalable large datasets achieved using augmented prior via inducingvariable approach underpinning sparse approximations along parallel computation stochastic optimization evaluate approach quantitatively qualitatively experiments small datasets mediumscale datasets large datasets showing competitiveness different likelihood models sparsity levels largescale experiments involving prediction airline delays classification handwritten digits show method par stateoftheart hardcoded approaches scalable regression classification\",\"new bayesian approach linear system identification proposed series recent papers main idea frame linear system identification predictor estimation infinite dimensional space aid regularizationbayesian techniques approach guarantees identification stable predictors based prediction error minimization unluckily stability predictors guarantee stability impulse response system paper propose compare various techniques address issue simulations results comparing techniques provided\",\"latent variable timeseries models among heavily used tools machine learning applied statistics models advantage learning latent structure noisy observations temporal ordering data assumed meaningful correlation structure exists across time highlystructured models linear dynamical system lineargaussian observations closedform inference procedures kalman filter case exception general rule exact posterior inference complex generative models intractable consequently much work timeseries modeling focuses approximate inference procedures one particular class models extend recent developments stochastic variational inference develop blackbox approximate inference technique latent variable models latent dynamical structure propose structured gaussian variational approximate posterior carries intuition standard kalman filtersmoother importantly permits use inference approach approximate posterior much general nonlinear latent variable generative models show approach recovers accurate estimates case basic models closedform posteriors interestingly performs well comparison variational approaches designed bespoke fashion specific nonconjugate models\",\"large multilayer neural networks trained backpropagation recently achieved stateoftheart results wide range problems however using backprop neural net learning still disadvantages tune large number hyperparameters data lack calibrated probabilistic predictions tendency overfit training data principle bayesian approach learning neural networks problems however existing bayesian techniques lack scalability large dataset network sizes work present novel scalable method learning bayesian neural networks called probabilistic backpropagation pbp similar classical backpropagation pbp works computing forward propagation probabilities network backward computation gradients series experiments ten realworld datasets show pbp significantly faster techniques offering competitive predictive abilities experiments also show pbp provides accurate estimates posterior variance network weights\",\"improve recently published results resources restricted boltzmann machines rbm deep belief networks dbn required make universal approximators show distribution set binary vectors length arbitrarily well approximated rbm hidden units minimal number pairs binary vectors differing one entry union contains support set important cases number half cardinality support set construct dbn nnb logn hidden layers width capable approximating distribution arbitrarily well confirms conjecture presented roux bengio\",\"paper describes expectation propagation method multiclass classification gaussian processes scales well large datasets method estimate logmarginallikelihood involves sum across data instances enables efficient training using stochastic gradients minibatches type training used computational cost depend number data instances furthermore extra assumptions approximate inference process make memory cost independent consequence proposed method used datasets millions instances compare empirically method alternative approaches approximate required computations using variational inference results show performs similar even better techniques sometimes give significantly worse predictive distributions terms test loglikelihood besides training process proposed approach also seems converge smaller number iterations\",\"using knearest neighbors method one often ignores uncertainty choice account uncertainty holmes adams proposed bayesian framework knearest neighbors knn bayesian knn bknn approach uses pseudolikelihood function standard markov chain monte carlo mcmc techniques draw posterior samples holmes adams focused performance bknn terms misclassification error assess ability quantify uncertainty present evidence show bknn still significantly underestimates model uncertainty\",\"introduce general constructive setting density ratio estimation problem solution multidimensional integral equation equation right hand side known approximately also integral operator defined approximately show illposed problem rigorous solution obtain solution closed form key element solution novel vmatrix captures geometry observed samples compare method three wellknown previously proposed ones experimental results demonstrate good potential new approach\",\"consider class misspecified dynamical models governing term approximately known assumption observations systems evolution accessible various initial conditions goal infer nonparametric correction misspecified driving term faithfully represent system dynamics devise system evolution predictions unobserved initial conditions model unknown correction term gaussian process analyze problem efficient experimental design find optimal correction term constraints limited experimental budget suggest novel formulation experimental design gaussian process show approximately optimal constant factor designs may efficiently derived utilizing results literature submodular optimization numerical experiments exemplify effectiveness techniques\",\"dropout used practical tool obtain uncertainty estimates large vision models reinforcement learning tasks obtain wellcalibrated uncertainty estimates gridsearch dropout probabilities necessary prohibitive operation large models impossible one propose new dropout variant gives improved performance better calibrated uncertainties relying recent developments bayesian deep learning use continuous relaxation dropouts discrete masks together principled optimisation objective allows automatic tuning dropout probability large models result faster experimentation cycles allows agent adapt uncertainty dynamically data observed analyse proposed variant extensively range tasks give insights common practice field larger dropout probabilities often used deeper model layers\",\"paper considers quantification prediction performance gaussian process regression standard approach base prediction error bars theoretical predictive variance lower bound mean squareerror mse approach however take account statistical model learned data show omission leads systematic underestimation prediction errors starting generalization cramerrao bound derive accurate mse bound provides measure uncertainty prediction gaussian processes improved bound easily computed illustrate using synthetic real data examples uncertainty prediction gaussian processes illustrate using synthetic real data examples\",\"interest multioutput kernel methods increasing whether guise multitask learning multisensor networks structured output data gaussian process perspective multioutput mercer kernel covariance function correlated output functions one way constructing kernels based convolution processes key problem approach efficient inference alvarez lawrence recently presented sparse approximation cps enabled efficient inference paper extend work two directions introduce concept variational inducing functions handle potential nonsmooth functions involved kernel construction consider alternative approach approximate inference based variational methods extending work titsias multiple output case demonstrate approaches prediction school marks compiler performance financial time series\",\"gaussian process state space model gpssm nonlinear dynamical system unknown transition andor measurement mappings described gps research gpssms focussed state estimation problem computing posterior latent state given model however key challenge gpssms satisfactorily addressed yet system identification learning model address challenge impose structured gaussian variational posterior distribution latent states parameterised recognition model form bidirectional recurrent neural network inference structure allows recover posterior smoothed sequences data provide practical algorithm efficiently computing lower bound marginal likelihood using reparameterisation trick allows use arbitrary kernels within gpssm demonstrate learnt gpssm efficiently generate plausible future trajectories identified system observing small number episodes true system\",\"consider gaussian process formulation multiple kernel learning problem goal select convex combination kernel matrices best explains data improve generalisation unseen data sparsity kernel weights obtained adopting hierarchical bayesian approach gaussian process priors imposed latent functions generalised inverse gaussians associated weights construction equivalent imposing product heavytailed process priors function space variational inference algorithm derived regression binary classification\",\"kernel leastmeansquare klms algorithm appealing tool online identification nonlinear systems due simplicity robustness addition choosing reproducing kernel setting filter parameters designing klms adaptive filter requires select socalled dictionary order get finiteorder model dictionary significant impact performance requires careful consideration theoretical analysis klms function dictionary setting rarely ever addressed literature analysis previously published authors dictionary elements assumed governed probability density function input data paper modify study considering dictionary part filter parameters set theoretical analysis paves way future investigations klms dictionary design\",\"circular variables arise multitude datamodelling contexts ranging robotics social sciences largely overlooked machine learning community paper partially redresses imbalance extending standard probabilistic modelling tools circular domain first introduce new multivariate distribution circular variables called multivariate generalised von mises mgvm distribution distribution constructed restricting renormalising general multivariate gaussian distribution unit hypertorus previously proposed multivariate circular distributions shown special cases construction second introduce new probabilistic model circular regression inspired gaussian processes method probabilistic principal component analysis circular hidden variables models leverage standard modelling tools covariance functions methods automatic relevance determination third show posterior distribution models mgvm distribution enables development efficient variational freeenergy scheme performing approximate inference approximate maximumlikelihood learning\",\"bayesian neural networks bnns latent variables probabilistic models automatically identify complex stochastic patterns data describe study models decomposition predictive uncertainty epistemic aleatoric components first show decomposition arises naturally bayesian active learning scenario following information theoretic approach second use similar decomposition develop novel risk sensitive objective safe reinforcement learning objective minimizes effect model bias environments whose stochastic dynamics described bnns latent variables experiments illustrate usefulness resulting decomposition active learning safe settings\",\"gaussian process regression generally scale beyond thousands data points without applying sort kernel approximation method approximations focus high eigenvalue part spectrum kernel matrix leads bad performance length scale kernel small paper introduce multiresolution kernel approximation mka first true broad bandwidth kernel approximation algorithm important points mka memory efficient direct method means also makes easy approximate mathoptextrmdetk\",\"learning using privileged information attractive problem setting helps many learning scenarios real world stateoftheart method gaussian process classification gpc privileged information gpc incorporates privileged information noise term likelihood drawback gpc requires numerical quadrature calculate posterior distribution latent function extremely timeconsuming overcome limitation propose novel classification method privileged information based gaussian processes called softlabeltransferred gaussian process sltgp basic idea construct another learning task predicting soft labels continuous values obtained privileged information perform transfer learning task target task predicting hard labels derive pacbayesian bound proposed method justifies optimizing hyperparameters empirical bayes method also experimentally show usefulness proposed method compared gpc gpc\",\"propose active set selection framework gaussian process classification cases dataset large enough render inference prohibitive scheme consists two step alternating procedure active set update rules hyperparameter optimization based upon marginal likelihood maximization active set update rules rely ability predictive distributions gaussian process classifier estimate relative contribution datapoint either included removed model means use include points potentially high impact classifier decision process removing less relevant introduce two active set rules based different criteria first one prefers model interpretable active set parameters whereas second puts computational complexity first thus model active set parameters directly control complexity also provide theoretical empirical support active set selection strategy good approximation full gaussian process classifier extensive experiments show approach compete stateoftheart classification techniques reasonable time complexity source code publicly available httpcogsysimmdtudkpassgp\",\"paper considers generation prediction intervals pis neural networks quantifying uncertainty regression tasks axiomatic highquality pis narrow possible whilst capturing specified portion data derive loss function directly axiom requires distributional assumption show form derives likelihood principle used gradient descent model uncertainty accounted ensembled form benchmark experiments show method outperforms current stateoftheart uncertainty quantification methods reducing average width\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"0_gaussian_process_model\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"0_gaussian_process_model\"],\"textfont\":{\"size\":12},\"x\":[12.210087,11.499163,11.536279,11.33807,11.865779,12.440641,12.476145,11.370288,11.3468685,11.0054,11.8277235,12.19562,12.386335,11.687311,10.955451,11.115912,11.801423,12.236103,11.355731,11.342911,12.545337,13.7547865,12.079637,11.945894,11.079538,12.444659,11.634992,11.911183,11.522092,12.139918,12.091775,11.446007,11.027107,11.297103,11.384664,11.21124,11.770269,11.655533,11.055455,11.458191,12.35641,11.518499,11.554882,12.03052,11.400901,11.759384,11.676878,12.080801,11.2264595,11.297526,12.37229,11.105426,11.501487,11.138259,11.396063,11.068504,11.319845,11.751322,11.158427,11.202553,12.3969755,12.484244,12.037983,12.620511,10.464215,12.454939,12.2267475,11.038072,11.117724,11.43248,12.4549265,11.88672,12.170812,11.522981,11.822172,11.623641,12.419525,11.483529,11.708454,11.404531,11.561016,11.528613,12.241527,11.519353,11.062744,11.25821,11.2109,11.890595,11.509563,12.394698,11.291752,11.532993,11.776163,11.174996,11.172473,11.320405,11.464341,11.7879715,11.946318,11.489444,12.177875,11.160014,12.378875,11.824718,11.443049,11.148622,12.375472,11.172014,11.239358,11.896482,11.764877,12.4273815,11.778287,11.37154,11.796527,11.881042,11.056951,11.578455,10.887416,12.195624,11.497397,11.351759,11.578635,12.51208,11.230568,11.202271,11.580366,11.575685,11.733681,11.771587,11.346423,11.948199,12.215936,12.529268,11.95058,12.200868,10.875623,11.230658,12.3338,11.702905,11.0915,11.631214,11.106939,11.216806,11.282286,12.21276,11.283738,11.107824,10.869749,12.122685,11.667458],\"y\":[10.031812,9.093541,9.209309,9.772926,9.462396,10.32943,10.31032,8.475364,9.428577,9.24177,9.261511,10.085479,10.311189,9.555373,9.285831,9.055977,9.602523,10.046486,9.82171,9.514002,9.916952,7.651895,9.182543,9.203089,9.21074,10.386958,9.678425,9.426155,10.213213,9.128234,9.167613,9.418673,9.268893,9.26817,9.564963,8.948829,9.747196,9.332394,9.36554,9.758921,10.238749,9.232857,9.587822,8.907834,9.385052,9.64224,9.349545,10.043866,8.845941,9.909289,10.241865,9.269297,9.306823,9.51023,8.206799,9.3821745,9.640391,9.440752,9.48956,9.699926,10.304567,10.332215,9.112341,9.965277,9.063409,10.432497,9.365103,9.165729,9.467942,9.400513,10.3691635,9.287854,10.025659,9.546513,9.796963,9.058017,10.325304,9.454958,9.742816,9.24299,9.694136,9.284479,10.117871,10.227186,9.247735,8.978114,9.645671,9.985804,9.504279,10.19607,9.055142,9.434995,9.347921,9.518511,8.953286,9.621237,9.053949,9.804953,9.800384,9.695644,10.039546,9.466447,10.167581,8.654596,8.6555605,9.490172,10.179711,9.353634,8.239833,9.671189,9.5698395,10.357243,9.731284,9.34654,9.813546,9.255254,9.224689,9.32521,8.832782,10.135908,9.579447,9.812672,9.409303,10.21275,9.452797,9.465238,9.685392,9.653952,9.301073,9.135424,8.686678,9.625759,10.142989,10.271242,9.4012,10.1637335,8.766178,9.867289,10.114995,9.81085,9.261871,9.573149,9.184509,8.23431,9.326567,10.079552,9.239638,9.362078,9.317335,10.051532,9.529287],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"frankwolfe algorithm widely used solving nuclear norm constrained problems since require projections however often yields high rank intermediate iterates expensive time space costs large problems address issue propose rankdrop method nuclear norm constrained problems goal generate descent steps lead rank decreases maintaining lowrank solutions throughout algorithm moreover optimization problems constrained ensure rankdrop step also feasible readily incorporated projectionfree minimization method frankwolfe demonstrate incorporating rankdrop steps frankwolfe algorithm rank solution greatly reduced compared original frankwolfe common variants\",\"modified cholesky decomposition commonly used precision matrix estimation given specified order random variables however order variables often available cannot predetermined work propose address variable order issue modified cholesky decomposition sparse precision matrix estimation key idea effectively combine set estimates obtained multiple permutations variable orders efficiently encourage sparse structure resultant estimate thresholding technique ensemble cholesky factor matrix consistent property proposed estimate established weak regularity conditions simulation studies conducted evaluate performance proposed method comparison several existing approaches proposed method also applied linear discriminant analysis real data classification\",\"describe ways define calculate lnorm signal subspaces less sensitive outlying data lcalculated subspaces focus computation maximumprojection principal component data matrix containing signal samples dimension conclude general problem formally nphard asymptotically large prove however case engineering interest fixed dimension asymptotically large sample support present optimal algorithm complexity ond generalize multiple lmaxprojection components present explicit optimal subspace calculation algorithm form matrix nuclearnorm evaluations conclude illustrations lsubspace signal processing fields data dimensionality reduction directionofarrival estimation\",\"consider problem dictionary learning assumption observed signals represented sparse linear combinations columns single large dictionary matrix particular analyze minimax risk dictionary learning problem governs mean squared error mse performance learning scheme regardless computational complexity following established informationtheoretic method based fanos inequality derive lower bound minimax risk given dictionary learning problem lower bound yields characterization samplecomplexity lower bound required number observations consistent dictionary learning schemes exist bounds may compared performance given learning scheme allowing characterize far method optimal performance\",\"consider problem reconstructing signals images periodic nonlinearities problems design measurement scheme supports efficient reconstruction moreover method adapted extend compressive sensingbased signal image acquisition systems techniques potentially useful reducing measurement complexity high dynamic range hdr imaging systems little loss reconstruction quality several numerical experiments real data demonstrate effectiveness approach\",\"many areas machine learning becomes necessary find eigenvector decompositions large matrices discuss two methods reducing computational burden spectral decompositions venerable nystom extension newly introduced algorithm based random projections previous work centered ability reconstruct original matrix argue interesting relevant comparison relative performance clustering classification tasks using approximate eigenvectors features demonstrate performance task specific depends rank approximation\",\"using bayesian approach consider problem recovering sparse signals additive sparse dense noise typically sparse noise models outliers impulse bursts data loss handle sparse noise existing methods simultaneously estimate sparse signal interest sparse noise interest estimating sparse signal without need estimating sparse noise construct robust relevance vector machine rvm rvm sparse noise ever present dense noise treated combined noise model precision combined noise modeled diagonal matrix show new rvm update equations correspond nonsymmetric sparsity inducing cost function combined modeling found computationally efficient also extend method blocksparse signals noise known unknown block structures simulations show performance computation efficiency new rvm several applications recovery sparse block sparse signals housing price prediction image denoising\",\"many applications desirable extract relevant aspects data principled way information bottleneck method one seeks code maximizes information relevance variable constraining information encoded original data unfortunately however method computationally demanding data highdimensional andor nongaussian propose approximate variational scheme maximizing lower bound objective analogous variational using method derive algorithm recover features relevant sparse finally demonstrate kernelized versions algorithm used address broad range problems nonlinear relation\",\"consider problem jointly estimating parameters well structure binary valued markov random fields contrast earlier work focus one two problems formulate problem maximization ellregularized surrogate likelihood allows find sparse solution optimization technique efficiently incorporates cuttingplane algorithm order obtain tighter outer bound marginal polytope results improvement parameter estimates approximation marginals synthetic data compare algorithm two estimation tasks existing methods analyze method highdimensional setting number dimensions allowed grow number observations rate convergence estimate demonstrated depend explicitly sparsity underlying graph\",\"large number algorithms machine learning principal component analysis pca nonlinear kernel extensions recent spectral embedding support estimation methods rely estimating linear subspace samples paper introduce general formulation problem derive novel learning error estimates results rely natural assumptions spectral properties covariance operator associated data distribu tion hold wide class metrics subspaces special cases discuss sharp error estimates reconstruction properties pca spectral support estimation key analysis operator theoretic approach broad applicability spectral learning methods\",\"consider problem noisy bit matrix completion exact rank constraint true underlying matrix instead observing subset noisy continuousvalued entries matrix observe subset noisy bit binary measurements generated according probabilistic model consider constrained maximum likelihood estimation constraint entrywise infinitynorm exact rank constraint contrast previous work used convex relaxations rank provide upper bound matrix estimation error model compared existing results bound faster convergence rate matrix dimensions fraction revealed bit observations fixed independent matrix dimensions also propose iterative algorithm solving nonconvex optimization certificate global optimality limiting point algorithm based low rank factorization validate method synthetic real data improved performance existing methods\",\"vanishing ideal set polynomials takes zero value given data points originally proposed computer algebra vanishing ideal recently exploited extracting nonlinear structures data many applications avoid overfitting noisy data polynomials often designed approximately rather exactly equal zero designated data although approximations empirically demonstrate high performance sound algebraic structure vanishing ideal lost present paper proposes vanishing ideal tolerant noisy data also pursued better algebraic structure new problem simultaneously find set polynomials data points polynomials approximately vanish input data points almost exactly vanish discovered data points experimental classification tests method discovered much fewer lowerdegree polynomials existing stateoftheart method consequently method accelerated runtime classification tasks without degrading classification accuracy\",\"motivated problems arise number applications online marketing explosives detection observations usually modeled using poisson statistics model observation poisson random variable whose mean sparse linear superposition known patterns unlike many conventional problems observations identically distributed since associated different sensing modalities analyze performance maximum likelihood decoder poisson setting involves nonlinear optimization yet computationally tractable derive fundamental sample complexity bounds sparse recovery measurements contaminated poisson noise contrast leastsquares linear regression setting gaussian noise observe addition sparsity scale parameters also fundamentally impacts ell error poisson setting show tightness upper bounds theoretically experimentally particular derive minimax matching lower bound meansquared error show constrained decoder minimax optimal regime\",\"modeling data linear combinations elements learned dictionary focus much recent research machine learning neuroscience signal processing signals natural images admit sparse representations well established models well suited restoration tasks context learning dictionary amounts solving largescale matrix factorization problem done efficiently classical optimization tools approach also used learning features data purposes image classification tuning dictionary supervised way tasks proven difficult paper present general formulation supervised dictionary learning adapted wide variety tasks present efficient algorithm solving corresponding optimization problem experiments handwritten digit classification digital art identification nonlinear inverse image problems compressed sensing demonstrate approach effective largescale settings well suited supervised semisupervised classification well regression tasks data admit sparse representations\",\"problem lowrank matrix estimation recently received lot attention due challenging applications lot work done rankpenalized methods convex relaxation theoretical applied sides however papers considered bayesian estimation paper review different type priors considered matrices favour lowrank also prove obtained bayesian estimators suitable assumptions enjoys optimality properties ones based penalization\",\"study problem recovering structured signal mathbfx highdimensional data mathbfyifmathbfaitmathbfx nonlinear potentially unknown link function regressors mathbfai iid gaussian brillinger showed ordinary leastsquares estimates mathbfx constant proportionality muell depends recently plan vershynin extended result highdimensional setting deriving sharp error bounds generalized lasso unfortunately leastsquares lasso fail recover mathbfx muell example includes even link functions resolve issue proposing analyzing alternative convex recovery method nutshell method treats link functions linear lifted space higherdimension interestingly error analysis captures effect nonlinearity problems geometry simple summary parameters\",\"suppose two large multidimensional data sets noisy measurements underlying random process principle components analysis performed separately data sets reduce dimensionality circumstances may happen two lowerdimensional data sets inordinately large procrustean fittingerror purpose manuscript quantify incommensurability phenomenon particular specified conditions square procrustean fittingerror two normalized lowerdimensional data sets asymptotically convex combination via correlation parameter hausdorff distance projection subspaces maximum possible value square procrustean fittingerror normalized data show gives rise incommensurability phenomenon employ illustrative simulations well real data experiment explore incommensurability phenomenon may appreciable impact\",\"consider problem estimation lowrank matrix limited number noisy rankone projections particular propose two fast nonconvex emphproper algorithms matrix recovery support rigorous theoretical analysis show proposed algorithms enjoy linear convergence sample complexity independent condition number unknown true lowrank matrix leveraging recent advances lowrank matrix approximation techniques show algorithms achieve computational speedups existing methods finally complement theory numerical experiments\",\"paper consider low rank matrix estimation using either matrixversion dantzig selector hatalambdad matrixversion lasso estimator hatalambdal consider subgaussian measurements measurements xldotsxninmathbbrmtimes iid subgaussian entries suppose textrmrankar proved ngeq cmrvee rlogmlogn hatalambdad hatalambdal obtain optimal upper boundsexcept logarithmic terms estimation accuracy spectral norm applying metric entropy grassmann manifolds construct near matching minimax lower bound estimation accuracy spectral norm also give upper bounds matching minimax lower boundexcept logarithmic terms estimation accuracy schattenq norm every leq qleqinfty direct corollary show upper bounds minimax lower bounds estimation accuracy kyfank norms every leq kleq\",\"tensor decomposition methods popular tools learning latent variables given lowerorder moments data however standard assumption sufficient data estimate moments high accuracy work consider case certain dimensions data always observedcommon applied settings measurements may taken observationsresulting moment estimates varying quality derive weighted tensor decomposition approach computationally efficient nonweighted approach demonstrate outperforms methods appropriately leverage lessobserved dimensions\",\"dimensionality reduction methods common field high dimensional data analysis typically algorithms dimensionality reduction computationally expensive therefore applications analysis massive amounts data impractical example repeated computations due accumulated data computationally prohibitive paper outofsample extension scheme used complementary method dimensionality reduction presented describe algorithm performs outofsample extension newlyarrived data points unlike extension algorithms nystrom algorithm proposed algorithm uses intrinsic geometry data properties dimensionality reduction map prove error proposed algorithm bounded additionally outofsample extension algorithm provides degree abnormality newlyarrived data point\",\"nonnegative matrix factorization nmf shown identifiable separability assumption columnsor rows input data matrix belong convex cone generated columnsor rows real applications however separability assumption hard satisfy following paper look linear programming based reformulation locate extreme rays convex cone noisy setting furthermore order deal large scale data employ firstorder methods fom mitigate computational complexity primarily results large number constraints show performance algorithm real synthetic data sets\",\"spectral features empirical moment matrix constitute resourceful tool unveiling properties cloud points among density support latent structures already well known empirical moment matrix encodes great deal subtle attributes underlying measure starting object base observations combine ideas statistics real algebraic geometry orthogonal polynomials approximation theory opening new insights relevant machine learning problems data supported singular sets refined concepts results real algebraic geometry approximation theory empowering simple tool empirical moment matrix task solving nontrivial questions data analysis provide theoretical support numerical experiments connections real world data validation stamina empirical moment matrix approach\",\"study problem prediction evolving graph data formulate problem minimization convex objective encouraging sparsity lowrank solution reflect natural graph properties convex formulation allows obtain oracle inequalities efficient solvers provide empirical results algorithm comparison competing methods point two open questions related compressed sensing algebra lowrank sparse matrices\",\"modeling variability tensor decomposition methods one challenges source separation one possible solution account variations one data set another jointly analysed resort parafac model however far imposing constraints mode variability possible following manuscript relaxation parafac model introduced allows imposing nonnegativity constraints varying mode algorithm compute proposed flexible parafac model derived performance studied synthetic chemometrics data\",\"paper examine problem approximating general linear dimensionality reduction ldr operator represented matrix mathbbrm times partial circulant matrix rows related circular shifts partial circulant matrices admit fast implementations via fourier transform methods subsampling operations investigation motivated desire leverage potential computational improvements largescale data processing tasks establish fundamental result large ldr matrices whose row spaces uniformly distributed fact cannot well approximated partial circulant matrices propose natural generalization partial circulant approximation framework entails approximating range space given ldr operator restricted domain inputs using matrix formed product partial circulant matrix rows times post processing matrix introduce novel algorithmic technique based sparse matrix factorization identifying factors comprising approximations provide preliminary evidence demonstrate potential approach\",\"given full partial information collection points lie close union several subspaces subspace clustering refers process clustering points according subspace identifying subspaces one popular approach sparse subspace clustering ssc represents sample weighted combination samples weights minimal ell norm uses learned weights cluster samples ssc stable settings sample contaminated relatively small amount noise however significant amount additive noise considerable number entries missing theoretical guarantees scarce paper study robust variant ssc establish clustering guarantees presence corrupted missing data give explicit bounds amount noise missing data algorithm tolerate deterministic settings random generative model notably approach provides guarantees higher tolerance noise missing data existing analyses method design results hold even know locations missing data presenceonly data\",\"introduce bayesian multitensor factorization model first bayesian formulation joint factorization multiple matrices tensors research problem generalizes joint matrixtensor factorization problem arbitrary sets tensors depth including matrices interpreted unsupervised multiview learning multiple data tensors generalized relax usual trilinear tensor factorization assumptions result factorization set tensors factors shared subsets tensors factors private individual tensors demonstrate performance existing baselines multiple tensor factorization tasks structural toxicogenomics functional neuroimaging\",\"study problem recovery matrices simultaneously low rank row andor column sparse matrices appear recent applications cognitive neuroscience imaging computer vision macroeconomics genetics propose gdt gradient descent hard thresholding algorithm efficiently recover matrices structure minimizing biconvex function nonconvex set constraints show linear convergence iterates obtained gdt region within statistical error optimal solution application method consider multitask learning problems show statistical error rate obtained gdt near optimal compared minimax rate experiments demonstrate competitive performance much faster running speed compared existing methods simulations real data sets\",\"propose general framework reducedrank modeling matrixvalued data applying generalized nuclear norm penalty directly model lowdimensional latent variables associated rows columns framework flexibly incorporates row column features smoothing kernels sources side information penalizing deviations row column models moreover large class models estimated scalably using convex optimization computational bottleneck case one singular value decomposition per iteration large easytoapply matrix framework generalizes traditional convex matrix completion multitask learning methods well maximum posteriori estimation large class popular hierarchical bayesian models\",\"nonnegative matrix factorization nmf aims factorize matrix two optimized nonnegative matrices appropriate intended applications method widely used unsupervised learning tasks including recommender systems rating matrix users items document clustering weighting matrix papers keywords however traditional nmf methods typically assume number latent factors dimensionality loading matrices fixed assumption makes inflexible many applications paper propose nonparametric nmf framework mitigate issue using dependent indian buffet processes dibp nutshell apply correlation function generation two stick weights associated pair columns loading matrices still maintaining respective marginal distribution specified ibp consequence generation two loading matrices columnwise indirectly correlated framework two classes correlation function proposed using bivariate beta distribution using copula function methods allow adopt work various applications flexibly choosing appropriate parameter settings compared stateofthe art approaches area using gaussian process gpbased dibp work seen much flexible terms allowing two corresponding binary matrix columns greater variations nonzero entries experiments realworld synthetic datasets show three proposed models perform well document clustering task comparing standard nmf without predefining dimension factor matrices bivariate beta distributionbased copulabased models better flexibility gpbased model\",\"paper study effects different prior likelihood choices bayesian matrix factorisation focusing small datasets choices greatly influence predictive performance methods identify four groups approaches gaussianlikelihood realvalued priors nonnegative priors seminonnegative models finally poissonlikelihood approaches group review several models literature considering sixteen total discuss relations different priors matrix norms extensively compare methods eight realworld datasets across three application areas giving inter intragroup comparisons measure convergence runtime speed crossvalidation performance sparse noisy prediction performance model selection robustness offer several insights tradeoffs prior likelihood choices bayesian matrix factorisation small datasets poisson models give poor predictions nonnegative models constrained realvalued ones\",\"sparse subspace clustering ssc elegant approach unsupervised segmentation data points cluster located linear subspaces model applies instance motion segmentation restrictions camera model hold ssc requires problems based lnorm solved infer points belong subspace unknown subspaces wellseparated algorithm guaranteed succeed algorithm rests upon assumption points subspace well spread question happens condition violated yet investigated work effect particular distributions subspace analyzed shown ssc fails infer correct labels points subspace fall one cluster\",\"study problem demixing pair sparse signals noisy nonlinear observations superposition mathematically consider nonlinear signal observation model gaitx ildotsm phi wpsi denotes superposition signal phi psi orthonormal bases mathbbrn zinmathbbrn sparse coefficient vectors constituent signals represents noise moreover represents nonlinear link function aiinmathbbrn ith row measurement matrix ainmathbbrmtimes problems nature arise several applications ranging astronomy computer vision machine learning paper make concrete algorithmic progress demixing problem specifically consider two scenarios case demixing procedure knowledge link function case demixing algorithm perfect knowledge link function cases provide fast algorithms recovery constituents observations moreover support algorithms rigorous theoretical analysis derive nearly tight upper bounds sample complexity proposed algorithms achieving stable recovery component signals also provide range numerical simulations illustrate performance proposed algorithms real synthetic signals images\",\"paper considers emphvolume minimization volminbased structured matrix factorization smf volmin factorization criterion decomposes given data matrix basis matrix times structured coefficient matrix via finding minimumvolume simplex encloses columns data matrix recent work showed volmin guarantees identifiability factor matrices mild conditions realistic wide variety applications paper focuses theoretical practical aspects volmin theory side exact equivalence two independently developed sufficient conditions volmin identifiability proven thereby providing comprehensive understanding aspect volmin algorithm side computational complexity sensitivity outliers two key challenges associated realworld applications volmin addressed via new volmin algorithm handles volume regularization computationally simple way automatically detects iteratively downweights outliers simultaneously simulations realdata experiments using remotely sensed hyperspectral image reuters document corpus employed showcase effectiveness proposed algorithm\",\"simple interpretation matrix completion problem introduced based statistical models combined wellknown results missing data analysis interpretation indicates matrix completion still valid principled estimation procedure even without missing completely random mcar assumption almost current theoretical studies matrix completion assume\",\"paper presents novel algorithms exploit intrinsic algebraic combinatorial structure matrix completion task estimating missing tries general low rank setting positive data achieve results performing state art nuclear norm accuracy computational efficiency simulations task predicting athletic performance partially observed data\",\"restricted isometry property rip universal tool data recovery explore implication rip framework generalized sparsity group measurements introduced part paper turns given measurement instrument number measurements rip improved optimizing families banach spaces second investigate preservation difference two sparse vectors trivial generalized models third extend rip partial fourier measurements optimal scaling number measurements random sign far general group structured measurements lastly also obtain rip infinite dimension context fourier measurement concepts sparsity naturally replaced smoothness assumptions\",\"due space limitations submission source separation clustering phaselocked subspaces accepted publication ieee transactions neural networks presented results without proof proofs provided paper\",\"recent years structured matrix recovery problems gained considerable attention real world applications recommender systems computer vision much existing work focused matrices lowrank structure limited progress made matrices types structure paper present nonasymptotic analysis estimation generally structured matrices via generalized dantzig selector generic subgaussian measurements show estimation error always succinctly expressed terms geometric measures suitable sets depend structure underlying true matrix addition derive general bounds geometric measures structures characterized unitarily invariant norms large family covering matrix norms practical interest examples provided illustrate utility theoretical development\",\"past years robust pca established standard tool reliable lowrank approximation matrices presence outliers recently robust pca approach via nuclear norm minimization extended matrices linear structures appear applications system identification data series analysis time shown control rank structured approximation via matrix factorization approaches drawbacks methods either lie lack robustness outliers static nature repeated batchprocessing present robust structured lowrank approximation method grassmannian one hand allows fast reinitialization online setting due subspace identification manifolds robust outliers due smooth approximation ellpnorm cost function hand method evaluated online time series forecasting tasks simulated realworld data\",\"nongaussian component analysis ngca aimed identifying linear subspace projected data follows nongaussian distribution paper propose novel ngca algorithm based logdensity gradient estimation unlike existing methods proposed ngca algorithm identifies linear subspace using eigenvalue decomposition without iterative procedures thus computationally reasonable furthermore theoretical analysis prove identified subspace converges true subspace optimal parametric rate finally practical performance proposed algorithm demonstrated artificial benchmark datasets\",\"ensure interpretability extracted sources tensor decomposition introduce paper dictionarybased tensor canonical polyadic decomposition enforces one factor belong exactly known dictionary new formulation sparse coding proposed enables high dimensional tensors dictionarybased canonical polyadic decomposition benefits using dictionary tensor decomposition models explored terms parameter identifiability estimation accuracy performances proposed algorithms evaluated decomposition simulated data unmixing hyperspectral images\",\"dictionaryaided sparse regression approach recently emerged promising alternative hyperspectral unmixing remote sensing using available spectral library dictionary approach identifies underlying materials given hyperspectral image selecting small subset spectral samples dictionary represent whole image drawback current developments actual spectral signature scene often assumed zero mismatch corresponding dictionary sample assumption considered ideal practice paper tackle spectral signature mismatch problem proposing dictionaryadjusted nonconvex sparsityencouraging regression danser framework main idea incorporate dictionary correcting variables formulation simple low periteration complexity algorithm tailordesigned practical realization danser using dictionary correcting idea also propose robust subspace solution dictionary pruning extensive simulations realdata experiments show proposed method effective mitigating undesirable spectral signature mismatch effects\",\"propose unified framework solve general lowrank plus sparse matrix recovery problems based matrix factorization covers broad family objective functions satisfying restricted strong convexity smoothness conditions based projected gradient descent double thresholding operator proposed generic algorithm guaranteed converge unknown lowrank sparse matrices locally linear rate matching bestknown robustness guarantee tolerance sparsity core theory novel structural lipschitz gradient condition lowrank plus sparse matrices essential proving linear convergence rate algorithm believe independent interest prove fast rates general superpositionstructured models illustrate application framework two concrete examples robust matrix sensing robust pca experiments synthetic real datasets corroborate theory\",\"developed efficient algorithm maximum likelihood joint tracking association problem strong clutter gmti data using iterative procedure dynamic logic process vaguetocrisp new tracker overcomes combinatorial complexity tracking highlycluttered scenarios results significant improvement signaltoclutter ratio\",\"section incorrect removed submissions rewritten version posted future\",\"consider problems detection localization contiguous block weak activation large matrix small number noisy possibly adaptive compressive linear measurements closely related problem compressed sensing task estimate sparse vector using small number linear measurements contrary results compressed sensing shown neither adaptivity contiguous structure help much show reliable localization magnitude weakest signals strongly influenced structure ability choose measurements adaptively detection neither adaptivity structure reduce requirement magnitude signal characterize precise tradeoffs various problem parameters signal strength number measurements required reliably detect localize block activation sufficient conditions complemented information theoretic lower bounds\",\"dictionary learning proven powerful tool many image processing tasks atoms typically defined small image patches drawback dictionary encodes basic structures addition approach treats patches different locations one single set means loss information features wellaligned across signals case instance multitrial magneto electroencephalography meeg learning dictionary entire signals could make use alignement reveal higherlevel features case however small missalignements phase variations features would compensated paper propose extension common dictionary learning framework overcome limitations allowing atoms adapt position across signals method validated simulated real neuroelectric data\",\"paper consider problem hypersparse aggregation namely given dictionary functions look optimal aggregation algorithm writes tilde sumjm thetaj many zero coefficients thetaj possible problem particular interest contains many irrelevant functions appear tildef provide exact oracle inequality tilde two coefficients nonzero entails tilde optimal aggregation algorithm since selectors suboptimal aggregation procedures proves minimal number elements required construction optimal aggregation procedures every situations simulated example algorithm proposed dictionary obtained using lars problem selection regularization parameter lasso also give example use aggregation achieve minimax adaptation anisotropic besov spaces previously known minimax theory regression random design\",\"paper considers problem subspace clustering noise specifically study behavior sparse subspace clustering ssc either adversarial random noise added unlabelled input data points assumed union lowdimensional subspaces show modified version ssc emphprovably effective correctly identifying underlying subspaces even noisy data extends theoretical guarantee algorithm practical settings provides justification success ssc class real applications\",\"highdimensional tensors multiway data becoming prevalent areas biomedical imaging chemometrics networking bibliometrics traditional approaches finding lower dimensional representations tensor data include flattening data applying matrix factorizations principal components analysis pca employing tensor decompositions candecomp parafac tucker decompositions former lose important structure data latter higherorder pca hopca methods problematic highdimensions many irrelevant features introduce frameworks sparse tensor factorizations sparse hopca based heuristic algorithmic approaches solving penalized optimization problems related decomposition extensions approaches lead methods general regularized tensor factorizations multiway functional hopca generalizations hopca structured data illustrate utility methods dimension reduction feature selection signal recovery simulated data multidimensional microarrays functional mris\",\"sparse coding core building block many data analysis machine learning pipelines typically solved relying generic optimization techniques iterative soft thresholding algorithm accelerated version ista fista methods optimal class firstorder methods nonsmooth convex functions however exploit particular structure problem hand input data distribution acceleration using neural networks coined lista proposed gregor cun showed empirically one could achieve high quality estimates iterations modifying parameters proximal splitting appropriately paper study reasons acceleration mathematical analysis reveals related specific matrix factorization gram kernel dictionary attempts nearly diagonalise kernel basis produces small perturbation ell ball factorization succeeds prove resulting splitting algorithm enjoys improved convergence bound respect nonadaptive version moreover analysis also shows conditions acceleration occur mostly beginning iterative process consistent numerical experiments validate analysis showing dictionaries factorization exist adaptive acceleration fails\",\"motivated electricity consumption metering extend existing nonnegative matrix factorization nmf algorithms use linear measurements observations instead matrix entries objective estimate multiple time series fine temporal scale temporal aggregates measured individual series furthermore algorithm extended take account individual autocorrelation provide better estimation using recent convex relaxation quadratically constrained quadratic program extensive experiments synthetic realworld electricity consumption datasets illustrate effectiveness matrix recovery algorithms\",\"present unified framework lowrank matrix estimation nonconvex penalties first prove proposed estimator attains faster statistical rate traditional lowrank matrix estimator nuclear norm penalty moreover rigorously show certain condition magnitude nonzero singular values proposed estimator enjoys oracle property exactly recovers true rank matrix besides attaining faster rate far know first work establishes theory lowrank matrix estimation nonconvex penalties confirming advantages nonconvex penalties matrix completion numerical experiments synthetic real world datasets corroborate theory\",\"matrix factorization become common approach collaborative filtering due ease implementation scalability large data sets two existing drawbacks basic model incorporate side information either users items assumes common variance users extend work constrained probabilistic matrix factorization deriving gibbs updates side feature vectors items salakhutdinov minh show bayesian treatment constrained pmf model outperforms simple map estimation also consider extensions heteroskedastic precision introduced literature lakshminarayanan bouchard archambeau show tends result overfitting deterministic approximation algorithms variational inference observed entries user item matrix distributed nonuniform manner light propose truncated precision model experimental results suggest model tends delay overfitting\",\"robust tensor recovery plays instrumental role robustifying tensor decompositions multilinear data analysis outliers gross corruptions missing values diverse array applications paper study problem robust lowrank tensor recovery convex optimization framework drawing upon recent advances robust principal component analysis tensor completion propose tailored optimization algorithms global convergence guarantees solving constrained lagrangian formulations problem algorithms based highly efficient alternating direction augmented lagrangian accelerated proximal gradient methods also propose nonconvex model often improve recovery results convex models investigate empirical recoverability properties convex nonconvex formulations compare computational performance algorithms simulated data demonstrate number real applications practical effectiveness convex optimization framework robust lowrank tensor recovery\",\"highdimensional data often lie lowdimensional subspaces corresponding different classes belong finding sparse representations data points dictionary built using collection data helps uncover lowdimensional subspaces address problems clustering classification subset selection paper address problem recovering sparse representations noisy data points dictionary whose columns correspond corrupted data lying close union subspaces consider constrained ellminimization study conditions solution proposed optimization satisfies approximate subspacesparse recovery condition specifically show noisy data point perturbed subspace noise magnitude varepsilon reconstructed using data points subspace small error order ovarepsilon coefficients corresponding data points subspaces sufficiently small order ovarepsilon impose randomness assumption arrangement subspaces distribution data points subspace framework based novel generalization nullspace property setting data lie multiple subspaces number data points subspace exceeds dimension subspace data points corrupted noise moreover assuming random distribution data points show coefficients desired support reconstruct given point high accuracy also sufficiently large values order\",\"density matrices positively semidefinite hermitian matrices unit trace describe state quantum system goal paper develop minimax lower bounds error rates estimation low rank density matrices trace regression models used quantum state tomography particular case pauli measurements explicit dependence bounds rank complexity parameters bounds established several statistically relevant distances including quantum versions kullbackleibler divergence relative entropy distance hellinger distance called bures distance schatten pnorm distances sharp upper bounds oracle inequalities least squares estimator von neumann entropy penalization obtained showing minimax lower bounds attained logarithmic factors distances\",\"present numerical algorithm nonnegative matrix factorization nmf problems noisy separability nmf problem separability stated one finding vertices convex hull data points research interest paper find vectors close vertices possible situation noise added data points algorithm designed capture shape convex hull data points using enclosing ellipsoid show algorithm correctness robustness properties theoretical practical perspectives correctness means data points contain noise algorithm find vertices convex hull robustness means data points contain noise algorithm find nearvertices finally apply algorithm document clustering report experimental results\",\"tensor train decomposition provides spaceefficient representation higherorder tensors despite advantage face two crucial limitations apply decomposition machine learning problems lack statistical theory scalable algorithms paper address limitations first introduce convex relaxation decomposition problem derive error bound tensor completion task next develop alternating optimization method randomization technique time complexity efficient space complexity experiments numerically confirm derived bounds empirically demonstrate performance method real higherorder tensor\",\"introduce sparse random projection important dimensionreduction tool machine learning estimation discretechoice models highdimensional choice sets initially highdimensional data compressed lowerdimensional euclidean space using random projections subsequently estimation proceeds using cyclic monotonicity moment inequalities implied multinomial choice model estimation procedure semiparametric require explicit distributional assumptions made regarding random utility errors random projection procedure justified via johnsonlindenstrauss lemma pairwise distances data points preserved data compression exploit show convergence estimator estimator works well simulations application supermarket scanner dataset\",\"consider problem efficient randomized dimensionality reduction normpreservation guarantees specifically prove datadependent johnsonlindenstrausstype geometry preservation guarantees hos random subspace method data satisfy mild regularity condition extent estimated sampling data random subspace approximately preserves euclidean geometry data high probability guarantees order random projection namely required dimension projection logarithmic number data points larger constant term bound depends upon regularity challenging situation original data sparse representation since implies large projection dimension required show situation improved sparse binary data applying efficient densifying preprocessing neither changes euclidean geometry data requires explicit matrixmatrix multiplication corroborate theoretical findings experiments dense sparse highdimensional datasets several application domains\",\"assume data independently sampled mixture distribution unit ball ddimensional euclidean space components first component uniform distribution ball representing outliers components uniform distributions along ddimensional linear subspaces restricted ball study simultaneous recovery underlying subspaces recovery best subspace largest number points minimizing lpaveraged distances data points ddimensional subspaces ddimensional space unlike minimization problems minimization nonconvex thus requires different methods analysis show underlying subspaces best subspace precisely recovered minimization overwhelming probability result extends additive homoscedastic uniform noise around subspaces uniform distribution strip around near recovery error proportional noise level hand show underlying subspaces best subspace cannot recovered even nearly recovered relaxations also discussed use results paper partially justifying recent effective algorithms modeling data mixtures multiple subspaces well discussing effect using variants minimizations ransactype strategies single subspace recovery\",\"study problem lowrank tensor factorization presence missing data ask following question many sampled entries need efficiently exactly reconstruct tensor lowrank orthogonal decomposition propose novel alternating minimization based method iteratively refines estimates singular vectors show certain standard assumptions method recover threemode ntimes ntimes dimensional rankr tensor exactly log randomly sampled entries process proving result solve two challenging subproblems tensors missing data first process analyzing initialization step prove generalization celebrated result szemeredie spectrum random graphs next prove global convergence alternating minimization good initialization simulations suggest dependence sample size dimensionality indeed tight\",\"text investigates relations two wellknown family algorithms matrix factorisations recursive linear filters describing probabilistic model approximate inference corresponds matrix factorisation algorithm using probabilistic model derive matrix factorisation algorithm recursive linear filter precisely derive matrixvariate recursive linear filter order perform efficient inference high dimensions also show possible interpret algorithm nontrivial stochastic gradient algorithm demonstrations comparisons image restoration task given\",\"recently sparsitybased algorithms proposed superresolution spectrum estimation however achieve adequately high resolution realworld signal analysis dictionary atoms close frequency thereby resulting coherent design popular convex compressed sensing methods break presence high coherence large noise propose new regularization approach handle model collinearity obtain parsimonious frequency selection simultaneously takes advantage pairing structure sine cosine atoms frequency dictionary probabilistic spectrum screening also developed fast computation high dimensions dataresampling version highdimensional bayesian information criterion used determine regularization parameters experiments show efficacy efficiency proposed algorithms challenging situations small sample size high frequency resolution low signaltonoise ratio\",\"propose unified framework estimating lowrank matrices nonconvex optimization based gradient descent algorithm framework quite general applied noisy noiseless observations general case noisy observations show algorithm guaranteed linearly converge unknown lowrank matrix minimax optimal statistical error provided appropriate initial estimator generic noiseless setting algorithm converges unknown lowrank matrix linear rate enables exact recovery optimal sample complexity addition develop new initialization algorithm provide desired initial estimator outperforms existing initialization algorithms nonconvex lowrank matrix estimation illustrate superiority framework three examples matrix regression matrix completion onebit matrix completion also corroborate theory extensive experiments synthetic data\",\"sparse coding consists representing signals sparse linear combinations atoms selected dictionary consider extension framework atoms assumed embedded tree achieved using recently introduced treestructured sparse regularization norm proven useful several applications norm leads regularized problems difficult optimize propose paper efficient algorithms solving precisely show proximal operator associated norm computable exactly via dual approach viewed composition elementary proximal operators procedure complexity linear close linear number atoms allows use accelerated gradient techniques solve treestructured sparse approximation problem computational cost traditional ones using lnorm method efficient scales gracefully millions variables illustrate two types applications first consider fixed hierarchical dictionaries wavelets denoise natural images apply optimization tools context dictionary learning learned dictionary elements naturally organize prespecified arborescent structure leading better performance reconstruction natural image patches applied text documents method learns hierarchies topics thus providing competitive alternative probabilistic topic models\",\"many high dimensional sparse learning problems formulated nonconvex optimization popular approach solve nonconvex optimization problems convex relaxations linear semidefinite programming paper study statistical limits convex relaxations particularly consider two problems mean estimation sparse principal submatrix edge probability estimation stochastic block model exploit sumofsquares relaxation hierarchy sharply characterize limits broad class convex relaxations result shows statistical optimality needs compromised achieving computational tractability using convex relaxations compared existing results computational lower bounds statistical problems consider general polynomialtime algorithms rely computational hardness hypotheses problems like planted clique detection theory focuses broad class convex relaxations rely unproven hypotheses\",\"determinantal point processes dpps enable modeling repulsion provide diverse sets points repulsion encoded kernel seen matrix storing similarity points diversity comes fact inclusion probability subset equal determinant submatrice exact algorithm sample dpps uses spectral decomposition computation becomes costly dealing high number points present alternative exact algorithm discrete setting avoids eigenvalues eigenvectors computation instead relies cholesky decompositions two steps strategy first samples bernoulli point process appropriate distribution samples target dpp distribution thinning procedure method used innovative algorithm competitive original algorithm even faster applications specified\",\"nonparametric extension tensor regression proposed nonlinearity highdimensional tensor space broken simple local functions incorporating lowrank tensor decomposition compared naive nonparametric approaches formulation considerably improves convergence rate estimation maintaining consistency function class specific conditions estimate local functions develop bayesian estimator gaussian process prior experimental results show theoretical properties high performance terms predicting summary statistic real complex network\",\"paper present unified analysis matrix completion general lowdimensional structural constraints induced norm regularization consider two estimators general problem structured matrix completion provide unified upper bounds sample complexity estimation error analysis relies results generic chaining establish two intermediate results independent interest characterizing size complexity low dimensional subsets high dimensional ambient space certain partial complexity measure encountered analysis matrix completion problems characterized terms well understood complexity measure gaussian widths shown form restricted strong convexity holds matrix completion problems general norm regularization provide several nontrivial examples structures included framework notably recently proposed spectral ksupport norm\",\"consider demixing problem two highdimensional vectors nonlinear observations number observations far less ambient dimension underlying vectors specifically demonstrate algorithm stably estimate underlying components general emphstructured sparsity assumptions components specifically show certain types structured superposition models method provably recovers components given merely mathcalos samples denotes number nonzero entries underlying components moreover method achieves fast linear convergence rate also exhibits fast nearlinear periteration complexity certain types structured models also provide range simulations illustrate performance proposed algorithm\",\"study problem estimating lowrank matrices linear measurements aka matrix sensing nonconvex optimization propose efficient stochastic variance reduced gradient descent algorithm solve nonconvex optimization problem matrix sensing algorithm applicable noisy noiseless settings case noisy observations prove algorithm converges unknown lowrank matrix linear rate minimax optimal statistical error noiseless setting algorithm guaranteed linearly converge unknown lowrank matrix achieves exact recovery optimal sample complexity notably overall computational complexity proposed algorithm defined iteration complexity times per iteration time complexity lower stateoftheart algorithms based gradient descent experiments synthetic data corroborate superiority proposed algorithm stateoftheart algorithms\",\"present technique significantly speeding alternating least squares als gradient descent two widely used algorithms tensor factorization exploiting properties khatrirao product show efficiently address computationally challenging substep algorithms algorithm dfacto requires two sparse matrixvector products easy parallelize dfacto scalable also average times faster competing algorithms variety datasets instance dfacto takes seconds machines perform one iteration als algorithm seconds perform one iteration algorithm million million million dimensional tensor billion nonzero entries\",\"paper aims achieving good estimator gradient function highdimensional space often functions sensitive coordinates gradient function almost sparse propose method gradient estimation combines ideas spalls simultaneous perturbation stochastic approximation compressive sensing aim obtain good estimator without many function evaluations application estimating gradient outer product matrix well standard optimization problems illustrated via simulations\",\"mixed linear regression involves recovery two unknown vectors unlabeled linear measurements sample comes exactly one vectors know one classic problem natural empirically popular approach solution algorithm settings prone bad local minima however iteration fast alternating guessing labels solving labels paper provide new initialization procedure based finding leading two eigenvectors appropriate matrix show resampled version algorithm provably converges correct vectors natural assumptions sampling distribution nearly optimal unimprovable sample complexity provides first characterization ems performance also much lower sample complexity compared standard randomly initialized methods problem\",\"joint blind source separation jbss emerging datadriven technique multiset datafusion paper jbss addressed tensorial perspective show using secondorder multiset statistics jbss specific double coupled canonical polyadic decomposition dccpd problem formulated propose algebraic dccpd algorithm based coupled rank detection mapping algorithm converts possibly underdetermined dccpd set overdetermined cpds latter solved algebraically via generalized eigenvalue decomposition based scheme therefore algorithm deterministic returns exact solution noiseless case noisy case used effectively initialize optimization based dccpd algorithms addition obtain determini stic generic uniqueness conditions dccpd shown relaxed cpd counterpart experiment results given illustrate superiority dccpd standard cpd based bss methods several existing jbss methods regards uniqueness accuracy\",\"study sparse nonnegative least squares snnls problem snnls occurs naturally wide variety applications unknown nonnegative quantity must recovered linear measurements present unified framework snnls based rectified power exponential scale mixture prior sparse codes show proposed framework encompasses large class snnls algorithms provide computationally efficient inference procedure based multiplicative update rules update rules convenient solving large sets snnls problems simultaneously required contexts like sparse nonnegative matrix factorization snmf provide theoretical justification proposed approach showing local minima objective function optimized sparse snnls algorithms presented guaranteed converge set stationary points objective function extend framework snmf showing framework leads many well known snmf algorithms specific choices prior providing guarantee popular subclass proposed algorithms converges set stationary points objective function finally study performance proposed approaches synthetic realworld data\",\"paper considers problem robust subspace recovery given set points mathbbrd many lie ddimensional subspace recover underlying subspace show tylers mestimator used recover underlying subspace percentage inliers larger data points lie general position empirically tylers mestimator compares favorably convex subspace recovery algorithms simulations experiments real data sets\",\"motivated largescale collaborativefiltering applications present noncommuting latent factor nclf tensorcompletion approach modeling threeway arrays diagonal like standard parafac wherein different terms distinguish different kinds threeway relations coclusters determined permutations latent factors first key component algebraic representation usage two noncommutative real trilinear operations building blocks approximation operations standard three dimensional tripleproduct trilinear product twodimensional real vector space representation real clifford algebra certain majorana spinor operations purely ternary cannot decomposed two groupoperations relevant spaces second key component method combining operations using permutationsymmetry preserving linear combinations apply model movielens fannie mae datasets find outperforms parafac model propose future directions unsupervisedlearning\",\"nonnegative matrix factorization nmf common tool audio source separation learning nmf large audio databases one major drawback complexity time ofkn updating dictionary dimension input power spectrograms number basis spectra thus forbidding application signals longer hour provide online algorithm complexity ofk time memory updates dictionary show audio simulations online approach faster short audio signals allows analyze audio signals several hours\",\"many applications data analysis rely decomposition data matrix lowrank sparse component existing methods tackle task use nuclear norm lcost functions convex relaxations rank constraint sparsity measure respectively employ thresholding techniques propose method allows reconstructing tracking subspace upperbounded dimension incomplete corrupted observations require priori information number outliers core algorithm intrinsic conjugate gradient method set orthogonal projection matrices socalled grassmannian nonconvex sparsity measures used outlier detection leads improved performance terms robustly recovering tracking lowrank matrix particular approach cope outliers underlying matrix higher rank stateoftheart methods\",\"synthesis model signals represented sparse combinations atoms dictionary dictionary learning describes acquisition process underlying dictionary given set training samples ideally would achieved optimizing expectation factors underlying distribution training data practice necessary information distribution available therefore real world applications achieved minimizing empirical average available samples main goal paper provide sample complexity estimate controls extent empirical average deviates cost function estimate provides suitable estimate accuracy representation learned dictionary presented approach exemplifies general results proposed authors sample complexity dictionary learning matrix factorizations gribonval gives concrete bounds sample complexity dictionary learning cover variety sparsity measures employed learning procedure\",\"tensor decomposition important technique capturing highorder interactions among multiway data multilinear tensor composition methods tucker decomposition candecompparafac assume complex interactions among objects multilinear thus insufficient represent nonlinear relationships data another assumption methods predefined rank known however rank tensors hard estimate especially cases missing values address issues design bayesian generative model tensor decomposition different traditional bayesian methods highorder interactions tensor entries modeled variational autoencoder proposed model takes advantages neural networks nonparametric bayesian models replacing multilinear product traditional bayesian tensor decomposition complex nonlinear function via neural networks whose parameters learned data experimental results synthetic data realworld chemometrics tensor data demonstrated new model achieve significantly higher prediction performance stateoftheart tensor decomposition approaches\",\"paper study general problem optimizing convex function set times matrices subject rank constraints however existing firstorder methods solving problems either slow converge require multiple invocations singular value decompositions hand factorizationbased nonconvex algorithms much faster require stringent assumptions emphcondition number optimum paper provide novel algorithmic framework achieves best worlds asymptotically fast factorization methods requiring dependency condition number instantiate general framework three important matrix estimation problems impact several practical applications emphnonlinear variant affine rank minimization logistic pca iii precision matrix estimation probabilistic graphical model learning derive explicit bounds sample complexity well running time approach show achieves best possible bounds cases also provide extensive range experimental results demonstrate algorithm provides attractive tradeoff estimation accuracy running time\",\"study theoretical properties learning dictionary signals mathbf xiin mathbb via lminimization assume mathbf xis iid random linear combinations columns complete square invertible reference dictionary mathbf mathbb rktimes random linear coefficients generated either ssparse gaussian model bernoulligaussian model first population case establish sufficient almost necessary condition reference dictionary mathbf locally identifiable local minimum expected lnorm objective function condition covers sparse dense cases random linear coefficients significantly improves sufficient condition gribonval schnass addition show complete mucoherent reference dictionary dictionary absolute pairwise column innerproduct muin local identifiability holds even random linear coefficient vector omu nonzeros average moreover local identifiability results also translate finite sample case high probability provided number signals scales oklog\",\"consider demixing problem two structured highdimensional vectors limited number nonlinear observations nonlinearity due either periodic aperiodic function study certain families structured superposition models propose method provably recovers components given nearly mathcalos samples denotes sparsity level underlying components strictly improves upon previous nonlinear demixing techniques asymptotically matches best possible sample complexity also provide range simulations illustrate performance proposed algorithms\",\"simple computationally efficient scheme treestructured vector quantization presented unlike previous methods quantization error depends intrinsic dimension data distribution rather apparent dimension space data happen lie\",\"latent variable models hidden binary units appear various applications learning models particular presence noise challenging computational problem paper propose novel spectral approach problem based eigenvectors second order moment matrix third order moment tensor observed data prove mild nondegeneracy conditions method consistently estimates model parameters optimal parametric rate tensorbased method generalizes previous orthogonal tensor decomposition approaches hidden units assumed either statistically independent mutually exclusive illustrate consistency method simulated data demonstrate usefulness learning common model population mixtures genetics\",\"density matrices positively semidefinite hermitian matrices unit trace describe states quantum systems many quantum systems physical interest represented highdimensional low rank density matrices popular problem quantum state tomography qst estimate unknown low rank density matrix quantum system conducting pauli measurements main contribution twofold first establish minimax lower bounds schatten pnorms leq pleq infty low rank density matrices estimation pauli measurements previous paper minimax lower bounds proved trace regression model gaussian noise noise assumed common variance paper prove bounds binomial observation model meets actual model qst second study dantzig estimator estimating unknown low rank density matrix binomial observation model using pauli measurements previous papers studied least squares estimator projection estimator proved optimal convergence rates least squares estimator schatten pnorms leq pleq stronger condition optimal convergence rates projection estimator schatten pnorms leq pleq infty paper show results two distinct estimators simultaneously obtained dantzig estimator moreover better convergence rates schatten norm distances proved dantzig estimator conditions weaker needed previous papers objective function replaced negative von neumann entropy obtain sharp convergence rate kullbackleibler divergence\",\"due challenging applications collaborative filtering matrix completion problem widely studied past years different approaches rely different structure assumptions matrix hand focus completion possibly lowrank matrix binary entries socalled bit matrix completion problem approach relies tools machine learning theory empirical risk minimization convex relaxations propose algorithm compute variational approximation pseudoposterior thanks convex relaxation corresponding minimization problem biconvex thus method behaves well practice also study performance variational approximation pacbayesian learning bounds contrary previous works focused upper bounds estimation error various matrix norms able derive analysis pac bound prediction error algorithm focus essentially convex relaxation hinge loss present complete analysis complete simulation study test movielens data set however also discuss variational approximation deal logistic loss\",\"orthogonal matching pursuit omp orthogonal least squares ols widely used sparse signal reconstruction underdetermined linear regression problems performance compressed sensing algorithms depends crucially textita priori knowledge either sparsity signal noise variance sigma sigma unknown general extremely difficult estimate determined models limits application omp ols many practical situations article develop two computationally efficient frameworks namely tfigp rrtigp using omp ols even sigma unavailable tfigp rrtigp analytically shown accomplish successful sparse recovery set restricted isometry conditions design matrix required ompols textita priori knowledge sigma numerical simulations also indicate highly competitive performance tfigp rrtigp comparison ompols textita priori knowledge sigma\",\"johnsonlindenstrauss lemma allows projection points pdimensional euclidean space onto kdimensional euclidean space fracln emphnepsilonepsilon pairwise distances preserved within factor pmepsilon working directly distributions random distances rather resorting moment generating function technique improvement lower bound obtained additional reduction dimension compared bounds found literature least cases additional reduction achieved using moment generating function technique provide lower bound using pairwise distances space points projected pairwise distances space projected points comparison results obtained literature shows bound presented provides additional reduction\",\"tensor data multidimension arrays lowrank decompositionbased regression methods tensor predictors exploit structural information tensor predictors significantly reducing number parameters tensor regression propose method named nact noise augmentation ell regularization core tensor tucker decomposition regularize parameters tensor regression coupled tucker decomposition establish theoretically nact achieves exact ell regularization core tensor tucker decomposition linear generalized linear knowledge nact first tucker decompositionbased regularization method achieve ell core tensors nact implemented iterative procedure involves two straightforward steps iteration generating noisy data based core tensor tucker decomposition updated parameter estimate running regular glm noiseaugmented data vectorized predictors demonstrate implementation nact ell regularization effect simulation studies real data applications results suggest nact improve predictions compared decompositionbased approaches without regularization identifies important predictors though designed purpose\",\"paper propose online algorithm compute matrix factorizations proposed algorithm updates dictionary matrix associated coefficients using single observation time algorithm performs lowrank updates dictionary matrix derive algorithm defining simple objective function minimize whenever observation arrived extend algorithm handling missing data also provide minibatch extension enables compute matrix factorization big datasets demonstrate efficiency algorithm real dataset give comparisons wellknown algorithms stochastic gradient matrix factorization nonnegative matrix factorization nmf\",\"propose novel sparse tensor decomposition method namely tensor truncated power ttp method incorporates variable selection estimation decomposition components sparsity achieved via efficient truncation step embedded tensor power iteration method applies broad family high dimensional latent variable models including high dimensional gaussian mixture mixtures sparse regressions thorough theoretical investigation conducted particular show final decomposition estimator guaranteed achieve local statistical rate strengthen global statistical rate introducing proper initialization procedure high dimensional regimes obtained statistical rate significantly improves shown existing nonsparse decomposition methods empirical advantages ttp confirmed extensive simulated results two real applications clickthrough rate prediction highdimensional gene clustering\",\"superresolution classical problem image processing numerous applications remote sensing image enhancement address superresolution irregularlysampled remote sensing images using optimal interpolation lowresolution reconstruction explore locallyadapted multimodal convolutional models investigate different dictionarybased decompositions namely based principal component analysis pca sparse priors nonnegativity constraints consider application reconstruction sea surface height ssh fields two information sources alongtrack altimeter data sea surface temperature sst data reported experiments demonstrate relevance proposed model especially locallyadapted parametrizations nonnegativity constraints outperform optimallyinterpolated reconstructions\",\"paper investigates phase retrieval problem aims recover signal magnitudes linear measurements develop statistically computationally efficient algorithms situation measurements corrupted sparse outliers take arbitrary values propose novel approach robustify gradient descent algorithm using sample median guide pruning spurious samples initialization local search adopting poisson loss reshaped quadratic loss respectively obtain two algorithms termed mediantwf medianrwf provably recover signal nearoptimal number measurements measurement vectors composed iid gaussian entries logarithmic factor even constant fraction measurements adversarially corrupted show algorithms stable presence additional dense bounded noise analysis accomplished developing nontrivial concentration results medianrelated quantities may independent interest provide numerical experiments demonstrate effectiveness approach\",\"consider factoring lowrank tensors presence outlying slabs problem important practice data collected many realworld applications speech fluorescence social network data fit paradigm prior work tackles problem iteratively selecting fixed number slabs fitting procedure may converge formulate problem groupsparsity promoting point view propose alternating optimization framework handle corresponding ellp pleq minimizationbased lowrank tensor factorization problem proposed algorithm features similar periteration complexity plain trilinear alternating least squares tals algorithm convergence proposed algorithm also easy analyze framework alternating optimization variants addition regularization constraints easily incorporated make use empha priori information latent loading factors simulations real data experiments blind speech separation fluorescence data analysis social network mining used showcase effectiveness proposed algorithm\",\"consider structured matrix factorization model one factor restricted columns lying unit simplex simplexstructured matrix factorization ssmf model associated factorization techniques spurred much interest research topics different areas hyperspectral unmixing remote sensing topic discovery machine learning name paper develop new theoretical ssmf framework whose idea study maximum volume ellipsoid inscribed convex hull data points maximum volume inscribed ellipsoid mvie idea attempted prior literature show sufficient condition mvie framework guarantees exact recovery factors sufficient recovery condition show mvie much relaxed separable nonnegative matrix factorization purepixel search coincidentally also identical minimum volume enclosing simplex known powerful ssmf framework nonseparable problem instances also show mvie practically implemented performing facet enumeration solving convex optimization problem potential mvie framework illustrated numerical results\",\"define discuss first sparse coding algorithm based closedform updates continuous latent variables underlying generative model consists standard spikeandslab prior gaussian noise model closedform solutions mstep equations derived generalizing probabilistic pca resulting algorithm take modes potentially multimodal posterior account computational cost algorithm scales exponentially number hidden dimensions however current computational resources still possible efficiently learn model parameters mediumscale problems thus model applied typical range source separation tasks numerical experiments artificial data verify likelihood maximization show derived algorithm recovers sparse directions standard sparse coding distributions source separation benchmarks comprised realistic data show algorithm competitive recent methods\",\"paper considers problem matrix completion observed entries noisy contain outliers begins introducing new optimization criterion recovered matrix defined solution criterion uses celebrated huber function robust statistics literature downweigh effects outliers practical algorithm developed solve optimization involved algorithm fast straightforward implement monotonic convergent furthermore proposed methodology theoretically shown stable well defined sense promising empirical performance demonstrated via sequence simulation experiments including image inpainting\",\"lowrank representationlrr significant method segmenting data generated union subspaces however known solving lrr program challenging terms time complexity memory footprint size nuclear norm regularized matrix nbyn number samples paper thereby develop fast online implementation lrr reduces memory cost opd ambient dimension estimated rankd crux end nonconvex reformulation lrr program pursues basis dictionary generates uncorrupted observations build theoretical guarantee sequence solutions produced algorithm converges stationary point empirical expected loss function asymptotically extensive experiments synthetic realistic datasets substantiate algorithm fast robust memory efficient\",\"paper study problem noisy tensor completion tensors admit canonical polyadic candecompparafac decomposition one factors sparse present general theoretical error bounds estimate obtained using complexityregularized maximum likelihood principle instantiate bounds case additive white gaussian noise also provide admmtype algorithm solving complexityregularized maximum likelihood problem validate theoretical finding via experiments synthetic data set\",\"consider problem discriminative factor analysis data general nongaussian bayesian model based ranks data proposed first introduce new maxmargin version ranklikelihood discriminative factor model developed integrating maxmargin ranklikelihood linear bayesian support vector machines also built maxmargin principle discriminative factor model extended nonlinear case mixtures local linear classifiers via dirichlet processes fully local conjugacy model yields efficient inference markov chain monte carlo variational bayes approaches extensive experiments benchmark real data demonstrate superior performance proposed model potential applications computational biology\",\"paper proposes subspace decomposition method based overcomplete dictionary sparse representation called sparse signal subspace decomposition method method makes use novel criterion based occurrence frequency atoms dictionary data set criterion well adapted subspacedecomposition dependent basis set adequately ects intrinsic characteristic regularity signal method combines variance sparsity component frequency criteria unified framework takes benefits using overcomplete dictionary preserves details subspace decomposition rejects strong noise method simple linear retrieval operation require prior knowledge distributions parameters applied image denoising demonstrates high performances preserving fine details suppressing strong noise\",\"study low rank matrix tensor completion propose novel algorithms employ adaptive sampling schemes obtain strong performance guarantees algorithms exploit adaptivity identify entries highly informative learning column space matrix tensor consequently results hold even row space highly coherent contrast previous analyses absence noise show one exactly recover times matrix rank merely omegan rlogr matrix entries also show one recover order tensor using omegan rtt logr entries noisy recovery algorithm consistently estimates low rank matrix corrupted noise using omegan textrmpolylogn entries complement study simulations verify theory demonstrate scalability algorithms\",\"propose method called ideal regression approximating arbitrary system polynomial equations system particular type using techniques approximate computational algebraic geometry show solve ideal regression directly without resorting numerical optimization ideal regression useful whenever solution learning problem described system polynomial equations example demonstrate formulate stationary subspace analysis ssa source separation problem terms ideal regression also yields consistent estimator ssa compare estimator simulations previous optimizationbased approaches ssa\",\"collaborative convex framework factoring data matrix nonnegative product sparse coefficient matrix proposed restrict columns dictionary matrix coincide certain columns data matrix thereby guaranteeing physically meaningful dictionary dimensionality reduction use linfty regularization select dictionary data show leads exact convex relaxation case distinct noise free data also show relax restrictiontox constraint initializing alternating minimization approach solution convex model obtaining dictionary close necessarily focus applications proposed framework hyperspectral endmember abundances identification also show application blind source separation nmr data\",\"concerned approximation problem symmetric positive semidefinite matrix due motivation class nonlinear machine learning methods discuss approximation approach call matrix ridge approximation particular define matrix ridge approximation incomplete matrix factorization plus ridge term moreover present probabilistic interpretations using normal latent variable model wishart model approximation approach idea behind latent variable model turn leads efficient iterative method handling matrix ridge approximation problem finally illustrate applications approximation approach multivariate data analysis empirical studies spectral clustering gaussian process regression show matrix ridge approximation iteration potentially useful\",\"paper problem onebit compressed sensing obcs formulated problem probably approximately correct pac learning shown vapnikchervonenkis dimension set halfspaces mathbbrn generated ksparse vectors bounded plus roundoff terms coupling estimate wellestablished results pac learning theory show consistent algorithm recover ksparse vector measurements given signs measurement vector result holds textitall probability measures mathbbrn shown random signflipping errors result increase constant estimate constructing consistent algorithm straightforward present heuristic based ellnorm support vector machine illustrate computational performance superior currently popular method\",\"propose set convex low rank inducing norms coupled matrices tensors hereafter coupled tensors shares information matrices tensors common modes specifically propose mixture overlapped trace norm latent norms matrix trace norm propose new completion algorithm based proposed norms key advantage proposed norms convex find globally optimal solution existing methods coupled learning nonconvex furthermore analyze excess risk bounds completion model regularized proposed norms show proposed norms exploit low rankness coupled tensors leading better bounds compared uncoupled norms synthetic realworld data experiments show proposed completion algorithm compares favorably existing completion algorithms\",\"provide general theory expectationmaximization algorithm inferring high dimensional latent variable models particular make two contributions parameter estimation propose novel high dimensional algorithm naturally incorporates sparsity structure parameter estimation appropriate initialization algorithm converges geometric rate attains estimator nearoptimal statistical rate convergence based obtained estimator propose new inferential procedures testing hypotheses constructing confidence intervals low dimensional components high dimensional parameters broad family statistical models framework establishes first computationally feasible approach optimal estimation asymptotic inference high dimensions theory supported thorough numerical results\",\"consider problem extracting lowdimensional linear latent variable structure highdimensional random variables specifically show mild conditions structure manifests linear space spans conditional means possible consistently recover structure using information second moments random variables finding specialized oneparameter exponential families whose variance function quadratic means allows derivation explicit estimator latent structure approach serves latent variable model estimator tool dimension reduction highdimensional matrix data composed many related variables theoretical results verified simulation studies application genomic data\",\"dictionary learning cuttingedge area imaging processing recently led stateoftheart results many signal processing tasks idea conduct linear decomposition signal using atoms learned usually overcompleted dictionary instead predefined basis determining proper size tobelearned dictionary crucial precision efficiency process existing dictionary learning algorithms choose size quite arbitrarily paper novel regularization method called grouped smoothly clipped absolute deviation gscad employed learning dictionary proposed method simultaneously learn sparse dictionary select appropriate dictionary size efficient algorithm designed based alternative direction method multipliers admm decomposes joint nonconvex problem nonconvex penalty two convex optimization problems several examples presented image denoising experimental results compared stateoftheart approaches\",\"consider following signal recovery problem given measurement matrix phiin mathbbrntimes noisy observation vector cin mathbbrn constructed phitheta epsilon epsilonin mathbbrn noise vector whose entries follow iid centered subgaussian distribution recover signal theta dtheta sparse rca linear transformation dinmathbbrmtimes one natural method using convex optimization solve following problem mintheta phitheta lambdadtheta paper provides upper bound estimate error shows consistency property method assuming design matrix phi gaussian random matrix specifically show noiseless case condition number bounded measurement number ngeq omegaslogp sparsity number true solution recovered high probability noisy case condition number bounded measurement increases faster slogp slogpon estimate error converges zero probability infinity results consistent special case dboldiptimes equivalently lasso improve existing analysis condition number plays critical role analysis consider condition numbers two cases including fused lasso random graph condition number fused lasso case bounded constant condition number random graph case bounded high probability mover textedgeover textvertex larger certain constant numerical simulations consistent theoretical results\",\"sparse representation classifier src utilized various classification problems makes use minimization works well image recognition satisfying subspace assumption paper propose new implementation src via screening establish equivalence original src regularity conditions prove classification consistency latent subspace model contamination results demonstrated via simulations real data experiments new algorithm achieves comparable numerical performance significantly faster\",\"sparse coding core building block many data analysis machine learning pipelines typically solved relying generic optimization techniques optimal class firstorder methods nonsmooth convex functions iterative soft thresholding algorithm accelerated version ista fista however methods dont exploit particular structure problem hand input data distribution acceleration using neural networks proposed citegregor coined lista showed empirically one could achieve high quality estimates iterations modifying parameters proximal splitting appropriately paper study reasons acceleration mathematical analysis reveals related specific matrix factorization gram kernel dictionary attempts nearly diagonalise kernel basis produces small perturbation ell ball factorization succeeds prove resulting splitting algorithm enjoys improved convergence bound respect nonadaptive version moreover analysis also shows conditions acceleration occur mostly beginning iterative process consistent numerical experiments validate analysis showing dictionaries factorization exist adaptive acceleration fails\",\"main goal paper propose novel method perform matrix completion online motivated wide variety applications ranging design recommender systems sensor network localization seismic data reconstruction consider matrix completion problem entries matrix interest observed gradually precisely place situation predictive rule refined incrementally rather recomputed scratch time sample observed entries increases extension existing matrix completion methods sequential prediction context indeed major issue big data era yet little addressed literature algorithm promoted article builds upon soft impute approach introduced mazumder major novelty essentially arises use randomised technique computing updating singular value decomposition svd involved algorithm though disarming simplicity method proposed turns efficient requiring reduced computations several numerical experiments based real datasets illustrating performance displayed together preliminary results giving theoretical basis\",\"extracting underlying lowdimensional space highdimensional signals often reside long center numerous algorithms signal processing machine learning literature past decades time working incomplete partly observed large scale datasets recently commonplace diverse reasons called big data era currently living calls devising online subspace learning algorithms suitably handle incomplete data envisaged objective recursively estimate unknown subspace processing streaming data sequentially thus reducing computational complexity obviating need storing whole dataset memory paper online variational bayes subspace learning algorithm partial observations presented account unawareness true rank subspace commonly met practice lowrankness explicitly imposed sought subspace data matrix exploiting sparse bayesian learning principles moreover sparsity simultaneously lowrankness favored subspace matrix sophisticated hierarchical bayesian scheme adopted proposed algorithm becomes adept dealing applications whereby underlying subspace may also sparse sparse dictionary learning problems shown new subspace tracking scheme outperforms stateoftheart counterparts terms estimation accuracy variety experiments conducted simulated real data\",\"consider generalization lowrank matrix completion case data belongs algebraic variety data point solution system polynomial equations case original matrix possibly highrank becomes lowrank mapping column higher dimensional space monomial features many wellstudied extensions linear models including affine subspaces union described variety model addition varieties used model richer class nonlinear quadratic higher degree curves surfaces study sampling requirements matrix completion variety model focus union affine subspaces also propose efficient matrix completion algorithm minimizes convex nonconvex surrogate rank matrix monomial features algorithm uses wellknown kernel trick avoid working directly highdimensional monomial matrix show proposed algorithm able recover synthetically generated data predicted sampling complexity bounds proposed algorithm also outperforms standard low rank matrix completion subspace clustering techniques experiments real data\",\"finding sparse solutions underdetermined systems linear equations fundamental problem signal processing statistics become subject interest recent years general systems infinitely many solutions however may shown sufficiently sparse solutions may identified uniquely words corresponding linear transformation invertible restrict domain sufficiently sparse vectors property may used example solve underdetermined blind source separation bss problem find sparse representation signal overcomplete dictionary primitive elements socalled atomic decomposition main drawback current methods finding sparse solutions computational complexity paper show detecting active components potential solution components considerable value framework fast solution problem may devised idea leads family algorithms called iterative detectionestimation ide converge solution successive detection estimation active part comparing performance ides one successful method date based linear programming improvement speed two three orders magnitude observed\",\"subdifferential convex functions singular spectrum real matrices widely studied matrix analysis optimization automatic control theory convex analysis optimization spaces tensors gaining much interest due potential applications signal processing statistics engineering goal paper present applications problem low rank tensor recovery based linear random measurement extending results tropp tensors setting\",\"present sparse estimation dictionary learning framework compressed fiber sensing based probabilistic hierarchical sparse model handle severe dictionary coherence selective shrinkage achieved using weibull prior related nonconvex optimization pnorm constraints addition leverage specific dictionary structure promote collective shrinkage based local similarity model incorporated form kernel function joint prior density sparse coefficients thereby establishing markov random fieldrelation approximate inference accomplished using hybrid technique combines hamilton monte carlo gibbs sampling estimate dictionary parameter pursue two strategies relying either deterministic probabilistic model dictionary parameter first strategy parameter estimated based alternating estimation second strategy jointly estimated along sparse coefficients performance evaluated comparison existing method various scenarios using simulations experimental data\",\"nonorthogonal joint diagonalization njd free prewhitening widely studied context blind source separation bss array signal processing etc however njd used retrieve jointly diagonalizable structure single set target matrices mostly formulized single dataset thus insufficient handle multiple datasets interset dependences scenario often encountered joint bss jbss applications present generalized njd gnjd algorithm simultaneously perform asymmetric njd upon multiple sets target matrices mutually linked loading matrices using decomposition successive rotations enable jbss multiple datasets indicationexploitation mutual dependences experiments synthetic realworld datasets provided illustrate performance proposed algorithm\",\"propose generic framework based new stochastic variancereduced gradient descent algorithm accelerating nonconvex lowrank matrix recovery starting appropriate initial estimator proposed algorithm performs projected gradient descent based novel semistochastic gradient specifically designed lowrank matrix recovery based upon mild restricted strong convexity smoothness conditions derive projected notion restricted lipschitz continuous gradient property prove algorithm enjoys linear convergence rate unknown lowrank matrix improved computational complexity moreover algorithm employed noiseless noisy observations optimal sample complexity minimax optimal statistical rate attained respectively illustrate superiority generic framework several specific examples theoretically experimentally\",\"note answer question lecue showing column normalization random matrix iid entries need lead good sparse recovery properties even generating random variable reasonable moment growth specifically every leq leq clog construct random vector iid meanzero variance coordinates satisfies supt xtlq leq csqrtq every leq leq show leq csqrtpdp tildegammard columnnormalized matrix generated independent copies probability least expcm tildegamma satisfy exact reconstruction property order\",\"ksupport norm regularizer successfully applied sparse vector prediction problems show belongs general class norms formulated parameterized infimum quadratics extend ksupport norm matrices observe special case matrix cluster norm using formulation derive efficient algorithm compute proximity operator norms improves upon standard algorithm ksupport norm allows apply proximal gradient methods cluster norm also describe solve regularization problems employ centered versions norms finally apply matrix regularizers different matrix completion multitask learning datasets results indicate spectral ksupport norm cluster norm give state art performance problems significantly outperforming trace norm elastic net penalties\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"1_matrix_dictionary_algorithm\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"1_matrix_dictionary_algorithm\"],\"textfont\":{\"size\":12},\"x\":[12.145502,11.09653,12.219085,11.433933,11.587796,12.382005,11.347239,11.2108965,11.855518,12.303023,12.122091,12.508808,11.675749,11.116187,12.204993,11.666345,12.713981,12.188936,12.099795,11.357904,12.435762,11.8852,11.387549,11.952392,11.318461,11.044593,12.870106,11.134865,11.994064,12.302763,12.487493,12.519814,12.913234,11.647213,11.833636,12.073971,12.246742,11.437386,12.928259,12.096097,12.21253,12.146989,11.357086,11.275818,11.826332,11.718302,12.861058,11.561141,11.244816,11.355234,12.893059,11.170361,11.153429,11.904639,12.199988,11.964501,11.740416,12.749873,12.054964,12.014237,11.701091,12.381048,12.461766,12.779214,11.738266,11.926297,11.337558,12.096344,11.177521,11.868494,12.496834,11.525004,12.152632,11.895983,12.07533,11.446704,11.653995,12.071308,11.318361,11.807307,12.808077,11.5760145,11.925496,12.157215,11.391356,11.341626,12.064899,11.52474,11.741695,12.442573,11.305584,12.094257,12.042397,11.298782,12.409111,11.417391,11.902201,11.262414,11.177336,11.602625,11.668023,11.882074,11.000858,12.139842,12.243007,11.6975355,12.6313925,11.309863,11.802223,12.475544,11.70121,12.442197,11.572148,11.7588825,11.350835,11.325256,11.285268,11.61252,12.859327,11.209412,12.055567,12.708913,12.499716,11.471616,11.897248,11.296895,11.477926,11.996006,12.303528,12.2050495,11.880212],\"y\":[5.7462907,5.6028247,5.2390676,5.909,5.947005,5.0040426,5.9999304,5.7686467,6.2536364,4.9726267,5.552302,5.394785,5.873379,5.7748847,5.809564,5.9692655,5.3094006,5.763802,5.7661495,4.787451,5.272464,5.491374,4.776198,5.961588,4.954015,5.6546645,5.361941,4.9007554,5.7098174,5.8739176,6.151099,6.2227454,5.3699327,5.912703,5.529482,5.3777742,5.4392524,6.092425,5.3519945,5.5654464,5.7370334,5.2066274,5.482758,5.630189,5.7387495,6.1445622,5.397848,5.9800115,5.7898307,6.118859,5.361087,4.9006357,5.709767,5.535102,5.7886415,5.4434896,4.9752765,5.361971,5.7153215,5.4389286,4.955443,5.308824,5.323797,5.3503757,4.9565864,5.4767485,5.812648,5.8624144,5.7359524,6.2887173,5.3008666,4.87216,5.400342,5.799553,5.85199,5.506486,6.163849,5.9662004,5.385993,5.6846013,5.362966,5.1701775,5.4691067,5.7785416,5.8634286,4.9320354,5.636252,5.9439216,5.8967066,5.2574816,4.746874,5.765062,5.437757,6.0719914,5.211921,5.0501184,5.444306,4.842198,5.6077743,5.954668,5.2071505,5.5717487,5.564834,5.483667,5.719412,4.9163327,6.2014647,5.727961,4.999442,5.4029045,5.5137973,5.053936,5.9250546,4.9810624,4.7351313,4.718334,5.7506795,5.910691,5.3498874,5.811554,5.44047,5.598417,5.322874,5.9459,5.819765,5.9877357,5.4340186,6.2883325,5.3835115,5.3498735,5.5417933],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"consider learning highdimensional multiresponse linear models structured parameters exploiting noise correlations among responses propose alternating estimation altest procedure estimate model parameters based generalized dantzig selector suitable sample size resampling assumptions show error estimates generated altest high probability converges linearly certain minimum achievable level tersely expressed geometric measures gaussian width sets related parameter structure best knowledge first nonasymptotic statistical guarantee altesttype algorithm applied estimation problem general structures\",\"lasso computationally efficient regression regularization procedure produce sparse estimators number predictors large oracle inequalities provide probability loss bounds lasso estimator deterministic choice regularization parameter bounds tend zero appropriately controlled thus commonly cited theoretical justification lasso ability handle highdimensional settings unfortunately practice regularization parameter selected deterministic quantity instead chosen using random datadependent procedure address shortcoming previous theoretical work study loss lasso estimator tuned optimally prediction assuming orthonormal predictors sparse true model prove probability best possible predictive performance lasso deteriorates increases positive arbitrarily close one given sufficiently high signal noise ratio sufficiently large demonstrate empirically amount deterioration performance far worse oracle inequalities suggest provide real data example deterioration observed\",\"address problem defining group sparse formulation principal components analysis pca equivalent formulations low rank approximation dictionary learning problems achieves compromise maximizing variance explained components promoting sparsity loadings propose first new definition variance explained non necessarily orthogonal components optimal aspect compatible principal components situation use specific regularization variance groupell norm define group sparse maximum variance gsmv formulation pca gsmv formulation achieves objective construction nice property inner non smooth optimization problem solved analytically thus reducing gsmv maximization smooth convex function unit norm orthogonality constraints generalizes journee group sparsity numerical comparison deflation synthetic data shows gsmv produces steadily slightly better robust results retrieval hidden sparse structures three times faster examples application real data shows interest group sparsity variables selection pca mixed data categoricalnumerical\",\"recent paper shown lasso algorithm exhibits nearideal behavior following sense suppose eta satisfies restricted isometry property rip sufficiently small constant vert eta vert leq epsilon minimizing vert vert subject vert vert leq epsilon leads estimate hatx whose error vert hatx vert bounded universal constant times error achieved oracle knows location nonzero components world optimization lasso algorithm generalized several directions group lasso sparse group lasso either without treestructured overlapping groups recently sorted lasso paper shown algorithm exhibits nearideal behavior sense provided norm used define sparsity index decomposable penalty norm minimized effort enforce sparsity gammadecomposable iii compressibility condition terms group restricted isometry property satisfied specifically group lasso sparse group lasso permissible overlap groups well sorted ellnorm minimization exhibit nearideal behavior explicit bounds residual error derived contain previously known results special cases\",\"present supervisedlearning algorithm graph data set graphs arbitrary twicedifferentiable loss functions sparse linear models possible subgraph features date shown possible subgraph features several types sparse learning adaboost lpboost larslasso sparse pls regression performed particularly emphasis placed simultaneous learning relevant features infinite set candidates first generalize techniques used preceding studies derive unifying bounding technique arbitrary separable functions carefully use bounding make block coordinate gradient descent feasible infinite subgraph features resulting fast converging algorithm solve wider class sparse learning problems graph data also empirically study differences existing approaches convergence property selected subgraph features searchspace sizes discuss several unnoticed issues sparse learning possible subgraph features\",\"introduce computationally effective algorithm linear model selection consisting three steps screeningorderingselection sos screening predictors based thresholded lasso penalized least squares screened predictors fitted using least squares ordered respect statistics finally model selected using greedy generalized information criterion gic penalized nested family induced ordering give nonasymptotic upper bounds error probability step sos algorithm terms penalties obtain selection consistency different scenarios conditions needed screening consistency lasso traditional setting give sanovtype bounds error probabilities orderingselection algorithm surprising consequence selection error greedy gic asymptotically larger exhaustive gic also obtain new bounds prediction estimation errors lasso proved parallel algorithm used practice formal version\",\"factorized information criterion fic recently developed information criterion based novel model selection methodology namely factorized asymptotic bayesian fab inference developed successfully applied various hierarchical bayesian models dirichlet process prior one well known representations chinese restaurant process crp derive another line model selection methods fic viewed prior distribution latent variable configurations view prove parameter dimensionality fic equivalent crp argue fic avoids inherent problem dpcrp data likelihood dominate impact prior thus model selection capability weaken increases however fic overestimates data likelihood result fic may overly biased towards models less components propose natural generalization fic finds middle ground crp fic may yield accurate model selection results fic\",\"model selection crucial issue machinelearning wide variety penalisation methods possibly data dependent complexity penalties recently introduced purpose however empirical performance generally well documented literature goal paper investigate extent recent techniques successfully used tuning regularisation kernel parameters support vector regression svr complexity measure regression trees cart task traditionally solved via vfold crossvalidation vfcv gives efficient results reasonable computational cost disadvantage however vfcv procedure known provide asymptotically suboptimal risk estimate number examples tends infinity recently penalisation procedure called vfold penalisation proposed improve vfcv supported theoretical arguments report extensive set experiments comparing vfold penalisation vfcv svrcart calibration several benchmark datasets highlight cases vfcv vfold penalisation provide poor estimates risk respectively introduce modified penalisation technique reduce estimation error\",\"multitask learning shown significantly enhance performance multiple related learning tasks variety situations present fused logistic regression sparse multitask learning approach binary classification specifically introduce sparsity inducing penalties parameter differences related logistic regression models encode similarity across related tasks resulting joint learning task cast form lends efficiently optimized recursive variant alternating direction method multipliers show results synthetic data describe regime settings multitask approach achieves significant improvements single task learning approach discuss implications applying fused logistic regression different real world settings\",\"propose communicationefficient distributed estimation method sparse linear discriminant analysis lda high dimensional regime method distributes data size machines estimates local sparse lda estimator machine using data subset size distributed estimation method aggregates debiased local estimators machines sparsifies aggregated estimator show aggregated estimator attains statistical rate centralized estimation method long number machines chosen appropriately moreover prove method attain model selection consistency milder condition centralized method experiments synthetic real datasets corroborate theory\",\"boosting gradient descent algorithms one popular method machine learning paper novel boostingtype algorithm proposed based restricted gradient descent structural sparsity control whose underlying dynamics governed differential inclusions particular present iterative regularization path structural sparsity parameter sparse linear transforms based variable splitting linearized bregman iteration hence called emphsplit lbi despite simplicity split lbi outperforms popular generalized lasso theory experiments theory path consistency presented equipped proper early stopping split lbi may achieve model selection consistency family irrepresentable conditions weaker necessary sufficient condition generalized lasso furthermore ell error bounds also given minimax optimal rates utility benefit algorithm illustrated several applications including image denoising partial order ranking sport teams world university grouping crowdsourced ranking data\",\"investigate properties estimators obtained minimization uprocesses lasso penalty highdimensional settings attention focused ranking problem popular machine learning related guessing ordering objects basis observed predictors prove oracle inequality excess risk considered estimator well bound distance estimator oracle\",\"describe simple efficient permutation based procedure selecting penalty parameter lasso procedure intended applications variable selection primary focus applied variety structural settings including generalized linear models briefly discuss connections permutation selection existing theory lasso addition present simulation study analysis three real data sets permutation selection compared crossvalidation bayesian information criterion bic selection method based recently developed testing procedures lasso\",\"study propose automatic learning method variables selection based lasso epidemiology context one aim approach overcome pretreatment experts medicine epidemiology collected data pretreatment consist recoding variables choose interactions based expertise approach proposed uses available explanatory variables without treatment generate automatically interactions lead high dimension use lasso one robust methods variable selection high dimension avoid fitting two levels crossvalidation used target variable account variable lasso estimators biased variables selected lasso debiased glm used predict distribution main vector malaria anopheles results show climatic environmental variables mains factors associated malaria risk exposure\",\"group lasso penalized regression method used regression problems covariates partitioned groups promote sparsity group level existing methods finding group lasso estimator either use gradient projection methods update entire coefficient vector simultaneously step update one group coefficients time using inexact line search approximate optimal value group coefficients groups coefficients fixed present new method computation group lasso linear regression case single line search sls algorithm operates computing exact optimal value group coefficients fixed one univariate line search perform simulations demonstrating sls algorithm often efficient existing computational methods also extend sls algorithm sparse group lasso problem via signed single line search ssls algorithm give theoretical results support algorithms\",\"paper consider problem learning highdimensional tensor regression problems lowrank structure one core challenges associated learning highdimensional models computation since underlying optimization problems often nonconvex convex relaxations could lead polynomialtime algorithms often slow practice hand limited theoretical guarantees exist nonconvex methods paper provide general framework provides theoretical guarantees learning highdimensional tensor regression models different lowrank structural assumptions using projected gradient descent algorithm applied potentially nonconvex constraint set theta terms emphlocalized gaussian width juxtapose theoretical results nonconvex projected gradient descent algorithms previous results regularized convex approaches two main differences convex nonconvex approach computational perspective whether nonconvex projection operator computable whether projection desirable contraction properties statistical upper bound perspective nonconvex approach superior rate number examples provide three concrete examples lowdimensional structure address issues explain pros cons nonconvex convex approaches supplement theoretical results simulations show several common settings generalized low rank tensor regression projected gradient descent approach superior terms statistical error runtime provided stepsizes projected descent algorithm suitably chosen\",\"propose new two stage algorithm ling large scale regression problems ling risk well known ridge regression fixed design setting computed much faster experiments shown ling performs well terms prediction accuracy computational efficiency compared large scale regression algorithms like gradient descent stochastic gradient descent principal component regression simulated real datasets\",\"compare risk ridge regression simple variant ordinary least squares one simply projects data onto finite dimensional subspace specified principal component analysis performs ordinary unregularized least squares regression subspace note shows risk ordinary least squares method within constant factor namely risk ridge regression\",\"many distributed learning problems heterogeneous loading computing machines may harm overall performance synchronous strategies paper propose effective asynchronous distributed framework minimization sum smooth functions machine performs iterations parallel local function updates shared parameter asynchronously way machines continuously work even though latest version shared parameter prove convergence consistency general distributed asynchronous method gradient iterations show efficiency matrix factorization problem recommender systems binary classification\",\"order identify important variables involved making optimal treatment decision proposed penalized least squared regression framework fixed number predictors robust misspecification conditional mean model two problems arise world explosively big data effective methods needed handle ultrahigh dimensional data set example dimension predictors nonpolynomial order sample size propensity score conditional mean models need estimated data dimensionality paper propose twostep estimation procedure deriving optimal treatment regime dimensionality steps penalized regressions employed nonconcave penalty function conditional mean model response given predictors may misspecified asymptotic properties weak oracle properties selection consistency oracle distributions proposed estimators investigated addition study limiting distribution estimated value function obtained optimal treatment regime empirical performance proposed estimation method evaluated simulations application depression dataset stard study\",\"sparse alphanorm regularization many datarich applications marketing economics alphanorm contrast lasso ridge regularization jumps sparse solution feature attractive ultra highdimensional problems occur demand estimation forecasting alphanorm objective nonconvex requires coordinate descent proximal operators find sparse solution study typical marketing demand forecasting problem grocery store sales salty snacks many dummy variables controls key predictors demand include price equivalized volume promotion flavor scent brand effects comparing many commonly used machine learning methods alphanorm regularization achieves goal providing accurate outofsample estimates promotion lift effects finally conclude directions future research\",\"regression models increasingly built using datasets follow design experiment instead data gathered automated monitoring technical system consequence already input data represents phenomena system violates statistical assumptions distributions input data show correlations clusters patterns distribution input data influences reliability regression models propose criteria quantify typical phenomena input data regression show suitability simulated benchmark datasets regressionen werden zunehmend auf datensatzen angewendet deren eingangsvektoren nicht durch eine statistische versuchsplanung festgelegt wurden stattdessen werden die daten beispielsweise durch die passive beobachtung technischer systeme gesammelt damit bilden bereits die eingangsdaten phanomene des systems und widersprechen statistischen verteilungsannahmen die verteilung der eingangsdaten hat einfluss auf die zuverlassigkeit eines regressionsmodells wir stellen deshalb bewertungskriterien fur einige typische phanomene eingangsdaten von regressionen vor und zeigen ihre funktionalitat anhand simulierter benchmarkdatensatze\",\"paper deals problem largescale linear supervised learning settings large number continuous features available propose combine wellknown trick onehot encoding continuous features new penalization called emphbinarsity group binary features coming onehot encoding single raw continuous feature penalization uses totalvariation regularization together extra linear constraint induces two interesting properties model weights onehot encoded features piecewise constant eventually block sparse nonasymptotic oracle inequalities generalized linear models proposed moreover sparse additive model assumption prove procedure matches stateoftheart setting numerical experiments illustrate good performances approach several datasets also noteworthy method numerical complexity comparable standard ell penalization\",\"multiple linear regression setting propose general framework termed weighted orthogonal components regression wocr encompasses many known methods special cases including ridge regression principal components regression wocr makes use monotonicity inherent orthogonal components parameterize weight function formulation allows efficient determination tuning parameters hence computationally advantageous moreover wocr offers insights deriving new better variants specifically advocate weighting components based correlations response leads enhanced predictive performance simulated studies real data examples provided assess illustrate advantages proposed methods\",\"popular sparse estimation methods based ellrelaxation lasso dantzig selector require knowledge variance noise order properly tune regularization parameter constitutes major obstacle applying methods several frameworkssuch time series random fields inverse problemsfor noise rarely homoscedastic level hard know advance paper propose new approach joint estimation conditional mean conditional variance highdimensional auto regression setting attractive feature proposed estimator efficiently computable even large scale problems solving secondorder cone program socp present theoretical analysis numerical results assessing performance proposed procedure\",\"minimum description length mdl principle states optimal model given data set compresses best due practial limitations model restricted class linear regression models address study formulations lasso forward stepwise regression interested sparsifying feature set preserving generalization ability derive wellprincipled set codes parameters error residuals along smooth approximations lengths codes allow gradient descent optimization description length show sparsification feature selection using approach faster lasso several datasets uci statlib repositories favorable generalization accuracy fully automatic requiring neither crossvalidation tuning regularization hyperparameters allowing even nonlinear expansion feature set followed sparsification\",\"linear regression models depend directly design matrix properties techniques efficiently estimate model coefficients partitioning rows design matrix increasingly popular largescale problems fit well modern parallel computing architectures propose simple measure concordance design matrix subset rows estimates well subset captures variancecovariance structure larger data set illustrate use measure heuristic method selecting row partition sizes balance statistical computational efficiency goals realworld problems\",\"propose loco algorithm largescale ridge regression distributes features across workers cluster important dependencies variables preserved using structured random projections cheap compute must communicated show loco obtains solution close exact ridge regression solution fixed design setting verify experimentally simulation study well application climate prediction furthermore show loco achieves significant speedups compared stateoftheart distributed algorithm largescale regression problem\",\"inferring predictive maps multiple input multiple output variables tasks innumerable applications data science multitask learning attempts learn maps several output tasks simultaneously information sharing propose novel multitask learning framework sparse linear regression full task hierarchy automatically inferred data assumption task parameters follow hierarchical tree structure leaves tree parameters individual tasks root global model approximates tasks apply proposed approach develop evaluate predictive models plant traits using largescale automated remote sensing data gwas methodologies mapping derived phenotypes lieu handmeasured traits demonstrate superior performance approach compared methods well usefulness discovering hierarchical groupings tasks results suggest richer genetic mapping indeed obtained remote sensing data addition discovered groupings reveal interesting insights plant science perspective\",\"although various distributed machine learning schemes proposed recently pure linear models fully nonparametric models little attention paid distributed optimization semiparamemetric models multiplelevel structures sparsity linearity nonlinearity address issues current paper proposes new communicationefficient distributed learning algorithm partially sparse linear models increasing number features proposed method based classical divide conquer strategy handing big data submethod defined subsample consists debiased estimation doubleregularized least squares approach proposed method theoretically prove global parametric estimator achieve optimal parametric rate semiparametric model given appropriate partition total data specially choice data partition relies underlying smoothness nonparametric component adaptive sparsity parameter even nondistributed setting develop new easilyread proof optimal estimation parametric error high dimensional partial linear model finally several simulated experiments implemented indicate comparable empirical performance debiased technique distributed setting\",\"sparse regularization ell regularization quite powerful widely used strategy high dimensional learning problems effectiveness sparse regularization supported practically theoretically several studies however one biggest issues sparse regularization performance quite sensitive correlations features ordinary ell regularization select variables correlated results deterioration generalization error also interpretability paper propose new regularization method independently interpretable lasso iilasso proposed regularizer suppresses selecting correlated variables thus active variable independently affects objective variable model hence interpret regression coefficients intuitively also improve performance avoiding overfitting analyze theoretical property iilasso show proposed method much advantageous sign recovery achieves almost minimax optimal convergence rate synthetic real data analyses also indicate effectiveness iilasso\",\"consider problem robustifying highdimensional structured estimation robust techniques key realworld applications often involve outliers data corruption focus trimmed versions structurally regularized mestimators highdimensional setting including popular least trimmed squares estimator well analogous estimators generalized linear models graphical models using possibly nonconvex loss functions present general analysis statistical convergence rates consistency take closer look trimmed versions lasso graphical lasso estimators special cases optimization side show extend algorithms mestimators fit trimmed variants provide guarantees numerical convergence generality competitive performance highdimensional trimmed estimators illustrated numerically simulated realworld genomics data\",\"concave regularization methods provide natural procedures sparse recovery however difficult analyze high dimensional setting recently sparse recovery results established specific local solutions obtained via specialized numerical procedures still fundamental relationship solutions whether identical relationship global minimizer underlying nonconvex formulation unknown current paper fills conceptual gap presenting general theoretical framework showing appropriate conditions global solution nonconvex regularization leads desirable recovery performance moreover suitable conditions global solution corresponds unique sparse local solution obtained via different numerical procedures unified framework present overview existing results discuss connections unified view work leads satisfactory treatment concave high dimensional sparse estimation procedures serves guideline developing numerical procedures concave regularization\",\"paper study problem recovering group sparse vector small number linear measurements past common approach use various group sparsityinducing norms group lasso norm purpose using theory convex relaxations show also possible use ellnorm minimization group sparse recovery introduce new concept called group robust null space property grnsp show suitable conditions group version restricted isometry property grip implies grnsp thus leads group sparse recovery groups equal size bounds less conservative known bounds moreover results apply even situations groups different sizes specialized conventional sparsity bounds reduce one wellknown best possible conditions sparse recovery relationship grnsp grip new even conventional sparsity substantially streamlines proofs known results using relationship derive bounds ellpnorm residual error vector measurement matrix consists random samples subgaussian random variable present bounds number measurements less conservative currently known bounds\",\"supervised linear feature extraction achieved fitting reduced rank multivariate model paper studies rank penalized rank constrained vector generalized linear models perspective thresholding rules build framework fitting singular value penalized models use feature extraction solving rank constraint form problem propose progressive feature space reduction fast computation high dimensions little performance loss novel projective crossvalidation proposed parameter tuning nonconvex setups real data applications given show power methodology supervised dimension reduction feature extraction\",\"number recent work studied effectiveness feature selection using lasso known restricted isometry properties rip lasso generally lead exact recovery set nonzero coefficients due looseness convex relaxation paper considers feature selection property nonconvex regularization solution given multistage convex relaxation scheme appropriate conditions show local solution obtained procedure recovers set nonzero coefficients without suffering bias lasso relaxation complements parameter estimation results procedure\",\"extend adaptive regression spline model incorporating saturation natural requirement function extend constant outside certain range fit saturating splines data using convex optimization problem space measures solve using efficient algorithm based conditional gradient method unlike many existing approaches algorithm solves original infinitedimensional splines degree least two optimization problem without prespecified knot locations adapt algorithm fit generalized additive models saturating splines coordinate functions show saturation requirement allows model simultaneously perform feature selection nonlinear function fitting finally briefly sketch method extended higher order splines different requirements extension outside data range\",\"study additive models built trend filtering additive models whose components regularized discrete total variation kth discrete derivative chosen integer geq results kth degree piecewise polynomial components gives piecewise constant components gives piecewise linear gives piecewise quadratic etc analogous advantages univariate case additive trend filtering favorable theoretical computational properties thanks large part localized nature discrete total variation regularizer uses theory side derive fast error rates additive trend filtering estimates show rates minimax optimal underlying function additive component functions whose derivatives bounded variation also show rates unattainable additive smoothing splines additive models built linear smoothers general computational side per standard additive models backfitting appealing method optimization particularly appealing additive trend filtering leverage highly efficient univariate trend filtering solvers going one step describe new backfitting algorithm whose iterations run parallel far know first kind lastly present experiments examine empirical performance additive trend filtering\",\"propose robust inferential procedure assessing uncertainties parameter estimation highdimensional linear models dimension grow exponentially fast sample size method combines debiasing technique composite quantile function construct estimator asymptotically normal hence used construct valid confidence intervals conduct hypothesis tests estimator robust require existence first second moment noise distribution also preserves efficiency sense worst case efficiency loss less compared squarelossbased debiased lasso estimator many cases estimator close better latter especially noise heavytailed debiasing procedure require solving lpenalized composite quantile regression instead allows firststage estimator desired convergence rate empirical sparsity paper also provides new proof techniques developing theoretical guarantees inferential procedures nonsmooth loss functions establish main results exploit local curvature conditional expectation composite quantile loss apply empirical process theories control difference empirical quantities conditional expectations results established weaker assumptions compared existing work inference highdimensional quantile regression furthermore consider highdimensional simultaneous test regression parameters applying gaussian approximation multiplier bootstrap theories also study distributed learning exploit divideandconquer estimator reduce computation complexity sample size massive finally provide empirical results verify theory\",\"propose nuclear norm penalty alternative ridge penalty regularized multinomial regression convex relaxation reducedrank multinomial regression advantage leveraging underlying structure among response categories make better predictions apply method nuclear penalized multinomial regression npmr major league baseball playbyplay data predict outcome probabilities based batterpitcher matchups interpretation results meshes well subjectarea expertise also suggests novel understanding differentiates players\",\"tuning parameter selection critical importance kernel ridge regression date data driven tuning method divideandconquer kernel ridge regression dkrr lacking literature limits applicability dkrr large data sets paper modifying generalized crossvalidation gcv wahba score propose distributed generalized crossvalidation dgcv datadriven tool selecting tuning parameters dkrr proposed dgcv computationally scalable massive data sets also shown mild conditions asymptotically optimal sense minimizing dgcv score equivalent minimizing true global conditional empirical loss averaged function estimator extending existing optimality results gcv divideandconquer framework\",\"theoretically investigate convergence rate support consistency correctly identifying subset nonzero coefficients large sample limit multiple kernel learning mkl focus mkl blockl regularization inducing sparse kernel combination blockl regularization inducing uniform kernel combination elasticnet regularization including blockl blockl regularization case true kernel combination sparse show sharper convergence rate blockl elasticnet mkl methods existing rate blockl mkl show elasticnet mkl requires milder condition consistent blockl mkl case optimal kernel combination exactly sparse prove elasticnet mkl achieve faster convergence rate blockl blockl mkl methods carefully controlling balance blockland blockl regularizers thus theoretical results overall suggest use elasticnet regularization mkl\",\"machine learning data mining linear models widely used model response parametric linear functions predictors relax stringent assumptions made parametric linear models additive models consider response summation unknown transformations applied predictors particular additive isotonic models aims assume unknown transformations monotone paper introduce sparse linear isotonic models slims highdimensional problems hybridizing ideas parametric sparse linear models aims enjoy appealing advantages highdimensional setting twostep algorithm proposed estimating sparse parameters well monotone functions predictors mild statistical assumptions show algorithm accurately estimate parameters promising preliminary experiments presented support theoretical results\",\"consider problem binary classification one particular cost choose classify observation present simple proof oracle inequality excess risk structural risk minimizers using lasso type penalty\",\"partial least squares pls methods heavily exploited analyse association two blocs data powerful approaches applied data sets number variables greater number observations presence high collinearity variables different sparse versions pls developed integrate multiple data sets simultaneously selecting contributing variables sparse modelling key factor obtaining better estimators identifying associations multiple data sets cornerstone sparsity version pls methods link svd matrix constructed deflated versions original matrices data least squares minimisation linear regression present accurate description popular pls methods alongside mathematical proofs unified algorithm proposed perform four types pls including regularised versions various approaches decrease computation time offered show whole procedure scalable big data sets\",\"consider empirical risk minimization problem linear supervised learning regularization structured sparsityinducing norms defined sums euclidean norms certain subsets variables extending usual ellnorm group ellnorm allowing subsets overlap leads specific set allowed nonzero patterns solutions problems first explore relationship groups defining norm resulting nonzero patterns providing forward backward algorithms back forth groups patterns allows design norms adapted specific prior knowledge expressed terms nonzero patterns also present efficient active set algorithm analyze consistency variable selection leastsquares linear regression low highdimensional settings\",\"study novel splinelike basis name falling factorial basis bearing many similarities classic truncated power basis advantage falling factorial basis enables rapid lineartime computations basis matrix multiplication basis matrix inversion falling factorial functions actually splines close enough splines provably retain favorable properties latter functions examine application two problems trend filtering arbitrary input points higherorder variant twosample kolmogorovsmirnov test\",\"distributed learning probabilistic models multiple data repositories minimum communication increasingly important study simple communicationefficient learning framework first calculates local maximum likelihood estimates mle based data subsets combines local mles achieve best possible approximation global mle given whole dataset study frameworks statistical properties showing efficiency loss compared global setting relates much underlying distribution families deviate full exponential families drawing connection theory information loss fisher rao efron show fullexponentialfamilyness represents lower bound error rate arbitrary combinations local mles achieved kldivergencebased combination method common linear combination method also study empirical properties methods showing method significantly outperforms linear combination practical settings issues model misspecification nonconvexity heterogeneous data partitions\",\"paper study nonconvex penalization using bernstein functions since bernstein function concave nonsmooth origin induce class nonconvex functions highdimensional sparse estimation problems derive threshold function based bernstein penalty give mathematical properties sparsity modeling show coordinate descent algorithm especially appropriate penalized regression problems bernstein penalty additionally prove bernstein function defined concave conjugate varphidivergence develop conjugate maximization algorithm finding sparse solution finally particularly exemplify family bernstein nonconvex penalties based generalized gamma measure conduct empirical analysis family\",\"investigate learning rate multiple kernel leaning mkl elasticnet regularization consists ellregularizer inducing sparsity ellregularizer controlling smoothness focus sparse setting total number kernels large number nonzero components ground truth relatively small prove elasticnet mkl achieves minimax learning rate ellmixednorm ball bound sharper convergence rates ever shown property smoother truth faster convergence rate\",\"compressed sensing order recover sparse nearly sparse vector possibly noisy measurements popular approach ellnorm minimization upper bounds ell norm error true estimated vectors given reviewed bounds ellnorm given unknown vector conventionally sparse group sparse instead variety alternatives ellnorm proposed literature including group lasso sparse group lasso group lasso tree structured overlapping groups however error bounds available modified objective functions present paper unified approach presented deriving upper bounds error true vector approximation based notion decomposable gammadecomposable norms bounds presented cover norms mentioned also provide guideline choosing norms future accommodate alternate forms sparsity\",\"propose framework perform streaming covariance selection approach employs regularization constraints timevarying sparsity parameter iteratively estimated via stochastic gradient descent allows regularization parameter efficiently learnt online manner proposed framework developed linear regression models extended graphical models via neighbourhood selection mild assumptions able obtain convergence results nonstochastic setting capabilities approach demonstrated using synthetic data well neuroimaging data\",\"stochastic gradient descent sgd algorithm widely used statistical estimation largescale data due computational memory efficiency existing works focus convergence objective function error obtained solution investigate problem statistical inference true model parameters based sgd population loss function strongly convex satisfies certain smoothness conditions main contributions twofold first fixed dimension setup propose two consistent estimators asymptotic covariance average iterate sgd plugin estimator batchmeans estimator computationally efficient uses iterates sgd proposed estimators allow construct asymptotically exact confidence intervals hypothesis tests second highdimensional linear regression using variant sgd algorithm construct debiased estimator regression coefficient asymptotically normal gives onepass algorithm computing sparse regression coefficients confidence intervals computationally attractive applicable online data\",\"introduce application group lasso design experiments note trying explain experimental design group lasso conversely explain use idea group lasso experimental design showing problem constructing optimal design matrix transformed problem group lasso numerical examples show obtain orthogonal arrays solutions group lasso problems\",\"deep learning methods multitask neural networks recently applied ligandbased virtual screening drug discovery applications using set industrial admet datasets compare neural networks standard baseline models analyze multitask learning effects random crossvalidation relevant temporal validation scheme confirm multitask learning provide modest benefits singletask models show smaller datasets tend benefit larger datasets multitask learning additionally find adding massive amounts side information guaranteed improve performance relative simpler multitask learning results emphasize multitask effects highly datasetdependent suggesting use datasetspecific models maximize overall performance\",\"generalised degrees freedom gdf defined jasa represent sensitivity model fits perturbations data computed statistical model making possible principle derive number parameters machinelearning approaches defined originally normally distributed data investigate potential approach bernoullidata gdfvalues models simulated real data compared model complexityestimates crossvalidation similarly computed gdfbased aicc randomforest neural networks boosted regression trees demonstrated similarity crossvalidation gdfestimates binary data unstable inconsistently sensitive number data points perturbed simultaneously time extremely computerintensive calculation repeated fold crossvalidation robust based fewer assumptions faster compute findings suggest gdfapproach readily transfer bernoulli data wider range regression approaches\",\"sparse bayesian learning sbl gaussian scale mixtures gsms used model sparsityinducing priors realize class concave penalty functions regression task realvalued signal models motivated relative scarcity formal tools sbl complexvalued models paper proposes gsm model bessel model induces concave penalty functions estimation complex sparse signals properties bessel model analyzed applied type type estimation analysis reveals tuning parameters mixing pdf different penalty functions invoked depending estimation type used value noise variance whether real complex signals estimated using bessel model derive sparse estimator based modification expectationmaximization algorithm formulated type estimation estimator includes special instance algorithms proposed tipping faul babacan numerical results show superiority proposed estimator stateoftheart estimators terms convergence speed sparseness reconstruction error robustness low medium signaltonoise ratio regimes\",\"life sciences experts generally use empirical knowledge recode variables choose interactions perform selection classical approach aim work perform automatic learning algorithm variables selection lead know experts help decision simply replaced machine improve knowledge results lasso method detect optimal subset variables estimation prediction conditions paper propose novel approach uses automatically variables available interactions double crossvalidation combine lasso select best subset variables glm simple crossvalidation perform predictions algorithm assures stability consistency estimators\",\"temporal group lasso example multitask regularized regression approach prediction response variables vary time aim work introduce reader concepts behind temporal group lasso related methods well type potential applications healthcare setting method argue method attractive ability reduce overfitting select predictors learn smooth effect patterns time finally simplicity\",\"subset selection multiple linear regression aims construct regression model minimizes errors selecting small number explanatory variables model built various statistical tests diagnostics conducted validate model determine whether regression assumptions met traditional approaches require human decisions step example user adding removing variable satisfactory model obtained however trialanderror strategy cannot guarantee subset minimizes errors satisfying regression assumptions found paper propose fully automated model building procedure multiple linear regression subset selection integrates model building validation based mathematical programming proposed model minimizes mean squared errors ensuring majority important regression assumptions met also propose efficient constraint approximate constraint coefficient ttest subset satisfies considered regression assumptions model provides alternative subset satisfies assumptions computational results show model yields better solutions satisfying regression assumptions compared stateoftheart benchmark models maintaining similar explanatory power\",\"propose method finding alternate features missing lasso optimal solution ordinary lasso problem one global optimum obtained resulting features interpreted taskrelevant features however overlook possibly relevant features selected lasso proposed method provide lasso optimal solution also possible alternate features lasso solution show alternate features computed efficiently avoiding redundant computations also demonstrate proposed method works newsgroup data shows reasonable features found alternate features\",\"propose approach multivariate nonparametric regression generalizes reduced rank regression linear models additive model estimated dimension qdimensional response shared pdimensional predictor variable control complexity model employ functional form kyfan nuclear norm resulting set function estimates low rank backfitting algorithms derived justified using nonparametric form nuclear norm subdifferential oracle inequalities excess risk derived exhibit scaling behavior procedure high dimensional setting methods illustrated gene expression data\",\"significant attention given minimizing penalized least squares criterion estimating sparse solutions large linear systems equations penalty responsible inducing sparsity natural choice socalled norm paper develop momentumized iterative shrinkage thresholding mist algorithm minimizing resulting nonconvex criterion prove convergence local minimizer simulations large data sets show superior performance proposed method methods\",\"paper develops general theoretical framework analyze structured sparse recovery problems using notation dual certificate although certain aspects dual certificate idea already used previous work due lack general coherent theory analysis far carried limited scopes specific problems context current paper makes two contributions first introduce general definition dual certificate use develop unified theory sparse recovery analysis convex programming second present class structured sparsity regularization called structured lasso calculations readily performed theoretical framework new theory includes many seemingly loosely related previous work special cases also implies new results improve existing ones even standard formulations regularization\",\"propose novel algorithm greedy forward feature selection regularized leastsquares rls regression classification also known leastsquares support vector machine ridge regression algorithm call greedy rls starts empty feature set iteration adds feature whose addition provides best leaveoneout crossvalidation performance method considerably faster previously proposed ones since time complexity linear number training examples number features original data set desired size set selected features therefore side effect obtain new training algorithm learning sparse linear rls predictors used large scale learning speed possible due matrix calculus based shortcuts leaveoneout feature addition experimentally demonstrate scalability algorithm ability find good quality feature sets\",\"paper introduce new optimization formulation sparse regression compressed sensing called clot combined lone two wherein regularizer convex combination ell ellnorms formulation differs elastic net formulation regularizer convex combination ell ellnorm squared shown context compressed sensing formulation achieve robust recovery sparse vectors whereas new clot formulation achieves robust recovery also like unlike lasso clot formulation achieves grouping effect wherein coefficients highly correlated columns measurement design matrix assigned roughly comparable values already known lasso grouping effect therefore clot formulation combines best features lasso robust sparse recovery grouping effect clot formulation special case another one called sgl sparse group lasso introduced literature previously without analysis either grouping effect robust sparse recovery shown sgl achieves robust sparse recovery also achieves version grouping effect coefficients highly correlated columns belonging group measurement design matrix assigned roughly comparable values\",\"recent studies literature paid much attention sparsity linear classification tasks one motivation imposing sparsity assumption linear discriminant direction rule noninformative features making hardly contribution classification problem work focused scenarios binary classification presence multiclass data preceding researches recommended individually pairwise sparse linear discriminant analysislda however sparsity explored paper estimator grouped lasso type proposed take advantage sparsity multiclass data enjoys appealing nonasymptotic properties allows insignificant correlations among features estimator exhibits superior capability simulated real data\",\"multitask sparse feature learning aims improve generalization performance exploiting shared features among tasks successfully applied many applications including computer vision biomedical informatics existing multitask sparse feature learning algorithms formulated convex sparse regularization problem usually suboptimal due looseness approximating elltype regularizer paper propose nonconvex formulation multitask sparse feature learning based novel nonconvex regularizer solve nonconvex optimization problem propose multistage multitask feature learning msmtfl algorithm also provide intuitive interpretations detailed convergence reproducibility analysis proposed algorithm moreover present detailed theoretical analysis showing msmtfl achieves better parameter estimation error bound convex formulation empirical studies synthetic realworld data sets demonstrate effectiveness msmtfl comparison state art multitask sparse feature learning algorithms\",\"performance orthogonal matching pursuit omp variable selection analyzed random designs contrasted deterministic case since performance measured averaging distribution design matrix one far less stringent sparsity constraints coefficient vector demonstrate exact sparse vectors performance omp similar known results lasso algorithm textitieee trans inform theory textbf moreover variable selection relaxed sparsity assumption coefficient vector whereby one control ell norm smaller coefficients also analyzed consequence results also show coefficient estimate satisfies strong oracle type inequalities\",\"given observation highdimensional ornsteinuhlenbeck process continuous time proceed inference drift parameter rowsparsity assumption towards aim consider negative loglikelihood process penalized ellpenalization lasso adaptive lasso provide nonasymptotic asymptotic results procedure means sharp oracle inequality limit theorem longtime asymptotics including asymptotic consistency variable selection byproduct point fact ornsteinuhlenbeck process one need assumption restricted eigenvalue type order derive fast rates lasso wellknown mandatory linear regression instance numerical results illustrate benefits penalized procedure compared standard maximum likelihood approaches simulations realworld financial data\",\"sparsity learning known grouping structure received considerable attention due wide modern applications highdimensional data analysis although advantages using group information wellstudied shrinkagebased approaches benefits group sparsity welldocumented greedytype methods much limits understanding use important class methods paper generalizing popular forwardbackward greedy approach propose new interactive greedy algorithm group sparsity learning prove proposed greedytype algorithm attains desired benefits group sparsity high dimensional settings estimation error bound refining existing methods guarantee group support recovery also established simultaneously addition incorporate general mestimation framework introduce interactive feature allow extra algorithm flexibility without compromise theoretical properties promising use proposal demonstrated numerical evaluations including real industrial application human activity recognition home supplementary materials article available online\",\"consider problem multivariate regression setting relevant predictors could shared among different responses propose algorithm decomposes coefficient matrix product long matrix wide matrix elastic net penalty former ell penalty latter first matrix linearly transforms predictors set latent factors second one regresses responses factors algorithm simultaneously performs dimension reduction coefficient estimation automatically estimates number latent factors data formulation results nonconvex optimization problem despite flexibility impose effective lowdimensional structure difficult even impossible solve exactly reasonable time specify optimization algorithm based alternating minimization three different sets updates solve nonconvex problem provide theoretical results convergence optimality finally demonstrate effectiveness algorithm via experiments simulated real data\",\"graphical lasso popular method learning structure undirected graphical model based regularization technique objective paper compare computationallyheavy technique numericallycheap heuristic method based simply thresholding sample covariance matrix end two notions signconsistent inverseconsistent matrices developed shown thresholding methods equivalent thresholded sample covariance matrix signconsistent inverseconsistent gap largest thresholded smallest unthresholded entries sample covariance matrix small building upon result proved methodas conic optimization problemhas explicit closedform solution thresholded sample covariance matrix acyclic structure result generalized arbitrary sparse support graphs formula found obtain approximate solution furthermore shown approximation error derived explicit formula decreases exponentially fast respect length minimumlength cycle sparsity graph developed results demonstrated synthetic data functional mri data traffic flows transportation networks massive randomly generated data sets show proposed method obtain accurate approximation instances sizes large times billion variables less minutes standard laptop computer running matlab stateoftheart methods converge within hours\",\"propose nonconvex estimator joint multivariate regression precision matrix estimation high dimensional regime sparsity constraints gradient descent algorithm hard thresholding developed solve nonconvex estimator attains linear rate convergence true regression coefficients precision matrix simultaneously statistical error compared existing methods along line research little theoretical guarantee proposed algorithm computationally much efficient provable convergence guarantee also attains optimal finite sample statistical rate logarithmic factor thorough experiments synthetic real datasets back theory\",\"paper study nonconvex penalization using bernstein functions whose firstorder derivatives completely monotone bernstein function induce class nonconvex penalty functions highdimensional sparse estimation problems derive thresholding function based bernstein penalty discuss important mathematical properties sparsity modeling show coordinate descent algorithm especially appropriate regression problems penalized bernstein function also consider application bernstein penalty classification problems devise proximal alternating linearized minimization method based theory kurdykalojasiewicz inequality conduct convergence analysis alternating iteration procedures particularly exemplify family bernstein nonconvex penalties based generalized gamma measure conduct empirical analysis family\",\"focus maximum regularization parameter anisotropic totalvariation denoising corresponds minimum value regularization parameter solution remains constant value well know lasso critical value investigated details totalvariation though importance tuning regularization parameter allows fixing upperbound grid optimal parameter sought establish closed form expression onedimensional case well upperbound twodimensional case appears reasonably tight practice problem directly linked computation pseudoinverse divergence quickly obtained performing convolutions fourier domain\",\"consider multitask learning simultaneously learns related prediction tasks improve generalization performance factorize coefficient matrix product two matrices based lowrank assumption matrices sparsities simultaneously perform variable selection learn overlapping group structure among tasks resulting biconvex objective function minimized alternating optimization subproblems solved using alternating direction method multipliers accelerated proximal gradient descent moreover provide performance bound proposed method effectiveness proposed method validated synthetic realworld datasets\",\"paper propose study family sparsityinducing penalty functions since penalty functions related kinetic energy special relativity call emphkinetic energy plus kep functions construct kep function using concave conjugate chidistance function present several novel insights kep function particular derive thresholding operator based kep function prove mathematical properties asymptotic properties sparsity modeling moreover show coordinate descent algorithm especially appropriate kep function additionally discuss relationship kep penalty functions ell mcp theoretical empirical analysis validates kep function effective efficient highdimensional data modeling\",\"propose novel application simultaneous orthogonal matching pursuit somp procedure sparsistant variable selection ultrahigh dimensional multitask regression problems screening variables introduced citefansis efficient highly scalable way remove many irrelevant variables set variables retaining relevant variables somp applied problems hundreds thousands variables number variables reduced manageable size computationally demanding procedure used identify relevant variables regression outputs knowledge first attempt utilize relatedness multiple outputs perform fast screening relevant variables main theoretical contribution prove asymptotically somp guaranteed reduce ultrahigh number variables sample size without losing true relevant variables also provide formal evidence modified bayesian information criterion bic used efficiently determine number iterations somp provide empirical evidence benefit variable selection using multiple regression outputs jointly opposed performing variable selection output separately finite sample performance somp demonstrated extensive simulation studies genetic association mapping problem keywords adaptive lasso greedy forward regression orthogonal matching pursuit multioutput regression multitask learning simultaneous orthogonal matching pursuit sure screening variable selection\",\"study problem learning sparse linear regression vector additional conditions structure sparsity pattern problem relevant machine learning statistics signal processing well known linear regression benefit knowledge underlying regression vector sparse combinatorial problem selecting nonzero components vector relaxed regularizing squared error convex penalty function like ell norm however many applications additional conditions structure regression vector sparsity pattern available incorporating information learning method may lead significant decrease estimation error paper present family convex penalty functions encode prior knowledge structure vector formed absolute values regression coefficients family subsumes ell norm flexible enough include different models sparsity patterns practical theoretical importance establish basic properties penalty functions discuss examples computed explicitly moreover present convergent optimization algorithm solving regularized least squares penalty functions numerical simulations highlight benefit structured sparsity advantage offered approach lasso method related methods\",\"taking account highorder interactions among covariates valuable many practical regression problems however computationally challenging task number highorder interaction features considered would extremely large unless number covariates sufficiently small paper propose novel efficient algorithm lassobased sparse learning highorder interaction models basic strategy reducing number features employ idea recently proposed safe feature screening sfs rule sfs rule property feature satisfies rule feature guaranteed nonactive lasso solution meaning safely screenedout prior lasso training process large number features screenedout training lasso computational cost memory requirment dramatically reduced however applying sfs rule extremely large number highorder interaction features would computationally infeasible key idea solving computational issue exploit underlying tree structure among highorder interaction features specifically introduce pruning condition called safe feature pruning sfp rule property rule satisfied certain node tree highorder interaction features corresponding descendant nodes guaranteed nonactive optimal solution algorithm extremely efficient making possible work order interactions original covariates number possible highorder interaction features greater\",\"problem learning sparse model conceptually interpreted process identifying active featuressamples optimizing model recently introduced safe screening allows identify part nonactive featuressamples far safe screening individually studied either feature screening sample screening paper introduce new approach safely screening features samples simultaneously alternatively iterating feature sample screening steps significant advantage considering simultaneously rather individually synergy effect sense results previous safe feature screening exploited improving next safe sample screening performances viceversa first theoretically investigate synergy effect illustrate practical advantage intensive numerical experiments problems large numbers features samples\",\"least absolute shrinkage selection operator lasso method adapted recently networkstructured datasets particular network lasso method allows learn graph signals small number noisy signal samples using total variation graph signal regularization efficient scalable implementations network lasso available little known conditions underlying network structure ensure network lasso accurate leveraging concepts compressed sensing address gap derive precise conditions underlying network topology sampling set guarantee network lasso particular loss function deliver accurate estimate entire underlying graph signal also quantify error incurred network lasso terms two constants reflect connectivity sampled nodes\",\"propose new sparsitysmoothness penalty highdimensional generalized additive models combination sparsity smoothness crucial mathematical theory well performance finitesample data present computationally efficient algorithm provable numerical convergence properties optimizing penalized likelihood furthermore provide oracle results yield asymptotic optimality estimator high dimensional sparse additive models finally adaptive version sparsitysmoothness penalized approach yields large additional performance gains\",\"subset selection multiple linear regression aims choose subset candidate explanatory variables tradeoff fitting error explanatory power model complexity number variables selected build mathematical programming models regression subset selection based mean square absolute errors minimalredundancymaximalrelevance criteria proposed models tested using linearprogrambased branchandbound algorithm tailored valid inequalities big values compared algorithms literature high dimensional cases iterative heuristic algorithm proposed based mathematical programming models core set concept randomized version algorithm derived guarantee convergence global optimum computational experiments find models quickly find quality solution rest time spent prove optimality iterative algorithms find solutions relatively short time competitive compared stateoftheart algorithms using adhoc big values recommended\",\"paper provides estimation inference methods conditional average treatment effects cate characterized highdimensional parameter homogeneous crosssectional unitheterogeneous dynamic panel data settings leading example model cate interacting base treatment variable explanatory variables first step procedure orthogonalization partial controls unit effects outcome base treatment take crossfitted residuals step uses novel generic crossfitting method design weakly dependent time series panel data method leaves neighbors fitting nuisance components theoretically power using strassens coupling result rely modern machine learning method first step provided learns residuals well enough second construct orthogonal residual learner cate lasso cate regresses outcome residual vector interactions residualized treatment explanatory variables complexity cate function simpler firststage regression orthogonal learner converges faster singlestage regressionbased learner third perform simultaneous inference parameters cate function using debiasing also use ordinary least squares last two steps cate lowdimensional heterogeneous panel data settings model unobserved unit heterogeneity weakly sparse deviation mundlak model correlated unit effects linear function timeinvariant covariates make use lpenalization estimate models demonstrate methods estimating price elasticities groceries based scanner data note results new even crosssectional iid case\",\"additive nonparametric regression models provide attractive tool variable selection high dimensions relationship response predictors complex offer greater flexibility compared parametric nonlinear regression models better interpretability scalability nonparametric regression models however achieving sparsity simultaneously number nonparametric components well variables within nonparametric component poses stiff computational challenge article develop novel bayesian additive regression model using combination hard soft shrinkages separately control number additive components variables within component efficient algorithm developed select importance variables estimate interaction network excellent performance obtained simulated real data examples\",\"sparse mapping key methodology many highdimensional scientific problems multiple tasks share set relevant features learning jointly group drastically improves quality relevant feature selection however practice technique used limitedly since grouping information usually hidden paper goal recover group structure sparsity patterns leverage information sparse learning toward formulate joint optimization problem task parameter group membership constructing appropriate regularizer encourage sparse learning well correct recovery task groups demonstrate proposed method recovers groups sparsity patterns task parameters accurately extensive experiments\",\"structured sparsity recently emerged statistics machine learning signal processing promising paradigm learning highdimensional settings existing methods learning assumption structured sparsity rely prior knowledge weight penalize individual subsets variables subset selection process available general inferring group weights data key open research problem structured sparsityin paper propose bayesian approach problem group weight learning model group weights hyperparameters heavytailed priors groups variables derive approximate inference scheme infer hyperparameters empirically show able recover model hyperparameters data generated model demonstrate utility learning weights synthetic real denoising problems\",\"forward regression statistical model selection estimation procedure inductively selects covariates add predictive power working statistical regression model model selected unknown regression parameters estimated least squares paper analyzes forward regression highdimensional sparse linear models probabilistic bounds prediction error norm number selected covariates proved analysis paper gives sharp rates require betamin irrepresentability conditions\",\"propose data aggregationbased algorithm monotonic convergence global optimum generalized version lnorm error fitting model assumption fitting function proposed algorithm generalizes recent algorithm literature aggregate iterative disaggregate aid selectively solves three specific lnorm error fitting problems proposed algorithm lnorm error fitting model solved optimally follows form lnorm error fitting problem fitting function satisfies assumption proposed algorithm also solve multidimensional fitting problems arbitrary constraints fitting coefficients matrix generalized problem includes popular models regression orthogonal procrustes problem results computational experiment show proposed algorithms faster stateoftheart benchmarks lnorm regression subset selection lnorm regression sphere relative performance proposed algorithm improves data size increases\",\"present method variable selection sparse generalized additive model method doesnt assume specific functional form select large number candidates takes form incremental forward stagewise regression given functional form assumed devised approach termed roughening adjust residuals iterations simulations show new method competitive popular machine learning approaches also demonstrate performance using real datasets method available part nlnet package cran httpscranrprojectorgpackagenlnet\",\"paper combine two important extensions ordinary least squares regression regularization optimal scaling optimal scaling sometimes also called optimal scoring originally developed categorical data process finds quantifications categories optimal regression model sense maximize multiple correlation although optimal scaling method developed initially variables limited number categories optimal transformations continuous variables special case consider variety transformation types typically use step functions categorical variables smooth spline functions continuous variables types functions restricted monotonic preserving ordinal information data addition optimal scaling three regularization methods considered ridge regression lasso elastic net resulting method called ros regression regularized optimal scaling regression show basic algorithm provides straightforward efficient estimation regularized regression coefficients automatically gives group lasso blockwise sparse regression extends monotonicity properties show optimal scaling linearizes nonlinear relationships predictors outcome improves upon condition predictor correlation matrix increasing average conditional independence predictors alternative options regularization either regression coefficients category quantifications mentioned extended examples provided keywords categorical data optimal scaling conditional independence step functions splines monotonic transformations regularization lasso elastic net group lasso blockwise sparse regression\",\"nonparametric methods widely applicable statistical inference problems since rely modeling assumptions context fresh look advocated permeates benefits variable selection compressive sampling robustify nonparametric regression outliers data markedly deviating postulated models variational counterpart leasttrimmed squares regression shown closely related lpseudonormregularized estimator encourages sparsity vector explicitly modeling outliers connection suggests efficient solvers based convex relaxation lead naturally variational mtype estimator equivalent leastabsolute shrinkage selection operator lasso outliers identified judiciously tuning regularization parameters amounts controlling sparsity outlier vector along whole robustification path lasso solutions reduced bias enhanced generalization capability attractive features improved estimator obtained replacing lpseudonorm nonconvex surrogate novel robust splinebased smoother adopted cleanse load curve data key task aiding operational decisions envisioned smart grid system computer simulations tests real load curve data corroborate effectiveness novel sparsitycontrolling robust estimators\",\"consider problem estimating regression function common situation number features small interpretability model high priority simple linear additive models fail provide adequate performance address problem present gaptv approach conceptually related cart recent crisp algorithm stateoftheart alternative method interpretable nonlinear regression gaptv divides feature space blocks constant value fits value blocks jointly via convex optimization routine method fully dataadaptive incorporates highly robust routines tuning hyperparameters automatically compare approach cart crisp demonstrate gaptv finds much better tradeoff accuracy interpretability\",\"propose bayesian regression method accounts multiway interactions arbitrary orders among predictor variables model makes use factorization mechanism representing regression coefficients interactions among predictors interaction selection guided prior distribution random hypergraphs construction generalizes finite feature model present posterior inference algorithm based gibbs sampling establish posterior consistency regression model method evaluated extensive experiments simulated data demonstrated able identify meaningful interactions applications genetics retail demand forecasting\",\"many applied settings empirical economics involve simultaneous estimation large number parameters particular applied economists often interested estimating effects manyvalued treatments like teacher effects location effects treatment effects many groups prediction models many regressors settings machine learning methods combine regularized estimation datadriven choices regularization parameters useful avoid overfitting article analyze performance class machine learning estimators includes ridge lasso pretest contexts require simultaneous estimation many parameters analysis aims provide guidance applied researchers choice regularized estimators practice datadriven selection regularization parameters address characterize risk mean squared error regularized estimators derive relative performance function simple features data generating process address show datadriven choices regularization parameters based steins unbiased risk estimate crossvalidation yield estimators risk uniformly close risk attained optimal unfeasible choice regularization parameters use data recent examples empirical economics literature illustrate practical applicability results\",\"consider problem estimating regression function common situation number features small interpretability model high priority simple linear additive models fail provide adequate performance address problem present maximum variance total variation denoising mvtv approach conceptually related cart recent crisp algorithm stateoftheart alternative method interpretable nonlinear regression mvtv divides feature space blocks constant value fits value blocks jointly via convex optimization routine method fully dataadaptive incorporates highly robust routines tuning hyperparameters automatically compare approach cart crisp via complexityaccuracy tradeoff metric human study demonstrating mvtv powerful interpretable method\",\"shown aictype criteria asymptotically efficient selectors tuning parameter nonconcave penalized regression methods assumption population variance known consistent estimator available relax assumption prove aic asymptotically efficient study performance finite samples classical regression known aic tends select overly complex models dimension maximum candidate model large relative sample size simulation studies suggest aic suffers shortcomings used penalized regression therefore propose use classical corrected aic aicc alternative prove maintains desired asymptotic properties broaden results prove efficiency aic penalized likelihood methods context generalized linear models dispersion parameter similar results exist literature restricted set candidate models employing results classical literature maximumlikelihood estimation misspecified models able establish result general set candidate models use simulations assess performance aic aicc well selectors finite samples scadpenalized lasso regressions real data example considered\",\"consistency doubly robust estimators relies consistent estimation least one two nuisance regression parameters moderate large dimensions use flexible dataadaptive regression estimators may aid achieving consistency however nconsistency doubly robust estimators guaranteed one nuisance estimators inconsistent paper present doubly robust estimator survival analysis novel property converges gaussian variable nrate large class dataadaptive estimators nuisance parameters assumption least one consistently estimated nrate result achieved adaptation recent ideas semiparametric inference amount gaussianizing making asymptotically linear drift term arises asymptotic analysis doubly robust estimator using crossfitting avoid entropy conditions nuisance estimators present formula asymptotic variance estimator allows computation doubly robust confidence intervals pvalues illustrate finitesample properties estimator simulation studies demonstrate use phase iii clinical trial estimating effect novel therapy treatment positive breast cancer\",\"present two sets theoretical results grouped lasso overlap jacob obozinski vert linear regression setting method allows joint selection predictors sparse regression allowing complex structured sparsity predictors encoded set groups flexible framework suggests arbitrarily complex structures encoded intricate set groups results show strategy results unexpected theoretical consequences procedure particular give two sets results finite sample bounds prediction estimation asymptotic distribution selection sets results give insight consequences choosing increasingly complex set groups procedure well happens set groups cannot recover true sparsity pattern additionally results demonstrate differences similarities grouped lasso procedure without overlapping groups analysis shows set groups must chosen caution overly complex set groups damage analysis\",\"existing methods sparse channel estimation typically provide estimate computed solution maximizing objective function defined sum loglikelihood function penalization term proportional lnorm parameter interest however penalization terms proven strong sparsityinducing properties work design pilotassisted channel estimators ofdm wireless receivers within framework sparse bayesian learning defining hierarchical bayesian prior models lead sparsityinducing penalization terms estimators result application variational messagepassing algorithm factor graph representing signal model extended hierarchical prior models numerical results demonstrate superior performance channel estimators compared traditional stateoftheart sparse methods\",\"present extension sparse pca sparse dictionary learning sparsity patterns dictionary elements structured constrained belong prespecified set shapes emphstructured sparse pca based structured regularization recently introduced classical sparse priors deal textitcardinality regularization use encodes higherorder information data propose efficient simple optimization procedure solve problem experiments two practical tasks face recognition study dynamics protein complex demonstrate benefits proposed structured approach unstructured approaches\",\"highdimensional andor nonparametric regression problems regularization penalization used control model complexity induce desired structure penalty weight parameter indicates strongly structure corresponding penalty enforced typically parameters chosen minimize error separate validation set using simple grid search gradientfree optimization method efficient tune parameters gradient determined often difficult problems nonsmooth penalty functions show many penalized regression problems validation loss actually smooth almosteverywhere respect penalty parameters therefore apply modified gradient descent algorithm tune parameters simulation studies example regression problems find increasing number penalty parameters tuning using method decrease generalization error\",\"convex sparsityinducing regularizations ubiquitous highdimensional machine learning solving resulting optimization problems slow accelerate solvers stateoftheart approaches consist reducing size optimization problem hand context regression achieved either discarding irrelevant features screening techniques prioritizing features likely included support solution working set techniques duality comes play several steps techniques propose extrapolation technique starting sequence iterates dual leads construction improved dual points enables tighter control optimality used stopping criterion well better screening performance gap safe rules finally propose working set strategy based aggressive use gap safe screening rules thanks new dual point construction show significant computational speedups multiple realworld problems\",\"sharply characterize performance different penalization schemes problem selecting relevant variables multitask setting previous work focuses regression problem conditions design matrix complicate analysis clearer simpler picture emerges studying normal means model model often used field statistics simplified model provides laboratory studying complex procedures\",\"introduce new approach variable selection called predictive correlation screening predictor design predictive correlation screening pcs implements false positive control selected variables well suited small sample sizes scalable high dimensions establish asymptotic bounds familywise error rate fwer resultant mean square error linear predictor selected variables apply predictive correlation screening following twostage predictor design problem experimenter wants learn multivariate predictor gene expressions based successive biological samples assayed mrna arrays assays whole genome samples assays selects small number variables using predictive correlation screening reduce assay cost subsequently assays selected variables remaining samples learn predictor coefficients show superiority predictive correlation screening relative lasso correlation learning sometimes popularly referred literature marginal regression simple thresholding terms performance computational complexity\",\"performance lasso well understood assumptions standard linear model homoscedastic noise however several applications standard model describe important features data paper examines lasso performs nonstandard model motivated medical imaging applications applications variance noise scales linearly expectation observation like heteroscedastic models noise terms poissonlike model textitnot independent design matrix specifically paper studies sign consistency lasso sparse poissonlike model addition studying sufficient conditions sign consistency lasso estimate paper also gives necessary conditions sign consistency sets conditions comparable results homoscedastic model showing measure signal noise ratio large lasso performs well poissonlike data homoscedastic data simulations reveal lasso performs equally well terms model selection performance poissonlike data homoscedastic data properly scaled noise variance across range parameterizations taken whole results suggest lasso robust poissonlike heteroscedastic noise\",\"compute approximate solutions regularized linear regression using regularization also known lasso initialization step algorithm lass lasszero uses computationally efficient stepwise search determine locally optimal solution given regularization solution present theoretical results consistency orthogonality appropriate handling redundant features empirically use synthetic data demonstrate lass solutions closer true sparse support regularization models additionally realworld data lass finds parsimonious solutions regularization maintaining similar predictive accuracy\",\"popular cubic smoothing spline estimate regression function arises minimizer penalized sum squares sumjyj mutj lambdaintab mut data tjyj minimization taken infinitedimensional function space space functions square integrable second derivatives calculations carried finitedimensional space reduction minimizing infinite dimensional space minimizing finite dimensional space occurs general objective functions data may related function another way sum squares may replaced suitable expression penalty intab mut might take different form paper reviews reproducing kernel hilbert space structure provides finitedimensional solution general minimization problem particular attention paid penalties based linear differential operators case one sometimes easily calculate minimizer explicitly using greens functions\",\"develop new method called discriminated hub graphical lasso dhgl based hub graphical lasso hgl providing prior information hubs apply new method two situations known hubs without known hubs compare dhgl hgl using several measures performance hubs known always estimate precision matrix better via dhgl hgl hubs known use graphical lasso provide information hubs find performance dhgl always better hgl correct prior information given seldom degenerate prior information wrong\",\"article considers problem multigroup classification setting number variables larger number observations several methods proposed literature address problem however variable selection performance either unknown suboptimal results known twogroup case work provide sharp conditions consistent recovery relevant variables multigroup case using discriminant analysis proposal gaynanova achieve rates convergence attain optimal scaling sample size number variables sparsity level rates significantly faster best known results multigroup case moreover coincide optimal minimax rates twogroup case validate theoretical results numerical analysis\",\"consider task fitting regression model involving interactions among potentially large set covariates wish enforce strong heredity propose family general framework task proposal generalization several existing methods vanish radchenko james hiernet bien allpairs lasso lasso using main effects formulated solution convex optimization problem solve using efficient alternating directions method multipliers admm algorithm algorithm guaranteed convergence global optimum easily specialized convex penalty function interest allows straightforward extension setting generalized linear models derive unbiased estimator degrees freedom family explore performance simulation study hiv sequence data set\",\"introduce gamsel generalized additive model selection penalized likelihood approach fitting sparse generalized additive models high dimension method interpolates null linear additive models allowing effect variable estimated either zero linear lowcomplexity curve determined data present blockwise coordinate descent procedure efficiently optimizing penalized likelihood objective dense grid tuning parameter producing regularization path additive models demonstrate performance method real simulated data examples compare existing techniques additive model selection\",\"propose scalable efficient statistically motivated computational framework graphical lasso friedman covariance regularization framework received significant attention statistics community past years existing algorithms trouble scaling dimensions larger thousand proposal significantly enhances stateoftheart moderate sized problems gracefully scales larger problems algorithms become practically infeasible requires key new ideas operate primal problem use subtle variation blockcoordinatemethods drastically reduces computational complexity orders magnitude provide rigorous theoretical guarantees convergence complexity algorithm demonstrate effectiveness proposal via experiments believe framework extends applicability graphical lasso largescale modern applications like bioinformatics collaborative filtering social networks among others\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"2_lasso_regression_sparse\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"2_lasso_regression_sparse\"],\"textfont\":{\"size\":12},\"x\":[11.765684,10.888387,11.075854,11.137326,11.501837,10.542998,12.689625,10.476101,11.066493,11.670389,11.186089,10.703964,10.548762,10.4273405,11.038593,11.887464,10.59745,10.607044,11.7584095,10.6009245,11.045462,10.678197,11.099684,10.547624,11.047019,10.945402,10.632451,10.602474,8.198602,11.6573105,11.007373,10.556449,11.436191,11.38375,10.536399,10.985392,11.211776,11.309003,10.685009,10.730984,10.393529,11.0166645,11.254056,10.653256,10.763745,11.041214,11.326168,11.643181,11.598099,11.026489,11.379762,10.84851,11.631806,11.109005,10.986284,10.372481,11.781936,10.3564415,10.869205,10.770645,10.851839,10.874353,11.530665,11.396886,10.693031,11.395713,11.150066,11.069373,11.246172,10.942072,11.267103,10.735625,10.441839,11.668148,11.572994,11.10652,11.141807,11.602954,10.360726,11.149797,10.123543,10.071392,11.60012,11.3250885,10.813453,10.653565,11.194846,11.227872,11.462177,10.63668,10.867722,11.212613,10.54064,10.840078,10.763869,12.673557,10.669778,10.8610935,10.750581,10.653875,11.116755,11.711303,11.198082,10.86045,11.240761,10.68535,10.150667,10.931614,10.901466,11.231786,10.476981,11.180802,10.444693,11.237745,10.500336,10.993903],\"y\":[6.31928,6.4520435,6.1740203,6.312844,6.28045,6.4440355,6.5872817,7.052036,6.5922427,6.791973,6.505586,6.5892467,6.3910303,6.444877,6.303325,6.7810483,6.190713,6.0708613,6.9500327,7.203044,6.4886537,7.208284,6.34688,6.136436,6.3128057,6.4114537,6.134591,6.2115016,8.137513,6.8515067,6.426716,6.308762,6.207195,6.1555386,5.9442344,6.3594146,7.0857487,7.0544696,7.295016,6.8312206,7.1007214,6.8039904,6.822154,6.6119995,6.0824404,7.1283226,7.009238,6.82362,6.438861,6.783259,6.1692147,6.31377,6.6408606,6.2715316,6.565013,7.1939516,6.608659,6.446691,6.412204,6.9679036,6.4006248,6.7920475,6.285356,6.143348,6.3239655,6.1372337,6.347023,6.432897,6.17219,6.5364494,6.403599,6.2032,6.0366507,6.4573936,6.4757643,6.4884186,6.6000934,6.494182,6.329212,6.4236665,6.5314503,7.1241927,6.4292064,6.786691,6.9658017,7.1832075,6.8414083,6.332278,6.4230094,6.6807737,7.045874,6.8557844,6.0861106,7.0702367,7.0169463,6.66183,7.1238675,7.05162,7.1426573,7.2727222,6.2886677,6.528473,6.0726357,6.915996,7.2307196,6.860082,6.138525,6.397626,6.4176493,7.173607,6.3111567,6.33774,6.4165974,6.9691896,6.343905,6.5917377],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"minimizing empirical risk popular training strategy learning tasks data may noisy heavytailed one may require many observations order generalize well achieve better performance less stringent requirements introduce procedure constructs robust approximation risk gradient use iterative learning routine using highprobability bounds excess risk algorithm show update deviate far ideal gradientbased update empirical tests using controlled simulations realworld benchmark data show diverse settings proposed procedure learn efficiently using less resources iterations observations generalizing better\",\"study strictly proper scoring rules reproducing kernel hilbert space propose general kernel scoring rule associated kernel divergence consider conditions kernel score strictly proper demonstrate kernel score includes maximum mean discrepancy special case also consider connections kernel score minimum risk proper loss function show kernel score incorporates information pertaining projected embedded distributions compared maximum mean discrepancy finally show integrate information provided different kernel divergences proposed bhattacharyya kernel divergence using oneclass classifier improved twosample hypothesis testing results\",\"twosample feature selection problem finding features describe difference two probability distributions ubiquitous problem scientific engineering studies however existing methods limited applicability restrictive assumptions data distributoins computational difficulty paper resolve difficulties formulating problem sparsest ksubgraph problem proposed method nonparametric assume specific parametric models data distributions show proposed method computationally efficient require extra computation model selection moreover prove proposed method provides consistent estimator features mild conditions experimental results show proposed method outperforms current method regard accuracy computation time\",\"autoencoder neural network implemented estimate missing data genetic algorithm implemented network optimization estimating missing data missing data treated missing random mechanism implementing maximum likelihood algorithm network performance determined calculating mean square error network prediction network optimized implementing decision forest impact missing data investigated decision forrests found improve results\",\"wild bootstrap method nonparametric hypothesis tests based kernel distribution embeddings proposed bootstrap method used construct provably consistent tests apply random processes naive permutationbased bootstrap fails applies large group kernel tests based vstatistics degenerate null hypothesis nondegenerate elsewhere illustrate approach construct twosample test instantaneous independence test multiple lag independence test time series experiments wild bootstrap gives strong performance synthetic examples audio data performance benchmarking gibbs sampler\",\"propose class nonparametric twosample tests cost linear sample size two tests given based ensemble distances analytic functions representing distributions first test uses smoothed empirical characteristic functions represent distributions second uses distribution embeddings reproducing kernel hilbert space analyticity implies differences distributions may detected almost surely finite number randomly chosen locationsfrequencies new tests consistent larger class alternatives previous lineartime tests based nonsmoothed empirical characteristic functions much faster current stateoftheart quadratictime kernelbased energy distancebased tests experiments artificial benchmarks challenging realworld testing problems demonstrate tests give better powertime tradeoff competing approaches cases better outright power even expensive quadratictime tests performance advantage retained even high dimensions cases difference distributions observable low order statistics\",\"kernel methods one mainstays machine learning problem kernel learning remains challenging heuristics little theory particular importance methods based estimation kernel mean embeddings probability measures characteristic kernels include commonly used ones kernel mean embedding uniquely determines probability measure used design powerful statistical testing framework includes nonparametric twosample independence tests practice however performance tests sensitive choice kernel lengthscale parameters address central issue propose new probabilistic model kernel mean embeddings bayesian kernel embedding model combining gaussian process prior reproducing kernel hilbert space containing mean embedding conjugate likelihood function thus yielding closed form posterior mean embedding posterior mean model closely related recently proposed shrinkage estimators kernel mean embeddings posterior uncertainty new interesting feature various possible applications critically purposes kernel learning model gives simple closed form marginal pseudolikelihood observed data given kernel hyperparameters marginal pseudolikelihood either optimized inform hyperparameter choice fully bayesian inference used\",\"connect shiftinvariant characteristic kernels infinitely divisible distributions mathbbrd characteristic kernels play important role machine learning applications kernel means distinguish two probability measures contribution paper twofold first show using levykhintchine formula shiftinvariant kernel given bounded continuous symmetric probability density function pdf infinitely divisible distribution mathbbrd characteristic also present closure property characteristic kernels addition pointwise product convolution second developing various kernel mean algorithms fundamental compute following values kernel mean values mpx mathcalx kernel mean rkhs inner products leftlangle rightranglemathcalh probability measures kernel gaussians computation results gaussian pdfs tractable generalize gaussian combination general cases class infinitely divisible distributions introduce conjugate kernel convolution trick pdf form expecting tractable computation least cases specific instances explore alphastable distributions rich class generalized hyperbolic distributions laplace cauchy studentt distributions included\",\"paper study statistical properties semisupervised learning considered important problem community machine learning standard supervised learning labeled data observed classification regression problems formalized supervised learning semisupervised learning unlabeled data also obtained addition labeled data hence exploiting unlabeled data important improve prediction accuracy semisupervised learning problems regarded semiparametric estimation problem missing data discriminative probabilistic models considered unlabeled data useless improve estimation accuracy recently revealed weighted estimator using unlabeled data achieves better prediction accuracy comparison learning method using labeled data especially discriminative probabilistic model misspecified improvement semiparametric model missing data possible semiparametric model misspecified paper apply densityratio estimator obtain weight function semisupervised learning benefit approach proposed estimator require wellspecified probabilistic models probability unlabeled data based statistical asymptotic theory prove estimation accuracy method outperforms supervised learning using labeled data numerical experiments present usefulness methods\",\"vapnikchervonenkis dimension fundamental measure generalization capacity learning algorithms however apart special cases hard impossible calculate analytically vapnik proposed technique estimating dimension empirically approach behaves well simulations could used bound generalization risk classifiers bounds estimation error dimension rectify omission providing high probability concentration results proposed estimator deriving corresponding generalization bounds\",\"massive amount available data potentially used discover patters machine learning challenge kernel based algorithms respect runtime storage capacities local approaches might help relieve issues statistical point view local approaches allow additionally deal different structures data different ways paper analyses properties localized kernel based nonparametric statistical machine learning methods particular support vector machines svms methods close show locally learnt kernel methods universal consistent furthermore give upper bound maxbias order show statistical robustness proposed method\",\"propose vectorvalued regression problem whose solution equivalent reproducing kernel hilbert space rkhs embedding bayesian posterior distribution equivalence provides new understanding kernel bayesian inference moreover optimization problem induces new regularization posterior embedding estimator faster comparable performance squared regularization kernel bayes rule regularization coincides former thresholding approach used kernel pomdps whose consistency remains established theoretical work solves open problem provides consistency analysis regression settings based optimizational formulation propose flexible bayesian posterior regularization framework first time enables put regularization distribution level apply method nonparametric statespace filtering tasks extremely nonlinear dynamics show performance gains baselines\",\"nonlinear similarity measures defined kernel space correntropy extract higherorder statistics data offer potentially significant performance improvement linear counterparts especially nongaussian signal processing machine learning work propose new similarity measure kernel space called kernel risksensitive loss krsl provide important properties apply krsl adaptive filtering investigate robustness develop mkrsl algorithm analyze mean square convergence performance compared correntropy krsl offer efficient performance surface thereby enabling gradient based method achieve faster convergence speed higher accuracy still maintaining robustness outliers theoretical analysis results superior performance new algorithm confirmed simulation\",\"simple framework probabilistic multiview graph embedding pmvge proposed multiview feature learning manytomany associations generalizes various existing multiview methods pmvge probabilistic model predicting new associations via graph embedding nodes data vectors links associations multiview data vectors manytomany associations transformed neural networks feature vectors shared space probability new association two data vectors modeled inner product feature vectors existing multiview feature learning techniques treat either manytomany association nonlinear transformation pmvge treat simultaneously combining mercers theorem universal approximation theorem prove pmvge learns wide class similarity measures across views likelihoodbased estimator enables efficient computation nonlinear transformations data vectors largescale datasets minibatch sgd numerical experiments illustrate pmvge outperforms existing multiview methods\",\"work studies class algorithms learning sideinformation emerge extending generative models embedded contextrelated variables using finite mixture models fmm prototypical bayesian network show maximumlikelihood estimation mle parameters expectationmaximization improves regular unsupervised case approach performances supervised learning despite absence explicit ground truth data labeling direct application missing information principle mip algorithms performances proven range conventional supervised unsupervised mle extremities proportionally information content contextual assistance provided acquired benefits regard higher estimation precision smaller standard errors faster convergence rates improved classification accuracy regression fitness shown various scenarios also highlighting important properties differences among outlined situations applicability showcased three realworld unsupervised classification scenarios employing gaussian mixture models importantly exemplify natural extension methodology type generative model deriving equivalent contextaware algorithm variational autoencoders vas thus broadening spectrum applicability unsupervised deep learning artificial neural networks latter contrasted neuralsymbolic algorithm exploiting sideinformation\",\"survey introduction positive definite kernels set methods inspired machine learning literature namely kernel methods first discuss properties positive definite kernels well reproducing kernel hibert spaces natural extension set functions kxcdotxinmathcalx associated kernel defined space mathcalx discuss length construction kernel functions take advantage wellknown statistical models provide overview numerous dataanalysis methods take advantage reproducing kernel hilbert spaces discuss idea combining several kernels improve performance certain tasks also provide short cookbook different kernels particularly useful certain datatypes images graphs speech segments\",\"despite recent progress towards efficient multiple kernel learning mkl structured output case remains open research front current approaches involve repeatedly solving batch learning problem makes inadequate large scale scenarios propose new family online proximal algorithms mkl well grouplasso variants thereof overcomes drawback show regret convergence generalization bounds proposed method experiments handwriting recognition dependency parsing testify successfulness approach\",\"many applications particular information systems pattern recognition machine learning cheminformatics bioinformatics name assessment uncertainty essential estimation underlying probability distribution function often form function unknown becomes necessary nonparametrically constructestimate given sample one methods choice nonparametrically estimate unknown probability distribution function given random variable defined binary space expansion estimation function rademacherwalsh polynomial basis functions paper demonstrate expansion probability distribution function estimation rademacherwalsh polynomial basis functions equivalent expansion function estimation set dirac kernel functions latter approach ameliorate computational bottleneck notational awkwardness often associated rademacherwalsh polynomial basis functions approach particular binary input space large\",\"propose nonparametric statistical test goodnessoffit given set samples test determines likely generated target density function measure goodnessoffit divergence constructed via steins method using functions reproducing kernel hilbert space test statistic based empirical estimate divergence taking form vstatistic terms log gradients target density kernel derive statistical test iid noniid samples estimate null distribution quantiles using wild bootstrap procedure apply test quantifying convergence approximate markov chain monte carlo methods statistical model criticism evaluating quality fit model complexity nonparametric density estimation\",\"vast majority neural network literature focuses predicting point values given set response variables conditioned feature vector many cases need model full joint conditional distribution response variables rather simply making point predictions paper present two novel approaches conditional density estimation cde multiscale nets msns cde trend filtering multiscale nets transform cde regression task hierarchical classification task decomposing density series halfspaces learning boolean probabilities split cde trend filtering applies kth order graph trend filtering penalty unnormalized logits multinomial classifier network edge graph corresponding neighboring point discretized version density compare methods plain multinomial classifier networks mixture density networks mdns simulated dataset three realworld datasets results suggest two methods complementary msns work well highdataperfeature regime cdetf well suited fewsamplesperfeature scenarios overfitting primary concern\",\"paper introduces kernel mixture network new method nonparametric estimation conditional probability densities using neural networks model arbitrarily complex conditional densities linear combinations family kernel functions centered subset training points weights determined outer layer deep neural network trained minimizing negative log likelihood generalizes popular quantized softmax approach seen kernel mixture network square nonoverlapping kernels test performance method two important applications namely bayesian filtering generative modeling bayesian filtering example show method used filter complex nonlinear nongaussian signals defined manifolds resulting kernel mixture network filter outperforms quantized softmax filter extended kalman filter terms model likelihood finally experiments generative models show given architecture kernel mixture network leads higher test set likelihood less overfitting diversified realistic generated samples quantized softmax approach\",\"propose likelihood ratio based inferential framework high dimensional semiparametric generalized linear models framework addresses variety challenging problems high dimensional data analysis including incomplete data selection bias heterogeneous multitask learning work three main contributions develop regularized statistical chromatography approach infer parameter interest proposed semiparametric generalized linear model without need estimating unknown base measure function propose new framework construct postregularization confidence regions tests low dimensional components high dimensional parameters unlike existing postregularization inferential methods approach based novel directional likelihood particular framework naturally handles generic regularized estimators nonconvex penalty functions used infer least false parameters misspecified models iii develop new concentration inequalities normal approximation results ustatistics unbounded kernels independent interest demonstrate consequences general theory using example missing data problem extensive simulation studies real data analysis provided illustrate proposed approach\",\"paper propose family tractable kernels dense family bounded positive semidefinite functions approximate bounded kernel arbitrary precision start discussing case stationary kernels propose family spectral kernels extends existing approaches spectral mixture kernels sparse spectrum kernels extension two primary advantages firstly unlike existing spectral approaches yield infinite differentiability kernels introduce allow learning degree differentiability latent function gaussian process models functions reproducing kernel hilbert space rkhs kernel methods secondly show kernels propose require fewer parameters existing spectral kernels accuracy thereby leading faster robust inference finally generalize approach propose flexible tractable family spectral kernels prove approximate continuous bounded nonstationary kernel\",\"integrating visual linguistic information single multimodal representation unsolved problem widereaching applications natural language processing computer vision paper present simple method build multimodal representations learning languagetovision mapping using output build multimodal embeddings sense method provides cognitively plausible way building representations consistent inherently reconstructive associative nature human memory using seven benchmark concept similarity tests show mapped vectors implicitly encode multimodal information also outperform strong unimodal baselines stateoftheart multimodal methods thus exhibiting humanlike judgmentsparticularly zeroshot settings\",\"modeling complex conditional distributions critical variety settings despite long tradition research conditional density estimation current methods employ either simple parametric forms difficult learn practice paper employs normalising flows flexible likelihood model presents efficient method fitting complex densities estimators must tradeoff modeling distributional complexity functional complexity heteroscedasticity without overfitting recognize tradeoffs modeling decisions develop bayesian framework placing priors conditional density estimators using variational bayesian neural networks evaluate method several small benchmark regression datasets obtains state art performance finally apply method two spatial density modeling tasks million datapoints using new york city yellow taxi dataset chicago crime dataset\",\"present probabilistic viewpoint multiple kernel learning unifying wellknown regularised risk approaches recent advances approximate bayesian inference relaxations framework proposes general objective function suitable regression robust regression classification lower bound marginal likelihood contains many regularised risk approaches special cases furthermore derive efficient provably convergent optimisation algorithm\",\"paper describes novel method approximate polynomial coefficients regression functions particular interest multidimensional classification derivation simple offers fast robust classification technique resistant overfitting\",\"paper deals problem nonparametric independence testing fundamental decisiontheoretic problem asks two arbitrary possibly multivariate random variables independent question comes many fields like causality neuroscience quantities like correlation test univariate linear independence natural alternatives like mutual information hard estimate due serious curse dimensionality recent approach avoiding issues estimates norms textitoperator reproducing kernel hilbert spaces rkhss main contribution strong empirical evidence employing textitshrunk operators sample size small one attain improvement power low false positive rates analyze effects stein shrinkage popular test statistic called hsic hilbertschmidt independence criterion observations provide insights two recently proposed shrinkage estimators scose fcose prove scose essentially optimal linear shrinkage method textitestimating true operator however nonlinearly shrunk fcose usually achieves greater improvements textittest power work important powerful nonparametric detection subtle nonlinear dependencies small samples\",\"derive upper bound local rademacher complexity ellpnorm multiple kernel learning yields tighter excess risk bound global approaches previous local approaches aimed analyzed case analysis covers cases leq pleqinfty assuming different feature mappings corresponding different kernels uncorrelated also show lower bound shows bound tight derive consequences regarding excess loss namely fast convergence rates order onfracalphaalpha alpha minimum eigenvalue decay rate individual kernels\",\"paper give new generalization error bound multiple kernel learning mkl general class regularizations discuss kind regularization gives favorable predictive accuracy main target paper dense type regularizations including ellpmkl according recent numerical experiments sparse regularization necessarily show good performance compared dense type regularizations motivated fact paper gives general theoretical tool derive fast learning rates mkl applicable arbitrary mixednormtype regularizations unifying manner enables compare generalization performances various types regularizations consequence observe homogeneity complexities candidate reproducing kernel hilbert spaces rkhss affects regularization strategy ell dense preferred fact homogeneous complexity settings complexities rkhss evenly ellregularization optimal among isotropic norms hand inhomogeneous complexity settings dense type regularizations show better learning rate sparse ellregularization also show learning rate achieves minimax lower bound homogeneous complexity settings\",\"consider learning algorithms general source condition polynomial decay eigenvalues integral operator vectorvalued function setting discuss upper convergence rates tikhonov regularizer general source condition corresponding increasing monotone index function convergence issues studied general regularization schemes using concept operator monotone index functions minimax setting also address minimum possible error learning algorithm\",\"last years due growing ubiquity unlabeled data much effort spent machine learning community develop better understanding improve quality classifiers exploiting unlabeled data following manifold regularization approach laplacian support vector machines lapsvms shown state art performance semisupervised classification paper present two strategies solve primal lapsvm problem order overcome issues original dual formulation whereas training lapsvm dual requires two steps using primal form allows collapse training single step moreover computational complexity training algorithm reduced using preconditioned conjugate gradient combined number labeled unlabeled examples speed training using early stopping strategy based prediction unlabeled data available labeled validation examples allows algorithm quickly compute approximate solutions roughly classification accuracy optimal ones considerably reducing training time due simplicity training lapsvm primal starting point additional enhancements original lapsvm formulation dealing large datasets present extensive experimental evaluation real world data showing benefits proposed approach\",\"propose nonparametric sequential test aims address two practical problems pertinent online randomized experiments hypothesis test complex metrics prevent type error inflation continuous monitoring proposed test require knowledge underlying probability distribution generating data use bootstrap estimate likelihood blocks data followed mixture sequential probability ratio test validate procedure data major online ecommerce website show proposed test controls type error time good power robust misspecification distribution generating data allows quick inference online randomized experiments\",\"apply wild bootstrap method lancaster threevariable interaction measure order detect factorisation joint distribution three variables forming stationary random process existing permutation bootstrap method fails iid case lancaster test found outperform existing tests cases two independent variables individually weak influence third considered jointly influence strong main contributions paper twofold first prove lancaster statistic satisfies conditions required estimate quantiles null distribution using wild bootstrap second manner proved novel simpler existing methods applied statistics\",\"concentration inequalities indispensable tools studying generalization capacity learning models hoeffdings mcdiarmids inequalities commonly used giving bounds independent data distribution although makes widely applicable drawback bounds loose specific cases although efforts devoted improving bounds find bounds tightened distributiondependent scenarios conditions inequalities relaxed particular propose four types conditions probabilistic boundedness bounded differences derive several distributiondependent extensions hoeffdings mcdiarmids inequalities extensions provide bounds functions satisfying conditions existing inequalities special cases tighter bounds furthermore obtain generalization bounds unbounded hierarchybounded loss functions finally discuss potential applications extensions learning theory\",\"study learning problems involving arbitrary classes functions distributions targets proper learning procedures procedures allowed select functions tend perform poorly unless problem satisfies additional structural property convex consider unrestricted learning procedures free choose functions outside given class present new unrestricted procedure optimal strong sense required sample complexity essentially best one hope estimate holds almost problem including heavytailed situations moreover sample complexity coincides one would expect convex even convex procedure turns proper thus unrestricted procedure actually optimal realms convex classes proper procedure arbitrary classes unrestricted procedure\",\"study density estimation problem observations generated certain dynamical systems admit unique underlying invariant lebesgue density observations drawn dynamical systems independent moreover usual mixing concepts may appropriate measuring dependence among observations employing mathcalcmixing concept measure dependence conduct statistical analysis consistency convergence kernel density estimator main results follows first show properly chosen bandwidth kernel density estimator universally consistent lnorm second establish convergence rates estimator respect several classes dynamical systems lnorm analysis density function assumed holder continuous weak assumption literature nonparametric density estimation also realistic dynamical system context last least prove convergence rates estimator linftynorm lnorm achieved density function holder continuous compactly supported bounded bandwidth selection problem kernel density estimator dynamical system also discussed study via numerical simulations\",\"direct way express arbitrary dependencies datasets estimate joint distribution apply afterwards argmaxfunction obtain mode corresponding conditional distribution method practice difficult requires global optimization complicated function joint distribution fixed input variables article proposes method finding global maxima joint distribution modeled kernel density estimation experiments show advantages shortcomings resulting regression method comparison standard nadarayawatson regression technique approximates optimum expectation value\",\"kernel dependence measures yield accurate estimates nonlinear relations random variables also endorsed solid theoretical properties convergence rates besides empirical estimates easy compute closed form involving linear algebra operations however hampered two important problems high computational cost involved two kernel matrices sample size computed stored interpretability measure remains hidden behind implicit feature map address two issues introduce sensitivity maps sms hilbertschmidt independence criterion hsic sensitivity maps allow explicitly analyze visualize relative relevance examples features dependence measure also present randomized hsic rhsic corresponding sensitivity maps cope large scale problems build upon framework random features bochners theorem approximate involved kernels canonical hsic power rhsic measure scales favourably number samples approximates hsic sensitivity maps efficiently convergence bounds measure sensitivity map also provided proposal illustrated synthetic examples challenging real problems dependence estimation feature selection causal inference empirical data\",\"additive models play important role semiparametric statistics paper gives learning rates regularized kernel based methods additive models learning rates compare favourably particular high dimensions recent results optimal learning rates purely nonparametric regularized kernel based quantile regression using gaussian radial basis function kernel provided assumption additive model valid additionally concrete example presented show gaussian function depending one variable lies reproducing kernel hilbert space generated additive gaussian kernel belong reproducing kernel hilbert space generated multivariate gaussian kernel variance\",\"concerned obtaining novel concentration inequalities missing mass total probability mass outcomes observed sample derive first time distributionfree bernsteinlike deviation bounds sublinear exponents deviation size missing mass also improve results mcallester ortiz andberend kontorovich small deviations interesting case learning theory known majority standard inequalities cannot directly used analyze heterogeneous sums sums whose terms large difference magnitude generic intuitive approach shows heterogeneity issue introduced mcallester ortiz resolvable least case missing mass via regulating terms using novel thresholding technique\",\"discuss meanfield theory cellular automata model metalearning metalearning process combining outcomes individual learning procedures order determine final decision higher accuracy single learning method method constructed ensemble interacting learning agents acquire process incoming information using various types different versions machine learning algorithms abstract learning space agents located constructed using fully connected model couples agents random strength values cellular automata network simulates higher level integration information acquired independent learning trials final classification incoming input data therefore defined stationary state metalearning system using simple majority rule yet minority clusters share opposite classification outcome observed system therefore probability selecting proper class given input data estimated even without prior knowledge affiliation fuzzy logic easily introduced system even learning agents build simple binary classification machine learning algorithms calculating percentage agreeing agents\",\"semisupervised learning powerful technique leveraging unlabeled data improve machine learning models affected presence informative labels occur classes likely labeled others missing data literature labels called missing random paper propose novel approach address issue estimating missingdata mechanism using inverse propensity weighting debias ssl algorithm including using data augmentation also propose likelihood ratio test assess whether labels indeed informative finally demonstrate performance proposed methods different datasets particular two medical datasets design pseudorealistic missing data scenarios\",\"given reproducing kernel hilbert space realvalued functions suitable measure source space subset decompose sum subspace centered functions orthogonal decomposition leads special case anova kernels functional anova representation best predictor elegantly derived either interpolation regularization framework proposed kernels appear particularly convenient analyzing ffect group variables computing sensitivity indices without recursivity\",\"consider problem uncertainty assessment low dimensional components high dimensional models specifically propose decorrelated score function handle impact high dimensional nuisance parameters consider hypothesis tests confidence regions generic penalized mestimators unlike existing inferential methods tailored individual models approach provides general framework high dimensional inference applicable wide range applications testing perspective develop general theorems characterize limiting distributions decorrelated score test statistic null hypothesis local alternatives results provide asymptotic guarantees type errors local powers proposed test furthermore show decorrelated score function used construct point confidence region estimators semiparametrically efficient also generalize framework broaden applications first extend handle high dimensional null hypothesis number parameters interest increase exponentially fast sample size second establish theory model misspecification third beyond likelihood framework introducing generalized score test based general loss functions thorough numerical studies conducted back developed theoretical results\",\"problem supervised classification discrimination functional data considered special interest popular knearest neighbors knn classifier first relying recent result cerou guyader prove consistency knn classifier functional data whose distribution belongs broad family gaussian processes triangular covariance functions second practical side check behavior knn method compared functional classifiers carried small simulation study analysis several real functional data sets global uniform winner emerges comparisons overall performance knn method together sound intuitive motivation relative simplicity suggests could represent reasonable benchmark classification problem functional data\",\"study prediction estimation problems using empirical risk minimization relative general convex loss function obtain sharp error rates even concentration false restricted example heavytailed scenarios results show error rate depends two parameters one captures intrinsic complexity class essentially leads error rate noisefree realizable problem measures interactions class members target loss dominant problem far realizable also explain one may deal outliers choosing loss way calibrated intrinsic complexity class noiselevel problem latter measured distance target class\",\"many machine learning problems characterized mutual contamination models problems one observes several random samples different convex combinations set unknown base distributions interest decontaminate mutual contamination models recover base distributions either exactly permutation paper considers general setting base distributions defined arbitrary probability spaces examine decontamination problem two mutual contamination models describe popular machine learning tasks recovering base distributions permutation mixed membership model recovering base distributions exactly partial label model classification give necessary sufficient conditions identifiability mutual contamination models algorithms problems infinite finite sample cases introduce novel proof techniques based affine geometry\",\"problem domain generalization labeled training data sets several related prediction problems goal make accurate predictions future unlabeled data sets known learner problem arises several applications data distributions fluctuate environmental technical sources variation introduce formal framework argue viewed kind supervised learning problem augmenting original feature space marginal distribution feature vectors framework several connections conventional analysis supervised learning algorithms several unique aspects require new methods analysis work lays learning theoretic foundations domain generalization building earlier conference paper problem introduced blanchard present two formal models data generation corresponding notions risk distributionfree generalization error analysis focusing attention kernel methods also provide quantitative results universally consistent algorithm efficient implementation provided algorithm experimentally compared pooling strategy one synthetic three realworld data sets\",\"modelling real world complexity music challenge machine learning address task modeling melodic sequences music genre perform comparative analysis two probabilistic models dirichlet variable length markov model dirichletvmm time convolutional restricted boltzmann machine tcrbm show tcrbm learns descriptive music features underlying chords typical melody transitions dynamics assess models future prediction compare performance vmm current state art melody generation show models perform significantly better vmm dirichletvmm marginally outperforming tcrbm finally evaluate short order statistics models using kullbackleibler divergence test sequences model samples show proposed methods match statistics music genre significantly better vmm\",\"kernel methods widely applied machine learning questions approximating unknown function finite sample data ensure arbitrary accuracy approximation various denseness conditions imposed selected kernel note contributes study universal characteristic cuniversal kernels first give simple direct description difference relation among three kinds universalities kernels focus translationinvariant weighted polynomial kernels simple shorter proof known characterization characteristic translationinvariant kernels presented main purpose note give delicate discussion universalities weighted polynomial kernels\",\"analyze generalization robustness batched weighted average algorithm vgeometrically ergodic markov data algorithm good alternative empirical risk minimization algorithm latter suffers overfitting optimizing empirical risk hard generalization algorithm prove pacstyle bound training sample size expected lloss converge optimal loss training data vgeometrically ergodic markov chains robustness show training target variables values contain bounded noise generalization bound algorithm deviates range noise results applied regression problem classification problem case exists unknown deterministic target hypothesis\",\"formulate supervised learning problem referred continuous ranking continuous realvalued label assigned observable taking values feature space mathcalx goal order possible observations mathcalx means scoring function smathcalxrightarrow mathbbr tend increase decrease together highest probability problem generalizes bimultipartite ranking certain extent task finding optimal scoring functions naturally cast optimization dedicated functional criterion called iroc curve maximization kendall tau related pair theoretical side describe optimal elements problem provide statistical guarantees empirical kendall tau maximization appropriate conditions class scoring function candidates also propose recursive statistical learning algorithm tailored empirical iroc curve optimization producing piecewise constant scoring function fully described oriented binary tree preliminary numerical experiments highlight difference nature regression continuous ranking provide strong empirical evidence performance empirical optimizers criteria proposed\",\"incorporating spatial information hyperspectral unmixing procedures shown positive effects due inherent spatialspectral duality hyperspectral scenes current research works consider spatial information mainly focused linear mixing model paper investigate variational approach incorporating spatial correlation nonlinear unmixing procedure nonlinear algorithm operating reproducing kernel hilbert spaces associated ell local variation norm spatial regularizer derived experimental results synthetic real data illustrate effectiveness proposed scheme\",\"regularized kernel methods support vector machines leastsquares support vector regression constitute important class standard learning algorithms machine learning theoretical investigations concerning asymptotic properties manly focused rates convergence last years limited asymptotic results statistical inference far serious limitation use mathematical statistics goal article fill gap based asymptotic normality many methods article derives strongly consistent estimator unknown covariance matrix limiting normal distribution way obtain asymptotically correct confidence sets psifplambda fplambda denotes minimizer regularized risk reproducing kernel hilbert space psihrightarrowmathdsrm hadamarddifferentiable functional applications include multivariate pointwise confidence sets values fplambda confidence sets gradients integrals norms\",\"develop approach feature elimination statistical learning kernel machines based recursive elimination featureswe present theoretical properties method show uniformly consistent finding correct feature space certain generalized assumptionswe present four case studies show assumptions met practical situations present simulation results demonstrate performance proposed approach\",\"new non parametric approach problem testing independence two random process developed test statistic hilbert schmidt independence criterion hsic used previously testing independence iid pairs variables asymptotic behaviour hsic established computed samples drawn random processes shown earlier bootstrap procedures worked iid case fail random processes alternative consistent estimate pvalues proposed tests artificial data realworld forex data indicate new test procedure discovers dependence missed linear approaches earlier bootstrap procedure returns elevated number false positives code available online httpsgithubcomkacperchwialkowskihsic\",\"highdimensional estimation prediction methods propose minimize cost function empirical risk written sum losses associated data point paper focus case nonconvex losses practically important still poorly understood classical empirical process theory implies uniform convergence empirical risk population risk uniform convergence implies consistency resulting mestimator ensure latter computed efficiently order capture complexity computing mestimators propose study landscape empirical risk namely stationary points properties establish uniform convergence gradient hessian empirical risk population counterparts soon number samples becomes larger number unknown parameters modulo logarithmic factors consequently good properties population risk carried empirical risk establish onetoone correspondence stationary points demonstrate several problems nonconvex binary classification robust regression gaussian mixture model result implies complete characterization landscape empirical risk convergence properties descent algorithms extend analysis highdimensional setting number parameters exceeds number samples provide characterization empirical risk landscape nearly informationtheoretically minimal condition namely number samples exceeds sparsity unknown parameters vector modulo logarithmic factors suitable uniform convergence result takes place apply result nonconvex binary classification robust regression highdimension\",\"give comprehensive theoretical characterization nonparametric estimator divergence two continuous distributions first bound rate convergence estimator showing sqrtnconsistent provided densities sufficiently smooth smooth regime show estimator asymptotically normal construct asymptotic confidence intervals establish berryesseen style inequality characterizing rate convergence normality also show estimator minimax optimal\",\"consider statistical inverse learning problem observe image function linear operator iid random design points superposed additive noise distribution design points unknown general analyze simultaneously direct estimation inverse estimation learning problems general framework obtain strong weak minimax optimal rates convergence number observations grows large large class spectral regularization methods regularity classes defined appropriate source conditions improves completes previous results obtained related settings optimality obtained rates shown exponent also explicit dependency constant factor variance noise radius source condition set\",\"lay theoretical foundations new database release mechanisms allow thirdparties construct consistent estimators population statistics ensuring privacy individual contributing database protected proposed framework rests two main ideas first releasing estimate kernel mean embedding data generating random variable instead database still allows thirdparties construct consistent estimators wide class population statistics second algorithm satisfy definition differential privacy basing released kernel mean embedding entirely synthetic data points controlling accuracy metric available reproducing kernel hilbert space describe two instantiations proposed framework suitable different scenarios prove theoretical results guaranteeing differential privacy resulting algorithms consistency estimators constructed outputs\",\"consider setting linear regression high dimension focus problem constructing adaptive honest confidence sets sparse parameter theta want construct confidence set theta contains theta high probability small possible diameter confidence set depend sparsity theta larger wider confidence set however practice unknown paper focuses constructing confidence set theta contains theta high probability whose diameter adaptive unknown sparsity implementable practice\",\"twosample hypothesis testing problem studied challenging scenario high dimensional data sets small sample sizes show twosample hypothesis testing problem posed oneclass set classification problem set classification problem goal classify set data points assumed common class prove average probability error given set less equal bayes error decreases power number sample data points set use positive definite set kernel directly mapping sets data associated reproducing kernel hilbert space without need learn probability distribution specifically solve twosample hypothesis testing problem using oneclass svm conjunction proposed set kernel compare proposed method maximum mean discrepancy ftest ttest methods number challenging simulated high dimensional small sample size data also perform twosample hypothesis testing experiments six cancer gene expression data sets achieve zero typei typeii error results data sets\",\"present work new family kernels compare positive measures arbitrary spaces xcal endowed positive kernel kappa translates naturally kernels histograms clouds points first cover case xcal euclidian focus kernels take account variance matrix mixture two measures compute similarity kernels define semigroup kernels sense use sum two measures compare spectral sense use eigenspectrum variance matrix mixture show family kernels close bonds laplace transforms nonnegativevalued functions defined cone positive semidefinite matrices present closed formulas derived special cases integral expressions focusing functions invariant addition null eigenvalue spectrum variance matrix define kernels atomic measures arbitrary spaces xcal endowed kernel kappa using directly eigenvalues centered gram matrix joined support compared measures provide explicit formulas suited applications present preliminary experiments illustrate interest approach\",\"many machine learning problems characterized mutual contamination models problems one observes several random samples different convex combinations set unknown base distributions goal infer base distributions paper considers general setting base distributions defined arbitrary probability spaces examine three popular machine learning problems arise general setting multiclass classification label noise demixing mixed membership models classification partial labels case give sufficient conditions identifiability present algorithms infinite finite sample settings associated performance guarantees\",\"paper presents general vectorvalued reproducing kernel hilbert spaces rkhs framework problem learning unknown functional dependency structured input space structured output space formulation encompasses vectorvalued manifold regularization coregularized multiview learning providing particular unifying framework linking two important learning approaches case least square loss function provide closed form solution obtained solving system linear equations case support vector machine svm classification formulation generalizes particular binary laplacian svm multiclass multiview settings multiclass simplex cone svm semisupervised multiview settings solution obtained solving single quadratic optimization problem standard svm via sequential minimal optimization smo approach empirical results obtained task object recognition using several challenging datasets demonstrate competitiveness algorithms compared stateoftheart methods\",\"recent years kernel density estimation exploited computer scientists model machine learning problems kernel density estimation based approaches interest due low time complexity either onlogn constructing classifier number sampling instances concerning design kernel density estimators one essential issue fast pointwise mean square error mse andor integrated mean square error imse diminish number sampling instances increases article shown proposed kernel function feasible make pointwise mse density estimator converge regardless dimension vector space provided probability density function point interest meets certain conditions\",\"hypothesis tests models whose dimension far exceeds sample size formulated much like classical studentized tests initial bias estimation removed successfully theory debiased estimators developed context quantile regression models fixed quantile value however frequently desirable formulate tests based quantile regression process leads robust tests stable confidence sets additionally inference quantile regression requires estimation called sparsity function depends unknown density error paper consider debiasing approach uniform testing problem develop highdimensional regression rank scores show use estimate sparsity function well adapt inference involving quantile regression process furthermore develop kolmogorovsmirnov test locationshift highdimensional models confidence sets uniformly valid many quantile values main technical result development bahadur representation debiasing estimator uniform range quantiles uniform convergence quantile process brownian bridge process independent interest simulation studies illustrate finite sample properties procedure\",\"kernel methods ubiquitous tools machine learning however often little reason common practice selecting kernel priori even universal approximating kernel selected quality finite sample estimator may greatly affected choice kernel furthermore directly applying kernel methods one typically needs compute times gram matrix pairwise kernel evaluations work dataset instances computation gram matrix precludes direct application kernel methods large datasets makes kernel learning especially difficult paper introduce bayesian nonparmetric kernellearning bank generic datadriven framework scalable learning kernels bank places nonparametric prior spectral distribution random frequencies allowing learn kernels scale large datasets show framework used large scale regression classification tasks furthermore show bank outperforms several scalable approaches kernel learning variety real world datasets\",\"study fundamental class regression models called second order linear model slm slm extends linear model high order functional space attracted considerable research interest recently yet efficiently learn slm full generality using nonconvex solver still remains open question due several fundamental limitations conventional gradient descent learning framework study try attack problem gradientfree approach call momentestimationsequence mes method show conventional gradient descent heuristic biased skewness distribution therefore longer best practice learning slm based mes framework design nonconvex alternating iteration process train ddimension rankk slm within okd memory onepass dataset proposed method converges globally linearly achieves epsilon recovery error retrieving okdcdotmathrmpolylogkdepsilon samples furthermore theoretical analysis reveals slms learned every subgaussian distribution instances sampled socalled taumip distribution slm learned optau samples tau positive constants depending skewness kurtosis distribution nonmip distribution addition diagonalfree oracle necessary sufficient guarantee learnability slm numerical simulations verify sharpness bounds sampling complexity linear convergence rate algorithm\",\"nonparametric kernelbased method realizing bayes rule proposed based representations probabilities reproducing kernel hilbert spaces probabilities uniquely characterized mean canonical map rkhs prior conditional probabilities expressed terms rkhs functions empirical sample explicit parametric model needed quantities posterior likewise rkhs mean weighted sample estimator expectation function posterior derived rates consistency shown representative applications kernel bayes rule presented including baysian computation without likelihood filtering nonparametric statespace model\",\"propose novel algebraic framework treating probability distributions represented cumulants mean covariance matrix example consider unsupervised learning problem finding subspace several probability distributions agree instead minimizing objective function involving estimated cumulants show treating cumulants elements polynomial ring directly solve problem lower computational cost higher accuracy moreover algebraic viewpoint probability distributions allows invoke theory algebraic geometry demonstrate compact proof identifiability criterion\",\"study twolevel multiview learning two views pacbayesian framework approach sometimes referred late fusion consists learning sequentially multiple viewspecific classifiers first level combining viewspecific classifiers second level main theoretical result generalization bound risk majority vote exhibits term diversity predictions viewspecific classifiers result comes controlling tradeoff diversity accuracy key element multiview learning complements results multiview learning finally experiment principle multiview datasets extracted reuters rcvrcv collection\",\"study paper consequences using mean absolute percentage error mape measure quality regression models prove existence optimal mape model show universal consistency empirical risk minimization based mape also show finding best model mape equivalent weighted mean absolute error mae regression apply weighting strategy kernel regression behavior mape kernel regression illustrated simulated data\",\"paper introduce conformal prediction method construct prediction sets oneshot federated learning setting specifically define quantileofquantiles estimator prove distribution possible output prediction sets desired coverage one round communication mitigate privacy issues also describe locally differentially private version estimator finally wide range experiments show method returns prediction sets coverage length similar obtained centralized setting overall results demonstrate method particularly wellsuited perform conformal predictions oneshot federated learning setting\",\"work constructs hypothesis test detecting whether datagenerating function rightarrow belongs specific reproducing kernel hilbert space mathcalh structure mathcalh partially known utilizing theory reproducing kernels reduce hypothesis simple onesided score test scalar parameter develop testing procedure robust misspecification kernel functions also propose ensemblebased estimator null model guarantee test performance small samples demonstrate utility proposed method apply test problem detecting nonlinear interaction groups continuous features evaluate finitesample performance test different datagenerating functions estimation strategies null model results reveal interesting connections notions machine learning model underfitoverfit statistical inference type errorpower hypothesis test also highlight unexpected consequences common model estimating strategies estimating kernel hyperparameters using maximum likelihood estimation model inference\",\"kernel embeddings distributions maximum mean discrepancy mmd resulting distance distributions useful tools fully nonparametric twosample testing learning distributions however rarely possible differences samples interest discovered differences due different types measurement noise data collection artefacts irrelevant sources variability propose distances distributions encode invariance additive symmetric noise aimed testing whether assumed true underlying processes differ moreover construct invariant features distributions leading learning algorithms robust impairment input distributions symmetric additive noise\",\"give improved constants data dependent variance sensitive confidence bounds called empirical bernstein bounds extend inequalities hold uniformly classes functionswhose growth function polynomial sample size bounds lead consider sample variance penalization novel learning method takes account empirical variance loss function give conditions sample variance penalization effective particular present bound excess risk incurred method using argue situations excess risk method order excess risk empirical risk minimization order sqrtn show experimental results confirm theory finally discuss potential application results sample compression schemes\",\"important aspect classifier error rate quantifies predictive capacity thus accuracy error estimation critical error estimation problematic smallsample classifier design error must estimated using data classifier designed use prior knowledge form prior distribution uncertainty class featurelabel distributions true unknown featuredistribution belongs facilitate accurate error estimation meansquare sense circumstances accurate completely modelfree error estimation impossible paper provides analytic asymptotically exact finitesample approximations various performance metrics resulting bayesian minimum meansquareerror mmse error estimator case linear discriminant analysis lda multivariate gaussian model performance metrics include first second cross moments bayesian mmse error estimator true error lda therefore rootmeansquare rms error estimator lay theoretical groundwork kolmogorov doubleasymptotics bayesian setting enables derive asymptotic expressions desired performance metrics produce analytic finitesample approximations demonstrate accuracy via numerical examples various examples illustrate behavior approximations use determining necessary sample size achieve desired rms supplementary material contains derivations equations added figures\",\"paper presents approximate confidence intervals function parameters banach space based bootstrap algorithm apply kernel density approach estimate persistence landscape addition evaluate quality distribution function estimator random variables using integrated mean square error imse results simulation studies show significant improvement achieved approach compared standard version confidence intervals algorithm next step provide several algorithms solve model finally real data analysis shows accuracy method compared previous works computing confidence interval\",\"study highdimensional asymptotic performance limits binary supervised classification problems class conditional densities gaussian unknown means covariances number signal dimensions scales faster number labeled training samples show bayes error namely minimum attainable error probability complete distributional knowledge equally likely classes arbitrarily close zero yet limiting minimax error probability every supervised learning algorithm better random coin toss contrast related studies classification difficulty bayes error made vanish hold constant taking highdimensional limits contrast vcdimension based minimax lower bounds consider worst case error probability distributions fixed bayes error worst case family gaussian distributions constant bayes error also show nontrivial asymptotic minimax error probability attained parametric subsets zero measure suitable measure space results expose fundamental importance prior knowledge suggest unless impose strong structural constraints sparsity parametric space supervised learning may ineffective high dimensional small sample settings\",\"nonparametric family conditional distributions introduced generalizes conditional exponential families using functional parameters suitable rkhs algorithm provided learning generalized natural parameter consistency estimator established well specified case experiments new method generally outperforms competing approach consistency guarantees competitive deep conditional density model datasets exhibit abrupt transitions heteroscedasticity\",\"paper reviews functional aspects statistical learning theory main point consideration nature hypothesis set prior information available data within framework first discuss hypothesis set vectorial space set pointwise defined functions evaluation functional set continuous mapping based principles original theory developed generalizing notion reproduction kernel hilbert space non hilbertian sets shown hypothesis set learning machine generalized reproducing set therefore thanks general representer theorem solution learning problem still linear combination kernel furthermore way design kernels given illustrate framework examples reproducing sets kernels given\",\"monograph deals adaptive supervised classification using tools borrowed statistical mechanics information theory stemming pacbayesian approach pioneered david mcallester applied conception statistical learning theory forged vladimir vapnik using convex analysis set posterior probability measures show get local measures complexity classification model involving relative entropy posterior distributions respect gibbs posterior measures discuss relative bounds comparing generalization error two classification rules showing margin assumption mammen tsybakov replaced empirical measure covariance structure classification modelwe show associate posterior distribution effective temperature relating gibbs prior distribution level expected error rate estimate effective temperature data resulting estimator whose expected error rate converges according best possible power sample size adaptively margin parametric complexity assumptions describe study alternative selection scheme based relative bounds estimators present two step localization technique handle selection parametric model family show extend systematically results obtained inductive setting transductive learning use improve vapniks generalization bounds extending case sample made independent nonidentically distributed pairs patterns labels finally review briefly construction support vector machines show derive generalization bounds measuring complexity either number support vectors value transductive inductive margin\",\"learning rates leastsquares regression typically expressed terms lnorms paper extend rates norms stronger lnorm without requiring regression function contained hypothesis space special case sobolev reproducing kernel hilbert spaces used hypotheses spaces stronger norms coincide fractional sobolev norms used sobolev space consequence target function also derivatives estimated without changing algorithm technical point view combine wellknown integral operator techniques embedding property far used combination empirical process arguments combination results new finite sample bounds respect stronger norms finite sample bounds rates easily follow finally prove asymptotic optimality results many cases\",\"main goal article address bipartite ranking issue perspective functional data analysis fda given training set independent realizations possibly sampled secondorder random function locally smooth autocorrelation structure binary label randomly assigned objective learn scoring function optimal roc curve based linearnonlinear waveletbased approximations shown select compact finite dimensional representations input curves adaptively order build accurate ranking rules using recent advances ranking problem multivariate data binary feedback beyond theoretical considerations performance learning methods functional bipartite ranking proposed paper illustrated numerical experiments\",\"prove density function gradient sufficiently smooth function omega subset mathbbrd rightarrow mathbbr obtained via random variable transformation uniformly distributed random variable increasingly closely approximated normalized power spectrum phiexpleftfracistauright free parameter tau rightarrow result shown using stationary phase approximation standard integration techniques requires proper ordering limits highlight relationship wellknown characteristic function approach density estimation detail result distinct approach\",\"kernelbased quadrature rules becoming important machine learning statistics achieve supersqrtn convergence rates numerical integration thus provide alternatives monte carlo integration challenging settings integrands expensive evaluate integrands high dimensional rules based assumption integrand certain degree smoothness expressed integrand belongs certain reproducing kernel hilbert space rkhs however assumption violated practice integrand black box function general theory established convergence kernel quadratures misspecified settings contribution proving kernel quadratures consistent even integrand belong assumed rkhs integrand less smooth assumed specifically derive convergence rates depend unknown lesser smoothness integrand degree smoothness expressed via powers rkhss via sobolev spaces\",\"kernel bayes rule proposed nonparametric kernelbased method realize bayesian inference reproducing kernel hilbert spaces however demonstrate theoretically experimentally prediction result kernel bayes rule cases unnatural consider phenomenon part due fact assumptions kernel bayes rule hold general\",\"smallball method introduced way obtaining high probability isomorphic lower bound quadratic empirical process weak assumptions indexing class key assumption class members satisfy uniform smallball estimate prf geq kappafl geq delta given constants kappa delta extend smallball method obtain high probability almostisometric rather isomorphic lower bound quadratic empirical process scope result considerably wider smallball method need class members satisfy uniform smallball condition moreover motivated notion tournament learning procedures result stable majority vote\",\"nonparametric classification regression problems regularized kernel methods particular support vector machines attract much attention theoretical applied statistics abstract sense regularized kernel methods simply called svms seen regularized mestimators parameter typically infinite dimensional reproducing kernel hilbert space smooth loss functions shown difference estimator empirical svm theoretical svm asymptotically normal rate sqrtn standardized difference converges weakly gaussian process reproducing kernel hilbert space common real applications choice regularization parameter may depend data proof done application functional deltamethod showing svmfunctional suitably hadamarddifferentiable\",\"first encountered pacbayesian concentration inequalities seemed rather disconnected good oldfashioned results like hoeffdings bernsteins inequalities least one flavour pacbayesian bounds actually close relation main innovation continuous version union bound along ingenious applications heres gist whats going presented machine learning perspective\",\"density ratio defined ratio two probability densities study inference problem density ratios apply semiparametric densityratio estimator twosample homogeneity test proposed test procedure fdivergence two probability densities estimated using densityratio estimator fdivergence estimator exploited twosample homogeneity test derive optimal estimator fdivergence sense asymptotic variance investigate relation proposed test procedure existing score test based empirical likelihood estimator numerical studies illustrate adequacy asymptotic theory finitesample inference\",\"study paper consequences using mean absolute percentage error mape measure quality regression models show finding best model mape equivalent weighted mean absolute error mae regression also show asumptions universal consistency empirical risk minimization remains possible using mape\",\"derive new discrepancy statistic measuring differences two probability distributions based combining steins identity reproducing kernel hilbert space theory apply result test well probabilistic model fits set observations derive new class powerful goodnessoffit tests widely applicable complex high dimensional distributions even computationally intractable normalization constants theoretical empirical properties methods studied thoroughly\",\"novel concentration inequalities obtained missing mass total probability mass outcomes observed sample derive distributionfree deviation bounds sublinear exponents deviation size missing mass improve results berend kontorovich yari saeed khanloo haffari small deviations important case learning theory\",\"statistical test independence may constructed using hilbertschmidt independence criterion hsic test statistic hsic defined distance embedding joint distribution embedding product marginals reproducing kernel hilbert space rkhs previously shown kernel used defining joint embedding characteristic embedding joint distribution feature space injective hsicbased test consistent particular sufficient product kernels individual domains characteristic joint domain note established via result lyons hsicbased independence tests consistent kernels marginals characteristic respective domains even product kernels characteristic joint domain\",\"response problem vidyasagar state criterion pac learnability concept class mathscr family nonatomic diffuse measures domain omega uniform glivenkocantelli property respect nonatomic measures longer necessary condition consistent learnability cannot general expected criterion stated terms combinatorial parameter vcmathscr cmathrmmodomega call dimension mathscr modulo countable sets new parameter obtained thickening single points definition dimension uncountable clusters equivalently vcmathscr cmoddomegaleq every countable subclass mathscr dimension leq outside countable subset omega new parameter also expressed classical dimension mathscr calculated suitable subset compactification omega make measurability assumptions mathscr assuming instead validity martins axiom similar results obtained function learning terms fatshattering dimension modulo countable sets like classical distributionfree case finiteness parameter sufficient necessary pac learnability nonatomic measures\",\"random sinusoidal features popular approach speeding kernelbased inference large datasets prior inference stage approach suggests performing dimensionality reduction first multiplying data vector random gaussian matrix computing elementwise sinusoid theoretical analysis shows collecting sufficient number features reliably used subsequent inference kernel classification regression work demonstrate mild increase dimension embedding also possible reconstruct data vector random sinusoidal features provided underlying data sparse enough particular propose numerically stable algorithm reconstructing data vector given nonlinear features analyze sample complexity algorithm extended types structured inverse problems demixing pair sparse incoherent vectors support efficacy approach via numerical experiments\",\"propose ksparse exhaustive search esk method ksparse approximate exhaustive search method aesk selecting variables linear regression methods ksparse combinations variables tested exhaustively assuming optimal combination explanatory variables ksparse collecting results exhaustively computing esk various approximate methods selecting sparse variables summarized density states density states compare different methods selecting sparse variables relaxation sampling large problems combinatorial explosion explanatory variables crucial aesk method enables density states effectively reconstructed using replicaexchange monte carlo method multiple histogram method applying esk aesk methods type supernova data confirmed conventional understanding astronomy appropriate given beforehand however found difficulty determine data using virtual measurement analysis argue caused data shortage\",\"investigate kernel regularization methods achieve minimax convergence rates source condition regularity assumption target function questions considered past literature specific assumptions decay typically polynomial spectrum kernel mapping covariance operator perspective distributionfree results investigate issue much weaker assumption eigenvalue decay allowing complex behavior reflect different structure data different scales\",\"paper concerned obtaining distributionfree concentration inequalities mixture independent bernoulli variables incorporate notion variance missing mass total probability mass associated outcomes seen given sample important quantity connects density estimates obtained sample population discrete distributions therefore specifically motivated apply method study concentration missing mass expressed mixture bernoulli novel way derive first time bernsteinlike large deviation bounds missing mass whose exponents behave almost linearly respect deviation size also sharpen mcallester ortiz berend kontorovich large sample sizes case small deviations interesting case learning theory meantime approach shows heterogeneity issue introduced mcallester ortiz resolvable case missing mass sense one use standard inequalities may lead strong results thus postulate results general applied provide potentially sharp bernsteinlike bounds constraints\",\"provide theoretical foundation nonparametric estimation functions random variables using kernel mean embeddings show continuous function consistent estimators mean embedding random variable lead consistent estimators mean embedding matern kernels sufficiently smooth functions also provide rates convergence results extend functions multiple random variables variables dependent require estimator mean embedding joint distribution starting point independent sufficient separate estimators mean embeddings marginal distributions either case results cover mean embeddings based iid samples well reduced set expansions terms dependent expansion points latter serves justification using expansions limit memory resources applying approach basis probabilistic programming\",\"paper aims formulating issue ranking multivariate unlabeled observations depending degree abnormality unsupervised statistical learning task situation problem usually tackled means tail estimation techniques univariate observations viewed abnormal located far tails underlying probability distribution would desirable well dispose scalar valued scoring function allowing comparing degree abnormality multivariate observations formulate issue scoring anomalies mestimation problem means novel functional performance criterion referred mass volume curve curve short whose optimal elements strictly increasing transforms density almost everywhere support density first study statistical estimation curve given scoring function provide strategy build confidence regions using smoothed bootstrap approach optimization functional criterion set piecewise constant scoring functions next tackled boils estimating sequence empirical minimum volume sets whose levels chosen adaptively data adjust variations optimal curve controling bias approximation stepwise curve generalization bounds established difference sup norm curve empirical scoring function thus obtained optimal curve\",\"selection validation basis full dataset often required industrial use supervised machine learning algorithm validation basis serve realize independent evaluation machine learning model select basis propose adopt design experiments point view using statistical criteria show support points concept based maximum mean discrepancy criteria particularly relevant industrial test case company edf illustrates practical interest methodology\",\"introduce mondrian kernel fast random feature approximation laplace kernel suitable batch online learning admits fast kernelwidthselection procedure random features reused efficiently kernel widths features constructed sampling trees via mondrian process roy teh highlight connection mondrian forests lakshminarayanan trees also sampled via mondrian process fit independently link provides new insight relationship kernel methods random forests\",\"paper introduces youtubem video understanding challenge hosted kaggle competition also describes approach experimenting various models experiments provide score result well possible improvements made towards end paper discuss various ensemble learning techniques applied dataset significantly boosted overall competition score last discuss exciting future video understanding research also many applications research could significantly improve\",\"propose investigate test statistics testing homogeneity reproducing kernel hilbert spaces asymptotic null distributions null hypothesis derived consistency fixed local alternatives assessed finally experimental evidence performance proposed approach artificial data speaker verification task provided\",\"consider problem learning set random samples show relevant geometric topological properties set studied analytically using concepts theory reproducing kernel hilbert spaces new kind reproducing kernel call separating kernel plays crucial role study analyzed detail prove new analytic characterization support distribution naturally leads family provably consistent regularized learning algorithms discuss stability methods respect random sampling numerical experiments show approach competitive often better state art techniques\",\"paper give new sharp generalization bound lpmkl generalized framework multiple kernel learning mkl imposes lpmixednorm regularization instead lmixednorm regularization utilize localization techniques obtain sharp learning rate bound characterized decay rate eigenvalues associated kernels larger decay rate gives faster convergence rate furthermore give minimax learning rate ball characterized lpmixednorm product space show derived learning rate lpmkl achieves minimax optimal rate lpmixednorm ball\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"3_kernel_learning_kernels\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"3_kernel_learning_kernels\"],\"textfont\":{\"size\":12},\"x\":[10.119789,10.38186,10.140327,9.464796,10.503403,10.5095625,10.384705,10.48288,9.485542,9.830749,9.803564,10.714889,10.897524,9.55944,10.162527,10.036427,9.890785,9.960084,10.718847,10.825648,10.398632,10.386351,10.471059,9.46355,10.78613,10.432946,10.434087,10.441439,10.492292,10.498943,10.572885,9.397245,10.471583,10.571439,9.797099,9.754797,10.881024,10.558338,10.33587,10.261635,9.819783,9.449704,9.350358,10.359172,10.624452,9.935322,10.256365,9.640004,9.51437,10.182198,10.067042,10.252098,9.354158,10.89417,10.276874,9.930653,10.518521,10.27636,10.635986,11.229511,9.956177,10.64374,9.629543,10.435694,9.589268,9.754403,10.548216,10.656753,10.051263,10.318999,10.713297,10.753273,9.432986,10.311121,9.888155,10.28635,10.432856,9.931227,10.696138,10.517063,9.552749,10.7941265,9.875059,9.676693,10.405672,9.416811,10.651214,10.3143835,10.384084,9.826357,10.178021,9.790266,10.790186,10.388656,10.46483,9.794836,10.417133,9.70105,10.091409,9.904047,10.542867,9.814251,10.545507,10.023579,9.918019,9.867626,9.31618,10.432026,9.84676,10.490801,10.187359],\"y\":[7.811019,8.134833,7.2964306,8.073317,8.279252,8.264192,8.133833,8.236248,8.120391,7.8754425,7.572087,8.36856,7.955927,7.3075495,8.56437,7.700578,7.4327865,7.935466,8.241186,8.466671,8.670435,7.6128936,7.9518604,6.6030383,8.449772,7.8985143,7.6742287,8.243669,7.677028,7.6059537,7.6577344,7.9604936,8.364693,8.308644,8.089955,8.0311365,8.174148,8.255657,8.115974,7.8798575,8.118554,8.083192,8.1123905,7.879572,7.4100814,7.847234,7.610675,7.848077,7.8006454,8.641799,7.7887383,7.6348314,8.051468,7.9433985,7.7293053,7.705547,8.33815,7.4869847,8.126597,8.1431465,7.8056035,7.345,7.800804,7.9980497,7.8997874,7.382591,7.99383,7.4921985,7.7752943,7.5404634,8.406928,8.7370825,7.3872757,7.6150727,7.8858604,8.156466,8.165555,7.9934564,7.6008034,8.3665905,7.9136505,8.4483,7.7490525,8.105971,7.753748,7.980022,8.220057,7.7860646,8.157555,8.089939,7.78032,8.097184,8.183564,7.4799247,8.219779,8.098417,8.193384,8.108856,7.729776,8.31362,7.7110863,8.123541,8.1643915,7.920278,8.532763,7.6949844,8.103629,8.191256,7.7435417,7.659315,7.9536266],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"problem finding overlapping communities networks gained much attention recently optimizationbased approaches use nonnegative matrix factorization nmf variants global optimum cannot provably attained general modelbased approaches popular mixedmembership stochastic blockmodel mmsb airoldi use parameters node specify overlapping communities standard inference techniques cannot guarantee consistency link two approaches establishing sufficient conditions symmetric nmf optimization unique solution mmsb proposing computationally efficient algorithm called geonmf provably optimal hence consistent broad parameter regime demonstrate accuracy simulated realworld datasets\",\"present two graphbased algorithms multiclass segmentation highdimensional data algorithms use diffuse interface model based ginzburglandau functional related total variation compressed sensing image processing multiclass extension introduced using gibbs simplex functionals doublewell potential modified handle multiclass case first algorithm minimizes functional using convex splitting numerical scheme second algorithm uses graph adaptation classical numerical merrimanbenceosher mbo scheme alternates diffusion thresholding demonstrate performance algorithms experimentally synthetic data grayscale color images several benchmark data sets mnist coil webkb also make use fast numerical solvers finding eigenvectors eigenvalues graph laplacian take advantage sparsity matrix experiments indicate results competitive better current stateoftheart multiclass segmentation algorithms\",\"nonparametric detection existence anomalous structure network investigated nodes corresponding anomalous structure one exists receive samples generated distribution different distribution generating samples nodes anomalous structure exist nodes receive samples generated assumed distributions arbitrary unknown goal design statistically consistent tests probability errors converging zero network size becomes asymptotically large kernelbased tests proposed based maximum mean discrepancy measures distance mean embeddings distributions reproducing kernel hilbert space detection anomalous interval line network first studied sufficient conditions minimum maximum sizes candidate anomalous intervals characterized order guarantee proposed test consistent also shown certain necessary conditions must hold guarantee test universally consistent comparison sufficient necessary conditions yields proposed test orderlevel optimal nearly optimal respectively terms minimum maximum sizes candidate anomalous intervals generalization results networks developed numerical results provided demonstrate performance proposed tests\",\"paper aims justifying lwf amp chain graphs showing represent arbitrary independence models specifically show every chain graph inclusion optimal wrt intersection independence models represented set directed acyclic graphs conditioning implies independence model represented chain graph accounted set causal models subject selection bias turn accounted system switches different regimes configurations\",\"existing approaches analyzing asymptotics graph laplacians typically assume wellbehaved kernel function smoothness assumptions remove smoothness assumption generalize analysis graph laplacians include previously unstudied graphs including knn graphs also introduce kernelfree framework analyze graph constructions shrinking neighborhoods general apply analyze locally linear embedding lle also describe given limiting laplacian operator desirable properties convergent spectrum sparseness achieved choosing appropriate graph construction\",\"consider problem embedding unweighted directed knearest neighbor graphs lowdimensional euclidean space knearest neighbors vertex provides ordinal information distances points distances use ordinal information along lowdimensionality recover coordinates points arbitrary similarity transformations rigid transformations scaling furthermore also illustrate possibility robustly recovering underlying density via total variation maximum penalized likelihood estimation tvmple method make existing approaches scalable using instance localtoglobal algorithm based group synchronization recently proposed literature context sensor network localization structural biology augment scaling synchronization step demonstrate scalability approach large graphs show compares local ordinal embedding loe algorithm recently proposed recovering configuration cloud points pairwise ordinal comparisons sparse set distances\",\"statistical inference graphs burgeoning field applied theoretical statistics communities well throughout wider world science engineering business etc many applications faced reality errorfully observed graphs existence edge two vertices based imperfect assessment paper consider graph wish perform inference task inference task considered vertex classification however observe rather potential edge vchoose observe edgefeature use classify edgenotedge thus errorfully observe observe graph widetildeg vwidetildee edges widetildee arise classifications edgefeatures expected errorful moreover face quantityquality tradeoff regarding edgefeatures observe informative edgefeatures expensive hence number potential edges assessed decreases quality edgefeatures studied problem formulating quantityquality tradeoff simple class random graphs model namely stochastic blockmodel consider simple optimal vertex classifier classifying derive optimal quantityquality operating point subsequent graph inference face tradeoff optimal operating points quantityquality tradeoff surprising illustrate issue methods intermediate tasks chosen maximize performance ultimate inference task finally investigate quantityquality tradeoff errorful obesrvations elegans connectome graph\",\"propose spectral clustering method based local principal components analysis pca performing local pca selected neighborhoods algorithm builds nearest neighbor graph weighted according discrepancy principal subspaces neighborhoods applies spectral clustering opposed standard spectral methods based solely pairwise distances points algorithm able resolve intersections establish theoretical guarantees simpler variants within prototypical mathematical framework multimanifold clustering evaluate algorithm various simulated data sets\",\"consider two networks overlapping nonidentical vertex sets given vertices interest first network seek identify corresponding vertices exist second network moderately sized networks graph matching methods applied directly recover missing correspondences herein present principled methodology appropriate situations networks large bruteforce graph matching methodology identifies vertices local neighborhood vertices interest first network verifiable corresponding vertices second network leveraging known correspondences referred seeds match induced subgraphs network generated neighborhoods verified seeds rank vertices second network terms likely matches original vertices interest demonstrate applicability methodology simulations real data examples\",\"laplacian eigenvectors graph constructed data set used many spectral manifold learning algorithms diffusion maps spectral clustering given graph constructed random sample ddimensional compact submanifold mathbbrd establish spectral convergence rate graph laplacian implies consistency spectral clustering algorithm via standard perturbation argument simple numerical study indicates necessity denoising step applying spectral algorithms\",\"many complex ecosystems formed multiple microbial taxa involve intricate interactions amongst various subcommunities basic relationships frequently modeled cooccurrence networks nodes represent various players community weighted edges encode levels interaction setting composition community may viewed probability distribution nodes network paper develops methods modeling organization data well euclidean counterparts across spatial scales using notion diffusion distance introduce diffusion frechet functions diffusion frechet vectors associated probability distributions euclidean space vertex set weighted network respectively prove functional statistics stable respect wasserstein distance probability measures thus yielding robust descriptors shapes apply methodology investigate bacterial communities human gut seeking characterize divergence intestinal homeostasis patients clostridium difficile infection cdi effects fecal microbiota transplantation treatment used cdi patients proven significantly effective traditional treatment antibiotics proposed method proves useful deriving biomarker might help elucidate mechanisms drive processes\",\"propose novel class network models temporal dyadic interaction data goal capture number important features often observed social interactions sparsity degree heterogeneity community structure reciprocity propose family models based selfexciting hawkes point processes events depend history process key component conditional intensity function hawkes process captures fact interactions may arise response past interactions reciprocity due shared interests individuals community structure order capture sparsity degree heterogeneity base non time dependent part intensity function builds compound random measures following todeschini conduct experiments variety realworld temporal interaction data show proposed model outperforms many competing approaches link prediction leads interpretable parameters\",\"volume data generated internet social networks increasing every day clear need efficient ways extracting useful information data take different forms important use available data representations prediction paper focus attention supervised classification using regular plain tabular data structural information coming network structure techniques investigated compared study divided three classes first one uses plain data build classification model second uses graph structure last uses information sources relative performances three cases investigated furthermore effect using graph embedding wellknown indicators spatial statistics also studied possible applications automatic classification web pages linked documents people social network proteins biological complex system name based comparison draw general conclusions advices tackle particular classification task datasets better explained graph structure graphdriven feature set featuresdriven efficient methods discussed cases\",\"paper focus stochastic block model sbma probabilistic tool describing interactions nodes network using latent clusters sbm assumes networkhas stationary structure connections time varying intensity taken account words interactions two groups forced features whole observation time overcome limitationwe propose partition whole time horizon interactions observed develop non stationary extension sbmallowing simultaneously cluster nodes network along fixed time intervals interactions take place number clusters nodes time intervals well class memberships finallyobtained maximizing completedata integrated likelihood means greedy search approach showing model works properly simulated data focus real data set thus consider three days acm hypertext conference held turinjune july proximity interactions attendees first day modelled interestingclustering daily hours finally obtained times social gathering coffee breaks recovered approach applications large networks limited due computational complexity greedy search dominated bythe number kmax dmax clusters used initialization thereforeadvanced clustering tools considered reduce number clusters expected data making greedy search applicable large networks\",\"consider problem unveiling implicit network structure node interactions user interactions social network based highfrequency timestamps inference based minimization leastsquares loss associated multivariate hawkes model penalized ell trace norm interaction tensor provide first theoretical analysis problem includes sparsity lowrank inducing penalizations result involves new datadriven concentration inequality matrix martingales continuous time observable variance result independent interest broad range possible applications since extends matrix martingales former results restricted scalar case consequence analysis construction sharply tuned ell tracenorm penalizations leads datadriven scaling variability information available users numerical experiments illustrate significant improvements achieved use datadriven penalizations\",\"recent papers formulated problem learning graphs data inverse covariance estimation graph laplacian constraints problems convex existing methods cannot guarantee solutions specific graph topology properties kpartite desirable applications fact problem learning graph given topology properties finding kpartite graph best matches data general nonconvex paper develop novel theoretical results provide performance guarantees approach solve problems solution decomposes problem two subproblems efficient solutions known specifically graph topology inference gti step employed select feasible graph topology one desired property graph weight estimation gwe step performed solving generalized graph laplacian estimation problem edges constrained topology found gti step main result bound error gwe step function error gti step error bound indicates gti step solved using algorithm approximates similarity matrix another matrix whose entries thresholded zero desired type graph topology gti stage leverage existing methods state art approaches graph coloring typically based minimizing total weight removed edges since gwe stage formulated inverse covariance estimation problem linear constraints solved using existing convex optimization methods demonstrate two step approach achieve good results synthetic texture image data\",\"stochastic blockmodels among prominent statistical models cluster analysis complex networks clusters defined groups nodes statistically similar link probabilities within groups recent extension karrer newman incorporates node degree correction model degree heterogeneity within group although demonstrably leads better performance several networks obvious whether modelling node degree always appropriate necessary formulate degree corrected stochastic blockmodel nonparametric bayesian model incorporating parameter control amount degree correction inferred data additionally formulation yields principled ways inferring number groups well predicting missing links network used quantify models predictive performance synthetic data demonstrate including degree correction yields better performance recovering true group structure predicting missing links degree heterogeneity present whereas performance par data degree heterogeneity within clusters seven real networks ground truth group structure available show predictive performance equal whether degree correction included however networks significantly fewer clusters discovered correcting degree indicating data compactly explained clusters heterogenous degree nodes\",\"consider structure learning problem graphical models call loosely connected markov random fields number short paths pair nodes small present new conditional independence test based algorithm learning underlying graph structure novel maximization step algorithm ensures true edges detected correctly even short cycles graph number samples required algorithm clog size graph constant depends parameters model show several previously studied models examples loosely connected markov random fields algorithm achieves lower computational complexity previously designed algorithms individual cases also get new results general graphical models particular algorithm learns general ising models erdosrenyi random graph correctly running time onp\",\"consider learning multiagent hawkes processes model containing multiple hawkes processes shared endogenous impact functions different exogenous intensities framework stochastic maximum likelihood estimation explore associated risk bound consider superposition hawkes processes within model demonstrate certain conditions operation beneficial tightening risk bound accordingly propose stochastic optimization algorithm assisted diversitydriven superposition strategy achieving better learning results improved convergence properties effectiveness proposed method verified synthetic data potential solve coldstart problem sequential recommendation systems demonstrated realworld data\",\"random graphs distributed according stochastic blockmodels special case latent position graphs adjacency spectral embedding followed appropriate vertex classification asymptotically bayes optimal approach requires knowledge critically depends model dimension paper propose sparse representation vertex classifier require information model dimension classifier represents test vertex sparse combination vertices training set uses recovered coefficients classify test vertex prove consistency proposed classifier stochastic blockmodels demonstrate sparse representation classifier predict vertex labels higher accuracy adjacency spectral embedding approaches via simulation studies real data experiments results demonstrate robustness effectiveness proposed vertex classifier model dimension unknown\",\"spectral analysis neighborhood graphs one widely used techniques exploratory data analysis applications ranging machine learning social sciences applications typical first encode relationships data samples using appropriate similarity function popular neighborhood construction techniques knearest neighbor knn graphs known sensitive choice parameters importantly susceptible noise varying densities paper propose use quantile analysis obtain local scale estimates neighborhood graph construction end build autoencoding neural network approach inferring conditional quantiles similarity function subsequently used obtain robust estimates local scales addition highly resilient noise outlying data proposed approach require extensive parameter tuning unlike several existing methods using applications spectral clustering singleexample label propagation show proposed neighborhood graphs outperform existing locally scaled graph construction approaches\",\"consider problem grouping multiple graphs several clusters using singular value thesholding nonnegative factorization derive model selection information criterion estimate number clusters demonstrate approach using swimmer data set well simulated data set compare performance two standard clustering algorithms\",\"many graph clustering quality functions suffer resolution limit inability find small clusters large graphs called resolutionlimitfree quality functions limit property previously introduced hard clustering graph partitioning investigate resolutionlimitfree property context nonnegative matrix factorization nmf hard soft graph clustering use nmf hard clustering setting common approach assign node highest membership cluster show case symmetric nmf resolutionlimitfree becomes hardness constraints used part optimization resulting function strongly linked constant potts model soft clustering nodes belong one cluster varying degrees membership setting resolutionlimitfree turns strong property therefore introduce locality roughly states changing one part graph affect clustering parts graph argue desirable property provide conditions nmf quality functions local propose novel class local probabilistic nmf quality functions soft graph clustering\",\"present work deals active sampling graph nodes representing training data binary classification graph may given constructed using similarity measures among nodal features leveraging graph classification builds premise labels across neighboring nodes correlated according categorical markov random field mrf model relaxed gaussian gmrf labels taking continuous values approximation mitigates combinatorial complexity categorical model also offers optimal unbiased soft predictors unlabeled nodes proposed sampling strategy based querying node whose label disclosure expected inflict largest change gmrf sense informative average strategy subsumes several measures expected model change including uncertainty sampling variance minimization sampling based sigmaoptimality criterion simple yet effective heuristic also introduced increasing exploration capabilities sampler reducing bias resultant classifier taking account confidence model label predictions novel sampling strategies based quantities readily available without need model retraining rendering computationally efficient scalable large graphs numerical tests using synthetic real data demonstrate proposed methods achieve accuracy comparable superior stateoftheart even reduced runtime\",\"traditionally community detection graphs solved using spectral methods posterior inference probabilistic graphical models focusing random graph families stochastic block model recent research unified approaches identified statistical computational detection thresholds terms signaltonoise ratio recasting community detection nodewise classification problem graphs also study learning perspective present novel family graph neural networks gnns solving community detection problems supervised learning setting show datadriven manner without access underlying generative models match even surpass performance belief propagation algorithm binary multiclass stochastic block models believed reach computational threshold particular propose augment gnns nonbacktracking operator defined line graph edge adjacencies models also achieve good performance realworld datasets addition perform first analysis optimization landscape training linear gnns community detection problems demonstrating certain simplifications assumptions loss values local global minima far apart\",\"natural approach analyze interaction data form whatconnectstowhatwhen create timeseries rather sequence graphs temporal discretization bandwidth selection spatial discretization vertex contraction discretization together nonnegative factorization techniques useful obtaining clustering graphs motivating application performing clustering graphs opposed vertex clustering found neuroscience social network analysis also used enhance community detection vertex clustering way conditioning cluster labels paper formulate problem clustering graphs model selection problem approach involves information criteria nonnegative matrix factorization singular value thresholding illustrate techniques using real simulated data\",\"many popular network models rely assumption vertex exchangeability distribution graph invariant relabelings vertices however aldoushoover theorem guarantees graphs dense empty probability one whereas many realworld graphs sparse present alternative notion exchangeability random graphs call edge exchangeability distribution graph sequence invariant order edges demonstrate edgeexchangeable models unlike models traditionally vertex exchangeable exhibit sparsity outline general framework graph generative models contrast pioneering work caron fox models within framework stationary across steps graph sequence particular model grows graph instantiating latent atoms single random measure dataset size increases rather adding new atoms measure\",\"define class euclidean distances weighted graphs enabling perform thermodynamic soft graph clustering class constructed form raw coordinates encountered spectral clustering extended means higherdimensional embeddings schoenberg transformations geographical flow data properly conditioned illustrate procedure well visualization aspects\",\"hypergraph partitioning lies heart number problems machine learning network sciences many algorithms hypergraph partitioning proposed extend standard approaches graph partitioning case hypergraphs however theoretical aspects methods seldom received attention literature compared extensive studies guarantees graph partitioning instance consistency results spectral graph partitioning stochastic block model well known paper present planted partition model sparse random nonuniform hypergraphs generalizes stochastic block model derive error bound spectral hypergraph partitioning algorithm model using matrix concentration inequalities best knowledge first consistency result related partitioning nonuniform hypergraphs\",\"latent block model lbm flexible probabilistic tool describe interactions node sets bipartite networks account interactions time varying intensity nodes unknown classes paper propose non stationary temporal extension lbm clusters simultaneously two node sets bipartite network constructs classes time intervals interactions stationary number clusters well membership classes obtained maximizing exact completedata integrated likelihood relying greedy search approach experiments simulated real data carried order assess proposed methodology\",\"propose simulation method multidimensional hawkes processes based superposition theory point processes formulation allows design efficient simulations hawkes processes differing exponentially decaying intensities demonstrate interarrival times decomposed simpler auxiliary variables sampled directly giving exact simulation approximation establish auxiliary variables provides information parent process event time algorithm correctness shown verifying simulated intensities theoretical moments modular inference procedure consisting gibbs samplers auxiliary variable augmentation adaptive rejection sampling presented finally compare proposed simulation method existing methods find significant improvement terms algorithm speed inference algorithm used discover strengths mutually excitations real dark networks\",\"paper propose bayesian nonparametric approach modelling sparse timevarying networks positive parameter associated node network models sociability node sociabilities assumed evolve time modelled via dynamic point process model model able capture long term evolution sociabilities moreover yields sparse graphs number edges grows subquadratically number nodes evolution sociabilities described tractable timevarying generalised gamma process provide theoretical insights model apply three datasets simulated network network hyperlinks communities reddit network cooccurences words reuters news articles september attacks\",\"unbalanced data arises many learning tasks clustering multiclass data hierarchical divisive clustering semisupervised learning graphbased approaches popular tools problems graph construction important aspect graphbased learning show graphbased algorithms fail unbalanced data many popular graphs knn epsilonneighborhood fullrbf graphs propose novel graph construction technique encodes global statistical information node degrees ranking scheme rank data sample estimate pvalue proportional total number data samples smaller density ranking scheme serves surrogate density reliably estimated indicates whether data sample close valleysmodes rankmodulated degreermd scheme able significantly sparsify graph near valleys provides adaptive way cope unbalanced data theoretically justify method limit cut analysis unsupervised semisupervised experiments synthetic real data sets demonstrate superiority method\",\"spectral clustering graphbased semisupervised learning ssl algorithms sensitive graphs constructed data particular data proximal unbalanced clusters algorithms lead poor performance wellknown graphs knn fullrbf epsilongraphs objectives ratiocut rcut normalized cut ncut attempt tradeoff cut values cluster sizes tailored unbalanced data propose novel graph partitioning framework parameterizes family graphs adaptively modulating node degrees knn graph propose model selection scheme choose sizable clusters separated smallest cut values framework able adapt varying levels unbalancedness data naturally used small cluster detection theoretically justify ideas limit cut analysis unsupervised semisupervised experiments synthetic real data sets demonstrate superiority method\",\"network clustering reveals organization network corresponding complex system elements represented vertices interactions edges directed weighted graph although notion clustering somewhat loose network clusters groups generally considered nodes enriched interactions edges sharing common patterns statistical inference often treats groups latent variables observed networks generated latent group structure termed stochastic block model regardless definitions statistical inference either translated modularity maximization provably npcomplete problem present scalable reliable algorithms recover hierarchical stochastic block models fast accurately algorithm scales almost linearly number edges inferred models accurate scalable methods\",\"vertex clustering stochastic blockmodel graph wide applicability subject extensive research thispaper provide short proof adjacency spectral embedding used obtain perfect clustering stochastic blockmodel degreecorrected stochastic blockmodel also show analogous result general random dot product graph model\",\"present method based orthogonal symmetric nonnegative matrix trifactorization normalized laplacian matrix community detection complex networks exact factorization given order may exist hard compute obtain approximate factorization solving optimization problem establish connection factors obtained factorization nonnegative basis invariant subspace estimated matrix drawing parallel spectral clustering using factorization clustering networks motivated analyzing blockdiagonal laplacian matrix blocks representing connected components graph method shown consistent community detection graphs generated stochastic block model degree corrected stochastic block model simulation results real data analysis show effectiveness methods wide variety situations including sparse highly heterogeneous graphs usual spectral clustering known fail method also performs better state art popular benchmark network datasets political web blogs karate club data\",\"spectral clustering popular versatile clustering method based relaxation normalised graph cut objective despite popularity however single agreed upon method tuning important scaling parameter determining automatically number clusters extract popular heuristics exist corresponding theoretical results scarce paper investigate asymptotic value normalised cut increasing sample assumed arise underlying probability distribution based result provide recommendations improving spectral clustering methodology corresponding algorithm proposed strong empirical performance\",\"given time series graphs fixed vertex set represents actors edge vertex vertex time represents existence communications event actors tth time period wish detect anomalies andor change points consider collection graph features invariants demonstrate adaptive fusion provides superior inferential efficacy compared naive equal weighting certain class anomaly detection problems simulation results using latent process model time series graphs well illustrative experimental results time series graphs derived enron email data show fusion statistic provide superior inference compared individual invariants alone results also demonstrate adaptive weighting scheme fusion invariants performs better naive equal weighting\",\"spectral embedding uses eigenfunctions discrete laplacian weighted graph obtain coordinates embedding abstract data set euclidean space propose new preprocessing step first using eigenfunctions simulate lowfrequency wave moving data using position well change time wave obtain refined metric classical methods dimensionality reduction applied motivated behavior waves symmetries wave equation hunting technique bats shown effective practice also works partial differential equations method yields improved results even classical heat equation\",\"mincut clustering based minimizing one two heuristic costfunctions proposed shi malik spawned tremendous research analytic algorithmic graph partitioning image segmentation communities last decade however unclear heuristics derived general principle facilitating generalization new problem settings motivated existing graph partitioning framework derive relationships optimizing relevance information defined information bottleneck method regularized cut kpartitioned graph fast mixing graphs show cost functions introduced shi malik well approximated rate loss predictive information location random walkers graph graphs generated stochastic algorithm designed model community structure optimal information theoretic partition optimal mincut partition shown high probability\",\"propose new yet natural algorithm learning graph structure general discrete graphical models aka markov random fields samples algorithm finds neighborhood node sequentially adding nodes produce largest reduction empirical conditional entropy greedy sense choice addition based reduction achieved iteration sequential nature gives lower computational complexity compared existing comparisonbased techniques involve exhaustive searches every node set certain size main result characterizes sample complexity procedure function node degrees graph size girth factorgraph representation subsequently specialize result case ising models provide simple transparent characterization sample complexity function model graph parameters tree graphs algorithm classical chowliu algorithm sense considered extension graphs cycles\",\"consider problem accelerating distributed optimization multiagent networks sequentially adding edges specifically extend distributed dual averaging dda subgradient algorithm evolving networks growing connectivity analyze corresponding improvement convergence rate known convergence rate dda influenced algebraic connectivity underlying network better connectivity leads faster convergence however impact network topology design convergence rate dda fully understood paper begin designing network topologies via edge selection scheduling edge selection determine best set candidate edges achieves optimal tradeoff growth network connectivity usage network resources dynamics network evolution incurred edge scheduling provide tractable approach analyze improvement convergence rate dda induced growth network connectivity analysis reveals connection network topology design convergence rate dda provides quantitative evaluation dda acceleration distributed optimization absent existing analysis lastly numerical experiments show dda significantly accelerated using sequence welldesigned networks theoretical predictions well matched empirical convergence behavior\",\"present model random simple graphs degree distribution obeys power law heavytailed attain behavior edge probabilities graph constructed bertoinfujitaroynetteyor bfry random variables recently utilized bayesian statistics construction power law models several applications construction readily extends capture structure latent factors similarly stochastic blockmodels maintaining power law degree distribution bfry random variables well approximated gamma random variables variational bayesian inference routine apply several network datasets power law degree distributions natural assumption learning parameters bfry distribution via probabilistic inference able automatically select appropriate power law behavior data order scale inference procedure adopt stochastic gradient ascent routines gradients computed minibatches subsets edges graph\",\"stochastic block model sbm widely used random graph model networks communities despite recent burst interest recovering communities sbm statistical computational points view still gaps understanding fundamental information theoretic computational limits recovery paper consider sbm full generality restriction number sizes communities grow number nodes well connection probabilities inside across communities generality allows move past artifacts homogenous sbm understand right parameters relative densities communities define various recovery thresholds outline implications generalizations via set illustrative examples instance log considered standard lower bound cluster size exact recovery via convex methods homogenous sbm show possible right circumstances sizes spread smaller cluster denser recover small clusters sqrtlog size polylogarithmic\",\"spectral clustering sensitive graphs constructed data particularly proximal imbalanced clusters present show ratiocut rcut normalized cut ncut objectives tailored imbalanced data since tend emphasize cut sizes cut values propose graph partitioning problem seeks minimum cut partitions minimum size constraints partitions deal imbalanced data approach parameterizes family graphs adaptively modulating node degrees fixed node set yield set parameter dependent cuts reflecting varying levels imbalance solution problem obtained optimizing parameters present rigorous limit cut analysis results justify approach demonstrate superiority method unsupervised semisupervised experiments synthetic real data sets\",\"several problems network intrusion community detection disease outbreak described observations attributed nodes edges graph applications presence intrusion community disease outbreak characterized novel observations unknown connected subgraph problems formulated terms optimization suitable objectives connected subgraphs problem generally computationally difficult overcome combinatorics connectivity embedding connected subgraphs linear matrix inequalities lmi computationally efficient tests realized optimizing convex objective functions subject lmi constraints prove means novel euclidean embedding argument tests minimax optimal exponential family distributions lattices show internal conductance connected subgraph family plays fundamental role characterizing detectability\",\"network metrics form fundamental part network analysis toolbox used quantitatively measure different aspects network metrics give insights underlying network structure function work connect network metrics modern probabilistic machine learning focus centrality metric used wide variety applications web search geneanalysis first formulate eigenvectorbased bayesian centrality model determining node importance compared existing methods probabilistic model allows assimilation multiple edge weight observations inclusion priors extraction uncertainties enable tractable inference develop variational lower bound vbc demonstrated effective variety networks two synthetic five realworld graphs bridge model sparse gaussian processes sparse variational bayesian centrality gaussian process vbcgp learns mapping node attributes latent centrality hence capable predicting centralities node features potentially represent large number nodes using limited number inducing inputs experiments show vbcgp learns highquality mappings compares favorably twostep baseline full trained node attributes precomputed centralities finally present two casestudies using vbcgp first ascertain relevant features taxi transport network second distribute limited number vaccines mitigate severity viral outbreak\",\"consider problem estimating undirected trianglefree graphs high dimensional distributions trianglefree graphs form rich graph family allows arbitrary loopy structures cliques inferential tractability propose graphical fermats principle regularize distribution family principle enforces existence distributiondependent pseudometric two nodes smaller distance two nodes geodesic path include two nodes guided principle show greedy strategy able recover true graph resulting algorithm requires pairwise distance matrix input computationally even efficient calculating minimum spanning tree consider graph estimation problems different settings including discrete nonparametric distribution families thorough numerical results provided illustrate usefulness proposed method\",\"interaction transitivity sparsity two common features empirical networks implies local regions large sparse networks dense call blessing transitivity consequences modeling inference extant research suggests statistical inference stochastic blockmodel difficult edges sparse however conclusion confounded fact asymptotic limit previous studies merely sparse also nontransitive retain transitivity blocks cannot grow faster expected degree thus sparse models blocks must remain asymptotically small previous algorithmic research demonstrates small local clusters amenable computation visualization interpretation compared global graph partitions paper provides first statistical results demonstrate small transitive clusters also amenable statistical estimation theorem shows local clustering algorithm high probability detect transitive stochastic block fixed size nodes embedded large graph constraint ambient graph large sparseit could generated random adversarysuggesting theoretical explanation robust empirical performance local clustering algorithms\",\"consider problem estimating consensus community structure combining information multiple layers multilayer network using methods based spectral clustering lowrank matrix factorization general theme intermediate fusion methods involve obtaining low column rank matrix optimizing objective function using columns matrix clustering however theoretical properties methods remain largely unexplored absence statistical guarantees objective functions difficult determine algorithms optimizing objectives return good community structures investigate consistency properties global optimizer objective functions multilayer stochastic blockmodel purpose derive several new asymptotic results showing consistency intermediate fusion techniques along spectral clustering mean adjacency matrix high dimensional setup number nodes number layers number communities multilayer graph grow numerical study shows intermediate fusion techniques outperform late fusion methods namely spectral clustering aggregate spectral kernel module allegiance matrix sparse networks outperform spectral clustering mean adjacency matrix multilayer networks contain layers homophilic heterophilic communities\",\"graphbased semisupervised learning one popular methods machine learning theoretical properties bounds generalization error convergence graph laplacian regularizer studied computer science statistics literatures however fundamental statistical property consistency estimator method proved article study consistency problem nonparametric framework prove consistency graphbased learning case estimated scores enforced equal observed responses labeled data sample sizes labeled unlabeled data allowed grow result estimated scores required equal observed responses tuning parameter used balance loss function graph laplacian regularizer give counterexample demonstrating estimator case inconsistent theoretical findings supported numerical studies\",\"consider problem clustering longestleg path distance llpd metric informative elongated irregularly shaped clusters prove finitesample guarantees performance clustering respect metric random samples drawn multiple intrinsically lowdimensional clusters highdimensional space presence large number highdimensional outliers combining results spectral clustering respect llpd provide conditions laplacian eigengap statistic correctly determines number clusters large class data sets prove guarantees labeling accuracy proposed algorithm methods quite general provide performance guarantees spectral clustering ultrametric also introduce efficient easy implement approximation algorithm llpd based multiscale analysis adjacency graphs allows runtime llpd spectral clustering quasilinear number data points\",\"prove criterion markov equivalence provided zhao may involve set features graph exponential number vertices\",\"present method estimate block membership nodes random graph generated stochastic blockmodel use embedding procedure motivated random dot product graph model particular example latent position model embedding associates node vector vectors clustered via minimization square error criterion prove method consistent assigning nodes blocks negligible number nodes misassigned prove consistency method directed undirected graphs consistent block assignment makes possible consistent parameter estimation stochastic blockmodel extend result setting number blocks grows slowly number nodes method also computationally feasible even large graphs compare method laplacian spectral clustering analysis simulated data graph derived wikipedia documents\",\"community detection graphs subject many algorithms recent methods want optimize modularity function shows maximum relationships within communities found minimum intercommunity relations algorithms applied unipartite multipartite directed graphs however given npcompleteness problem algorithms heuristics guarantee optimum paper introduce algorithm based approximate solution obtained efficient detection algorithm modifie achieve local optimum based function reassignment function potential function therefore computed optimum nash equilibrium supplement method overlap function allows simultaneously two detection modes several experiments show interest approach\",\"graph clustering involves task dividing nodes clusters edge density higher within clusters opposed across clusters natural classic popular statistical setting evaluating solutions problem stochastic block model also referred planted partition model paper present new algorithma convexified version maximum likelihoodfor graph clustering show classic stochastic block model setting outperforms existing methods polynomial factors cluster size allowed general scalings fact within logarithmic factors known lower bounds spectral methods evidence suggesting polynomial time algorithm would significantly better show guarantee carries general extension stochastic block model method handle settings semirandom graphs heterogeneous degree distributions unequal cluster sizes unaffiliated nodes partially observed graphs planted cliquecoloring etc particular results provide best exact recovery guarantees date planted partition planted kdisjointcliques planted noisy coloring models general cluster sizes settings match best existing results logarithmic factors\",\"labeled stochastic block model random graph model representing networks community structure interactions multiple types simplest form consists two communities approximately equal size edges drawn labeled random probability depending whether two endpoints belong community conjectured citeheimlicher correlated reconstruction identification partition correlated true partition underlying communities would feasible model parameter exceeds threshold prove one half conjecture reconstruction impossible threshold positive direction introduce weighted graph exploit label information suitable choice weight function show threshold specific constant reconstruction achieved minimum bisection semidefinite relaxation minimum bisection spectral method combined removal edges incident vertices high degree furthermore show hypothesis testing labeled stochastic block model labeled erdhosrenyi random graph model exhibits phase transition conjectured reconstruction threshold\",\"explosion interest statistical models analyzing network data considerable interest class exponential random graph erg models especially connection difficulties computing maximum likelihood estimates issues associated difficulties relate broader structure discrete exponential families paper reexamines issues two parts first consider closure kdimensional exponential families distribution discrete base measure polyhedral convex support mathrmp show normal fan mathrmp geometric object plays fundamental role deriving statistical geometric properties corresponding extended exponential families discuss relevance maximum likelihood estimation theoretical computational standpoint second apply results analysis erg models particular means detailed example provide characterization properties erg models particular certain behaviors erg models known degeneracy\",\"modeling structure complex networks using bayesian nonparametrics makes possible specify flexible model structures infer adequate model complexity observed data paper provides gentle introduction nonparametric bayesian modeling complex networks using infinite mixture model running example steps deriving model infinite limit finite parametric model inferring model parameters markov chain monte carlo checking models fit predictive performance explain advanced nonparametric models complex networks derived point relevant literature\",\"estimation probabilities network edges observed adjacency matrix important applications predicting missing links network denoising usually addressed estimating graphon function determines matrix edge probabilities illdefined without strong assumptions network structure propose novel computationally efficient method based neighborhood smoothing estimate expectation adjacency matrix directly without making structural assumptions graphon estimation requires neighborhood smoothing method requires little tuning competitive meansquared error rate outperforms many benchmark methods link prediction simulated real networks\",\"consider principal component analysis pca decomposable gaussian graphical models exploit prior information models order distribute computation purpose reformulate problem sparse inverse covariance concentration domain solve global eigenvalue problem using sequence local eigenvalue problems cliques decomposable graph demonstrate application methodology context decentralized anomaly detection abilene backbone network based topology network propose approximate statistical graphical model distribute computation pca\",\"paper present framework fitting multivariate hawkes processes largescale problems number events observed history number event types dimensions proposed lowrank hawkes process lrhp framework introduces lowrank approximation kernel matrix allows perform nonparametric learning triggering kernels using ondr operations rank approximation comes major improvement existing stateoftheart inference algorithms ond furthermore lowrank approximation allows lrhp learn representative patterns interaction event types may valuable analysis complex processes real world datasets efficiency scalability approach illustrated numerical experiments simulated well real datasets\",\"article study spectral methods community detection based alphaparametrized normalized modularity matrix hereafter called lalpha heterogeneous graph models show regime community detection asymptotically trivial lalpha well approximated tractable random matrix falls family spiked random matrices analysis equivalent spiked random matrix allows improve spectral methods community detection assess performances regime study particular prove existence optimal value alpharm opt parameter alpha detection communities best ensured provide online estimation alpharm opt based knowledge graph adjacency matrix unlike classical spectral methods community detection clustering performed eigenvectors associated extreme eigenvalues show theoretical analysis regularization instead performed eigenvectors prior clustering heterogeneous graphs finally deeper study regularized eigenvectors used clustering assess performances new algorithm community detection numerical simulations course article show methods outperform stateoftheart spectral methods dense heterogeneous graphs\",\"latent space model family random graphs assigns realvalued vectors nodes graph edge probabilities determined latent positions latent space models provide natural statistical framework graph visualizing clustering latent space model particular interest random dot product graph rdpg fit using efficient spectral method however method based heuristic fail even simple cases consider closely related latent space model logistic rdpg uses logistic link function map latent positions edge likelihoods model show asymptotically exact maximum likelihood inference latent position vectors achieved using efficient spectral method method involves computing top eigenvectors normalized adjacency matrix scaling eigenvectors using regression step novel regression scaling step essential part proposed method simulations show proposed method accurate robust common practices also show effectiveness approach standard real networks karate club political blogs\",\"partitioning graph groups vertices within group densely connected vertices assigned different groups known graph clustering often used gain insight organisation large scale networks visualisation purposes whereas large number dedicated techniques recently proposed static graphs design online graph clustering methods tailored evolving networks challenging problem much less documented literature motivated broad variety applications concerned ranging study biological networks analysis networks scientific references exploration communications networks world wide web main purpose paper introduce novel computationally efficient approach graph clustering evolutionary context namely method promoted article viewed incremental eigenvalue solution spectral clustering method described incremental eigenvalue solution general technique finding approximate eigenvectors symmetric matrix given change well outlining approach detail present theoretical bound quality approximate eigenvectors using perturbation theory derive novel spectral clustering algorithm called incremental approximate spectral clustering iasc iasc algorithm simple implement efficacy demonstrated synthetic real datasets modelling evolution hiv epidemic citation network purchase history graph ecommerce website\",\"laplacian mixture models identify overlapping regions influence unlabeled graph network data scalable computationally efficient way yielding useful lowdimensional representations combining laplacian eigenspace finite mixture modeling methods provide probabilistic fuzzy dimensionality reductions domain decompositions variety input data types including mixture distributions feature vectors graphs networks provable optimal recovery using algorithm analytically shown nontrivial class cluster graphs heuristic approximations scalable highperformance implementations described empirically tested connections pagerank community detection network analysis demonstrate wide applicability approach origins fuzzy spectral methods beginning generalized heat diffusion equations physics reviewed summarized comparisons dimensionality reduction clustering methods challenging unsupervised machine learning problems also discussed\",\"performance spectral clustering considerably improved via regularization demonstrated empirically amini provide attempt quantifying improvement theoretical analysis stochastic block model sbm extensions previous results spectral clustering relied minimum degree graph sufficiently large good performance examining scenario regularization parameter tau large show minimum degree assumption potentially removed special case sbm two blocks results require maximum degree large grow faster log opposed minimum degree importantly show usefulness regularization situations nodes belong welldefined clusters results rely biasvariancelike tradeoff arises understanding concentration sample laplacian eigen gap function regularization parameter byproduct bounds propose datadriven technique textitdkest standing estimated daviskahan bounds choosing regularization parameter technique shown work well simulations real data set\",\"develop model interactions nodes dynamic network counted non homogeneous poisson processes block modelling perspective nodes belong hidden clusters whose number unknown intensity functions counting processes depend clusters nodes order make inference tractable move discrete time partitioning entire time horizon interactions observed fixedlength time subintervals first derive exact integrated classification likelihood criterion maximize relying greedy search approach allows estimate memberships clusters number clusters simultaneously maximumlikelihood estimator developed estimate non parametrically integrated intensities discuss overfitting problems model propose regularized version solving issues experiments real simulated data carried order assess proposed methodology\",\"many real world graphs graphs molecules exhibit structure multiple different scales existing kernels graphs either purely local purely global character contrast building hierarchy nested subgraphs multiscale laplacian graph kernels mlg kernels define paper account structure range different scales heart mlg construction another new graph kernel called feature space laplacian graph kernel flg kernel property lift base kernel defined vertices two graphs kernel graphs mlg kernel applies flg kernels subgraphs recursively make mlg kernel computationally feasible also introduce randomized projection procedure similar nystrom method rkhs operators\",\"present probabilistic framework overlapping community discovery link prediction relational data given graph proposed framework deep architecture enables infer multiple layers latent featurescommunities node providing superior link prediction performance complex networks better interpretability latent features regression model allows directly conditioning node latent features side information available form node attributes framework handles via clean unified model enjoys full local conjugacy via data augmentation facilitates efficient inference via closed form gibbs sampling moreover inference cost scales number edges attractive massive sparse networks framework also easily extendable model weighted networks countvalued edges compare various stateoftheart methods report results quantitative qualitative several benchmark data sets\",\"superposition temporal point processes studied many years although usefulness models practical applications fully developed investigate superposed hawkes process important class models properties studied framework least squares estimation superposition hawkes processes demonstrated beneficial tightening upper bound excess risk certain conditions show feasibility benefit typical situations usefulness superposed hawkes processes verified synthetic data potential solve coldstart problem recommendation systems demonstrated realworld data\",\"given vertex interest network vertex nomination problem seeks find corresponding vertex interest exists second network vertex nomination scheme produces list vertices ranked according likely judged corresponding vertex interest vertex nomination problem related information retrieval tasks attracted much attention machine learning literature numerous applications social biological networks however current framework often confined comparatively small class network models concept statistically consistent vertex nomination schemes shallowly explored paper extend vertex nomination problem general statistical model graphs drawing inspiration longestablished classification framework pattern recognition literature provide definitions key notions bayes optimality consistency extended vertex nomination framework including derivation bayes optimal vertex nomination scheme addition prove universally consistent vertex nomination schemes exist illustrative examples provided throughout\",\"recent years increased interest statistical analysis data multiple types relations among set entities multirelational data represented multilayer graphs set vertices represents entities multiple types edges represent different relations among community detection multilayer graphs consider two random graph models multilayer stochastic blockmodel mlsbm model restricted parameter space restricted multilayer stochastic blockmodel rmlsbm derive consistency results community assignments maximum likelihood estimators mles models mlsbm assumed true model either number nodes number types edges grow compare mles two models baseline approaches separate modeling layers aggregating layers majority voting rmlsbm shown advantage mlsbm either growth rate number communities high growth rate average degree component graphs multigraph low also derive minimax rates error sharp thresholds achieving consistency community detection models used compare multilayer models baseline model aggregate stochastic block model simulation studies real data applications confirm superior performance multilayer approaches comparison baseline procedures\",\"prove central limit theorem components eigenvectors corresponding largest eigenvalues normalized laplacian matrix finite dimensional random dot product graph corollary show stochastic blockmodel graphs rows spectral embedding normalized laplacian converge multivariate normals furthermore mean covariance matrix row functions associated vertexs block membership together prior results eigenvectors adjacency matrix compare via chernoff information multivariate normal distributions choice embedding method impacts subsequent inference demonstrate neither embedding method dominates respect inference task recovering latent block assignments\",\"paper exact linear relation leading eigenvectors modularity matrix singular vectors uncentered data matrix developed based analysis concept modularity component defined properties developed shown modularity component analysis used cluster data similar traditional principal component analysis used except modularity component analysis require data centering\",\"paper presents novel spectral algorithm additive clustering designed identify overlapping communities networks algorithm based geometric properties spectrum expected adjacency matrix random graph model call stochastic blockmodel overlap sbmo adaptive version algorithm require knowledge number hidden communities proved consistent sbmo degrees graph slightly logarithmic algorithm shown perform well simulated data realworld graphs known overlapping communities\",\"given graph vertices deemed interesting priori vertex nomination task order remaining vertices nomination list concentration interesting vertices top list previous work yielded several approaches problem theoretical results setting graph drawn stochastic block model sbm including vertex nomination analogue bayes optimal classifier paper prove maximum likelihood mlbased vertex nomination consistent sense performance mlbased scheme asymptotically matches bayes optimal scheme prove theorems form model parameters known unknown additionally introduce prove consistency related scalable restrictedfocus vertex nomination scheme finally incorporate vertex edge features mlbased vertex nomination briefly explore empirical effectiveness approach\",\"consider problem vertex classification graphs constructed latent position model shown previously approach embedding graphs euclidean space followed classification space yields universally consistent vertex classifier however major technical difficulty approach arises classifying unlabeled outofsample vertices without including embedding stage paper studied outofsample extension graph embedding step impact subsequent inference tasks show latent position graph model sufficiently large mapping outofsample vertices close true latent position demonstrate successful inference outofsample vertices possible\",\"paper design nonparametric online algorithm estimating triggering functions multivariate hawkes processes unlike parametric estimation evolutionary dynamics exploited fast computation gradient unlike typical function learning representer theorem readily applicable upon proper regularization objective function nonparametric estimation faces challenges inefficient evaluation gradient lack representer theorem iii computationally expensive projection necessary guarantee positivity triggering functions paper offer solutions challenges design online estimation algorithm named npolemhp outputs estimations mathcalot regret mathcalot stability furthermore design algorithm npolemmhp estimation multivariate marked hawkes processes test performance npolemhp various synthetic real datasets demonstrate different evaluation metrics npolemhp performs good optimal maximum likelihood estimation mle run time little parametric online algorithms\",\"variety machine learning taskseg matrix factorization topic modelling feature allocationcan viewed learning parameters probability distribution bipartite graphs recently new class models networks sparse exchangeable graphs introduced resolve important pathologies traditional approaches statistical network modelling notably inability model sparsity asymptotic sense present paper explains practical insights arising work first show check sparsity relevant modelling given fixed size dataset using network subsampling identify simple signature sparsity discuss implications sparse exchangeable subsampling theory testtrain dataset splitting argue common approaches lead biased results propose principled alternative finally study sparse exchangeable poisson matrix factorization worked example particular show adapt mean field variational inference sparse exchangeable setting allowing scale inference huge datasets\",\"detection anomalous activity graphs statistical problem arises many applications network surveillance disease outbreak detection activity monitoring social networks beyond wide applicability graph structured anomaly detection serves case study difficulty balancing computational complexity statistical power work develop first principles generalized likelihood ratio test determining well connected region activation vertices graph gaussian noise test computationally infeasible provide relaxation called lovasz extended scan statistic less uses submodularity approximate intractable generalized likelihood ratio demonstrate connection less maximum aposteriori inference markov random fields provides polytime algorithm less using electrical network theory able control type error less prove conditions less risk consistent finally consider specific graph models torus knearest neighbor graphs epsilonrandom graphs show graphs results provide nearoptimal performance matching results known lower bounds\",\"many statistical methods network data parameterize edgeprobability attributing latent traits vertices block structure assume exchangeability sense aldoushoover representation theorem empirical studies networks indicate many realworld networks powerlaw distribution vertices turn implies number edges scale slower quadratically number vertices assumptions fundamentally irreconcilable aldoushoover theorem implies quadratic scaling number edges recently caron fox proposed use different notion exchangeability due kallenberg obtained network model admits powerlaw behaviour retaining desirable statistical properties however model capture latent vertex traits blockstructure work reintroduce use blockstructure network models obeying kallenbergs notion exchangeability thereby obtain model admits inference blockstructure edge inhomogeneity derive simple expression likelihood efficient sampling method obtained model significantly difficult implement existing approaches blockmodelling performs well real network datasets\",\"given two graphs graph matching problem align two vertex sets minimize number adjacency disagreements two graphs seeded graph matching problem graph matching problem first given partial alignment tasked completing paper modify stateoftheart approximate graph matching algorithm faq vogelstein make fast approximate seeded graph matching algorithm adapt applicability include graphs differently sized vertex sets extend algorithm provide individual vertex nomination list likely matches demonstrate effectiveness algorithm via simulation real data experiments indeed knowledge even seeds extremely effective seeded graph matching algorithm used recover naturally existing alignment partially observed\",\"visual rendering graphs key task mapping complex network data although graph drawing algorithms emphasize aesthetic appeal certain applications traveltime maps place importance visualization structural network properties present paper advocates two graph embedding approaches centrality considerations comply node hierarchy problem formulated first one constrained multidimensional scaling mds solved via block coordinate descent iterations successive approximations guaranteed convergence kkt point addition regularization term enforcing graph smoothness incorporated goal reducing edge crossings second approach leverages locallylinear embedding lle algorithm assumes graph encodes data sampled lowdimensional manifold closedform solutions resulting centralityconstrained optimization problems determined yielding meaningful embeddings experimental results demonstrate efficacy approaches especially visualizing large networks order thousands nodes\",\"work develops generic framework called bagofpaths bop link network data analysis central idea assign probability distribution set paths network precisely gibbsboltzmann distribution defined bag paths network representation considers paths independently show distribution probability drawing path connecting two nodes easily computed closed form simple matrix inversion probability captures notion relatedness nodes graph two nodes considered highly related connected many preferably lowcost paths application two families distances nodes derived bop probabilities interestingly second distance family interpolates shortest path distance resistance distance addition extends bellmanford formula computing shortest path distance order integrate suboptimal paths simply replacing minimum operator soft minimum operator experimental results semisupervised classification show new distance families competitive stateoftheart approaches addition distance measures studied paper bagofpaths framework enables straightforward computation many relevant network measures\",\"lately several suggestions parametrized distances graph generalize shortest path distance commute time resistance distance need developing distances risen observation abovementioned common distances many situations fail take account global structure graph article develop theory one family graph node distances known randomized shortest path dissimilarity foundation statistical physics show randomized shortest path dissimilarity easily computed closed form pairs nodes graph moreover come new definition distance measure call free energy distance free energy distance seen upgrade randomized shortest path dissimilarity defines metric addition satisfies graphgeodetic property derivation computation free energy distance also straightforward make comparison set generalized distances interpolate shortest path distance commute time resistance distance comparison focuses applicability distances graph node clustering classification comparison general shows parametrized distances perform well tasks particular see results obtained free energy distance among best experiments\",\"many popular dimensionality reduction procedures outofsample extensions allow practitioner apply learned embedding observations seen initial training sample work consider problem obtaining outofsample extension adjacency spectral embedding procedure embedding vertices graph euclidean space present two different approaches problem one based leastsquares objective based maximumlikelihood formulation show graph interest drawn according certain latent position model called random dot product graph outofsample extensions estimate true latent position outofsample vertex error rate prove central limit theorem leastsquaresbased extension showing estimate asymptotically normal truth largegraph limit\",\"suppose one particular block stochastic block model interest block labels observed vertices network utilizing graph realized model observed block labels vertex nomination task order vertices unobserved block labels ranked nomination list goal abundance interesting vertices near top list vertex nomination schemes literature including optimally precise canonical nomination schememathcallc consistent spectral partitioning nomination schememathcallp canonical nomination scheme mathcallc provably optimally precise computationally intractable impractical implement even modestly sized graphs mind approximation canonical schemedenoted canonical sampling nomination scheme mathcallcsis introduced mathcallcs relies scalable markov chain monte carlobased approximation mathcallc converges mathcallc amount sampling goes infinity spectral partitioning nomination scheme also extended extended spectral partitioning nomination scheme mathcallep introduces novel semisupervised clustering framework improve upon precision mathcallp realdata simulation experiments employed illustrate precision vertex nomination schemes well empirical computational complexity keywords vertex nomination markov chain monte carlo spectral partitioning mclust msc\",\"community detection fundamental problem network analysis made challenging overlaps communities often occur practice propose general flexible interpretable generative model overlapping communities thought generalization degreecorrected stochastic block model develop efficient spectral algorithm estimating community memberships deals overlaps employing kmedians algorithm rather usual kmeans clustering spectral domain show algorithm asymptotically consistent networks sparse overlaps communities large numerical experiments simulated networks many real social networks demonstrate method performs well compared number benchmark methods overlapping community detection\",\"consider problem model selection gaussian markov fields sample deficient scenario benchmark informationtheoretic results case dregular graphs require number samples least proportional logarithm number vertices allow consistent graph recovery number samples less amount reliable detection edges impossible many applications important learn distribution edge coupling parameters network specific locations edges assuming entire graph partitioned number spatial regions similar edge parameters reasonably regular boundaries develop new informationtheoretic sample complexity bounds show bounded number samples sufficient consistently recover regions finally introduce analyze efficient region growing algorithm capable recovering regions high accuracy show consistent demonstrate performance benefits synthetic simulations\",\"paper develops exact linear relationship leading eigenvector unnormalized modularity matrix eigenvectors adjacency matrix propose method approximating leading eigenvector modularity matrix derive error approximation also complete proof equivalence normalized adjacency clustering normalized modularity clustering numerical experiments show normalized adjacency clustering twice efficient normalized modularity clustering\",\"paper consider problem link prediction timeevolving graphs assume certain graph features node degree follow vector autoregressive var model propose use information improve accuracy prediction strategy involves joint optimization procedure space adjacency matrices var matrices takes account sparsity low rank properties matrices oracle inequalities derived illustrate tradeoffs choice smoothing parameters modeling joint effect sparsity low rank property estimate computed efficiently using proximal methods generalized forwardbackward agorithm\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"4_graph_graphs_network\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"4_graph_graphs_network\"],\"textfont\":{\"size\":12},\"x\":[11.061233,11.365342,10.91263,10.742904,11.402756,11.394188,10.890161,11.455836,10.954642,11.560288,10.65509,10.661355,10.490015,10.7572365,10.673442,10.851888,10.862428,10.690656,10.851251,11.046717,11.489833,11.287918,11.283057,10.969165,10.891211,11.2856455,10.8445215,11.431507,11.244985,10.682059,10.85148,10.725477,11.387202,11.424305,10.940285,11.149332,11.2217655,11.421024,10.8961525,11.572202,11.328269,10.706434,10.7439785,10.754494,10.861775,11.4156065,10.953976,10.690518,10.715501,10.960126,11.174996,11.380847,11.51352,10.946845,11.114758,11.168321,11.161947,10.89708,10.842574,10.736417,10.890344,11.082183,10.851524,11.280803,11.122852,11.279886,11.3961315,11.4114275,10.723404,11.269409,10.93806,10.852133,10.986306,10.823776,11.161925,11.3302,11.124901,11.048853,11.091376,10.8514385,10.911584,10.939433,10.814181,10.93517,11.321015,10.74474,10.807827,11.276431,11.129402,11.072466,10.707187,11.332709,10.771232,11.039004],\"y\":[1.9211369,1.9705085,2.3338249,2.3339665,2.1705706,2.117818,2.42569,2.0618763,2.3299425,2.125073,2.5205746,2.1359808,2.5545917,2.097055,2.171246,2.4350097,2.0824733,2.779337,12.323108,2.1595306,1.9478024,1.9322022,1.936343,2.4439416,2.2084022,1.9294759,2.3383412,2.003142,1.9936877,2.0612168,12.327746,2.1976335,1.9636511,1.9192473,1.9997116,2.0066159,1.9118891,1.8941439,2.4461508,2.1935613,1.9413624,2.7735674,2.304747,2.7298648,2.0211928,1.8947557,2.2289162,2.466626,2.76293,1.9755944,1.8852627,2.0150807,2.0052035,2.4148984,2.0328896,1.9284075,1.9653465,2.1175008,2.4623632,2.279675,2.2378235,2.1494813,12.328047,1.886513,2.0132637,1.9057956,1.9248337,1.9200355,2.10892,2.1370623,2.1245284,12.328467,2.3987002,2.2726178,2.0184884,1.858952,1.9057225,2.3953078,2.2232008,12.328307,2.3333786,2.3074644,2.394369,2.4045,2.152452,2.4895163,2.4119956,2.1584082,2.0673437,1.9239764,2.7786381,1.8266762,2.1779103,2.718313],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"melanoma deadliest form skin cancer computer systems assist melanoma detection widespread clinical practice open challenge classification dermoscopic images skin lesions announced training set images corresponding class labels semiautomaticmanual segmentation masks released challenge independent test set images used rank participants article demonstrates impact ranking criteria segmentation method classifier highlights clinical perspective compare five different measures diagnostic accuracy analysing resulting ranking computer systems challenge choice performance measure great impact ranking systems ranked among top three one measure dropped bottom half changing performance measure nevus doctor computer system previously developed authors used investigate impact segmentation classifier unexpected small impact automatic versus semiautomaticmanual segmentation suggests improvements automatic segmentation method wrt resemblance semiautomaticmanual segmentation improve diagnostic accuracy substantially small set similar classification algorithms used investigate impact classifier diagnostic accuracy variability diagnostic accuracy different classifier algorithms larger variability segmentation methods suggests focus future investigations clinical perspective misclassification melanoma benign far greater cost misclassification benign lesion computer systems clinical impact performance ranked highsensitivity measure\",\"paper analyzes use convolutional neural networks brain tumor segmentation images address problem using three different architectures combine fine coarse features obtain final segmentation compare three different networks use multiresolution features terms design performance show improve singleresolution counterparts\",\"combining information different sources common way improve classification accuracy braincomputer interfacing bci instance small sample settings useful integrate data subjects sessions order improve estimation quality spatial filters classifier since data different subjects may show large variability crucial weight contributions according importance many multisubject learning algorithms determine optimal weighting separate step using heuristics however without ensuring selected weights optimal respect classification work apply multiple kernel learning mkl problem mkl widely used feature fusion computer vision allows simultaneously learn classifier optimal weighting compare mkl method two baseline approaches investigate reasons performance improvement\",\"case control comparisons classical approach study neurological diseases however patients fall cleanly either group instead clinicians typically find patients cannot classified clearly progressed disease state subjects little said brain function basis analyses group differences describe intermediate brain function requires models interpolate disease states chosen gaussian processes regression obtain continuous spectrum brain activation extract unknown disease progression profile models incorporate spatial distribution measures activation correlation fmri trace input stimulus constitute ultrahigh multivariate regressors applied gps model fmri image phenotypes across alzheimers disease behavioural measures mmse ace etc scores obtained predictions nonobserved mmseace values overall model confirmed known reduction spatial extent activity response reading versus falsefont stimulation predictive uncertainty indicated worsening confidence intervals behavioural scores distance used training thus model indicated type patient behavioural score would need included training data improve models predictions\",\"understanding relationships different properties data whether connectome genome information disease status becoming increasingly important modern biological datasets existing approaches test whether two properties related often require unfeasibly large sample sizes real data scenarios provide insight procedure reached decision approach multiscale graph correlation mgc dependence test juxtaposes previously disparate data science techniques including knearest neighbors kernel methods support vector machines multiscale analysis wavelets methods typically require double triple number samples achieve statistical power mgc benchmark suite including highdimensional nonlinear relationships spanning polynomial linear quadratic cubic trigonometric sinusoidal circular ellipsoidal spiral geometric square diamond wshape functions dimensionality ranging moreover mgc uniquely provides simple elegant characterization potentially complex latent geometry underlying relationship providing insight maintaining computational efficiency several real data applications including brain imaging cancer genetics mgc method detect presence dependency provide specific guidance next experiment andor analysis conduct\",\"paper considers problem brain disease classification based connectome data connectome network representation human brain typical connectome classification problem challenging small sample size high dimensionality data propose use simultaneous approximate diagonalization adjacency matrices order compute eigenstructures stable way obtained approximate eigenvalues used features classification proposed approach demonstrated efficient detection alzheimers disease outperforming simple baselines competing stateoftheart approaches brain disease classification\",\"propose novel classification model weak signal data building upon recent model bayesian multiview learning group factor analysis gfa instead assuming data come single gfa model allow latent clusters different gfa model producing different class distribution show sharing information across clusters sharing factors increases classification accuracy considerably shared factors essentially form flexible noise model explains away part data related classification motivation setting comes singletrial functional brain imaging data low signaltonoise ratio natural multiview setting different sensors measurement modalities eeg meg fmri possible auxiliary information views demonstrate model meg dataset\",\"inverse inference brain reading recent paradigm analyzing functional magnetic resonance imaging fmri data based pattern recognition statistical learning predicting cognitive variables related brain activation maps approach aims decoding brain activity inverse inference takes account multivariate information voxels currently way assess precisely cognitive information encoded activity neural populations within whole brain however relies prediction function plagued curse dimensionality since far features samples voxels fmri volumes address problem different methods proposed among others univariate feature selection feature agglomeration regularization techniques paper consider sparse hierarchical structured regularization specifically penalization use constructed tree obtained spatiallyconstrained agglomerative clustering approach encodes spatial structure data different scales regularization makes overall prediction procedure robust intersubject variability regularization used induces selection spatially coherent predictive brain regions simultaneously different scales test algorithm real data acquired study mental representation objects show proposed algorithm delineates meaningful brain regions yields well better prediction accuracy reference methods\",\"manuscript consider problem jointly estimating multiple graphical models high dimensions assume data collected subjects consists possibly dependent observations graphical models subjects vary assumed change smoothly corresponding measure closeness subjects propose kernel based method jointly estimating graphical models theoretically double asymptotic framework dimension increase provide explicit rate convergence parameter estimation characterizes strength one borrow across different individuals impact data dependence parameter estimation empirically experiments synthetic real resting state functional magnetic resonance imaging rsfmri data illustrate effectiveness proposed method\",\"development computed tomography image reconstruction methods significantly reduce patient radiation exposure maintaining high image quality important area research lowdose ldct imaging propose new penalized weighted least squares pwls reconstruction method exploits regularization based efficient union learned transforms pwlsultra union square transforms prelearned numerous image patches extracted dataset images volumes proposed pwlsbased cost function optimized alternating image reconstruction step sparse coding clustering step image reconstruction step accelerated relaxed linearized augmented lagrangian method orderedsubsets reduces number forward back projections simulations axial scans extended cardiactorso phantom helical chest abdomen scans show normaldose lowdose levels proposed method significantly improves quality reconstructed images compared pwls reconstruction nonadaptive edgepreserving regularizer pwlsep pwls regularization based union learned transforms leads better image reconstructions using single learned square transform also incorporate patchbased weights pwlsultra enhance image quality help improve image resolution uniformity proposed approach achieves comparable better image quality compared learned overcomplete synthesis dictionaries importantly much faster computationally efficient\",\"integrative analysis disparate data blocks measured common set experimental subjects major challenge modern data analysis data structure naturally motivates simultaneous exploration joint individual variation within data block resulting new insights instance strong desire integrate multiple genomic data sets cancer genome atlas characterize common also unique aspects cancer genetics cell biology source paper introduce anglebased joint individual variation explained capturing joint individual variation within data block major improvement earlier approaches challenge terms new conceptual understanding much better adaption data heterogeneity fast linear algebra computation important mathematical contributions use score subspaces principal descriptors variation structure use perturbation theory guide variation segmentation leads exploratory data analysis method insensitive heterogeneity among data blocks require separate normalization application cancer data reveals different behaviors type signal characterizing tumor subtypes application mortality data set reveals interesting historical lessons software data available github httpsgithubcommeileijiangajiveproject\",\"explosion interest functional magnetic resonance imaging mri past two decades naturally accompanied many major advances understanding human connectome advances served pose novel challenges well open new avenues research one promising exciting avenues study functional mri realtime studies recently gained momentum applied wide variety settings ranging training healthy subjects selfregulate neuronal activity suggested potential treatments clinical populations date vast majority studies focused single region time due part many challenges faced estimating dynamic functional connectivity networks realtime work propose novel methodology accurately track changes functional connectivity networks realtime adapt recently proposed single algorithm estimating sparse temporally homo geneous dynamic networks applicable realtime proposed method applied motor task data human connectome project well realtime data tained exploring virtual environment show algorithm able estimate significant taskrelated changes network structure quickly enough useful future braincomputer interface applications\",\"decoding prediction brain images signals calls empirical evaluation predictive power evaluation achieved via crossvalidation method also used tune decoders hyperparameters paper review crossvalidation procedures decoding neuroimaging includes didactic overview relevant theoretical considerations practical aspects highlighted extensive empirical study common decoders withinand acrosssubject predictions multiple datasets anatomical functional mri meg simulations theory experiments outline popular leaveoneout strategy leads unstable biased estimates repeated random splits method preferred experiments outline large error bars crossvalidation neuroimaging settings typical confidence intervals nested crossvalidation tune decoders parameters avoiding circularity bias however find favorable use sane defaults particular nonsparse decoders\",\"matrix factorisation methods decompose multivariate observations linear combinations latent feature vectors indian buffet process ibp provides way model number latent features required good approximation terms regularised reconstruction error previous work focussed latent feature vectors independent entries extend model include nondiagonal latent covariance structures representing characteristics smoothness done using simulations demonstrate appropriate conditions smoothness prior helps recover true latent features denoising accurately demonstrate method real neuroimaging dataset computational tractability sufficient challenge efficient strategy presented essential\",\"substantial evidence indicates major psychiatric disorders associated distributed neural dysconnectivity leading strong interest using neuroimaging methods accurately predict disorder status work specifically interested multivariate approach uses features derived wholebrain resting state functional connectomes however functional connectomes reside high dimensional space complicates model interpretation introduces numerous statistical computational challenges traditional feature selection techniques used reduce data dimensionality blind spatial structure connectomes propose regularization framework structure functional connectome explicitly taken account via fused lasso graphnet regularizer method restricts loss function convex marginbased allowing nondifferentiable loss functions hingeloss used using fused lasso graphnet regularizer hingeloss leads structured sparse support vector machine svm embedded feature selection introduce novel efficient optimization algorithm based augmented lagrangian classical alternating direction method solve fused lasso graphnet regularized svm little modification also demonstrate inner subproblems algorithm solved efficiently analytic form coupling variable splitting strategy data augmentation scheme experiments simulated data resting state scans large schizophrenia dataset show proposed approach identify predictive regions spatially contiguous connectome space offering additional layer interpretability could provide new insights various disease processes\",\"extracting information functional magnetic resonance fmri images major area research two decades goal work present new method analysis fmri data sets capable incorporate priori available information via efficient optimization framework tests synthetic data sets demonstrate significant performance gains existing methods kind\",\"use machinelearning neuroimaging offers new perspectives early diagnosis prognosis brain diseases although multivariate methods capture complex relationships data traditional approaches provide irregular penalty scattered penalty predictive pattern limited relevance penalty like total variation exploits natural structure images increase spatial coherence weight map however penalization leads nonsmooth optimization problems hard minimize propose optimization framework minimizes combination penalties preserving exact penalty algorithm uses nesterovs smoothing technique approximate penalty smooth function loss penalties minimized exact accelerated proximal gradient algorithm propose original continuation algorithm uses successively smaller values smoothing parameter reach prescribed precision achieving best possible convergence rate algorithm used losses penalties algorithm applied classification problem adni dataset observe penalty necessarily improve prediction provides major breakthrough terms support recovery predictive brain regions\",\"systems biomedicine experimenter encounters different potential sources variation data individual samples multiple experimental conditions multivariable networklevel responses multiparametric cytometry often used analyzing patient samples issues critical computational methods identify cell populations individual samples without ability automatically match across samples difficult compare characterize populations typical experiments responding various stimulations distinctive particular patients timepoints especially many samples joint clustering matching jcm multilevel framework simultaneous modeling registration populations across cohort jcm models every population robust multivariate probability distribution simultaneously jcm fits randomeffects model construct overall batch template used registering populations across samples classifying new samples tackling systemslevel variation jcm supports practical biomedical applications involving large cohorts\",\"correlated component analysis proposed dmochowski tool investigating brain process similarity responses multiple views given stimulus correlated components identified assumption involved spatial networks identical propose hierarchical probabilistic model infer level universality multiview data completely unrelated representations corresponding canonical correlation analysis identical representations correlated component analysis new model denote bayesian correlated component analysis evaluates favourably three relevant algorithms simulated data wellestablished benchmark eeg dataset used validate new model infer variability spatial representations across multiple subjects\",\"symmetric binary matrices representing relations among entities commonly collected many areas focus dynamically evolving binary relational matrices interest inference relationship structure prediction propose nonparametric bayesian dynamic model reduces dimensionality characterizing binary matrix lowerdimensional latent space representation latent coordinates evolving continuous time via gaussian processes using logistic mapping function probability matrix space latent relational space obtain flexible computational tractable formulation employing polyagamma data augmentation efficient gibbs sampler developed posterior computation dimension latent space automatically inferred provide theoretical results flexibility model illustrate performance via simulation experiments also consider application comovements world financial markets\",\"introduce novel kernel models inputdependent couplings across multiple latent processes pairwise joint kernel measures covariance along inputs across different latent signals mutuallydependent fashion latent correlation gaussian process lcgp model combines nonstationary latent components multiple outputs inputdependent mixing matrix probit classification support multiple observation sets derived variational bayesian inference results several datasets indicate lcgp model recover correlations latent signals simultaneously achieving stateoftheart performance highlight latent covariances eeg classification dataset latent brain processes couplings simultaneously emerge model\",\"brain decoding involves determination subjects cognitive state associated stimulus functional neuroimaging data measuring brain activity setting cognitive state typically characterized element finite set neuroimaging data comprise voluminous amounts spatiotemporal data measuring aspect neural signal associated statistical problem one classification highdimensional data explore use functional principal component analysis mutual information networks persistent homology examining data exploratory analysis constructing features characterizing neural signal brain decoding review approach perspective incorporate features classifier based symmetric multinomial logistic regression elastic net regularization approaches illustrated application task infer brain activity measured magnetoencephalography meg type video stimulus shown subject\",\"volume collection contributions workshop machine learning interpretation neuroimaging mlini neural information processing systems nips conference modern multivariate statistical methods developed rapidly growing field machine learning increasingly applied various problems neuroimaging cognitive state detection clinical diagnosis prognosis multivariate pattern analysis methods designed examine complex relationships highdimensional signals brain images outcomes interest category stimulus type mental state subject specific mental disorder techniques contrast traditional massunivariate approaches dominated neuroimaging past treated individual imaging measurement isolation believe machine learning prominent role shaping questions neuroscience framed machinelearning mind set entering modern psychology behavioral studies also equally important practical applications fields motivate rapidly evolving line research machine learning community parallel intense interest learning brain function context rich naturalistic environments scenes efforts beyond highly specific paradigms pinpoint single function towards schemes measuring interaction natural varied scene made goal workshop pinpoint pressing issues common challenges across neuroscience neuroimaging psychology machine learning fields sketch future directions open questions light novel methodology\",\"sources variability experimentally derived data include measurement error addition physical phenomena interest measurement error combination systematic components originating measuring instrument random measurement errors several novel biological technologies mass cytometry singlecell rnaseq plagued systematic errors may severely affect statistical analysis data properly calibrated propose novel deep learning approach removing systematic batch effects method based residual network trained minimize maximum mean discrepancy mmd multivariate distributions two replicates measured different batches apply method mass cytometry singlecell rnaseq datasets demonstrate effectively attenuates batch effects\",\"gene expression data represents unique challenge predictive model building small number samples compared huge amount features property hampered application deep learning techniques disease outcome classification sparse learning incorporating external gene network information could potential solution issue still problem challenging tens thousands features hundreds training samples scalefree structure gene network unfriendly setup convolutional neural networks address issues build robust classification model propose graphembedded deep feedforward networks gedfn integrate external relational information features deep neural network architecture method able achieve sparse connection network layers prevent overfitting validate methods capability conducted simulation experiments real data analysis using breast cancer rnaseq dataset cancer genome atlas tcga resulting high classification accuracy easily interpretable feature selection results suggest method useful addition current classification models feature selection procedures method available httpsgithubcomyunchuankongnetworkneuralnetwork\",\"factor analysis provides linear factors describe relationships individual variables data set extend classical formulation linear factors describe relationships groups variables group represents either set related variables data set model also naturally extends canonical correlation analysis two sets way flexible previous extensions solution formulated variational inference latent variable model structural sparsity consists two hierarchical levels higher level models relationships groups whereas lower models observed variables given higher level show resulting solution solves group factor analysis problem accurately outperforming alternative factor analysis based solutions well straightforward implementations group factor analysis method demonstrated two life science data sets one brain activation systems biology illustrating applicability analysis different types highdimensional data sources\",\"propose macau powerful flexible bayesian factorization method heterogeneous data model factorize set entities relations represented relational model including tensors also multiple relations entity macau also incorporate side information specifically entity relation features crucial predicting sparsely observed relations macau scales millions entity instances hundred millions observations sparse entity features millions dimensions achieve scale specially designed sampling procedure entity relation features relies primarily noise injection linear regressions show performance advanced features macau set experiments including challenging drugprotein activity prediction task\",\"advances modern sensing sequencing technologies generate deluge high dimensional spacetemporal physiological nextgeneration sequencing ngs data physiological traits observed either continuous random functions dense grid referred functionvalued traits physiological ngs data highly correlated data inherent order spacing functional nature ignored traditional summarybased univariate multivariate regression methods designed quantitative genetic analysis scalar trait common variants capture morphological dynamic features data utilize dependent structure propose functional linear model flm trait curve modeled response function genetic variation genomic region gene modeled functional predictor genetic effects modeled function time genomic position flmf genetic analysis functionvalued trait gwas ngs data extensive simulations demonstrate flmf correct type error rates much higher power detect association existing methods flmf applied sleep data starr county health studies oxygen saturation measured seconds average individuals found genes significantly associated oxygen saturation functional trait pvalues ranging results clearly demonstrate flmf substantially outperforms traditional genetic models scalar trait\",\"multiple instance dictionary learning approach dictionary learning using functions multiple instances dlfumi used perform beattobeat heart rate estimation characterize heartbeat signatures ballistocardiogram bcg signals collected hydraulic bed sensor dlfumi estimates heartbeat concept represents individuals personal ballistocardiogram heartbeat pattern dlfumi formulates heartbeat detection heartbeat characterization multiple instance learning problem address uncertainty inherent aligning bcg signals ground truth training experimental results show estimated heartbeat concept found dlfumi effective heartbeat prototype achieves superior performance comparison algorithms\",\"neuroscientists enjoyed much success understanding brain functions constructing brain connectivity networks using data collected highly controlled experimental settings however experimental settings bear little resemblance reallife experience daytoday interactions surroundings address issue neuroscientists measuring brain activity natural viewing experiments subjects given continuous stimuli watching movie listening story main challenge approach measured signal consists stimulusinduced signal well intrinsicneural nonneuronal signals exploiting experimental design propose estimate stimuluslocked brain network treating nonstimulusinduced signals nuisance parameters many neuroscience applications often important identify brain regions connected many brain regions cognitive process propose inferential method test whether maximum degree estimated network larger prespecific number prove type error controlled power increases one asymptotically simulation studies conducted assess performance method finally analyze functional magnetic resonance imaging dataset obtained sherlock holmes movie stimuli\",\"proposed complex populations arise genomics studies may exhibit dependencies among observations well among variables gives rise challenging problem analyzing unreplicated highdimensional data unknown mean dependence structures matrixvariate approaches impose various forms inverse covariance sparsity allow flexible dependence structures estimated cannot directly applied mean covariance matrices estimated jointly present practical method utilizing generalized least squares penalized inverse covariance estimation address challenge establish consistency obtain rates convergence estimating mean parameters covariance matrices advantages approaches dependence graphs covariance structures estimated presence unknown mean structure mean structure becomes efficiently estimated accounting dependence structure among observations iii inferences mean parameters become correctly calibrated use simulation studies analysis genomic data twin study ulcerative colitis illustrate statistical convergence performance methods practical settings several lines evidence show test statistics differential gene expression produced methods correctly calibrated improve power conventional methods\",\"diffusionweighted magnetic resonance imaging dwi fiber tractography methods measure structure white matter living human brain diffusion signal modelled combined contribution many individual fascicles nerve fibers passing location white matter typically done via basis pursuit estimation exact directions limited due discretization difficulties inherent modeling dwi data shared many problems involving fitting nonparametric mixture models ekanadaham proposed approach continuous basis pursuit overcome discretization error dimensional case spikesorting propose general algorithm fits mixture models dimensionality without discretization algorithm uses principles lboost together refitting weights pruning parameters addition steps lboost accelerates algorithm assures accuracy refer resulting algorithm elastic basis pursuit ebp since expands contracts active set kernels needed show contrast existing approaches fitting mixtures boosting framework enables selection optimal biasvariance tradeoff along solution path scales highdimensional problems simulations dwi find ebp yields better parameter estimates nonnegative least squares nnls approach standard model used dwi tensor model serves basis diffusion tensor imaging dti demonstrate utility method dwi data acquired parts brain containing crossings multiple fascicles nerve fibers\",\"extend multiway multivariate anovatype analysis cases one covariate view features view coming different highdimensional domains different views assumed connected paired samples common setup recent bioinformatics experiments analyze metabolite profiles different conditions disease control treatment untreated different tissues views introduce multiway latent variable model new task extending generative model bayesian canonical correlation analysis cca take multiway covariate information account population priors reducing dimensionality integrated factor analysis assumes metabolites come correlated groups\",\"translating potential disease biomarkers multispecies omics experiments new direction biomedical research existing methods limited simple experimental setups basic healthydiseased comparisons methods also require priori matching variables genes metabolites species however many experiments complicated multiway experimental design often involving irregularlysampled timeseries measurements instance metabolites always known matchings organisms introduce bayesian modelling framework translating multiple species results omics experiments complex multiway timeseries experimental design underlying assumption unknown matching inferred response variables multiple covariates including time\",\"improving interpretability brain decoding approaches primary interest many neuroimaging studies despite extensive studies type present formal definition interpretability brain decoding models consequence quantitative measure evaluating interpretability different brain decoding methods paper present simple definition interpretability linear brain decoding models propose combine interpretability performance brain decoding new multiobjective criterion model selection preliminary results toy data show optimizing hyperparameters regularized linear classifier based proposed criterion results informative linear models presented definition provides theoretical background quantitative evaluation interpretability linear brain decoding\",\"learning children animals occurs effortlessly largely without obvious supervision successes automating supervised learning translated ambiguous realm unsupervised learning goals labels provided barlow suggested signal brains leverage unsupervised learning dependence redundancy sensory environment dependence characterized using informationtheoretic multivariate mutual information measure called total correlation principle total correlation explanation corex learn representations data explain much dependence data possible review manifestations principle along successes unsupervised learning problems across diverse domains including human behavior biology language\",\"profiling cellular phenotypes microscopic imaging provide meaningful biological information resulting various factors affecting cells one motivating application drug development morphological cell features captured images similarities different drug compounds applied different doses quantified general approach find function mapping images embedding space manageable dimensionality whose geometry captures relevant features input images important known issue methods separating relevant biological signal nuisance variation example embedding vectors tend correlated cells cultured imaged week different weeks despite identical drug compounds applied cases case particular batch set experiments conducted constitutes domain data ideal set image embeddings contain relevant biological information drug effects develop general framework adjusting image embeddings order forget domainspecific information preserving relevant biological information achieve minimize loss function based distances marginal distributions wasserstein distance embeddings across domains replicated treatment dataset present results replicated treatment happens negative control treatment expect treatmentinduced cell morphology changes find transformed embeddings underlying geometric structure preserved embeddings also carry improved biological signal less domainspecific information present\",\"problems machine learning involve noisy input data classification methods reached limiting accuracies based standard data sets consisting feature vectors classes greater accuracy require incorporation prior structural information data learning study methods regularize feature vectors unsupervised regularization methods analogous supervised regularization estimating functions study regularization denoising feature vectors using tikhonov regularization methods functions feature vector xxldotsxnxqqn viewed function index smoothed using prior information structure involve penalty functional feature vectors analogous statistical learning use proximity graph structure set indices feature vector regularization inherits property function denoising accuracy nonmonotonic denoising regularization parameter alpha assumptions noise level data structure show best reconstruction accuracy also occurs finite positive alpha index spaces graph structures adapt two standard function denoising methods used local averaging kernel regression general index space discrete set notion proximity metric space subset graphnetwork feature vectors functions notion continuity show improves feature vector recovery thus subsequent classification regression done give example gene expression analysis cancer classification genome index space network structure based proteinprotein interactions\",\"introduce general framework estimation inverse covariance precision matrices heterogeneous populations proposed framework uses laplacian shrinkage penalty encourage similarity among estimates disparate related subpopulations allowing differences among matrices propose efficient alternating direction method multipliers admm algorithm parameter estimation well extension faster computation high dimensions thresholding empirical covariance matrix identify joint block diagonal structure estimated precision matrices establish variable selection norm consistency proposed estimator distributions exponential polynomial tails extend applicability method settings unknown populations structure propose laplacian penalty based hierarchical clustering discuss conditions datadriven choice results consistent estimation precision matrices heterogenous populations extensive numerical studies applications gene expression data subtypes cancer distinct clinical outcomes indicate potential advantages proposed method existing approaches\",\"present study proposes deep learning model named deepsleepnet automatic sleep stage scoring based raw singlechannel eeg existing methods rely handengineered features require prior knowledge sleep analysis encode temporal information transition rules important identifying next sleep stages extracted features proposed model utilize convolutional neural networks extract timeinvariant features bidirectionallong shortterm memory learn transition rules among sleep stages automatically eeg epochs implement twostep training algorithm train model efficiently evaluated model using different singlechannel eegs feogleft fpzcz pzoz two public sleep datasets different properties sampling rate scoring standards aasm results showed model achieved similar overall accuracy macro fscore mass sleepedf compared stateoftheart methods mass sleepedf datasets demonstrated without changing model architecture training algorithm model could automatically learn features sleep stage scoring different raw singlechannel eegs different datasets without utilizing handengineered features\",\"genomewide interaction studies detect genegene interactions methods divided two folds single nucleotide polymorphisms snp based genebased methods basically methods based gene effective methods based single snp recent years kernel canonical correlation analysis classical kernel cca based statistic kccu proposed detect nonlinear relationship genes estimate variance kccu used resampling based methods highly computationally intensive addition classical kernel cca robust contaminated data therefore first discuss robust kernel mean element robust kernel covariance crosscovariance operators second propose method based influence function estimate variance kccu third propose nonparametric robust kccu method based robust kernel cca designed contaminated data less sensitive noise classical kernel cca finally investigate proposed methods synthesized data imaging genetic data set based gene ontology pathway analysis synthesized genetics analysis demonstrate proposed robust method shows superior performance stateoftheart methods\",\"predictive models used highdimensional brain images diagnosis clinical condition spatial regularization structured sparsity offers new perspectives context reduces risk overfitting model providing interpretable neuroimaging signatures forcing solution adhere domainspecific constraints total variation enforces spatial smoothness solution segmenting predictive regions background consider problem minimizing sum smooth convex loss nonsmooth convex penalty whose proximal operator known wide range possible complex nonsmooth convex structured penalties overlapping group lasso existing solvers either limited functions minimize practical capacity scale highdimensional imaging data nesterovs smoothing technique used minimize large number nonsmooth convex structured penalties reasonable precision requires small smoothing parameter slows convergence speed benefit versatility nesterovs smoothing technique propose first order continuation algorithm conesta automatically generates sequence decreasing smoothing parameters generated sequence maintains optimal convergence speed towards globally desired precision main contributions propose expression duality gap probe current distance global optimum order adapt smoothing parameter convergence speed provide convergence rate improvement classical proximal gradient smoothing methods demonstrate simulated highdimensional structural neuroimaging data conesta significantly outperforms many stateoftheart solvers regard convergence speed precision\",\"increasing size complexity scientific data could dramatically enhance discovery prediction basic scientific applications realizing potential however requires novel statistical analysis methods interpretable predictive introduce union intersections uoi flexible modular scalable framework enhanced model selection estimation methods based uoi perform model selection model estimation intersection union operations respectively show uoibased methods achieve lowvariance nearly unbiased estimation small number interpretable features maintaining highquality prediction accuracy perform extensive numerical investigation evaluate uoi algorithm uoilasso synthetic real data demonstrate extraction interpretable functional networks human electrophysiology recordings well accurate prediction phenotypes genotypephenotype data reduced features also show uoillogistic uoicur variants basic framework improved prediction parsimony classification matrix factorization several benchmark biomedical data sets results suggest methods based uoi framework could improve interpretation prediction datadriven discovery across scientific fields\",\"flow cytometry highthroughput technology used quantify multiple surface intracellular markers level single cell enables identify cell subtypes determine relative proportions improvements technology allow describe millions individual cells blood sample using multiple markers results highdimensional datasets whose manual analysis highly timeconsuming poorly reproducible several methods developed perform automatic recognition cell populations treat analyze sample independently however practice individual samples rarely independent longitudinal studies propose use bayesian nonparametric approach dirichlet process mixture dpm multivariate skew tdistributions perform model based clustering flowcytometry data dpm models directly estimate number cell populations data avoiding model selection issues skew tdistributions provides robustness outliers nonelliptical shape cell populations accommodate repeated measurements propose sequential strategy relying parametric approximation posterior illustrate good performance method simulated data experimental benchmark dataset new longitudinal data dalia trial evaluates therapeutic vaccine hiv benchmark dataset sequential strategy outperforms methods evaluated similarly leads improved performance dalia data made method available community package npflow\",\"estimating state dynamical system series noisecorrupted observations fundamental many areas science engineering wellknown method kalman smoother related kalman filter relies assumptions linearity gaussianity rarely met practice paper introduced new dynamical smoothing method exploits remarkable capabilities convolutional neural networks approximate complex nonlinear functions main idea generate training set composed latent states observations ensemble simulators train deep network recover former latter importantly method requires availability simulators therefore applied situations either latent dynamical model observation model cannot easily expressed closed form simulation studies show resulting convnet smoother almost optimal performance gaussian case even parameters unknown furthermore method successfully applied extremely nonlinear nongaussian systems finally empirically validate approach via analysis measured brain signals\",\"construction synthetic complexvalued signals realvalued observations important step many time series analysis techniques widely used approach based hilbert transform maps realvalued signal quadrature component paper define probabilistic generalization approach model observable realvalued signal real part latent complexvalued gaussian process order obtain appropriate statistical relationship real imaginary parts define two new classes complexvalued covariance functions analysis simulated chirplets stochastic oscillations show resulting gaussian process complexvalued signal provides better estimate instantaneous amplitude frequency established approaches furthermore complexvalued gaussian process regression allows incorporate prior information structure signal noise thereby tailor analysis features signal example analyze nonstationary dynamics brain oscillations alpha band measured using magnetoencephalography\",\"understanding type inhibitory interaction plays important role drug design therefore researchers interested know whether drug competitive noncompetitive interaction particular protein targets method analyze interaction types propose factorization method macau allows combine different measurement types single tensor together proteins compounds compounds characterized high dimensional ecfp fingerprints novelty proposed method using specially designed noise injection mcmc sampler incorporate high dimensional side information millions unique ecfp compound features even large scale datasets millions compounds without side information case tensor factorization would practically futile results using public data chembl trained model identify latent subspace separating two measurement types results suggest proposed method detect competitive inhibitory activity compounds proteins\",\"personalized treatment patients based tissuespecific cancer subtypes strongly increased efficacy chosen therapies even though amount data measured cancer patients increased last years cancer subtypes still diagnosed based individual data sources gene expression data propose unsupervised data integration method based kernel principal component analysis principal component analysis one widely used techniques data analysis unfortunately straightforward multiplekernel extension method leads use one input matrices fit goal gaining information data sources therefore present scoring function determine impact input matrix approach enables visualizing integrated data subsequent clustering cancer subtype identification due nature method free parameters set apply methodology five different cancer data sets demonstrate advantages terms results usability\",\"computing accurate estimates fourier transform analog signals discrete data points important many fields science engineering conventional approach performing discrete fourier transform data implicitly assumes periodicity bandlimitedness signal paper use gaussian process regression estimate fourier transform integral transform without making assumptions possible posterior expectation gaussian process regression maps finite set samples function defined whole real line expressed linear combination covariance functions estimate covariance function data using appropriately designed gradient ascent method constrains solution linear combination tractable kernel functions procedure results posterior expectation analog signal whose fourier transform obtained analytically exploiting linearity simulations show new method leads sharper precise estimation spectral density noisefree noisecorrupted signals validate method two realworld applications analysis yearly fluctuation atmospheric level analysis spectral content brain signals\",\"identifying altered pathways associated specific cancer types potentially bring significant impact cancer patient treatment accurate identification key altered pathways information used develop novel therapeutic agents well understand molecular mechanisms various types cancers better trimatrix factorization efficient tool learn associations two different entities cancer types pathways case data successfully apply trimatrix factorization methods biomedical problems biological prior knowledge pathway databases proteinprotein interaction ppi networks taken account factorization model however straightforward bayesian setting even though bayesian methods appealing point estimate methods maximum likelihood maximum posterior method sense calculate distributions variables robust overfitting propose bayesian seminonnegative matrix factorization model human cancer genomic data biological prior knowledge represented pathway database ppi network taken account factorization model finite dependent betabernoulli prior tested method cancer genome atlas tcga dataset found pathways identified method used prognostic biomarkers patient subgroup identification\",\"introduce factor analysis model summarizes dependencies observed variable groups instead dependencies individual variables standard factor analysis group may correspond one view set objects one many data sets tied cooccurrence set alternative variables collected statistics tables measure one property interest show assuming groupwise sparse factors active subset sets variation decomposed factors explaining relationships sets factors explaining away setspecific variation formulate assumptions bayesian model provides factors apply model two data analysis tasks neuroimaging chemical systems biology\",\"canonical correlation analysis cca one popular methods frequency recognition steadystate visual evoked potential ssvepbased braincomputer interfaces bcis despite efficiency potential problem using preconstructed sinecosine waves required reference signals cca method often result optimal recognition accuracy due lack features real eeg data address problem study proposes novel method based multiset canonical correlation analysis msetcca optimize reference signals used cca method ssvep frequency recognition msetcca method learns multiple linear transforms implement joint spatial filtering maximize overall correlation among canonical variates hence extracts ssvep common features multiple sets eeg data recorded stimulus frequency optimized reference signals formed combination common features completely based training data experimental study eeg data ten healthy subjects demonstrates msetcca method improves recognition accuracy ssvep frequency comparison cca method two competing methods multiway cca mwaycca phase constrained cca pcca especially small number channels short time window length superiority indicates proposed msetcca method new promising candidate frequency recognition ssvepbased bcis\",\"flow cytometry often used characterize malignant cells leukemia lymphoma patients traced level individual cell typically flow cytometric data analysis performed series dimensional projections onto axes data set years clinicians determined combinations different fluorescent markers generate relatively known expression patterns specific subtypes leukemia lymphoma cancers hematopoietic system viewing series dimensional projections highdimensional nature data rarely exploited paper present means determining lowdimensional projection maintains highdimensional relationships information differing oncological data sets using machine learning techniques allow clinicians visualize data low dimension defined linear combination available markers rather time provides aid diagnosing similar forms cancer well means variable selection exploratory flow cytometric research refer method information preserving component analysis ipca\",\"clinical neuroscientific studies systematic differences two populations brain networks investigated order characterize mental diseases processes networks usually represented graphs built neuroimaging data studied means graph analysis methods typical machine learning approach study brain graphs creates classifier tests ability discriminate two populations contrast approach work propose directly test whether two populations graphs different using kernel twosample test ktst without creating intermediate classifier claim general two approaches provides similar results ktst requires much less computation additionally regime low sample size claim ktst lower frequency type error classification approach besides providing algorithmic considerations support claims show strong evidence experiments one simulation\",\"many unsupervised kernel methods rely estimation kernel covariance operator kernel kernel crosscovariance operator kernel cco kernel kernel cco sensitive contaminated data even bounded positive definite kernels used best knowledge wellfounded robust kernel methods statistical unsupervised learning addition influence function estimator characterize robustness asymptotic properties standard error standard kernel canonical correlation analysis standard kernel cca derived yet fill gap first propose robust kernel covariance operator robust kernel robust kernel crosscovariance operator robust kernel cco based generalized loss function instead quadratic loss function second derive robust kernel cco standard kernel cca using standard kernel cca detect influential observations two sets data finally propose method based robust kernel robust kernel cco called robust kernel cca less sensitive noise standard kernel cca introduced principles also applied many kernel methods involving kernel kernel cco experiments synthesized data imaging genetics analysis demonstrate proposed standard kernel cca identify outliers also seen proposed robust kernel cca method performs better ideal contaminated data standard kernel cca\",\"brain segmentation neonatal mri images challenging task due large changes shape cerebral structures variations signal intensities reflecting gestational process context clear need segmentation techniques robust variations image contrast spatial configuration anatomical structures work evaluate potential synthetic learning contrastindependent model trained using synthetic images generated ground truth labels subjectswe base experiments dataset released developmental human connectome project highquality tweighted images available babies aged weeks postconception first confirm impressive performance standard unet trained tweighted volumes also confirm models learn intensityrelated features specific training domain evaluate synthetic learning approach confirm robustness variations image contrast reporting capacity model segment tweighted images individuals however observe clear influence age baby predictions improve performance model enriching synthetic training set realistic motion artifacts oversegmentation white matter based extensive visual assessment argue better performance model trained real data may due systematic errors ground truth propose original experiment combining two definitions ground truth allowing show learning real data reproduce systematic bias training set synthetic models avoid limitation overall experiments confirm synthetic learning effective solution segmenting neonatal brain mri adapted synthetic learning approach combines key features instrumental large multisite studies clinical applications\",\"principal component analysis pca exploratory tool widely used data analysis uncover dominant patterns variability within population despite ability represent data set lowdimensional space interpretability pca remains limited however neuroimaging essential uncover clinically interpretable phenotypic markers would account main variability brain images population recently alternatives standard pca approach sparse pca proposed aim limit density components nonetheless sparsity alone entirely solve interpretability problem since may yield scattered unstable components hypothesized incorporation prior information regarding structure data may lead improved relevance interpretability brain patterns therefore present simple extension popular pca framework adds structured sparsity penalties loading vectors order identify stable regions brain images accounting variability structured sparsity obtained combining total variation penalties regularization encodes higher order information structure data paper presents structured sparse pca denoted spcatv optimization framework resolution demonstrate efficiency versatility spcatv three different data sets gains spcatv unstructured approaches significantsince spcatv reveals variability within data set form intelligible brain patterns easy interpret stable across different samples\",\"many natural systems neurons firing brain basketball teams traversing court give rise time series data complex nonlinear dynamics gain insight systems decomposing data segments explained simpler dynamic units building switching linear dynamical systems slds present new model class discovers dynamical units also explains switching behavior depends observations continuous latent states recurrent switching linear dynamical systems provide insight discovering conditions unit deployed something traditional slds models fail leverage recent algorithmic advances approximate inference make bayesian inference models easy fast scalable\",\"study tested interaction effect multimodal datasets using novel method called kernel method detecting higher order interactions among biologically relevant mulitview data using semiparametric method reproducing kernel hilbert space rkhs used standard mixedeffects linear model derived scorebased variance component statistic tests higher order interactions multiview data proposed method offers intangible framework identification higher order interaction effects three way interaction genetics brain imaging epigenetic data extensive numerical simulation studies first conducted evaluate performance method finally method evaluated using data mind clinical imaging consortium mcic including single nucleotide polymorphism snp data functional magnetic resonance imaging fmri scans deoxyribonucleic acid dna methylation data respectfully schizophrenia patients healthy controls treated genederived snps region interest roi genederived dna methylation single testing unit combined triplets evaluation addition cardiovascular disease risk factors age gender body mass index assessed covariates hippocampal volume compared triplets method identified triplets pvalues leq included genederived snps rois genederived dna methylations correlated changes hippocampal volume suggesting triplets may important explaining schizophreniarelated neurodegeneration strong evidence pvalues leq triplet magi crblcrusl fbxo potential distinguish schizophrenia patients healthy control variations\",\"central goal neuroscience understand activity nervous system related features external world features nervous system common approach model neural responses weighted combination external features vice versa structure model weights provide insight neural representations often neural inputoutput relationships sparse inputs contributing output part account sparsity structured regularizers incorporated model fitting optimization however imposing priors structured regularizers make difficult interpret learned model parameters investigate simple minimally structured model estimation method accurate unbiased estimation sparse models based bootstrapped adaptive threshold selection followed ordinary leastsquares refitting boats extensive numerical investigations show method often performs favorably compared regularizers particular variety model distributions noise levels boats accurately recovers parameters sparse models leading parsimonious explanations outputs finally apply method task decoding human speech production ecog recordings\",\"analysis nonstationary time series great importance many scientific fields physics neuroscience recent years gaussian process regression attracted substantial attention robust powerful method analyzing time series paper introduce new framework analyzing nonstationary time series using locally stationary gaussian process analysis parameters coupled hidden markov model main advantage framework arbitrary complex nonstationary covariance functions obtained combining simpler stationary building blocks whose hidden parameters estimated closedform demonstrate flexibility method analyzing two examples synthetic nonstationary signals oscillations time varying frequency time series two dynamical states finally report example application real magnetoencephalographic measurements brain activity\",\"best knowledge general wellfounded robust methods statistical unsupervised learning unsupervised methods explicitly implicitly depend kernel covariance operator kernel kernel crosscovariance operator kernel cco sensitive contaminated data even using bounded positive definite kernels first propose robust kernel covariance operator robust kernel robust kernel crosscovariance operator robust kernel cco based generalized loss function instead quadratic loss function second propose influence function classical kernel canonical correlation analysis classical kernel cca third using influence function propose visualization method detect influential observations two sets data finally propose method based robust kernel robust kernel cco called robust kernel cca designed contaminated data less sensitive noise classical kernel cca principles describe also apply many kernel methods must deal issue kernel kernel cco experiments synthesized imaging genetics analysis demonstrate proposed visualization robust kernel cca applied effectively ideal data contaminated data robust methods show superior performance stateoftheart methods\",\"mechanistic models singleneuron dynamics extensively studied computational neuroscience however identifying models quantitatively reproduce empirically measured data challenging propose overcome limitation using likelihoodfree inference approaches also known approximate bayesian computation abc perform full bayesian inference singleneuron models approach builds recent advances abc learning neural network maps features observed data posterior distribution parameters learn bayesian mixturedensity network approximating posterior multiple rounds adaptively chosen simulations furthermore propose efficient approach handling missing features parameter settings simulator fails well strategy automatically learning relevant features using recurrent neural networks synthetic data approach efficiently estimates posterior distributions recovers groundtruth parameters invitro recordings membrane voltages recover multivariate posteriors biophysical parameters yield modelpredicted voltage traces accurately match empirical data approach enable neuroscientists perform bayesian inference complex neuron models without design modelspecific algorithms closing gap mechanistic statistical approaches singleneuron modelling\",\"present new model predictive state recurrent neural networks psrnns filtering prediction dynamical systems psrnns draw insights recurrent neural networks rnns predictive state representations psrs inherit advantages types models like many successful rnn architectures psrnns use potentially deeply composed bilinear transfer functions combine information multiple sources show bilinear functions arise naturally state updates bayes filters like psrs observations viewed gating belief states also show psrnns learned effectively combining backpropogation time bptt initialization derived statistically consistent learning algorithm psrs called twostage regression finally show psrnns factorized using tensor decomposition reducing model size suggesting interesting connections existing multiplicative architectures lstms applied psrnns datasets showed outperform several popular alternative approaches modeling dynamical systems cases\",\"diffusionweighted imaging dwi method currently measure connections different parts human brain vivo elucidate structure connections algorithms tracking bundles axonal fibers subcortical white matter rely local estimates fiber orientation distribution function fodf different parts brain functions describe relative abundance populations axonal fibers crossing location multiple models exist estimating fodfs quality resulting estimates quantified means suitable measure distance space fodfs however multiple distance metrics applied purpose including smoothed distances wasserstein metrics give four reasons use earth movers distance emd equipped arclength distance metric continued\",\"genomewide association study gwas correlates marker variation trait variation sample individuals study subject genotyped multitude snps single nucleotide polymorphisms spanning genome assume subjects unrelated collected random trait values normally distributed transformed normality past decade researchers remarkably successful applying gwas analysis hundreds traits massive amount data produced studies present unique computational challenges penalized regression lasso mcp penalties capable selecting handful associated snps millions potential snps unfortunately model selection corrupted false positives false negatives obscuring genetic underpinning trait paper introduces iterative hard thresholding iht algorithm gwas analysis continuous traits parallel implementation iht accommodates snp genotype compression exploits multiple cpu cores graphics processing units gpus allows statistical geneticists leverage commodity desktop computers gwas analysis avoid supercomputing evaluate iht performance simulated real gwas data conclude reduces false positive false negative rates remaining competitive computational time penalized regression source code freely available httpsgithubcomklkeysihtjl\",\"imaging genetic research essentially focused discovering unique coassociation effects typically ignoring identify outliers atypical objects genetic well nongenetics variables identifying significant outliers essential challenging issue imaging genetics multiple sources data analysis therefore need examine transcription errors identified outliers first address influence function kernel mean element kernel covariance operator kernel crosscovariance operator kernel canonical correlation analysis kernel cca multiple kernel cca second propose multiple kernel cca applied two datasets third propose visualization method detect influential observations multiple sources data based kernel cca multiple kernel cca finally proposed methods capable analyzing outliers subjects usually found biomedical applications number dimension large examine outliers use stemandleaf display experiments synthesized imaging genetics data snp fmri dna methylation demonstrate proposed visualization applied effectively\",\"measuring dependence two random variables important critical many applied areas variable selection brain network analysis however know kind functional relationship two covariates requires dependence measure equitable gives similar scores equally noisy relationship different types fact dependence score continuous random variable taking values thus theoretically impossible give similar scores paper introduce new definition equitability dependence measure powerequitable weakequitable show simulation hhg copula dependence coefficient cdc weakequitable\",\"malaria serious infectious disease responsible half million deaths yearly worldwide major cause mortalities late inaccurate diagnosis manual microscopy currently considered dominant diagnostic method malaria however time consuming prone human errors aim paper automate diagnosis process minimize human intervention developed hardware software costefficient malaria diagnostic system paper describes manufactured hardware also proposes novel software handle parasite detection lifestage identification motorized microscope developed take images giemsastained blood smears patchbased unsupervised statistical clustering algorithm proposed offers novel method classification different regions within blood images proposed method provides better robustness different imaging settings core proposed algorithm model called mixture independent component analysis manifold based optimization method proposed facilitates application model high dimensional data usually acquired medical microscopy method tested blood slides various imaging conditions speed method higher current supervised systems accuracy comparable better\",\"paper propose framework automatic classification patients multimodal genetic brain imaging data optimally combining additive models unadapted penalties classical group lasso penalty lmultiple kernel learning treat modalities manner result undesirable elimination specific modalities contributions unbalanced overcome limitation introduce multilevel model combines imaging genetics considers joint effects two modalities diagnosis prediction furthermore propose framework allowing combine several penalties taking account structure different types data group lasso penalty genetic modality lpenalty imaging modalities finally propose fast optimization algorithm based proximal gradient method model evaluated genetic single nucleotide polymorphismssnp imaging anatomical mri measures data adni database compared additive models exhibits good performances diagnosis time reveals relationships genes brain regions disease status\",\"typical cohorts brain imaging studies large enough systematic testing information contained images build testable working hypotheses investigators thus rely analysis previous work sometimes formalized socalled metaanalysis brain imaging approach underlies specification regions interest rois usually selected basis coordinates previously detected effects paper propose use database images rather coordinates frame problem transfer learning learning discriminant model reference task apply different related new task facilitate statistical analysis small cohorts use sparse discriminant model selects predictive voxels reference task thus provides principled procedure define rois benefits approach twofold first uses reference database prediction provide potential biomarkers clinical setting second increases statistical power new task demonstrate set pairs functional mri experimental conditions approach gives good prediction addition specific transfer situation involving different scanners different locations show voxel selection based transfer learning leads higher detection power small cohorts\",\"paper taskrelated fmri problem treated matrix factorization formulation focused dictionary learning approach new method allows incorporation priori knowledge associated experimental design well available brain atlases moreover proposed method efficiently cope uncertainties related hrf modeling addition proposed method bypasses one major drawbacks associated methods selection sparsityrelated regularization parameters formulation alternative sparsity promoting constraint employed bears direct relation number voxels spatial maps hence related parameters tuned using information available brain atlases proposed method evaluated several popular techniques including glm obtained performance gains reported via novel realistic synthetic fmri dataset well real data related challenging experimental design\",\"functional brain networks well described estimated data gaussian graphical models ggms using sparse inverse covariance estimators comparing functional connectivity subjects two populations calls comparing estimated ggms goal identify differences ggms known similar structure characterize uncertainty differences confidence intervals obtained using parametric distribution parameters sparse estimator sparse penalties enable statistical guarantees interpretable models even highdimensional lowsample settings characterizing distributions sparse models inherently challenging penalties produce biased estimator recent work invokes sparsity assumptions effectively remove bias sparse estimator lasso distributions used give confidence intervals edges ggms extension differences however case comparing ggms estimators make use assumed joint structure among ggms inspired priors brain functional connectivity derive distribution parameter differences joint penalty parameters known sparse difference leads introduce debiased multitask fused lasso whose distribution characterized efficient manner show debiased lasso multitask fused lasso used obtain confidence intervals edge differences ggms validate techniques proposed set synthetic examples well neuroimaging dataset created study autism\",\"neuroimaging data analysis gaussian graphical models often used model statistical dependencies across spatially remote brain regions known functional connectivity typically data collected across cohort subjects scientific objectives consist estimating population subjectspecific graphical models third objective often overlooked involves quantifying intersubject variability thus identifying regions subnetworks demonstrate heterogeneity across subjects information fundamental order thoroughly understand human connectome propose mixed neighborhood selection order simultaneously address three aforementioned objectives recasting covariance selection neighborhood selection problem able efficiently learn topology node introduce additional mixed effect component neighborhood selection order simultaneously estimate graphical model population subjects well individual subject proposed method validated empirically series simulations applied resting state data healthy subjects taken abide consortium\",\"despite fact consider temporal nature data classic dimensionality reduction techniques pca widely applied time series data paper introduce factor decomposition specific time series builds upon bayesian multivariate autoregressive model hence evades assumption data points mutually independent key find lowrank estimation autoregressive matrices probabilistic version factor models induces latent lowdimensional representation original data discuss possible generalisations alternatives relevant technique simultaneous smoothing dimensionality reduction illustrate potential applications apply model synthetic data set different types neuroimaging data eeg ecog\",\"present procedure effective estimation entropy mutual information smallsample data apply problem inferring highdimensional gene association networks specifically develop jamessteintype shrinkage estimator resulting procedure highly efficient statistically well computationally despite simplicity show outperforms eight entropy estimation procedures across diverse range sampling scenarios datagenerating models even cases severe undersampling illustrate approach analyzing coli gene expression data computing entropybased geneassociation network gene expression data computer program available implements proposed shrinkage estimator\",\"workshop explores interface cognitive neuroscience recent advances fields aim reproduce human performance natural language processing computer vision specifically deep learning approaches problems studying cognitive capabilities brain scientists follow system identification approach present different stimuli subjects try model response different brain areas stimulus goal understand brain trying find function expresses activity brain areas terms different properties stimulus experimental stimuli becoming increasingly complex people interested studying real life phenomena perception natural images natural sentences therefore need rich adequate vector representation properties stimulus obtain using advances machine learning parallel new approaches many deep learning inspired certain extent human behavior biological principles neural networks example originally inspired biological neurons recently processes attention used inspired human behavior however large bulk methods independent findings brain function unclear whether beneficial machine learning try emulate brain function order achieve tasks brain achieves\",\"response variables nominal populations crossclassified respect multiple polytomies questions often arise degree association responses explanatory variables populations known introduce nominal association vector matrix evaluate dependence response variable explanatory variable measures provide detailed evaluations nominal associations local global levels also define general class global association measures embraces well known association measure goodmankruskal proposed association matrix also gives rise expected generalized confusion matrix classification hierarchy equivalence relations defined association vector matrix also shown\",\"solve key biomedical problems experimentalists routinely measure millions billions features dimensions per sample hope data science techniques able build accurate datadriven inferences sample sizes typically orders magnitude smaller dimensionality data valid inferences require finding lowdimensional representation preserves discriminating information whether individual suffers particular disease lack interpretable supervised dimensionality reduction methods scale millions dimensions strong statistical theoretical guaranteeswe introduce approach xox extending principal components analysis incorporating classconditional moment estimates lowdimensional projection simplest version linear optimal lowrank projection lol incorporates classconditional means prove substantiate synthetic real data benchmarks lol generalizations xox framework lead improved data representations subsequent classification maintaining computational efficiency scalability using multiple brain imaging datasets consisting million features several genomics datasets features lol outperforms scalable linear dimensionality reduction techniques terms accuracy requiring minutes standard desktop computer\",\"highdimensional data structured noise caused observed unobserved factors affecting multiple target variables simultaneously imposes serious challenge modeling masking often weak signal therefore explaining away structured noise multipleoutput regression paramount importance additionally assumptions correlation structure regression weights needed note formulated natural way latent variable model interesting signal noise mediated latent factors assumption signal model borrows strength noise model encouraging similar effects correlated targets introduce hyperparameter emphlatent signaltonoise ratio turns important modelling weak signals ordered infinitedimensional shrinkage prior resolves rotational unidentifiability reducedrank regression models simulations prediction experiments metabolite gene expression fmri measurement macroeconomic time series data show model equals exceeds stateoftheart performance particular outperforms standard approach assuming independent noise signal models\",\"paper treats problem screening variables high correlations high dimensional data many fewer samples variables focus thresholdbased correlation screening methods three related applications screening variables large correlations within single treatment autocorrelation screening screening variables large crosscorrelations two treatments crosscorrelation screening screening variables persistently large autocorrelations two treatments persistentcorrelation screening novelty correlation screening identifies smaller number variables highly correlated others compared identifying number correlation parameters correlation screening suffers phase transition phenomenon correlation threshold decreases number discoveries increases abruptly obtain asymptotic expressions mean number discoveries phase transition thresholds function number samples number variables joint sample distribution also show weak dependency condition number discoveries dominated poisson random variable giving asymptotic expression false positive rate correlation screening approach bears tremendous dividends terms type strength asymptotic results obtained also overcomes major hurdles faced existing methods literature correlation screening naturally scalable high dimension numerical results strongly validate theory presented paper illustrate application correlation screening methodology large scale geneexpression dataset revealing influential variables exhibit significant amount correlation multiple treatments\",\"powerful approach understanding neural population dynamics extract lowdimensional trajectories population recordings using dimensionality reduction methods current approaches dimensionality reduction neural data limited single population recordings identify dynamics embedded across multiple measurements propose approach extracting lowdimensional dynamics multiple sequential recordings algorithm scales data comprising millions observed dimensions making possible access dynamics distributed across large populations multiple brain areas building subspaceidentification approaches dynamical systems perform parameter estimation minimizing momentmatching objective using scalable stochastic gradient descent algorithm model optimized predict temporal covariations across neurons across time show approach naturally handles missing data multiple partial recordings identify dynamics predict correlations even presence severe subsampling small overlap recordings demonstrate effectiveness approach simulated data wholebrain larval zebrafish imaging dataset\",\"regularized variants principal components analysis especially sparse pca functional pca among useful tools analysis complex highdimensional data many examples massive data sparse functional smooth aspects may benefit regularization scheme capture forms structure example neuroimaging data brains response stimulus may restricted discrete region activation spatial sparsity exhibiting smooth response within region propose unified approach regularized pca induce sparsity smoothness row column principal components framework generalizes much previous literature sparse functional twoway sparse twoway functional pca special cases approach method permits flexible combinations sparsity smoothness lead improvements feature selection signal recovery well interpretable pca factors demonstrate efficacy method simulated data neuroimaging example eeg data\",\"present general framework classifying partially observed dynamical systems based idea learning model space contrast existing approaches using model point estimates represent individual data items employ posterior distributions models thus taking account principled manner uncertainty due generative observational andor dynamic noise observation sampling time processes evaluate framework two testbeds biological pathway model stochastic doublewell system crucially show classifier performance impaired model class used inferring posterior distributions much simple observationgenerating model class provided reduced complexity inferential model class captures essential characteristics needed given classification task\",\"discovering correlation one variable another variable fundamental scientific practical interest existing correlation measures suitable discovering average correlation fail discover hidden potential correlations bridge gap postulate set natural axioms expect measure potential correlation satisfy show rate information bottleneck hypercontractivity coefficient satisfies proposed axioms iii provide novel estimator estimate hypercontractivity coefficient samples provide numerical experiments demonstrating proposed estimator discovers potential correlations among various indicators datasets robust discovering gene interactions gene expression time series data statistically powerful estimators correlation measures binary hypothesis testing canonical examples potential correlations\",\"many problem settings parameter vectors merely sparse dependent way nonzero coefficients tend cluster together refer form dependency region sparsity classical sparse regression methods lasso automatic relevance determination ard model parameters independent priori therefore exploit dependencies introduce hierarchical model smooth regionsparse weight vectors tensors linear regression setting approach represents hierarchical extension relevance determination framework add transformed gaussian process model dependencies prior variances regression weights combine structured model prior variances fourier coefficients eliminates unnecessary high frequencies resulting prior encourages weights regionsparse two different bases simultaneously develop laplace approximation monte carlo markov chain mcmc sampling provide efficient inference posterior furthermore twostage convex relaxation laplace approximation approach also provided relax inevitable nonconvexity optimization finally show substantial improvements comparable methods simulated real datasets brain imaging\",\"high throughput screening compounds chemicals essential part drug discovery involving thousands millions compounds purpose identifying candidate hits statistical tools including industry standard bscore method work individual compound plates exploit crossplate correlation statistical strength among plates present new statistical framework high throughput screening compounds based bayesian nonparametric modeling proposed approach able identify candidate hits multiple plates simultaneously sharing statistical strength among plates providing robust estimates compound activity flexibly accommodate arbitrary distributions compound activities applicable plate geometry algorithm provides principled statistical approach hit identification false discovery rate control experiments demonstrate significant improvements hit identification sensitivity specificity bscore method highly sensitive threshold choice framework implemented efficient extension package bhtspack suitable large scale data sets\",\"extend kernelized matrix factorization fully bayesian treatment ability work multiple side information sources expressed different kernels kernel functions introduced matrix factorization integrate side information rows columns objects users recommender systems necessary making outofmatrix cold start predictions discuss specifically bipartite graph inference output matrix binary extensions general matrices straightforward extend state art two key aspects fully conjugate probabilistic formulation kernelized matrix factorization problem enables efficient variational approximation whereas fully bayesian treatments computationally feasible earlier approaches multiple side information sources included treated different kernels multiple kernel learning additionally reveals side information sources informative method outperforms alternatives predicting drugprotein interactions two data sets show framework also used solving multilabel learning problems considering samples labels two domains matrix factorization operates algorithm obtains lowest hamming loss values multilabel classification data sets compared five stateoftheart multilabel learning algorithms\",\"objective work perform margin assessment human breast tissue optical coherence tomography oct images using deep neural networks dnns work simulates intraoperative setting breast cancer lumpectomy methods train dnns use stateoftheart methods weight decay dropout newly introduced regularization method based function norms commonly used methods fail small database available use function norm introduces direct control complexity function aim diminishing risk overfitting results neither code data previous results publicly available obtained results compared reported results literature conservative comparison moreover method applied locally collected data several data configurations reported results average different trials conclusion experimental results show use dnns yields significantly better results techniques evaluated terms sensitivity specificity score gmean matthews correlation coefficient function norm regularization yielded higher robust results competing methods significance demonstrated system shows high promise partially automated margin assessment human breast tissue equal error rate eer reduced approximately lowest reported literature reduction method computationally feasible intraoperative application less seconds per image\",\"machine learning methods used discover complex nonlinear relationships biological medical data however sophisticated learning models computationally unfeasible data millions features introduce first feature selection method nonlinear learning problems scale large ultrahigh dimensional biological data specifically scale novel hilbertschmidt independence criterion lasso hsic lasso handle millions features tens thousand samples proposed method guaranteed find optimal subset maximally predictive features minimal redundancy yielding higher predictive power improved interpretability effectiveness demonstrated applications classify phenotypes based module expression human prostate cancer patients detect enzymes among protein structures achieve high accuracy one million features dimensionality reduction algorithm implemented commodity cloud computing platforms dramatic reduction features may lead ubiquitous deployment sophisticated prediction models mobile health care applications\",\"adopt data structure form cover trees iteratively apply approximate nearest neighbour ann searches fast compressed sensing reconstruction signals living discrete smooth manifolds levering recent stability results inexact iterative projected gradient ipg algorithm using cover trees ann searches decrease projection cost ipg algorithm logarithmically growing data population low dimensional smooth manifolds apply results quantitative mri compressed sensing particular within magnetic resonance fingerprinting mrf framework similar sometimes better reconstruction accuracy report orders magnitude reduction computations compared standard iterative method uses bruteforce searches\",\"new technologies recording activity large neural populations complex behavior provide exciting opportunities investigating neural computations underlie perception cognition decisionmaking nonlinear state space models provide interpretable signal processing framework combining intuitive dynamical system probabilistic observation model provide insights neural dynamics neural computation development neural prosthetics treatment feedback control brings challenge learning latent neural state underlying dynamical system neither known neural systems priori developed flexible online learning framework latent nonlinear state dynamics filtered latent states using stochastic gradient variational bayes approach method jointly optimizes parameters nonlinear dynamical system observation model blackbox recognition model unlike previous approaches framework incorporate nontrivial distributions observation noise constant time space complexity features make approach amenable realtime applications potential automate analysis experimental design ways testably track modify behavior using stimuli designed influence learning\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"5_brain_data_kernel\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"5_brain_data_kernel\"],\"textfont\":{\"size\":12},\"x\":[9.245231,9.249734,8.91801,9.103835,9.175825,9.383798,9.208198,9.184463,9.819687,9.440369,9.196224,9.38762,9.091834,10.16059,9.348823,9.201957,9.243766,9.210893,9.399607,9.899494,9.439293,9.069303,8.966726,9.15223,8.937155,9.954841,9.785031,9.110443,8.629338,9.048295,9.275426,9.3760195,9.754728,9.3304825,9.047324,8.886251,9.157443,9.118625,9.260996,8.760648,9.348607,9.330792,9.000689,9.177795,8.625022,8.641259,9.749883,9.274697,8.651549,9.260319,9.932641,8.841891,9.190562,9.160255,9.593407,9.142728,9.36033,8.545633,9.131982,8.674207,8.642514,9.46278,8.564949,8.524095,9.375324,9.172872,9.269113,9.707584,9.236395,9.243972,9.0989065,9.243123,9.264522,9.402414,10.13586,9.186302,8.918083,9.96489,8.997012,9.203303,9.841544,8.568192,10.121423,8.518503,9.850012,9.3392935,9.615726,9.732456,9.321765,8.9273405,9.639795,8.531366,9.241155],\"y\":[5.2741594,4.8520026,4.894798,4.5784793,5.5457883,4.247354,4.5689034,4.6043415,4.0275073,4.9610457,5.5643377,4.242974,4.557282,4.8905206,4.30335,4.587822,4.8320785,5.5424185,4.51871,5.406672,4.479196,4.5389085,4.6418047,5.595268,6.1679454,4.9540977,5.457475,5.670158,5.853637,4.4890413,5.48616,4.8040466,5.201171,5.5070977,4.557225,4.67503,5.5781546,5.959202,5.4933,4.847387,5.585101,4.7826166,5.925195,5.5508122,4.9973807,5.034861,5.5530167,5.5898733,5.0438113,5.583883,5.05009,4.911935,5.574046,4.415345,5.7969503,4.74043,4.8073754,4.8051815,4.7559686,4.724079,5.025064,5.742441,4.773297,4.8084946,4.7218018,5.624652,5.356718,4.3293624,5.034743,4.835584,4.7273145,4.6180983,4.398977,4.273189,4.911504,5.439639,4.7004457,5.071,5.6571574,4.589446,5.234044,4.777788,4.889844,4.829347,5.048334,4.7932787,5.6344304,5.650467,5.0205317,6.187739,4.942266,4.81922,5.0396852],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"recently shown many existing quasinewton algorithms formulated learning algorithms capable learning local models cost functions importantly understanding allows safely start assembling probabilistic newtontype algorithms applicable situations access noisy observations cost function derivatives interest lies make contributions use nonparametric probabilistic gaussian process models solving stochastic optimisation problems specifically present new algorithm unites approximations together recent probabilistic line search routines deliver probabilistic quasinewton approach also show probabilistic optimisation algorithms deliver promising results challenging nonlinear system identification problems nature problem access cost function derivative via noisy observations since closedform expressions available\",\"present probabilistic model natural images based gaussian scale mixtures simple multiscale representation contrast dominant approach modeling whole images focusing markov random fields formulate model terms directed graphical model show able generate images interesting higherorder correlations trained natural images samples occlusion based model importantly directed model enables perform principled evaluation easy generate visually appealing images demonstrate model also yields best performance reported date evaluated respect crossentropy rate measure tightly linked average loglikelihood\",\"many statistical learning problems posed minimization sum two convex functions one typically composition nonsmooth linear functions examples include regression structured sparsity assumptions popular algorithms solving problems admm often involve nontrivial optimization subproblems smoothing approximation consider two classes primaldual algorithms incur difficulties unify perspective monotone operator theory unification propose continuum preconditioned forwardbackward operator splitting algorithms amenable parallel distributed computing entire region convergence whole continuum algorithms establish rates convergence known instances continuum analysis closes gap theory exploit unification propose continuum accelerated algorithms show whole continuum attains theoretically optimal rate convergence scalability proposed algorithms well convergence behavior demonstrated million variables distributed implementation\",\"recently number mostly ellnorm regularized least squares type deterministic algorithms proposed address problem emphsparse adaptive signal estimation system identification bayesian perspective task equivalent maximum posteriori probability estimation sparsity promoting heavytailed prior parameters interest following different approach paper develops unifying framework sparse emphvariational bayes algorithms employ heavytailed priors conjugate hierarchical form facilitate posterior inference resulting fully automated variational schemes first presented batch iterative form shown properly exploiting structure batch estimation task new sparse adaptive variational bayes algorithms derived ability impose track sparsity realtime processing timevarying environment important feature proposed algorithms completely eliminate need computationally costly parameter finetuning necessary ingredient sparse adaptive deterministic algorithms extensive simulation results provided demonstrate effectiveness new sparse variational bayes algorithms stateoftheart deterministic techniques adaptive channel estimation results show proposed algorithms numerically robust exhibit general superior estimation performance compared deterministic counterparts\",\"strategy early stopping regularization technique based choosing stopping time iterative algorithm focusing nonparametric regression reproducing kernel hilbert space analyze early stopping strategy form gradientdescent applied leastsquares loss function propose datadependent stopping rule involve holdout crossvalidation data prove upper bounds squared error resulting function estimate measured either lpn norm upper bounds lead minimaxoptimal rates various kernel classes including sobolev smoothness classes forms reproducing kernel hilbert spaces show simulation stopping rule compares favorably two stopping rules one based holdout data based steins unbiased risk estimate also establish tight connection early stopping strategy solution path kernel ridge regression estimator\",\"piecewise linearquadratic plq penalties widely used develop models statistical inference signal processing machine learning common examples plq penalties include least squares huber vapnik norm asymmetric generalizations properties estimators depend choice penalty shape parameters degree asymmetry quantile loss transition point linear quadratic pieces huber function paper develop statistical framework help modeler automatically tune shape parameters shape penalty chosen choice parameter informed basic notion penalty correspond true statistical density normalization constant inherent requirement helps inform optimization shape parameters giving joint optimization problem well primary parameters interest second contribution consider optimization methods joint problems show basic firstorder methods immediately brought bear design specialized extensions interior point methods plq problems quickly efficiently solve joint problem synthetic problems largerscale practical examples illustrate potential approach\",\"article addresses modeling reverberant recording environments context underdetermined convolutive blind source separation model contribution source mixture channels timefrequency domain zeromean gaussian random variable whose covariance encodes spatial characteristics source consider four specific covariance models including fullrank unconstrained model derive family iterative expectationmaximization algorithms estimate parameters model propose suitable procedures initialize parameters align order estimated sources across frequency bins based estimated directions arrival doa experimental results reverberant synthetic mixtures live recordings speech data show effectiveness proposed approach\",\"gradient descent optimization requires choose learning rate deeper deeper models tuning learning rate easily become tedious necessarily lead ideal convergence propose variation gradient descent algorithm learning rate fixed instead learn learning rate either another gradient descent firstorder method newtons method secondorder way gradient descent machine learning algorithm optimized\",\"pseudolikelihood method one popular algorithms learning sparse binary pairwise markov networks paper formulate regularized pseudolikelihood problem sparse multiple logistic regression problem way many insights optimization procedures sparse logistic regression applied learning discrete markov networks specifically use coordinate descent algorithm generalized linear models convex penalties combined strong screening rules solve pseudolikelihood problem regularization therefore substantial speedup without losing accuracy achieved furthermore method stable nodewise logistic regression approach unbalanced highdimensional data penalized small regularization parameters thorough numerical experiments simulated data real world data demonstrate advantages proposed method\",\"constrained adaptive filtering algorithms inculding constrained least mean square clms constrained affine projection cap constrained recursive least squares crls extensively studied many applications existing constrained adaptive filtering algorithms developed mean square error mse criterion ideal optimality criterion gaussian noises assumption however fails model behavior nongaussian noises found practice motivated robustness simplicity maximum correntropy criterion mcc nongaussian impulsive noises paper proposes new adaptive filtering algorithm called constrained maximum correntropy criterion cmcc specifically cmcc incorporates linear constraint mcc filter solve constrained optimization problem explicitly proposed adaptive filtering algorithm easy implement low computational complexity terms convergence accuracy say lower mean square deviation stability significantly outperform mse based constrained adaptive algorithms presence heavytailed impulsive noises additionally mean square convergence behaviors studied energy conservation relation sufficient condition ensure mean square convergence steadystate mean square deviation msd proposed algorithm obtained simulation results confirm theoretical predictions gaussian non gaussian noises demonstrate excellent performance novel algorithm comparing conventional methods\",\"certain situations shall undoubtedly common big data era datasets available massive computing statistics full sample hardly feasible unfeasible natural approach context consists using survey schemes substituting full data statistics counterparts based resulting random samples manageable size main purpose paper investigate impact survey sampling unequal inclusion probabilities stochastic gradient descentbased mestimation methods largescale statistical machinelearning problems precisely prove presence priori information one may significantly increase asymptotic accuracy choosing appropriate first order inclusion probabilities without affecting complexity striking results described limit theorems also illustrated numerical experiments\",\"work compute lower lipschitz bounds ellp pooling operators infty well ellp pooling operators preceded halfrectification layers give sufficient conditions design invertible neural network layers numerical experiments mnist image patches confirm pooling layers inverted phase recovery algorithms moreover regularity inverse pooling controlled lower lipschitz constant empirically verified nearest neighbor regression\",\"consider unknown smooth function rightarrow mathbbr say given noisymod samples fxi etaimod etai denotes noise given samples xiyiin goal recover smooth robust estimates clean samples fxi bmod formulate natural approach solving problem works representations mod values unit circle amounts solving quadratically constrained quadratic program qcqp nonconvex constraints involving points lying unit circle proposed approach based solving relaxation trustregion subproblem hence solvable efficiently demonstrate robustness noise approach via extensive simulations several synthetic examples provide detailed theoretical analysis\",\"consider problem minimizing sum functions convex parameter set mathcalc subset mathbbrp ngg pgg regime algorithms utilize subsampling techniques known effective paper use subsampling techniques together lowrank approximation design new randomized batch algorithm possesses comparable convergence rate newtons method yet much smaller periteration cost proposed algorithm robust terms starting point step size enjoys composite convergence rate namely quadratic convergence start linear convergence iterate close minimizer develop theoretical analysis also allows select nearoptimal algorithm parameters theoretical results used obtain convergence rates previously proposed subsampling based algorithms well demonstrate results apply wellknown machine learning problems lastly evaluate performance algorithm several datasets various scenarios\",\"goal extract meaningful transformations raw images varying thickness lines handwriting lighting portrait propose unsupervised approach learn transformations attempting reconstruct image linear combination transformations nearest neighbors handwritten digits celebrity portraits show even linear transformations method generates visually highquality modified images moreover since method semiparametric model data distribution learned transformations extrapolate training data applied new types images\",\"fundamental task general density estimation keen interest machine learning work attempt systematically characterize methods density estimation broadly speaking existing methods categorized either using textita autoregressive models estimate conditional factors chain rule pxi ldots textitb nonlinear transformations variables simple base distribution based study characteristics categories propose multiple novel methods category example proposed rnn based transformations model nonmarkovian dependencies comprehensive study real world synthetic data show jointly leveraging transformations variables autoregressive conditional models results considerable improvement performance illustrate use models outlier detection image modeling finally introduce novel data driven framework learning family distributions\",\"new shrinkagebased construction developed compressible vector boldsymbolxinmathbbrn cases components naturally associated tree structure important examples corresponds coefficients wavelet blockdct representation data method consider detail numerical results presented based increments gamma process however demonstrate general framework appropriate many types shrinkage priors within levy process family gamma process special case bayesian inference carried approximating posterior samples mcmc algorithm well constructing heuristic variational approximation posterior also consider expectationmaximization map point solution stateoftheart results manifested compressive sensing denoising applications latter spiky nongaussian noise\",\"subsampling methods recently proposed speed least squares estimation large scale settings however algorithms typically robust outliers corruptions observed covariates concept influence developed regression diagnostics used detect corrupted observations shown paper property influence also develop randomized approximation motivates proposed subsampling algorithm large scale corrupted linear regression limits influence data points since highly influential points contribute residual error general model corrupted observations show theoretically empirically variety simulated real datasets algorithm improves current stateoftheart approximation schemes ordinary least squares\",\"study inference learning based sparse coding model spikeandslab prior standard sparse coding model used assumes independent latent sources linearly combine generate data points however instead using standard sparse prior laplace distribution study application flexible spikeandslab distribution models absence presence sources contribution independently strength contributes investigate two approaches optimize parameters spikeandslab sparse coding novel truncated approach comparison approach based standard factored variational distributions truncated approach regarded variational approach truncated posteriors variational distributions applications source separation find approaches improve stateoftheart number standard benchmarks argues use spikeandslab priors corresponding data domains furthermore find truncated approach improves standard factored approach source separation taskswhich hints biases introduced assuming posterior independence factored variational approach likewise standard benchmark image denoising find truncated approach improves factored variational approach performance factored approach saturates increasing numbers hidden dimensions performance truncated approach improves stateoftheart higher noise levels\",\"given iid observations unknown absolute continuous distribution defined domain omega propose nonparametric method learn piecewise constant function approximate underlying probability density function density estimate piecewise constant function defined binary partition omega key ingredient algorithm use discrepancy concept originates quasi monte carlo analysis control partition process resulting algorithm simple efficient provable convergence rate empirically demonstrate efficiency density estimation method present applications wide range tasks including finding good initializations kmeans\",\"mixture models gamma inversegamma distributed mixture components useful medical image tissue segmentation posthoc models regression coefficients obtained linear regression within generalised linear modeling framework glm used case separate stochastic gaussian noise kind positive negative activation modeled gamma inversegamma distributed date common choice context gaussiangamma mixture models learned maximum likelihood approach recently extended algorithm mixture models inversegamma components introduce fully analytical variational bayes learning framework gamma andor inversegamma components use synthetic resting state fmri data compare performance algorithms terms area curve computational cost observed gaussiangamma model expensive specially considering high resolution images furthermore solutions highly variable occasionally overestimate activations severely bayesian gaussgamma general fastest algorithm provides dense solutions maximum likelihood gaussianinversegamma also fast provides general sparse solutions variational gaussianinversegamma mixture model robust cost acceptable even high resolution images presented methodology represents essential building block directly used complex inference tasks specially designed analyse mrifmri data models include example analytical variational mixture models adaptive spatial regularization better source models new spatial blind source separation approaches\",\"present efficient alternating direction method multipliers admm algorithm segmenting multivariate nonstationary time series structural breaks stationary regions draw recent work series assumed follow vector autoregressive model within segments convex estimation procedure may formulated using group fused lasso penalties admm approach first splits convex problem global quadratic program simple group lasso proximal update show global problem may parallelized rows time dependent transition matrices furthermore subproblem may rewritten form identical loglikelihood gaussian state space model consequently develop kalman smoothing algorithm solve global update time linear length series\",\"many problems lowlevel computer vision image processing denoising deconvolution tomographic reconstruction superresolution addressed maximizing posterior distribution sparse linear model slm show higherorder bayesian decisionmaking problems optimizing image acquisition magnetic resonance scanners addressed querying slm posterior covariance unrelated densitys mode propose scalable algorithmic framework slm posteriors full highresolution images approximated first time solving variational optimization problem convex iff posterior mode finding convex methods successfully drive optimization sampling trajectories realworld magnetic resonance imaging bayesian experimental design attempted methodology provides new insight similarities differences sparse reconstruction approximate bayesian inference important implications compressive sensing realworld images\",\"study fundamental tradeoffs computational tractability statistical accuracy general family hypothesis testing problems combinatorial structures based upon oracle model computation captures interactions algorithms data establish general lower bound explicitly connects minimum testing risk computational budget constraints intrinsic probabilistic combinatorial structures statistical problems lower bound mirrors classical statistical lower bound cam allows quantify optimal statistical performance achievable given limited computational budgets systematic fashion unified framework sharply characterize statisticalcomputational phase transition two testing problems namely normal mean detection sparse principal component detection normal mean detection consider two combinatorial structures namely sparse set perfect matching problems identify significant gaps optimal statistical accuracy achievable computational tractability constraints classical statistical lower bounds compared existing works computational lower bounds statistical problems consider general polynomialtime algorithms turing machines rely computational hardness hypotheses problems like planted clique detection focus oracle computational model covers broad range popular algorithms rely unproven hypotheses moreover result provides intuitive concrete interpretation intrinsic computational intractability highdimensional statistical problems one byproduct result lower bound strict generalization matrix permanent problem independent interest\",\"given iid samples unknown continuous density hyperrectangle attempt learn piecewise constant function approximates underlying density nonparametrically density estimate defined binary split built sequentially according discrepancy criteria key ingredient control discrepancy adaptively subrectangle achieve overall bound prove estimate even though simple appears preserves estimation power exploiting structure directly applied important pattern recognition tasks mode seeking density landscape exploration demonstrate applicability simulations examples\",\"applications related airborne radars simulation always played important role mainly two fold reason unavailability desired data difficulty associated collection data controlled environment simple example regarding collection pure multipolar radar data even phenomenal development field radar hardware design signal processing till collection pure multipolar data challenge radar system designers till recently power computer simulation radar signal return available selected heavy cost associated main line electro magnetic simulators radar signal simulation secondly many simulators restricted marketting however fast progress made field simulation many current generic simulators used simulate radar returns realistic targets current article expounds steps towards generating synthetic aperture radar sar image database ground targets using eneric simulator also demonstrates help example images quality sar mage generated using general purpose simulator\",\"saga fast incremental gradient method finite sum problem effectiveness tested vast applications paper analyze saga class nonstrongly convex nonconvex statistical problem lasso group lasso logistic regression ell regularization linear regression scad regularization correct lasso prove saga enjoys linear convergence rate statistical estimation accuracy assumption restricted strong convexity rsc significantly extends applicability saga convex nonconvex optimization\",\"hearing aid algorithms need tuned fitted match impairment specific patient lack fundamental fitting theory strong contributing factor unsatisfying sound experience hearing aid patients paper proposes probabilistic modeling approach design algorithms proposed method relies generative probabilistic model hearing loss problem provides automated inference corresponding signal processing algorithm fitting solution well principled performance evaluation metric three tasks realized message passing algorithms factor graph representation generative model principle allows fast implementation hearing aid mobile device hardware methods theoretically worked simulated custombuilt factor graph toolbox specific hearing loss model\",\"consider problem minimizing convex function closed convex set projected gradient descent pgd propose fully parameterfree version adagrad adaptive distance initialization optimum sum square norm subgradients algorithm able handle projection steps involve restarts reweighing along trajectory additional gradient evaluations compared classical pgd also fulfills optimal rates convergence cumulative regret logarithmic factors provide extension approach stochastic optimization conduct numerical experiments supporting developed theory\",\"present alternating augmented lagrangian method convex optimization problems cost function sum two terms one separable variable blocks second separable difference consecutive variable blocks examples problems include fused lasso estimation total variation denoising multiperiod portfolio optimization transaction costs iteration method first step involves separately optimizing variable block carried parallel second step separable variables carried efficiently apply algorithm segmentation data based changes inmean mean filtering changes variance variance filtering numerical example show implementation around times faster compared generic optimization solver sdpt\",\"problems outliers detection robust regression highdimensional setting fundamental statistics numerous applications following recent set works providing methods simultaneous robust regression outliers detection consider paper model linear regression individual intercepts highdimensional setting introduce new procedure simultaneous estimation linear regression coefficients intercepts using two dedicated sortedell penalizations also called slope develop complete theory problem first provide sharp upper bounds statistical estimation error vector individual intercepts regression coefficients second give asymptotic control false discovery rate fdr statistical power support selection individual intercepts consequence paper first introduce procedure guaranteed fdr statistical power control outliers detection meanshift model numerical illustrations comparison recent alternative approaches provided simulated several realworld datasets experiments conducted using opensource software written python\",\"present exploration rich theoretical connections several classes regularized models network flows recent results submodular function theory work unifies key aspects problems common theory leading novel methods working several important models interest statistics machine learning computer vision part review concepts network flows submodular function optimization theory foundational results examine connections network flows minimumnorm algorithm submodular optimization extending improving several current results leads concise representation structure large class pairwise regularized models important machine learning statistics computer vision part describe full regularization path class penalized regression problems dependent variables includes graphguided lasso total variation constrained models description also motivates practical algorithm allows efficiently find regularization path discretized version penalized models ultimately new algorithms scale highdimensional problems millions variables\",\"extend traditional worstcase minimax analysis stochastic convex optimization introducing localized form minimax complexity individual functions main result gives functionspecific lower upper bounds number stochastic subgradient evaluations needed optimize either function hardest local alternative given numerical precision bounds expressed terms localized computational analogue modulus continuity central statistical minimax analysis show computational modulus continuity explicitly calculated concrete cases relates curvature function optimum also prove superefficiency result demonstrates meaningful benchmark acting computational analogue fisher information statistical estimation nature practical implications results demonstrated simulations\",\"paper introduces general framework iterative optimization algorithms establishes general assumptions convergence asymptotically geometric also prove appropriate assumptions rate convergence lower bounded convergence geometric provide exact asymptotic convergence rate framework allows deal constrained optimization encompasses expectation maximization algorithm mirror descent algorithm well variants alphaexpectation maximization mirror prox algorithmfurthermore establish sufficient conditions convergence mirror prox algorithm method converges systematically unique minimizer convex function convex compact set\",\"consider problem solving largescale quadratically constrained quadratic program problems occur naturally many scientific web applications although efficient methods tackle problem mostly scalable paper develop method transforms quadratic constraint linear form sampling set lowdiscrepancy points transformed problem solved applying stateoftheart largescale quadratic programming solvers show convergence approximate solution true solution well finite sample error bounds experimental results also shown prove scalability well improved quality approximation practice\",\"give convergence guarantees estimating coefficients symmetric mixture two linear regressions expectation maximization particular show empirical iterates converge target parameter vector parametric rate provided algorithm initialized unbounded cone particular initial guess sufficiently large cosine angle target parameter vector samplesplitting version algorithm converges true coefficient vector high probability interestingly analysis borrows tools used problem estimating centers symmetric mixture two gaussians also show population operator mixtures two regressions anticontractive target parameter vector cosine angle input vector target parameter vector small thereby establishing necessity conic condition finally give empirical evidence supporting theoretical observation suggests sample based algorithm performs poorly initial guesses drawn accordingly simulation study also suggests algorithm performs well even model misspecification covariate error distributions violate model assumptions\",\"early stopping well known approach reduce time complexity performing training model selection large scale learning machines hand memoryspace rather time complexity main constraint many applications randomized subsampling techniques proposed tackle issue paper ask whether early stopping subsampling ideas combined fruitful way consider question least squares regression setting propose form randomized iterative regularization based early stopping subsampling context analyze statistical computational properties proposed method theoretical results complemented validated thorough experimental analysis\",\"signal processing problems involve challenging task multidimensional probability density function pdf estimation work propose solution problem using family rotationbased iterative gaussianization rbig transforms general framework consists sequential application univariate marginal gaussianization transform followed orthonormal transform proposed procedure looks differentiable transforms known pdf unknown pdf estimated point original domain particular aim zero mean unit covariance gaussian convenience rbig formally similar classical iterative projection pursuit algorithms however show unlike methods particular class rotations used special qualitative relevance context since looking interestingness critical issue pdf estimation key difference approach focuses univariate part marginal gaussianization problem rather multivariate part rotation difference implies one may select convenient rotation suited practical application differentiability invertibility convergence rbig theoretically experimentally analyzed relation methods radial gaussianization oneclass support vector domain description svdd deep neural networks dnn also pointed practical performance rbig successfully illustrated number multidimensional problems image synthesis classification denoising multiinformation estimation\",\"large sample size brings computation bottleneck modern data analysis subsampling one efficient strategies handle problem previous studies researchers make cus subsampling replacement ssr subsampling without replacement sswr paper investigate kind sswr poisson subsampling pss fast algorithm ordinary leastsquare problem establish nonasymptotic property error bound correspond ing subsample estimator provide tradeoff computation cost approximation efficiency besides nonasymptotic result provide asymptotic consistency normality subsample estimator methodologically propose twostep subsampling algorithm efficient respect statistical objective independent linear model assumption synthetic real data used empirically study proposed subsampling strategies argue empirical studies proposed twostep algorithm obvious advantage assumed linear model accurate pss strategy performs obviously better ssr subsampling ratio increases\",\"interpreting gradient methods fixedpoint iterations provide detailed analysis methods minimizing convex objective functions due conceptual algorithmic simplicity gradient methods widely used machine learning massive data sets big data particular stochastic gradient methods considered facto standard training deep neural networks studying gradient methods within realm fixedpoint theory provides powerful tools analyze convergence properties particular gradient methods using inexact noisy gradients stochastic gradient descent studied conveniently using wellknown results inexact fixedpoint iterations moreover demonstrate paper fixedpoint approach allows elegant derivation accelerations basic gradient methods particular show gradient descent accelerated fixedpoint preserving transformation operator associated objective function\",\"present distributionally robust optimization dro approach estimate robustified regression plane linear regression setting observed samples potentially contaminated adversarially corrupted outliers approach mitigates impact outliers hedging family distributions observed data assign low probabilities outliers set distributions consideration close empirical distribution sense wasserstein metric show dro formulation relaxed convex optimization problem encompasses class models selecting proper norm spaces wasserstein metric able recover several commonly used regularized regression models provide new insights regularization term give guidance selection regularization coefficient standpoint confidence region establish two types performance guarantees solution formulation mild conditions one related outofsample behavior prediction bias concerns discrepancy estimated true regression planes estimation bias extensive numerical results demonstrate superiority approach host regression models terms prediction estimation accuracies also consider application robust learning procedure outlier detection show approach achieves much higher auc area roc curve mestimation\",\"robust parameter estimation well studied parametric density estimation little investigation robust density estimation nonparametric setting present robust version popular kernel density estimator kde estimators robust version kde useful since sample contamination common issue datasets robustness means nonparametric density estimate straightforward topic explore paper construct robust kde scale traditional kde project nearest weighted kde norm yields scaled projected kde spkde squared norm penalizes pointwise errors superlinearly causes weighted kde allocate weight high density regions demonstrate robustness spkde numerical experiments consistency result shows asymptotically spkde recovers uncontaminated density sufficient conditions contamination\",\"maximum correntropy criterion mcc recently successfully applied robust regression classification adaptive filtering correntropy maximized instead minimizing wellknown mean square error mse improve robustness respect outliers impulsive noises considerable efforts devoted develop various robust adaptive algorithms mcc far little insight gained optimal solution affected outliers work study problem context parameter estimation simple linear errorsinvariables eiv model variables scalar certain conditions derive upper bound absolute value estimation error show optimal solution mcc close true value unknown parameter even outliers whose values arbitrarily large input output variables illustrative examples presented verify clarify theory\",\"recently blanchet kang murhy blanchet kang showed several machine learning algorithms squareroot lasso support vector machines regularized logistic regression among many others represented exactly distributionally robust optimization dro problems distributional uncertainty defined neighborhood centered empirical distribution propose methodology learns neighborhood natural datadriven way show rigorously framework encompasses adaptive regularization particular case moreover demonstrate empirically proposed methodology able improve upon wide range popular machine learning estimators\",\"practical model building processes often timeconsuming many different models must trained validated paper introduce novel algorithm used computing lower upper bounds model validation errors without actually training model key idea behind algorithm using side information available suboptimal model reasonably good suboptimal model available algorithm compute lower upper bounds many useful quantities making inferences unknown target model demonstrate advantage algorithm context model selection regularized learning problems\",\"classical stochastic gradient methods well suited minimizing expectedvalue objective functions however apply minimization nonlinear function involving expected values composition two expectedvalue functions problems form minx mathbfev fvbigmathbfew gwxbig order solve stochastic composition problem propose class stochastic compositional gradient descent scgd algorithms viewed stochastic versions quasigradient method scgd update solutions based noisy sample gradients fvgw use auxiliary variable track unknown quantity mathbfewgwx prove scgd converge almost surely optimal solution convex optimization problems long solution exists convergence involves interplay two iterations different time scales nonsmooth convex problems scgd achieve convergence rate general case strongly convex case taking samples smooth convex problems scgd accelerated converge rate general case strongly convex case nonconvex problems prove limit point generated scgd stationary point also provide convergence rate analysis indeed stochastic setting one wants optimize compositions expectedvalue functions common practice proposed scgd methods find wide applications learning estimation dynamic programming etc\",\"context large samples small number individuals might spoil basic statistical indicators like mean difficult detect automatically atypical individuals alternative strategy using robust approaches paper focuses estimating geometric median random variable robust indicator central tendency order deal large samples data arriving sequentially online stochastic newton algorithms estimating geometric median introduced give rates convergence since estimates median hessian matrix recursively updated also determine confidences intervals median designated direction perform online statistical tests\",\"paper addresses problem segmenting timeseries respect changes mean value variance first case time data modeled sequence independent normal distributed random variables unknown possibly changing mean value fixed variance main assumption mean value piecewise constant time task estimate change times mean values within segments second case mean value constant variance change assumption variance piecewise constant time want estimate change times variance values within segments find solutions problems study regularized maximum likelihood method related fused lasso method trend filtering parameters estimated free vary sample penalize variations estimated parameters lnorm time difference parameters used regularization term idea closely related total variation denoising main contribution convex formulation variance estimation problem parametrization based inverse variance formulated certain mean estimation problem implies results methods mean estimation applied challenging problem variance segmentationestimation\",\"obtain index complexity random sequence allowing role measure classical probability theory played function call generating mechanism typically generating mechanism finite automata generate set biased sequences applying finite state automata specified number states set binary sequences thus index complexity random sequence number states automata detail optimal algorithms predict sequences generated way\",\"consider problem maximizing unknown function compact convex set using observations possible observe optimization function essentially relies learning induced bipartite ranking rule based idea relate global optimization bipartite ranking allows address problems high dimensional input space well cases functions weak regularity properties paper introduces novel metaalgorithms global optimization rely choice bipartite ranking method theoretical properties provided well convergence guarantees equivalences various optimization methods obtained byproduct eventually numerical evidence given show main algorithm paper adapts empirically underlying ranking structure essentially outperforms existing stateoftheart global optimization algorithms typical benchmarks\",\"recently general method analyzing statistical accuracy algorithm developed applied simple latent variable models balakrishnan method basin attraction valid initialization required ball around truth using steins lemma extend results case estimating centers twocomponent gaussian mixture dimensions particular significantly expand basin attraction intersection half space ball around origin signaltonoise ratio least constant multiple sqrtdlog show random initialization strategy feasible\",\"focusing bound constrained global optimization problems whose objective functions computationally expensive blackbox functions multiple local minima recently popular metric stochastic response surface msrs algorithm proposed citeregissrbf based adaptive sequential learning based response surfaces revisited extended better performance case higher dimensional problems specifically propose new way generate candidate points next function evaluation point picked according metric criteria based new definition distance prove global convergence corresponding correspondingly adaptive implementation msrs named sosa presented sosa likely perturb sensitive coordinates generating candidate points instead perturbing coordinates simultaneously numerical experiments synthetic problems real problems demonstrate advantages new algorithm compared many state art alternatives\",\"paper prove probabilistic continuous complexity conjecture continuous complexity theory states complexity solving continuous problem probability approaching converges limit complexity solving problem worst case prove conjecture holds space problem elements uniformly convex nonuniformly convex case striking counterexample problem identifying brownian path wiener space shown probabilistic complexity converges half worst case complexity limit\",\"measurements made satellite remote sensing moderate resolution imaging spectroradiometer modis globally distributed aerosol robotic network aeronet compared comparison two datasets measurements aerosol optical depth values show biases two data products paper present general framework towards identifying relevant set variables responsible observed bias present general framework identify possible factors influencing bias might associated measurement conditions solar sensor zenith angles solar sensor azimuth scattering angles surface reflectivity various measured wavelengths etc specifically performed analysis remote sensing aqualand data set used machine learning technique neural network case perform multivariate regression groundtruth training data sets finally used mutual information observed predicted values measure similarity identify relevant set variables search brute force method consider possible combinations computations involves huge number crunching exercise implemented writing jobparallel program\",\"consider unknown smooth function rightarrow mathbbr say given noisy mod samples fxi etaimod etai denotes noise given samples xiyiin goal recover smooth robust estimates clean samples fxi bmod formulate natural approach solving problem works angular embeddings noisy mod samples unit circle inspired angular synchronization framework amounts solving smoothness regularized leastsquares problem quadratically constrained quadratic program qcqp variables constrained lie unit circle approach based solving relaxation trustregion subproblem hence solvable efficiently provide theoretical guarantees demonstrating robustness noise adversarial random gaussian bernoulli noise models best knowledge first theoretical results problem demonstrate robustness efficiency approach via extensive numerical simulations synthetic data along simple leastsquares solution unwrapping stage recovers original samples global shift shown perform well high levels noise taking input denoised modulo samples finally also consider two approaches denoising modulo samples leverage tools riemannian optimization manifolds including burermonteiro approach semidefinite programming relaxation formulation twodimensional version problem applications radar interferometry able solve instances realworld data million sample points seconds personal laptop\",\"paper first work propose network predict structured uncertainty distribution synthesized image previous approaches mostly limited predicting diagonal covariance matrices novel model learns predict full gaussian covariance matrix reconstruction permits efficient sampling likelihood evaluation demonstrate model accurately reconstruct ground truth correlated residual distributions synthetic datasets generate plausible high frequency samples real face images also illustrate use predicted covariances structure preserving image denoising\",\"effective accurate model selection important problem modern data analysis one major challenges computational burden required handle large data sets cannot stored processed one machine another challenge one may encounter presence outliers contaminations damage inference quality parallel divide conquer model selection strategy divides observations full data set roughly equal subsets perform inference model selection independently subset local subset inference method aggregates posterior model probabilities modelvariable selection criteria obtain final model using notion geometric median approach leads improved concentration finding correct model model parameters also provably robust outliers data contamination\",\"paper propose new volumepreserving flow show performs similarly linear general normalizing flow idea enrich linear inverse autoregressive flow introducing multiple lowertriangular matrices ones diagonal combining using convex combination experimental studies mnist histopathology data show proposed approach outperforms volumepreserving flows competitive current stateoftheart linear normalizing flow\",\"study two procedures reversemode forwardmode computing gradient validation error respect hyperparameters iterative learning algorithm stochastic gradient descent procedures mirror two methods computing gradients recurrent neural networks different tradeoffs terms running time space requirements formulation reversemode procedure linked previous work maclaurin require reversible dynamics forwardmode procedure suitable realtime hyperparameter updates may significantly speed hyperparameter optimization large datasets present experiments data cleaning learning task interactions also present one largescale experiment use previous gradientbased methods would prohibitive\",\"consider convexconcave saddle point problems separable structure nonstrongly convex functions propose efficient stochastic block coordinate descent method using adaptive primaldual updates enables flexible parallel optimization largescale problems method shares efficiency flexibility block coordinate descent methods simplicity primaldual methods utilizing structure separable convexconcave saddle point problem capable solving wide range machine learning applications including robust principal component analysis lasso feature selection group lasso etc theoretically empirically demonstrate significantly better performance stateoftheart methods applications\",\"survey present compare different approaches estimate mutual information data analyse general dependencies variables interest system demonstrate performance difference versus correlation analysis optimal case linear dependencies first use piecewise constant bayesian methodology using general dirichlet prior estimation method use twostage approach approximate probability distribution first calculate marginal joint entropies demonstrate performance bayesian approach versus others computing dependency different variables also compare linear correlation analysis finally apply correlation analysis identification bias determination aerosol optical depth aod satellite based moderate resolution imaging spectroradiometer modis ground based aerosol robotic network aeronet observe aod measurements two instruments might different location reason bias explored quantifying dependencies bias variables including cloud cover surface reflectivity others\",\"paper studies new bayesian algorithm joint reconstruction classification reflectance confocal microscopy rcm images application identification human skin lentigo proposed bayesian approach takes advantage distribution multiplicative speckle noise affecting true reflectivity images appropriate priors unknown model parameters markov chain monte carlo mcmc algorithm proposed jointly estimate model parameters image true reflectivity classifying images according distribution reflectivity precisely metropoliswhitingibbs sampler investigated sample posterior distribution bayesian model associated rcm images build estimators parameters including labels indicating class rcm image resulting algorithm applied synthetic data real images clinical study containing healthy lentigo patients\",\"comment fact gradient ascent logistic regression connection perceptron learning algorithm logistic learning soft variant perceptron learning\",\"consider statistical well algorithmic aspects solving largescale leastsquares problems using randomized sketching algorithms problem input data mathbbrn times times mathbbrn sketching algorithms use sketching matrix sinmathbbrr times rather solving problem using full data sketching algorithms solve problem using sketched data prior work typically adopted algorithmic perspective made statistical assumptions input instead assumed data fixed worstcase prior results show using sketching matrices random projections leveragescore sampling algorithms error solving original problem small constant statistical perspective typically consider meansquared error performance randomized sketching algorithms data generated according statistical model beta epsilon epsilon noise process provide rigorous comparison perspectives leading insights differ first develop framework assessing algorithmic statistical aspects randomized sketching methods consider statistical prediction efficiency statistical residual efficiency sketched estimator use framework provide upper bounds several types random projection random sampling sketching algorithms among results show upper bounded typically requires sample size substantially larger lower bounds developed subsequent results show upper bounds improved\",\"consider statistical algorithmic aspects solving largescale leastsquares problems using randomized sketching algorithms prior results show emphalgorithmic perspective using sketching matrices constructed random projections leveragescore sampling number samples much smaller original sample size worstcase error solving original problem small relative error emphstatistical perspective one typically considers meansquared error performance randomized sketching algorithms data generated according statistical linear model paper provide rigorous comparison perspectives leading insights differ first develop framework assessing unified manner algorithmic statistical aspects randomized sketching methods consider statistical prediction efficiency statistical residual efficiency sketched estimator use framework provide upper bounds several types random projection random sampling algorithms among results show upper bounded much smaller typically requires number samples substantially larger lower bounds developed subsequent work show upper bounds improved\",\"typical goal supervised dimension reduction find lowdimensional subspace input space projected input variables preserve maximal information output variables dependence maximization approach solves supervised dimension reduction problem maximizing statistical dependence projected input variables output variables wellknown statistical dependence measure mutual information based kullbackleibler divergence however known divergence sensitive outliers hand quadratic qmi variant based distance robust outliers divergence computationally efficient method estimate qmi data called leastsquares qmi lsqmi proposed recently reasons developing supervised dimension reduction method based lsqmi seems promising however qmi derivative qmi needed subspace search supervised dimension reduction derivative accurate qmi estimator necessarily good estimator derivative qmi paper propose directly estimate derivative qmi without estimating qmi show direct estimation derivative qmi accurate derivative estimated qmi finally develop supervised dimension reduction algorithm efficiently uses proposed derivative estimator demonstrate experiments proposed method robust outliers existing methods\",\"consider class optimization problems arising computationally intensive lregularized mestimators function gradient values expensive compute particular instance interest lregularized mle learning conditional random fields crfs popular class statistical models varied structured prediction problems sequence labeling alignment classification label taxonomy lregularized mles crfs particularly expensive optimize since computing gradient values requires expensive inference step work propose use carefully constructed proximal quasinewton algorithm computationally intensive mestimation problems employ aggressive active set selection technique key contribution paper show proximal quasinewton method provably superlinearly convergent even absence strong convexity leveraging restricted variant strong convexity experiments proposed algorithm converges considerably faster current stateoftheart problems sequence labeling hierarchical classification\",\"propose new stochastic dual coordinate ascent technique applied wide range regularized learning problems method based alternating direction multiplier method admm deal complex regularization functions structured regularizations although original admm batch method proposed method offers stochastic update rule iteration requires one sample observations moreover method naturally afford minibatch update gives speed convergence show mild assumptions method converges exponentially numerical experiments show method actually performs efficiently\",\"article derive new stepsize adaptation normalized least mean square algorithm nlms describing task linear acoustic echo cancellation bayesian network perspective similar wellknown kalman filter equations model acoustic wave propagation loudspeaker microphone latent state vector define linear observation equation model relation state vector observation well linear process equation model temporal progress state vector based additional assumptions statistics random variables observation process equation apply expectationmaximization algorithm derive nlmslike filter adaptation exploiting conditional independence rules bayesian networks reveal resulting emnlms algorithm stepsize update equivalent optimalstepsize calculation proposed yamamoto kitayama adopted many textbooks main difference instantaneous stepsize value estimated step algorithm instead approximated artificially extending acoustic echo path emnlms algorithm experimentally verified synthesized scenarios white noise male speech input signal\",\"provide theoretical analysis statistical computational properties penalized mestimators formulated solution possibly nonconvex optimization problem many important estimators fall category including least squares regression nonconvex regularization generalized linear models nonconvex regularization sparse elliptical random design regression problems intractable calculate global solution due nonconvex formulation paper propose approximate regularization pathfollowing method solving variety learning problems nonconvex objective functions unified analytic framework simultaneously provide explicit statistical computational rates convergence local solution attained algorithm computationally algorithm attains global geometric rate convergence calculating full regularization path optimal among firstorder algorithms unlike existing methods attain geometric rates convergence one single regularization parameter algorithm calculates full regularization path iteration complexity particular provide refined iteration complexity bound sharply characterize performance stage along regularization path statistically provide sharp sample complexity analysis approximate local solutions along regularization path particular analysis improves upon existing results providing refined sample complexity bound well exact support recovery result final estimator results show final estimator attains oracle statistical property due usage nonconvex penalty\",\"careful tuning regularization parameter indispensable many machine learning tasks significant impact generalization performances nevertheless current practice regularization parameter tuning art science hard tell many gridpoints would needed crossvalidation obtaining solution sufficiently small error paper propose novel framework computing lower bound errors function regularization parameter call regularization path error lower bounds proposed framework used providing theoretical approximation guarantee set solutions sense far error current best solution could away best possible error entire range regularization parameters demonstrate numerical experiments theoretically guaranteed choice regularization parameter sense possible reasonable computational costs\",\"paper introduces method efficiently inferring highdimensional distributed quantity observations quantity interest qoi approximated basis dictionary learned training set coefficients associated approximation qoi basis determined minimizing misfit observations obtain probabilistic estimate quantity interest bayesian approach employed qoi treated random field endowed hierarchical prior distribution closedform expressions obtained posterior distribution main contribution present work lies derivation empha representation basis consistent observation chain used infer associated coefficients resulting dictionary tailored observable sensors accurate approximating posterior mean algorithm deriving observable dictionary presented method illustrated estimation velocity field open cavity flow handful wallmounted point sensors comparison standard estimation approaches relying principal component analysis ksvd dictionaries provided illustrates superior performance present approach\",\"mixture experts moe model popular neural network architecture nonlinear regression classification class moe mean functions known uniformly convergent unknown target function assuming target function sobolev space sufficiently differentiable domain estimation compact unit hypercube provide alternative result shows class moe mean functions dense class continuous functions arbitrary compact domains estimation result viewed universal approximation theorem moe models\",\"goal paper design sequential strategies lead efficient optimization unknown function assumption finite lipschitz constant first identify sufficient conditions consistency generic sequential algorithms formulate expected minimax rate performance introduce analyze first algorithm called lipo assumes lipschitz constant known consistency minimax rates lipo proved well fast rates additional holder like condition adaptive version lipo also introduced realistic setup lipschitz constant unknown estimated along optimization similar theoretical guarantees shown hold adaptive lipo algorithm numerical assessment provided end paper illustrate potential strategy respect stateoftheart methods typical benchmark problems global optimization\",\"density ratio estimation vital tool machine learning statistical community however due unbounded nature density ratio estimation procedure vulnerable corrupted data points often pushes estimated ratio toward infinity paper present robust estimator automatically identifies trims outliers proposed estimator convex formulation global optimum obtained via subgradient descent analyze parameter estimation error estimator highdimensional settings experiments conducted verify effectiveness estimator\",\"briefly review recent progress techniques modeling analyzing hyperspectral images movies particular detecting plumes known unknown chemicals detecting chemicals known spectrum extend technique using single subspace modeling background mixture subspaces model tackle complicated background furthermore use partial least squares regression resampled training set boost performance detection unknown chemicals view problem anomaly detection problem use novel estimators lowsampled complexity intrinsically lowdimensional data highdimensions enable model normal spectra detect anomalies apply algorithms benchmark data sets made available automated target detection program cofunded nsf dtra nga compare applicable current stateoftheart algorithms favorable results\",\"consider forwardbackward greedy algorithms solving sparse feature selection problems general convex smooth functions stateoftheart greedy method forwardbackward greedy algorithm fobaobj requires solve large number optimization problems thus scalable largesize problems fobagdt algorithm uses gradient information feature selection forward iteration significantly improves efficiency fobaobj paper systematically analyze theoretical properties forwardbackward greedy algorithms main contributions derive better theoretical bounds existing analyses regarding fobaobj general smooth convex functions show fobagdt achieves theoretical performance fobaobj condition restricted strong convexity condition new bounds consistent bounds special case least squares fills previously existing theoretical gap general convex smooth functions show restricted strong convexity condition satisfied number independent samples barklog bark sparsity number dimension variable apply fobagdt conditional random field objective sensor selection problem human indoor activity recognition results show fobagdt outperforms methods including ones based forward greedy selection lregularization\",\"archetypal analysis represents set observations convex combinations pure patterns archetypes original geometric formulation finding archetypes approximating convex hull observations assumes real valued unfortunately compatible many practical situations paper revisit archetypal analysis basic principles propose probabilistic framework accommodates observation types integers binary probability vectors corroborate proposed methodology convincing realworld applications finding archetypal winter tourists based binary survey data archetypal disasteraffected countries based disaster count data document archetypes based termfrequency data also present appropriate visualization tool summarize archetypal analysis solution better\",\"speaker recognition scenarios find conversations recorded simultaneously multiple channels case interviews nist sre dataset take advantage propose modification plda model considers two different intersession variability terms first term tied recordings belonging conversation whereas second thus former mainly intends capture variability due phonetic content conversation latter tries capture channel variability document derive equations model model applied paper handling recordings acquired simultaneously multiple channels plda published interspeech\",\"estimation response functions important task dynamic medical imaging task arises example dynamic renal scintigraphy impulse response retention functions estimated functional magnetic resonance imaging hemodynamic response functions required functions observed directly estimation complicated recorded images subject superposition underlying signals therefore response functions estimated via blind source separation deconvolution performance algorithm heavily depends used models response functions response functions real image sequences rather complicated finding suitable parametric form problematic paper study estimation response functions using nonparametric bayesian priors priors designed favor desirable properties functions sparsity smoothness assumptions used within hierarchical priors blind source separation deconvolution algorithm comparison resulting algorithms priors performed synthetic dataset well real datasets dynamic renal scintigraphy shown flexible nonparametric priors improve estimation response functions cases matlab implementation resulting algorithms freely available download\",\"present revival interest bistatic radar systems research area gained momentum given strategic advantages bistatic configuration tech nological advances past years largescale implementation bistatic systems scope near future bistatic systems replace monostatic systems least par tially existing usages monostatic system manageable bistatic system detailed investigation possibilities automatic target recognition atr facil ity bistatic radar system presented lack data experiments carried simulated data still results positive make positive case introduction bistatic configuration first found contrary popular expectation bistatic atr performance might substantially worse monostatic atr performance bistatic atr performed fairly well though better monostatic atr second atr per formance deteriorate substantially increasing bistatic angle last polarimetric data bistatic scattering found distinct information contrary expert opinions along results suggestions also made stabilise bistaticatr per formance changing bistatic angle finally new fast robust atr algorithm developed present work presented\",\"prototypal analysis introduced overcome two shortcomings archetypal analysis sensitivity outliers nonlocality reduces applicability learning tool archetypal analysis prototypal analysis finds prototypes convex combination data points approximates data convex combination archetypes adds penalty using prototypes distant data points reconstruction prototypal analysis extendedvia kernel embeddingto probability distributions since convexity prototypes makes interpretable mixtures finally prototypal regression developed robust supervised procedure allows use distributions either features labels\",\"paper propose novel framework construction sparsityinducing priors particular define priors mixture exponential power distributions generalized inverse gaussian density epgig epgig variant generalized hyperbolic distributions special cases include gaussian scale mixtures laplace scale mixtures furthermore laplace scale mixtures subserve bayesian framework sparse learning nonconvex penalization densities epgig explicitly expressed moreover corresponding posterior distribution also follows generalized inverse gaussian distribution properties lead algorithms bayesian sparse learning show algorithms bear interesting resemblance iteratively reweighted ell ell methods addition present two extensions grouped variable selection logistic regression\",\"datadriven distributionally robust optimization dddro via optimal transport shown encompass wide range popular machine learning algorithms distributional uncertainty size often shown correspond regularization parameter type regularization norm used regularize corresponds shape distributional uncertainty propose datadriven robust optimization methodology inform transportation cost underlying definition distributional uncertainty show empirically additional layer robustification produces method called doubly robust datadriven distributionally robust optimization ddrdro allows enhance generalization properties regularized estimators reducing testing error relative stateoftheart classifiers wide range data sets\",\"stateoftheart speaker recognition relays models need large amount training data models successful tasks like nist sre sufficient data available however real applications usually much data many cases speaker labels unknown present method adapt plda model domain large amount labeled data another unlabeled data describe generative model produces sets data unknown labels modeled like latent variables used variational bayes estimate hidden variables derive equations model model used papers unsupervised adaptation plda using variational bayes methods publised icassp unsupervised training plda variational bayes published iberspeech variational bayesian plda speaker diarization mgb challenge published asru\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"6_algorithms_problems_algorithm\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"6_algorithms_problems_algorithm\"],\"textfont\":{\"size\":12},\"x\":[10.972891,9.74967,11.86452,11.9982605,10.937457,11.524024,12.295151,12.102189,11.721721,11.303497,11.158476,12.441422,11.6198845,11.79241,9.698334,10.847795,12.550659,11.413616,10.902356,10.879174,9.570247,11.922826,9.636233,12.24602,10.846198,9.996536,11.884805,12.273965,12.197937,11.923188,11.195718,11.870228,12.225674,12.049167,12.352469,11.571639,11.374768,10.149266,11.6725445,11.954037,11.152813,10.864484,11.194448,11.155923,11.057763,12.293324,11.184226,11.962168,12.200549,12.06932,11.579722,12.152645,12.138855,10.388678,11.368589,9.800122,11.054118,11.940256,12.004343,11.914786,10.416041,9.575045,11.780088,12.34283,12.398807,11.214258,11.688372,11.802198,12.277476,11.636575,11.581317,10.96943,11.490207,12.160411,10.887624,10.445985,11.840326,11.194542,12.323108,9.537399,10.013966,11.198968,11.9288025,11.624801,12.350104,11.421679],\"y\":[10.59141,5.9702773,7.10072,6.8920546,7.6976824,7.315735,6.9877906,7.8310723,7.2297635,8.060511,7.8624864,10.368082,7.481502,7.323219,6.353737,8.436118,9.173417,7.580697,5.5908318,8.313588,4.9989643,7.1351995,5.009006,8.185423,8.337523,6.3525224,7.0512123,7.005455,7.6424713,7.1006384,7.5100584,7.1784067,7.716796,7.430238,8.219636,7.8355694,7.564397,6.158391,7.7615685,7.3392525,7.5918074,8.226489,7.907209,7.6467853,7.579528,7.722661,7.5382786,7.149901,8.1876955,7.50955,7.8269005,7.649334,8.086653,8.96684,7.4191794,6.0021334,7.5680666,7.106779,7.930949,7.108548,8.943408,5.01306,7.288243,8.245908,8.285354,7.518328,7.572545,7.246488,7.042317,7.415447,7.3341126,8.322061,7.9269104,7.6218104,8.191065,9.021498,7.2787523,7.5210896,7.0033827,4.9427986,6.348175,7.4937434,6.5832124,7.967051,7.0526905,7.4776244],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"propose segmented ihmm sihmm hierarchical infinite hidden markov model ihmm supports simple efficient inference scheme sihmm well suited segmentation problems goal identify points time series transitions one relatively stable regime new regime conventional ihmms often struggle problems since mechanism distinguishing high lowlevel dynamics hierarchical hmms hhmms better require much complex expensive inference algorithms sihmm retains simplicity efficiency ihmm outperforms variety segmentation problems achieving performance matches exceeds complicated hhmm\",\"hamiltonian monte carlo hmc exploits hamiltonian dynamics construct efficient proposals markov chain monte carlo mcmc paper present generalization hmc exploits textitnoncanonical hamiltonian dynamics refer algorithm magnetic hmc since dimensions subset dynamics map onto mechanics charged particle coupled magnetic field establish theoretical basis use noncanonical hamiltonian dynamics mcmc construct symplectic leapfroglike integrator allowing implementation magnetic hmc finally exhibit several examples noncanonical dynamics lead improved mixing magnetic hmc relative ordinary hmc\",\"dasgupta shulman showed tworound variant algorithm learn mixture gaussian distributions near optimal precision high probability gaussian distributions well separated dimension sufficiently high paper generalize theory learning mixture highdimensional bernoulli templates template binary vector template generates examples randomly switching binary components independently certain probability computer vision applications binary vector feature map image binary component indicates whether local feature structure present absent within certain cell image domain bernoulli template considered statistical model images objects parts objects category show tworound algorithm learn mixture bernoulli templates near optimal precision high probability bernoulli templates sufficiently different number features sufficiently high illustrate theoretical results synthetic real examples\",\"range fields including geosciences molecular biology robotics computer vision one encounters problems involve random variables manifolds currently lack flexible probabilistic models manifolds fast easy train define extremely flexible class exponential family distributions manifolds torus sphere rotation groups show distributions gradient loglikelihood computed efficiently using noncommutative generalization fast fourier transform fft discuss applications bayesian camera motion estimation harmonic exponential families serve conjugate priors modelling spatial distribution earthquakes surface earth experimental results show harmonic densities yield significantly higher likelihood best competing method orders magnitude faster train\",\"edge partition model epm fundamental bayesian nonparametric model extracting overlapping structure binary matrix epm adopts gamma process gammap prior automatically shrink number active atoms however empirically found model shrinkage epm typically work appropriately leads overfitted solution analysis expectation epms intensity function suggested gamma priors epm hyperparameters disturb model shrinkage effect internal gammap order ensure model shrinkage effect epm works appropriate manner proposed two novel generative constructions epm cepm incorporating constrained gamma priors depm incorporating dirichlet priors instead gamma priors furthermore depms model parameters including infinite atoms gammap prior could marginalized thus possible derive truly infinite depm idepm efficiently inferred using collapsed gibbs sampler experimentally confirmed model shrinkage proposed models works well idepm indicated stateoftheart performance generalization ability link prediction accuracy mixing efficiency convergence speed\",\"paper investigates asymptotic behaviors gradient descent algorithms particularly accelerated gradient descent stochastic gradient descent context stochastic optimization arising statistics machine learning objective functions estimated available data show algorithms computationally modeled continuoustime ordinary stochastic differential equations establish gradient flow central limit theorems describe limiting dynamic behaviors computational algorithms largesample performances related statistical procedures number algorithm iterations data size infinity gradient flow central limit theorems governed linear ordinary stochastic differential equations like timedependent ornsteinuhlenbeck processes illustrate study provide novel unified framework joint computational statistical asymptotic analysis computational asymptotic analysis studies dynamic behaviors algorithms time number iterations algorithms statistical asymptotic analysis investigates large sample behaviors statistical procedures like estimators classifiers algorithms applied compute fact statistical procedures equal limits random sequences generated iterative algorithms number iterations goes infinity joint analysis results based obtained gradient flow central limit theorems identify four factors learning rate batch size gradient covariance hessian derive new theory regarding local minima found stochastic gradient descent solving nonconvex optimization problems\",\"paper addresses problem filtering statespace model standard approaches filtering assume probabilistic model observations observation model given explicitly least parametrically consider setting assumption satisfied assume knowledge observation model provided examples stateobservation pairs setting important appears state variables defined quantities different observations propose kernel monte carlo filter novel filtering method focused setting approach based framework kernel mean embeddings enables nonparametric posterior inference using stateobservation examples proposed method represents state distributions weighted samples propagates samples sampling estimates state posteriors kernel bayes rule resamples kernel herding particular sampling resampling procedures novel expressed using kernel mean embeddings theoretically analyze behaviors reveal following properties similar corresponding procedures particle methods performance sampling degrade effective sample size weighted sample small resampling improves sampling performance increasing effective sample size first demonstrate theoretical findings synthetic experiments show effectiveness proposed filter artificial real data experiments include visionbased mobile robot localization\",\"increasing availability vehicle gps data created potentially transformative opportunities traffic management route planning locationbased services critical utility data accuracy mapmatching process improving accuracy aligning gps data road network paper propose purely probabilistic approach mapmatching based sequential monte carlo algorithm known particle filters approach performs mapmatching producing range candidate solutions associated probability score outline implementation details thoroughly validate technique gps data varied quality\",\"applying standard markov chain monte carlo mcmc algorithms large data sets computationally expensive calculation acceptance probability creation informed proposals usually require iteration whole data set recently proposed stochastic gradient langevin dynamics sgld method circumvents problem generating proposals based subset data skipping acceptreject step using decreasing stepsizes sequence deltamm geq appropriate lyapunov conditions provide article rigorous mathematical framework analysing algorithm prove verifiable assumptions algorithm consistent satisfies central limit theorem clt asymptotic biasvariance decomposition characterized explicit functional stepsizes sequence deltamm geq leverage analysis give practical recommendations notoriously difficult tuning algorithm asymptotically optimal use stepsize sequence type deltam asymp leading algorithm whose mean squared error mse decreases rate mathcalom\",\"infinite hidden markov models ihmms attractive nonparametric generalization classical hidden markov model automatically infer number hidden states system however due infinitedimensional nature transition dynamics performing inference ihmm difficult paper present infinitestate particle gibbs algorithm resample state trajectories ihmm proposed algorithm uses efficient proposal optimized ihmms leverages ancestor sampling suppress degeneracy standard algorithm algorithm demonstrates significant convergence improvements synthetic real world data sets additionally infinitestate algorithm lineartime complexity number states sampler competing methods scale quadratically\",\"optimal transport based distances powerful tools machine learning compare probability measures manipulate using maps field setting interest semidiscrete source measure continuous target discrete recent works shown minimax rate map mathcalot using iid subsamples measure twosample setting open question whether better convergence rate achieved full information discrete measure known onesample setting work answer positively question proving mathcalot lower bound rate map using similarity laguerre cells estimation density support estimation proposing stochastic gradient descent sgd algorithm adaptive entropic regularization averaging acceleration nearly achieve desired fast rate characteristic nonregular parametric problems design entropic regularization scheme decreasing number samples another key step algorithm consists using projection step permits leverage local strong convexity regularized problem convergence analysis integrates online convex optimization stochastic gradient techniques complemented specificities semidual moreover computationally memory efficient vanilla sgd algorithm achieves unusual fast rates theory numerical experiments\",\"probabilistic programming languages simplify development machine learning techniques inference sufficiently scalable unfortunately bayesian parameter estimation highly coupled models regressions statespace models still scales poorly mcmc transition takes linear time number observations paper describes sublineartime algorithm making metropolishastings updates latent variables probabilistic programs approach generalizes recently introduced approximate techniques instead subsampling data items assumed independent subsamples edges dynamically constructed graphical model thus applies broader class problems interoperates generalpurpose inference techniques empirical results including confirmation sublinear pertransition scaling presented bayesian logistic regression nonlinear classification via joint dirichlet process mixtures parameter estimation stochastic volatility models state estimation via particle mcmc three applications use implementation requires lines probabilistic code\",\"study statistical inference distributionally robust solution methods stochastic optimization problems focusing confidence intervals optimal values solutions achieve exact coverage asymptotically develop generalized empirical likelihood frameworkbased distributional uncertainty sets constructed nonparametric fdivergence ballsfor hadamard differentiable functionals particular stochastic optimization problems consequences theory provide principled method choosing size distributional uncertainty regions provide one twosided confidence intervals achieve exact coverage also give asymptotic expansion distributionally robust formulation showing robustification regularizes problems variance finally show optimizers distributionally robust formulations study enjoy essentially consistency properties classical sample average approximations general approach applies quickly mixing stationary sequences including geometrically ergodic harris recurrent markov chains\",\"need reason uncertainty large complex multimodal datasets become increasingly common across modern scientific environments ability transform samples one distribution another distribution enables solution many problems machine learning bayesian inference generative modeling actively pursued theoretical computational application perspectives across fields information theory computer science biology performing transformations general still leads computational difficulties especially high dimensions consider problem computing measure transport maps efficient parallelizable methods mild assumptions need known sampled density known proportionality constant logconcave provide work convex optimization problem pertaining relative entropy minimization show empirical minimization formulation polynomial chaos map parameterization allow learning transport map distributed scalable methods also leverage findings nonequilibrium thermodynamics represent transport map composition simpler maps learned sequentially transport cost regularized version aforementioned problem formulation provide examples framework within context bayesian inference boston housing dataset generative modeling handwritten digit images mnist dataset\",\"introduce multivariate stochastic volatility model asset returns imposes restrictions structure volatility matrix treats elements functions latent stochastic processes number assets prohibitively large propose factor multivariate stochastic volatility model variances correlations factors evolve stochastically time inference achieved via carefully designed feasible scalable markov chain monte carlo algorithm combines two computationally important ingredients utilizes invariant prior metropolis proposal densities simultaneously updating latent paths quadratic rather cubic computational complexity evaluating multivariate normal densities required apply modelling computational methodology stock daily returns euro stoxx index data period years matlab software paper available httpwwwauebgrusersmtitsiascodemsvzip\",\"optimal transport theory informally described using words french mathematician gaspard monge worker shovel hand move large pile sand lying construction site goal worker erect sand target pile prescribed shape example giant sand castle naturally worker wishes minimize total effort quantified instance total distance time spent carrying shovelfuls sand mathematicians interested cast problem comparing two probability distributions two different piles sand volume consider many possible ways morph transport reshape first pile second associate global cost every transport using local consideration much costs move grain sand one place another recent years witnessed spread several fields thanks emergence approximate solvers scale sizes dimensions relevant data sciences thanks newfound scalability increasingly used unlock various problems imaging sciences color texture processing computer vision graphics shape manipulation machine learning regression classification density fitting short book reviews bias toward numerical methods applications data sciences sheds lights theoretical properties make particularly useful applications\",\"stochastic gradient markov chain monte carlo sgmcmc developed flexible family scalable bayesian sampling algorithms however little theoretical analysis impact minibatch size algorithms convergence rate paper prove limited computational budgettime larger minibatch size leads faster decrease mean squared error bound thus fastest one corresponds using full gradients motivates necessity variance reduction sgmcmc consequently borrowing ideas stochastic optimization propose practical variancereduction technique sgmcmc efficient computation storage develop theory prove algorithm induces faster convergence rate standard sgmcmc number largescale experiments ranging bayesian learning logistic regression deep neural networks validate theory demonstrate superiority proposed variancereduction sgmcmc framework\",\"monte carlo sampling algorithms extremely widelyused technique estimate expectations functions especially high dimensions control variates powerful technique reduce error estimates conventional form rely accurate approximation priori stacked monte carlo stackmc recently introduced technique designed overcome limitation fitting control variate data samples done naively forming control variate data would result overfitting typically worsening algorithms performance stackmc uses insample outsample techniques remove overfitting crucially postprocessing technique requiring additional samples applied data generated estimator preliminary experiments demonstrated stackmc improved estimates expectations used postprocess samples produces simple sampling estimator substantially extend earlier work provide indepth analysis stackmc algorithm use construct improved version original algorithm lower estimation error perform experiments stackmc several additional kinds estimators demonstrating improved performance samples generated via importance sampling latinhypercube sampling quasimonte carlo sampling also show extend stackmc combine multiple fitting functions apply discrete input spaces\",\"factorial hidden markov models fhmms powerful models sequential data scale well long sequences propose scalable inference learning algorithm fhmms draws ideas stochastic variational inference neural network copula literatures unlike existing approaches proposed algorithm requires message passing procedure among latent variables distributed network computers speed learning experiments corroborate proposed algorithm introduce approximation bias compared proven structured meanfield algorithm achieves better performance long sequences large fhmms\",\"many matching tracking sorting ranking problems require probabilistic reasoning possible permutations set grows factorially dimension combinatorial optimization algorithms may enable efficient point estimation fully bayesian inference poses severe challenge highdimensional discrete space surmount challenge start usual step relaxing discrete set permutation matrices convex hull birkhoff polytope set doublystochastic matrices introduce two novel transformations first invertible differentiable stickbreaking procedure maps unconstrained space birkhoff polytope second map rounds points toward vertices polytope transformations include temperature parameter limit concentrates densities permutation matrices exploit transformations reparameterization gradients introduce variational inference permutation matrices demonstrate utility series experiments\",\"hamiltonian monte carlo hmc popular markov chain monte carlo mcmc algorithm generates proposals metropolishastings algorithm simulating dynamics hamiltonian system however hmc sensitive large time discretizations performs poorly mismatch spatial geometry target distribution scales momentum distribution particular mass matrix hmc hard tune well order alleviate problems propose relativistic hamiltonian monte carlo version hmc based relativistic dynamics introduce maximum velocity particles also derive stochastic gradient versions algorithm show resulting algorithms bear interesting relationships gradient clipping rmsprop adagrad adam popular optimisation methods deep learning based develop relativistic stochastic gradient descent taking zerotemperature limit relativistic stochastic gradient hamiltonian monte carlo experiments show relativistic algorithms perform better classical newtonian variants adam\",\"estimation normalizing constants fundamental step probabilistic model comparison sequential monte carlo methods may used task advantage inherently parallelizable however standard choice using fixed number particles iteration suboptimal steps contribute disproportionately variance estimate introduce adaptive version resamplemove algorithm particle set adaptively expanded whenever better approximation intermediate distribution needed algorithm builds expression optimal number particles corresponding minimum variance found ideal conditions benchmark results challenging gaussian process classification restricted boltzmann machine applications show adaptive resamplemove arm estimates normalizing constant smaller variance using less computational resources either resamplemove fixed number particles annealed importance sampling advantage annealed importance sampling arm easier tune\",\"propose novel approach parameter estimation simulatorbased statistical models intractable likelihood proposed method involves recursive application kernel abc kernel herding observed data provide theoretical explanation regarding approach works showing population setting certain assumption point estimates obtained method converge true parameter recursion proceeds conducted variety numerical experiments including parameter estimation realworld pedestrian flow simulator show cases method outperforms existing approaches\",\"propose new sampling method thermostatassisted continuouslytempered hamiltonian monte carlo bayesian learning large datasets multimodal distributions simulates nosehoover dynamics continuouslytempered hamiltonian system built distribution interest significant advantage method able efficiently draw representative iid samples distribution contains multiple isolated modes capable adaptively neutralising noise arising minibatches maintaining accurate sampling properties method studied using synthetic distributions experiments three real datasets also demonstrated gain performance several strong baselines various types neural networks plunged\",\"annealed importance sampling ais common algorithm estimate partition functions useful stochastic models one important problem obtaining accurate ais estimates selection annealing schedule conventionally annealing schedule often determined heuristically simply set linearly increasing sequence paper propose algorithm optimal schedule deriving functional dominates ais estimation error numerically minimizing functional experimentally demonstrate proposed algorithm mostly outperforms conventional scheduling schemes large quantization numbers\",\"optimal transportation distances fundamental family parameterized distances histograms despite appealing theoretical properties excellent performance retrieval tasks intuitive formulation computation involves resolution linear program whose cost prohibitive whenever histograms dimension exceeds hundreds propose work new family optimal transportation distances look transportation problems maximumentropy perspective smooth classical optimal transportation problem entropic regularization term show resulting optimum also distance computed sinkhornknopps matrix scaling algorithm speed several orders magnitude faster transportation solvers also report improved performance classical optimal transportation distances mnist benchmark problem\",\"models complex systems often formalized sequential software simulators computationally intensive programs iteratively build probable system configurations given parameters initial conditions simulators enable modelers capture effects difficult characterize analytically summarize statistically however many realworld applications simulations need inverted match observed data typically requires custom design derivation implementation sophisticated inversion algorithms give framework inverting broad class complex software simulators via probabilistic programming automatic inference using lines probabilistic code approach based formulation inversion approximate inference simple sequential probabilistic model implement four inference strategies including metropolishastings sequentialized metropolishastings scheme particle markov chain monte carlo scheme requiring fewer lines probabilistic code demonstrate framework applying invert real geological software simulator oil gas industry\",\"paper consider statistical problem learning linear model noisy samples existing work focused approximating least squares solution using leveragebased scores importance sampling distribution however finite sample statistical guarantees computationally efficient optimal sampling strategies proposed evaluate statistical properties different sampling strategies propose simple yet effective estimator easy theoretical analysis useful multitask linear regression derive exact mean square error proposed estimator given sampling scores based minimizing mean square error propose optimal sampling scores estimator predictor show influenced noisetosignal ratio numerical simulations match theoretical analysis well\",\"propose kernel hamiltonian monte carlo kmc gradientfree adaptive mcmc algorithm based hamiltonian monte carlo hmc target densities classical hmc option due intractable gradients kmc adaptively learns targets gradient structure fitting exponential family model reproducing kernel hilbert space computational costs reduced two novel efficient approximations gradient asymptotically exact kmc mimics hmc terms sampling efficiency offers substantial mixing improvements stateoftheart gradient free samplers support claims experimental studies toy realworld applications including approximate bayesian computation exactapproximate mcmc\",\"regular particle filter algorithm sequential monte carlo smc methods initial weights traditionally dependent proposed distribution posterior distribution current timestamp sampled sequence target posterior distribution previous timestamp technically correct leads algorithms usually practical issues degeneracy particles eventually collapse onto single particle paper propose evaluate using means clustering attack even take advantage degeneracy specifically propose stochastic smc algorithm initializes set means providing initial centers chosen collapsed particles fight degeneracy adjust regular smc weights mediated cluster proportions correct retain expectation experimentally demonstrate approach better performance vanilla algorithms\",\"focus interpolation method referred bayesian reconstruction paper whereas standard interpolation methods missing data interpolated deterministically bayesian reconstruction missing data interpolated probabilistically using bayesian treatment paper address framework bayesian reconstruction application traffic data reconstruction problem field traffic engineering latter part paper describe evaluation statistical performance bayesian traffic reconstruction model using statistical mechanical approach clarify statistical behavior\",\"present analyse three online algorithms learning discrete hidden markov models hmms compare baldichauvin algorithm using kullbackleibler divergence measure generalisation error draw learning curves simplified situations performance learning drifting concepts one presented algorithms analysed compared baldichauvin algorithm situations brief discussion learning symmetry breaking based results also presented\",\"consider problem adaptive stratified sampling monte carlo integration differentiable function given finite number evaluations function construct sampling scheme samples often regions function oscillates allocating samples well spread domain notion shares similitude low discrepancy prove estimate returned algorithm almost similarly accurate estimate optimal oracle strategy would know variations function everywhere would return provide finitesample analysis\",\"recent decades seen interest prediction problems bayesian methodology used ubiquitously sampling approximating posterior predictive distribution bayesian model allows one make inferential statements potentially observable random quantities given observed data purpose note use statistical decision theory basis justify use posterior predictive distribution making point prediction\",\"distributions permutations arise applications ranging multiobject tracking ranking instances difficulty dealing distributions caused size domain factorial number considered entities makes direct definition multinomial distribution permutation space impractical small work propose embedding permutations given surface hypersphere defined mathbbmrn result embedding acquire ability define continuous distributions hypersphere benefits directional statistics provide polynomial time projections continuous hypersphere representation nelement permutation space framework provides way use continuous directional probability densities methods developed thereof establishing densities permutations demonstration benefits framework derive inference procedure statespace model permutations demonstrate approach applications\",\"propose restricted collapsed draw rcd sampler general markov chain monte carlo sampler simultaneous draws hierarchical chinese restaurant process hcrp restriction models require simultaneous draws hierarchical dirichlet process restriction infinite hidden markov models ihmm difficult enjoy benefits markergthe hcrp due combinatorial explosion calculating distributions coupled draws constructing proposal seating arrangements partitioning stochastically accepts proposal metropolishastings algorithm rcd sampler makes accurate sampling complex combination draws retaining efficiency hcrp representation based rcd sampler developed series sophisticated sampling algorithms ihmms including blocked gibbs sampling beam sampling splitmerge sampling outperformed conventional ihmm samplers experiments\",\"large matrix factorisation problems develop distributed markov chain monte carlo mcmc method based stochastic gradient langevin dynamics sgld call parallel sgld psgld psgld favourable scaling properties increasing data size comparable terms computational requirements optimisation methods based stochastic gradient descent psgld achieves high performance exploiting conditional independence structure models subsample data systematic manner allow parallelisation distributed computation provide convergence proof algorithm verify superior performance various architectures graphics processing units shared memory multicore systems multicomputer clusters\",\"propose datadriven coarsegraining formulation context equilibrium statistical mechanics contrast existing techniques based finetocoarse map adopt opposite strategy prescribing probabilistic coarsetofine map corresponds directed probabilistic model coarse variables play role latent generators fine scale allatom data informationtheoretic perspective framework proposed provides improvement upon relative entropy method capable quantifying uncertainty due information loss unavoidably takes place process furthermore readily extended fully bayesian model various sources uncertainties reflected posterior model parameters latter used produce point estimates finescale reconstructions macroscopic observables importantly predictive posterior distributions quantities predictive posterior distributions reflect confidence model function amount data level coarsegraining issues model complexity model selection seamlessly addressed employing hierarchical prior favors discovery sparse solutions revealing prominent features coarsegrained model flexible parallelizable monte carlo expectationmaximization mcem scheme proposed carrying inference learning tasks comparative assessment proposed methodology presented lattice spin system spce water model\",\"recently theoretical guarantees obtained matrix completion nonuniform sampling regime particular sampling distribution aligns underlying matrixs leverage scores high probability nuclear norm minimization exactly recover low rank matrix article analyze scenario nonuniform sampling distribution may may align underlying matrixs leverage scores explore learning parameters weighted nuclear norm minimization terms empirical sampling distribution provide sufficiency condition learned weights provide exact recovery guarantee weighted nuclear norm minimization established specific choice weights terms true sampling distribution allows weighted nuclear norm minimization exactly recover low rank matrix also allows quantifiable relaxation exact recovery conditions article extend quantifiable relaxation exact recovery conditions specific choice weights defined analogously terms empirical distribution opposed true sampling distribution accomplish employ concentration measure bound large deviation bound also present numerical evidence healthy robustness weighted nuclear norm minimization algorithm choice empirically learned weights numerical experiments show variety easily computable empirical weights weighted nuclear norm minimization outperforms unweighted nuclear norm minimization nonuniform sampling regime\",\"stochastic gradient markov chain monte carlo sgmcmc increasingly popular bayesian learning due ability deal large data standard sgmcmc algorithm simulates samples discretizedtime markov chain approximate target distribution however samples typically highly correlated due sequential generation process undesired property sgmcmc contrary stein variational gradient descent svgd directly optimizes set particles able approximate target distribution much fewer samples paper propose novel method directly optimize particles samples sgmcmc scratch specifically propose efficient methods solve corresponding fokkerplanck equation space probability distributions whose solution distribution approximated particles framework able show connections sgmcmc svgd well seemly unrelated generativeadversarialnet framework certain relaxations particle optimization sgmcmc interpreted extension standard svgd momentum\",\"manifold markov chain monte carlo algorithms introduced sample effectively challenging target densities exhibiting multiple modes strong correlations algorithms exploit local geometry parameter space thus enabling chains achieve faster convergence rate measured number steps however acquiring local geometric information often increase computational complexity per step extent sampling highdimensional targets becomes inefficient terms total computational time paper analyzes computational complexity manifold langevin monte carlo proposes geometric adaptive monte carlo sampler aimed balancing benefits exploiting local geometry computational cost achieve high effective sample size given computational cost suggested sampler discretetime stochastic process random environment random environment allows switch local geometric adaptive proposal kernels help schedule exponential schedule put forward enables frequent use geometric information early transient phases chain saving computational time late stationary phases average complexity manually set depending need geometric exploitation posed underlying model\",\"technical report explores estimation methodologies hyperparameters markov random field gaussian hidden markov random field first section briefly investigate theoretical framework metropolishastings algorithm next using algorithm simulate data ising model study hyperparameter estimation ising model enabled mcmc algorithm using pseudolikelihood approximation following section deals issue parameters estimation process gaussian hidden markov random field using map estimation algorithm also discusses problems found several experiments following section expand idea estimating parameters gaussian hidden markov spatialtemporal random field display results two performed experiments\",\"langevin diffusion commonly used tool sampling given distribution work establish target density log smooth strongly convex discrete langevin diffusion produces distribution klppleq epsilon tildeofracdepsilon steps dimension sample space also study convergence rate strongconvexity assumption absent considering langevin diffusion gradient flow space probability distributions obtain elegant analysis applies stronger property convergence kldivergence gives conceptually simpler proof bestknown convergence results weaker metrics\",\"approximate bayesian computation abc likelihoodfree monte carlo methods abc methods use comparison simulated data using different parameters drew prior distribution observed data comparison process based computing distance summary statistics simulated data observed data complex models usually difficult define methodology choosing constructing summary statistics recently nonparametric abc proposed uses dissimilarity measure discrete distributions based empirical kernel embeddings alternative summary statistics nonparametric abc outperforms methods including abc kernel abc synthetic likelihood abc however assumes probability distributions discrete robust dealing observations paper propose apply kernel embeddings using smoother density estimator parzen estimator comparing empirical data distributions computing abc posterior synthetic data real data used test bayesian inference method compare method respect stateoftheart methods demonstrate method robust estimator posterior distribution terms number observations\",\"propose novel sampling framework inference probabilistic models active learning approach converges quickly wallclock time markov chain monte carlo mcmc benchmarks central challenge probabilistic inference numerical integration average ensembles models unknown hyperparameters example compute marginal likelihood partition function mcmc provided approaches numerical integration deliver stateoftheart inference suffer sample inefficiency poor convergence diagnostics bayesian quadrature techniques offer modelbased solution problems uptake hindered prohibitive computation costs introduce warped model probabilistic integrands likelihoods known nonnegative permitting cheap active learning scheme optimally select sample locations algorithm demonstrated offer faster convergence seconds relative simple monte carlo annealed importance sampling synthetic realworld examples\",\"contrastive divergence algorithm achieved notable success training energybased models including restricted boltzmann machines played key role emergence deep learning idea algorithm approximate intractable term exact gradient loglikelihood function using short markov chain monte carlo mcmc runs approximate gradient computationallycheap biased whether algorithm provides asymptotically consistent estimate still open questions paper studies asymptotic properties algorithm canonical exponential families special cases energybased model suppose algorithm runs mcmc transition steps iteration iteratively generates sequence parameter estimates thetatt given iid data sample xiin sim pthetastar conditions commonly obeyed algorithm practice prove existence bounded limit point time average left sumst thetas right infty consistent estimate true parameter thetastar proof based fact thetatt homogenous markov chain conditional data sample xiin chain meets fosterlyapunov drift criterion converges random walk around maximum likelihood estimate range random walk shrinks zero rate mathcalosqrtn sample size infty\",\"introduce novel approach parallelizing mcmc inference models spatially determined conditional independence relationships existing techniques exploiting graphical model structure applicable approach motivated model seismic events signals events detected distant regions approximately independent given intermediate regions perform parallel inference coloring factor graph defined regions latent space rather individual model variables evaluating model seismic event detection achieve significant speedups serial mcmc degradation inference quality\",\"factorial hidden markov models fhmms powerful tools modeling sequential data learning fhmms yields challenging simultaneous model selection issue selecting number multiple markov chains dimensionality chain main contribution address model selection issue extending factorized asymptotic bayesian fab inference fhmms first offer better approximation marginal loglikelihood previous fab inference key idea integrate transition probabilities yet still apply laplace approximation emission probabilities second prove two similar hidden states fhmm one redundant fab almost surely shrink eliminate one making model parsimonious experimental results show fab fhmms significantly outperforms stateoftheart nonparametric bayesian ifhmm variational fhmm model selection accuracy competitive heldout perplexity\",\"learning hidden markov model hmm sequen tial observations often complemented realvalued summary response variables generated path hid den states settings arise numerous domains includ ing many applications biology like motif discovery genome annotation paper present flexible frame work jointly modeling latent sequence features functional mapping relates summary response variables hidden state sequence algorithm com patible rich set mapping functions results show availability additional continuous response vari ables simultaneously improve annotation quential observations yield good prediction performance synthetic data realworld datasets\",\"consider problem adaptive stratified sampling monte carlo integration noisy function given finite budget noisy evaluations function tackle paper problem adapting function time number samples stratum partition precisely interesting refine partition domain area noise function variations function heterogeneous hand refined stratification optimal indeed refined stratification difficult adjust allocation samples stratification sample points noise variations function larger provide paper algorithm selects online among large class partitions partition provides optimal tradeoff allocates samples almost optimally partition\",\"discuss bayesian formulation coarsegraining pdes coefficients material parameters exhibit random fine scale variability direct solution problems requires grids small enough resolve fine scale variability unavoidably requires repeated solution large systems algebraic equations establish physically inspired datadriven coarsegrained model learns low dimensional set microstructural features predictive finegrained model response learned features provide sharp distribution coarse scale effec tive coefficients pde suitable prediction fine scale model output ultimately allows replace computationally expensive generative proba bilistic model based evaluating much cheaper several times sparsity enforcing pri ors increase predictive efficiency reveal microstructural features important predicting response moreover model yields probabilistic rather singlepoint predictions enables quantification unavoidable epistemic uncertainty present due information loss occurs coarsegraining process\",\"propose analyze two new mcmc sampling algorithms vaidya walk john walk generating samples uniform distribution polytope random walks sampling algorithms derived interior point methods former based volumetriclogarithmic barrier introduced vaidya whereas latter uses johns ellipsoids show vaidya walk mixes significantly fewer steps logarithmicbarrier based dikin walk studied past work polytope mathbbrd defined linear constraints show mixing time warm start bounded mathcalond compared mathcalond mixing time bound dikin walk cost step vaidya walk order dikin walk twice large terms constant prefactors john walk prove mathcalodcdotlognd bound mixing time conjecture improved variant could achieve mixing time mathcalodcdottextpolylognd additionally propose variants vaidya john walks mix polynomial time deterministic starting point speedup vaidya walk dikin walk illustrated numerical examples\",\"probabilistic programming languages represent complex data intermingled models lines code efficient inference algorithms probabilistic programming languages make possible build unified frameworks compute interesting probabilities various large realworld problems structure model given constructing probabilistic program rather straightforward thus main focus learn best model parameters compute marginal probabilities paper provide new perspective build expressive probabilistic program continue time series data structure model given intuition behind method find descriptive covariance structure time series data nonparametric gaussian process regression report descriptive covariance structure efficiently derives probabilistic programming description accurately\",\"propose novel method semisupervised learning ssl based datadriven distributionally robust optimization dro using optimal transport metrics proposed method enhances generalization error using unlabeled data restrict support worst case distribution dro formulation enable implementation dro formulation proposing stochastic gradient descent algorithm allows easily implement training procedure demonstrate semisupervised dro method able improve generalization error natural supervised procedures stateoftheart ssl estimators finally include discussion large sample behavior optimal uncertainty region dro formulation discussion exposes important aspects role dimension reduction ssl\",\"expectation propagation provides framework approximate inference model consideration latent gaussian field approximation gaussian show approximations systematically corrected perturbative expansion made exact intractable correction applied models partition function moments interest correction expressed higherorder cumulants neglected eps local matching moments expansion see correct first order considering higher orders corrections increasing polynomial complexity applied approximation second order provides correction quadratic time apply array gaussian process ising models corrections generalize arbitrarily complex approximating families illustrate treestructured ising model approximations furthermore provide polynomialtime assessment approximation error also provide theoretical practical insights exactness solution\",\"stochastic gradient mcmc sgmcmc algorithms proven useful scaling bayesian inference large datasets assumption iid data instead develop sgmcmc algorithm learn parameters hidden markov models hmms timedependent data two challenges applying sgmcmc setting latent discrete states needing break dependencies considering minibatches consider marginal likelihood representation hmm propose algorithm harnesses inherent memory decay process demonstrate effectiveness algorithm synthetic experiments ion channel recording data runtimes significantly outperforming batch mcmc\",\"performing inference simulators generally intractable runtime means cannot compute marginal likelihood develop likelihoodfree inference method infer parameters cardiac simulator replicates electrical flow heart body surface improve fit stateoftheart simulator electrocardiogram ecg recorded real patient\",\"consider problem transforming samples one continuous source distribution samples another target distribution demonstrate optimal transport theory source distribution easily sampled target distribution logconcave tractably solved convex optimization show special case source prior target posterior bayesian inference tractably calculate normalization constant draw posterior iid samples remarkably bayesian tractability criterion simply log concavity prior likelihood criterion tractable calculation maximum posteriori point estimate simulated data demonstrate attain bayes risk simulations physiologic data demonstrate improvements point estimation intensive care unit outcome prediction electroencephalographybased sleep staging\",\"propose novel adaptive importance sampling algorithm incorporates stein variational gradient decent algorithm svgd importance sampling algorithm leverages nonparametric transforms svgd iteratively decrease divergence importance proposal target distribution advantages algorithm twofold first algorithm turns svgd standard algorithm allowing use standard diagnostic analytic tools evaluate interpret results second restrict choice importance proposal predefined distribution families like traditional adaptive methods empirical experiments demonstrate algorithm performs well evaluating partition functions restricted boltzmann machines testing likelihood variational autoencoders\",\"adaptive monte carlo schemes developed last years usually seek ensure ergodicity sampling process line mcmc tradition poses constraints possible terms adaptation general case ergodicity guaranteed adaptation diminished certain rate importance sampling approaches offer way circumvent limitation design sampling algorithms keep adapting present gradient informed variant smc special case population monte carlo static problems\",\"consider continuous time markovian processes populations individual agents interact stochastically according kinetic rules despite increasing prominence models fields ranging biology smart cities bayesian inference systems remains challenging continuous time discrete state systems potentially infinite statespace propose novel efficient algorithm joint state parameter posterior sampling population markov jump processes introduce class pseudomarginal sampling algorithms based random truncation method enables principled treatment infinite state spaces extensive evaluation number benchmark models shows approach achieves considerable savings compared state art methods retaining accuracy fast convergence also present results synthetic biology data set showing potential practical usefulness work\",\"recently stochastic gradient markov chain monte carlo sgmcmc methods proposed scaling monte carlo computations large data problems whilst approaches proven useful many applications vanilla sgmcmc might suffer poor mixing rates random variables exhibit strong couplings target densities big scale differences study propose novel sgmcmc method takes local geometry account using ideas quasinewton optimization methods second order methods directly approximate inverse hessian using limited history samples gradients method uses dense approximations inverse hessian keeping time memory complexities linear dimension problem provide formal theoretical analysis show proposed method asymptotically unbiased consistent posterior expectations illustrate effectiveness approach synthetic real datasets experiments two challenging applications show method achieves fast convergence rates similar riemannian approaches time low computational requirements similar diagonal preconditioning approaches\",\"probabilistic models often parameters translated scaled permuted otherwise transformed without changing model symmetries lead strong correlation multimodality posterior distribution models parameters pose challenges performing inference interpreting results work address automatic detection common problematic model symmetries introduce local symmetries cover many common cases amenable automatic detection show derive algorithms detect several broad classes local symmetries algorithms compatible probabilistic programming constructs arrays loops statements scale models many variables\",\"study dual volume sampling method selecting columns short wide matrix probability selection proportional volume spanned rows induced submatrix method proposed avron boutsidis showed promising method column subset selection multiple applications however wider adoption hampered lack polynomial time sampling algorithms remove hindrance developing exact randomized polynomial time sampling algorithm well derandomization thereafter study dual volume sampling via theory real stable polynomials prove distribution satisfies strong rayleigh property result numerous consequences including provably fastmixing markov chain sampler makes dual volume sampling much attractive practitioners sampler closely related classical algorithms popular experimental design methods date lacking theoretical analysis known empirically work well\",\"many random processes simulated output deterministic model accepting random inputs model usually describes complex mathematical physical stochastic system randomness introduced input variables model statistics output event known input variables chosen specific way output prescribed statistics probability distribution input random variables directly known dictated implicitly statistics output random variables problem usually intractable classical sampling methods based markov chain monte carlo propose novel method sample random inputs models introducing modification standard metropolishastings algorithm example consider system described stochastic differential equation sde demonstrate sample paths random process satisfying sde generated technique\",\"present new algorithms compute mean set empirical probability measures optimal transport metric mean known wasserstein barycenter measure minimizes sum wasserstein distances element set propose two original algorithms compute wasserstein barycenters build upon subgradient method direct implementation algorithms however costly would require repeated resolution large primal dual optimal transport problems compute subgradients extending work cuturi propose smooth wasserstein distance used definition wasserstein barycenters entropic regularizer recover strictly convex objective whose gradients computed considerably cheaper computational cost using matrix scaling algorithms use algorithms visualize large family images solve constrained clustering problem\",\"asymptotic pseudotrajectory approach stochastic approximation benaim hofbauer sorin extended asynchronous stochastic approximations setvalued mean field asynchronicity process incorporated mean field produce convergence results remain similar equivalent synchronous process addition allows many restrictive assumptions previously associated asynchronous stochastic approximation removed framework extended coupled asynchronous stochastic approximation process setvalued mean fields twotimescales arguments used similar manner original work area borkar applicability approach demonstrated learning markov decision process\",\"study probability measures induced set functions constraints measures arise variety realworld settings prior knowledge resource limitations pragmatic considerations impose constraints consider task rapidly sampling constrained measures develop fast markov chain samplers first main result mcmc sampling strongly rayleigh measures present sharp polynomial bounds mixing time corollary result yields fast mixing sampler determinantal point processes dpps yielding knowledge first provably fast mcmc sampler dpps since inception four decades ago beyond measures develop mcmc samplers probabilistic models hard constraints identify sufficient conditions chains mix rapidly illustrate claims empirically verifying dependence mixing times key factors governing theoretical bounds\",\"ability track moving vehicle crucial importance numerous applications task often approached importance sampling technique particle filters due ability model nonlinear nongaussian dynamics vehicle travelling road network good example particle filters perform poorly observations highly informative paper address problem proposing particle filters sample around recent observation proposal leads order magnitude improvement accuracy efficiency conventional particle filters especially observations infrequent lownoise\",\"develop automated variational inference method bayesian structured prediction problems gaussian process priors linearchain likelihoods approach need know details structured likelihood model scale large number observations furthermore show required expected likelihood term gradients variational objective elbo estimated efficiently using expectations lowdimensional gaussian distributions optimization elbo fully parallelizable sequences amenable stochastic optimization use along control variate techniques stateoftheart incremental optimization make framework useful practice results set natural language processing tasks show method good sometimes better hardcoded approaches including svmstruct crfs overcomes scalability limitations previous inference algorithms based sampling overall fundamental step developing automated inference methods bayesian structured prediction\",\"importance sampling widely used machine learning statistics power limited restriction using simple proposals importance weights tractably calculated address problem studying blackbox importance sampling methods calculate importance weights samples generated unknown proposal blackbox mechanism method allows use better richer proposals solve difficult problems somewhat counterintuitively also additional benefit improving estimation accuracy beyond typical importance sampling theoretical empirical analyses provided\",\"modern scale data brought new challenges bayesian inference particular conventional mcmc algorithms computationally expensive large data sets promising approach solve problem embarrassingly parallel mcmc epmcmc first partitions data multiple subsets runs independent sampling algorithms subset subset posterior draws aggregated via combining rules obtain final approximation existing epmcmc algorithms limited approximation accuracy difficulty resampling article propose new epmcmc algorithm part solves problems new algorithm applies random partition trees combine subset posterior draws distributionfree easy resample adapt multiple scales provide theoretical justification extensive experiments illustrating empirical performance\",\"paper demonstrate tempering markov chain monte carlo samplers bayesian models recursively subsampling observations without replacement improve performance baseline samplers terms effective sample size per computation present two tempering subsampling algorithms subsampled parallel tempering subsampled tempered transitions provide asymptotic analysis computational cost tempering subsampling verify tempering subsampling costs less traditional tempering demonstrate algorithms bayesian approaches learning mean high dimensional multivariate normal estimating gaussian process hyperparameters\",\"given family probability measures space probability measures hilbert space goal paper highlight one ore curves summarize efficiently family propose study problem optimal transport wasserstein geometry using curves restricted geodesic segments metric show concepts play key role euclidean pca data centering orthogonality principal directions find natural equivalent optimal transport geometry using wasserstein means differential geometry implementation ideas however computationally challenging achieve scalable algorithms handle thousands measures propose use relaxed definition geodesics regularized optimal transport distances interest approach demonstrated images seen either shapes color histograms\",\"wellestablished methods solution stochastic partial differential equations spdes typically struggle problems highdimensional inputsoutputs difficulties amplified largescale applications even tens fullorder model runs impracticable dimensionality reduction alleviate issues known many features highdimensional input actually predictive highdimensional output paper advocate bayesian formulation capable performing simultaneous dimension modelorder reduction consists component encodes highdimensional input lowdimensional set feature functions employing sparsityenforcing priors decoding component makes use solution coarsegrained model order reconstruct fullorder model components represented latent variables probabilistic graphical model simultaneously trained using stochastic variational inference methods model capable quantifying predictive uncertainty due information loss unavoidably takes place modelorderdimension reduction well uncertainty arising finitesized training datasets demonstrate capabilities context random media finescale fluctuations give rise random inputs tens thousands variables tens fullorder model simulations proposed model capable identifying salient physical features produce sharp predictions different boundary conditions full output consists thousands components\",\"recent advances bayesian learning largescale data witnessed emergence stochastic gradient mcmc algorithms sgmcmc stochastic gradient langevin dynamics sgld stochastic gradient hamiltonian mcmc sghmc stochastic gradient thermostat finitetime convergence properties sgld storder euler integrator recently studied corresponding theory general sgmcmcs explored paper consider general sgmcmcs highorder integrators develop theory analyze finitetime convergence properties asymptotic invariant measures theoretical results show faster convergence rates accurate invariant measures sgmcmcs higherorder integrators example proposed efficient ndorder symmetric splitting integrator mean square error mse posterior average sghmc achieves optimal convergence rate iterations compared sghmc sgld storder euler integrators furthermore convergence results decreasingstepsize sgmcmcs also developed convergence rates fixedstepsize counterparts specific decreasing sequence experiments synthetic real datasets verify theory show advantages proposed method two largescale real applications\",\"present nonparametric prior reversible markov chains use completely random measures specifically gamma processes construct countably infinite graph weighted edges enforcing symmetry make edges undirected define prior random walks graphs results reversible markov chain resulting prior infinite transition matrices closely related hierarchical dirichlet process enforces reversibility reinforcement scheme recently proposed similar properties finetti measure well characterised take alternative approach explicitly constructing mixing measure allows straightforward efficient inference cost longer closed form predictive distribution use process construct reversible infinite hmm apply two real datasets one epigenomics one ion channel recording\",\"article present elitist particle filter based evolutionary strategies epfes efficient approach nonlinear system identification epfes derived frequentlyemployed statespace model relevant information nonlinear system captured unknown state vector similar classical particle filtering epfes consists set particles respective weights represent different realizations latent state vector likelihood solution optimization problem main innovation epfes includes evolutionary elitistparticle selection combines longterm information instantaneous sampling approximated continuous posterior distribution article propose two advancements previouslypublished elitistparticle selection process epfes shown generalization widelyused gaussian particle filter thus evaluated respect latter two completely different scenarios first consider socalled univariate nonstationary growth model timevariant latent state variable evolutionary selection elitist particles evaluated nonrecursively calculated particle weights second problem nonlinear acoustic echo cancellation addressed simulated scenario speech input signal using longterm fitness measures highlight efficacy wellgeneralizing epfes estimating nonlinear system even large search spaces finally illustrate similarities epfes evolutionary algorithms outline future improvements fusing achievements fields research\",\"paper presents novel twostep approach fundamental problem learning optimal map one distribution another first learn optimal transport plan thought onetomany map two distributions end propose stochastic dual approach regularized show empirically scales better recent related approach amount samples large second estimate textitmonge map deep neural network learned approximating barycentric projection previouslyobtained plan parameterization allows generalization mapping outside support input measure prove two theoretical stability results regularized show estimations converge plan monge map underlying continuous measures showcase proposed approach two applications domain adaptation generative modeling\",\"ability compare two degenerate probability distributions two probability distributions supported two distinct lowdimensional manifolds living much higherdimensional space crucial problem arising estimation generative models highdimensional observations arising computer vision natural language known optimal transport metrics represent cure problem since specifically designed alternative information divergences handle problematic scenarios unfortunately training generative machines using raises formidable computational statistical challenges computational burden evaluating losses instability lack smoothness losses iii difficulty estimate robustly losses gradients high dimension paper presents first tractable computational method train large scale generative models using optimal transport loss tackles three issues relying two key ideas entropic smoothing turns original loss one computed using sinkhorn fixed point iterations algorithmic automatic differentiation iterations two approximations result robust differentiable approximation loss streamlined gpu execution entropic smoothing generates family losses interpolating wasserstein maximum mean discrepancy mmd thus allowing find sweet spot leveraging geometry favorable highdimensional sample complexity mmd comes unbiased gradient estimates resulting computational architecture complements nicely standard deep network generative models stack extra layers implementing loss function\",\"common problem disciplines applied statistics research astrostatistics estimating posterior distribution relevant parameters typically likelihoods models computed via expensive experiments cosmological simulations universe urgent challenge research domains develop methods estimate posterior likelihood evaluations paper study active posterior estimation bayesian setting likelihood expensive evaluate existing techniques posterior estimation based generating samples representative posterior methods consider efficiency terms likelihood evaluations order query efficient treat posterior estimation active regression framework propose two myopic query strategies choose evaluate likelihood implement using gaussian processes via experiments series synthetic real examples demonstrate approach significantly query efficient existing techniques heuristics posterior estimation\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"7_sampling_carlo_monte\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"7_sampling_carlo_monte\"],\"textfont\":{\"size\":12},\"x\":[13.459692,13.192232,10.993946,12.976154,12.575188,12.431581,12.205708,12.108136,12.97232,13.291009,12.422997,12.609486,11.703459,12.704766,10.119969,12.609016,13.063031,12.652254,13.621076,12.840511,13.186354,12.939627,12.172629,13.14349,12.944315,12.623305,12.679963,12.581399,13.124579,12.761788,12.083579,13.227801,12.835245,12.364945,12.877539,12.988066,13.001473,12.522141,12.682411,13.082866,12.88486,12.704932,12.9215765,12.159503,12.869261,13.012619,12.585593,13.429039,13.518677,12.903158,12.605831,12.922328,12.5728035,11.644134,12.171713,13.079284,12.395078,12.394988,13.083991,12.878566,13.005091,13.068382,12.648804,12.787679,12.796216,12.603304,12.783589,12.919762,12.1354885,13.137301,12.9320965,12.963896,12.739804,12.6422415,12.568995,13.040845,13.240182,11.970966,12.80256,12.796834,12.294618,12.703613],\"y\":[8.622129,8.906792,8.795372,7.860964,9.250736,7.945108,8.766877,8.615041,8.762854,8.654635,7.8210754,9.106928,8.03682,9.391433,9.6391325,7.838468,8.935579,8.604749,8.935272,7.6725464,8.926775,9.041961,8.723786,8.937206,9.162905,7.8002725,8.923997,8.412091,8.87741,8.813231,8.716953,8.52877,8.697361,8.8609,7.715225,8.608931,8.876595,9.210596,8.430572,9.008393,8.700917,8.877901,8.6321535,8.732215,8.849357,8.691143,9.027252,8.635382,8.73788,8.719843,9.428909,8.626999,9.076201,8.012214,9.2604475,8.843926,8.784152,8.87781,9.165449,8.752683,8.7332535,8.8321085,9.129529,8.471495,8.81484,7.792591,8.568673,8.715446,8.629521,9.128794,9.072131,8.858345,8.948224,7.801258,9.557749,8.727238,8.667646,8.562709,9.566185,9.604207,8.802759,8.738938],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"present two deep generative models based variational autoencoders improve accuracy drug response prediction models perturbation variational autoencoder semisupervised extension drug response variational autoencoder drvae learn latent representation underlying gene states drug application depend druginduced biological change gene overall treatment response outcome vaebased models outperform current published benchmarks field anywhere auroc aupr addition found better reconstruction accuracy necessarily lead improvement classification accuracy jointly trained models perform better models minimize reconstruction error independently\",\"reparameterization gradient become widely used method obtain monte carlo gradients optimize variational objective however technique easily apply commonly used distributions beta gamma without approximations practical applications reparameterization gradient fit gaussian distributions paper introduce generalized reparameterization gradient method extends reparameterization gradient wider class variational distributions generalized reparameterizations use invertible transformations latent variables lead transformed distributions weakly depend variational parameters results new monte carlo gradients combine reparameterization gradients score function gradients demonstrate approach variational inference two complex probabilistic models generalized reparameterization effective even single sample variational distribution enough obtain lowvariance gradient\",\"many inference problems involving questions optimality ask maximum minimum finite set unknown quantities technical report derives first two posterior moments maximum two correlated gaussian variables first two posterior moments two generating variables corresponding gaussian approximations minimizing relative entropy shown used build heuristic approximation maximum relationship finite set gaussian variables allowing approximate inference expectation propagation quantities\",\"laplace approximation calls computation second derivatives likelihood maximum maximum found emalgorithm convenient way compute derivatives likelihood gradient obtained emauxiliary hessian obtained gradient pearlmutter trick\",\"motivations using variational inference neural networks differ significantly latent variable models counterintuitive consequence expressive variational approximations provide significantly worse predictions compared less expressive families work make two contributions first identify cause performance gap variational overpruning second introduce theoretically grounded explanation phenomenon perspective sheds light several related published results provides intuition design effective variational approximations neural networks\",\"variational inference powerful concept underlies many iterative approximation algorithms expectation propagation meanfield methods belief propagations central themes school perceived unifying framework lectures manfred opper introduce archetypal example expectation propagation establishing connection approximation methods corrections expansion expectation propagation explained finally advanced inference topics applications explored final sections\",\"provide sharp empirical estimates expectation variance normal approximation class statistics whose variation argument change much another argument modified examples weak interactions furnished vstatistics lipschitz lstatistics various error functionals lregularized algorithms gibbs algorithms\",\"stochastic variational inference relatively well known scaling inference bayesian probabilistic models related methods also offer ways circumnavigate approximation analytically intractable expectations key challenge either setting controlling variance gradient estimates recent work shown continuous latent variables particularly multivariate gaussians achieved using gradient log posterior paper apply idea gamma distributed latent variables given gamma variational distributions enabling straightforward black box variational inference models sparsity nonnegativity appropriate demonstrate method recently proposed gamma process model network data well novel sparse factor analysis outperform generic sampling algorithms approach using gaussian variational distributions transformed variables\",\"automatic chemical design framework generating novel molecules optimized properties original scheme featuring bayesian optimization latent space variational autoencoder suffers pathology tends produce invalid molecular structures first demonstrate empirically pathology arises bayesian optimization scheme queries latent points far away data variational autoencoder trained secondly reformulating search procedure constrained bayesian optimization problem show effects pathology mitigated yielding marked improvements validity generated molecules posit constrained bayesian optimization good approach solving class training set mismatch many generative tasks involving bayesian optimization latent space variational autoencoder\",\"present approach deep estimation discrete conditional probability distributions models several applications including generative modeling audio image video data approach combines two main techniques dyadic partitioning graphbased smoothing discrete space recursively decomposing dimension series binary splits smoothing resulting distribution using graphbased trend filtering impose strict structure model achieve much higher sample efficiency demonstrate advantages model series benchmarks synthetic realworld datasets cases reducing error nearly half comparison popular methods literature models implemented tensorflow publicly available httpsgithubcomtanseysdp\",\"recurring problem building probabilistic latent variable models regularization model selection instance choice dimensionality latent space context belief networks latent variables problem adressed automatic relevance determination ard employing monte carlo inference present variational inference approach ard deep generative models using doubly stochastic variational inference provide fast scalable learning show empirical results standard dataset illustrating effects contracting latent space automatically show resulting latent representations significantly compact without loss expressive power learned models\",\"meanfield variational methods widely used approximate posterior inference many probabilistic models typical application meanfield methods approximately compute posterior coordinateascent optimization algorithm model conditionally conjugate coordinate updates easily derived closed form however many models interestlike correlated topic model bayesian logistic regressionare nonconjuate models meanfield methods cannot directly applied practitioners develop variational algorithms casebycase basis paper develop two generic methods nonconjugate models laplace variational inference delta method variational inference methods several advantages allow easily derived variational algorithms wide class nonconjugate models extend unify existing algorithms derived specific models work well realworld datasets studied methods correlated topic model bayesian logistic regression hierarchical bayesian logistic regression\",\"stochastic variational inference collapsed models recently successfully applied large scale topic modelling paper propose stochastic collapsed variational inference algorithm sequential data setting algorithm applicable finite hidden markov models hierarchical dirichlet process hidden markov models datasets generated emission distributions exponential family experiment results two discrete datasets show inference efficient accurate uncollapsed version stochastic variational inference\",\"introduce overdispersed blackbox variational inference method reduce variance monte carlo estimator gradient blackbox variational inference instead taking samples variational distribution use importance sampling take samples overdispersed distribution exponential family variational approximation approach general since readily applied exponential family distribution typical choice variational approximation run experiments two nonconjugate probabilistic models show method effectively reduces variance overhead introduced computation proposal parameters importance weights negligible find overdispersed importance sampling scheme provides lower variance blackbox variational inference even latter uses twice number samples results faster convergence blackbox inference procedure\",\"variational inference scalable technique approximate bayesian inference deriving variational inference algorithms requires tedious modelspecific calculations makes difficult automate propose automatic variational inference algorithm automatic differentiation variational inference advi user provides bayesian model dataset nothing else make conjugacy assumptions support broad class models algorithm automatically determines appropriate variational family optimizes variational objective implement advi stan code available probabilistic programming framework compare advi mcmc sampling across hierarchical generalized linear models nonconjugate matrix factorization mixture model train mixture model quarter million images advi use variational inference model write stan\",\"deep belief networks powerful way model complex probability distributions however learning structure belief network particularly one hidden units difficult indian buffet process used nonparametric bayesian prior directed structure belief network single infinitely wide hidden layer paper introduce cascading indian buffet process cibp provides nonparametric prior structure layered directed belief network unbounded depth width yet allows tractable inference use cibp prior nonlinear gaussian belief network unit additionally vary behavior discrete continuous representations provide markov chain monte carlo algorithms inference belief networks explore structures learned several image data sets\",\"two fundamental problems unsupervised learning efficient inference latentvariable models robust density estimation based large amounts unlabeled data algorithms two tasks normalizing flows generative adversarial networks gans often developed independently paper propose concept continuoustime flows ctfs family diffusionbased methods able asymptotically approach target distribution distinct normalizing flows gans ctfs adopted achieve two goals one framework theoretical guarantees framework includes distilling knowledge ctf efficient inference learning explicit energybased distribution ctfs density estimation tasks rely new technique distribution matching within amortized learning experiments various tasks demonstrate promising performance proposed ctf framework compared related techniques\",\"quantitatively assessing relationships latent variables observed variables important understanding developing generative models representation learning paper propose latentobserved dissimilarity lod evaluate dissimilarity probabilistic characteristics latent observed variables also define four essential types generative models different independenceconditional independence configurations experiments using tractable realworld data show lod effectively capture differences models reflect capability higher layer learning also show conditional independence latent variables given observed variables contributes improving transmission information characteristics lower layers higher layers\",\"variational inference approximates posterior distribution probabilistic model parameterized density maximizing lower bound model evidence modern solutions fit flexible approximation stochastic gradient descent using monte carlo approximation gradients enables variational inference arbitrary differentiable probabilistic models consequently makes variational inference feasible probabilistic programming languages work develop efficient inference algorithms task considering importance sampling estimates gradients show gradient respect approximation parameters often evaluated efficiently without needing recompute gradients model proceed derive practical algorithms use importance sampled estimates speed computationwe present importance sampled stochastic gradient descent outperforms standard stochastic gradient descent clear margin range models provide justifiable variant stochastic average gradients variational inference\",\"stochastic variational inference svi stateoftheart algorithm scaling variational inference largedatasets inherently serial moreover requires parameters fit memory single processor problematic number parameters billions paper propose extreme stochastic variational inference esvi asynchronous lockfree algorithm perform variational inference mixture models massive real world datasets esvi overcomes limitations svi requiring processor access subset data subset parameters thus providing data model parallelism simultaneously demonstrate effectiveness esvi running latent dirichlet allocation lda umbcb dataset vocabulary million token size billion experiments found esvi outperforms svi wallclocktime also achieves better quality solution addition propose strategy speed computation save memory fitting large number topics\",\"variational inference lies core many stateoftheart algorithms improve approximation posterior beyond parametric families proposed include mcmc steps variational lower bound work explore idea using steps hamiltonian monte carlo hmc algorithm efficient mcmc method particular incorporate acceptance step hmc algorithm guaranteeing asymptotic convergence true posterior additionally introduce extensions hmc algorithm geared towards faster convergence theoretical advantages modifications reflected performance improvements experimental results\",\"generative adversarial networks gans successful deep generative models gans based twoplayer minimax game however objective function derived original motivation changed obtain stronger gradients learning generator propose novel algorithm repeats density ratio estimation fdivergence minimization algorithm offers new perspective toward understanding gans able make use multiple viewpoints obtained research density ratio estimation divergence stable relative density ratio useful\",\"paper generalizes beta divergence beyond classical form associated power variance functions tweedie models generalized form represented compact definite integral function variance function exponential dispersion model compact integral form simplifies derivations many properties scaling translation expectation beta divergence show beta divergence half statistical deviance equivalent measures\",\"learning deep models using bayesian methods generated significant attention recently largely feasibility modern bayesian methods yield scalable learning inference maintaining measure uncertainty model parameters stochastic gradient mcmc algorithms sgmcmc family diffusionbased sampling methods largescale bayesian learning sgmcmc multivariate stochastic gradient thermostats msgnht augment parameter interest momentum thermostat variable maintain stationary distributions target posterior distributions number variables continuoustime diffusion increases numerical approximation error becomes practical bottleneck better use numerical integrator desirable end propose use efficient symmetric splitting integrator msgnht instead traditional euler integrator demonstrate proposed scheme accurate robust converges faster properties demonstrated desirable bayesian deep learning extensive experiments two canonical models deep extensions demonstrate proposed scheme improves general bayesian posterior sampling particularly deep models\",\"stochastic variational inference collapsed models recently successfully applied large scale topic modelling paper propose stochastic collapsed variational inference algorithm hidden markov models sequential data setting given collapsed hidden markov model break long markov chain set short subchains propose novel sumproduct algorithm update posteriors subchains taking account boundary transitions due sequential dependencies experiments two discrete datasets show collapsed algorithm scalable large datasets memory efficient significantly accurate existing uncollapsed algorithm\",\"study unsupervised generative modeling terms optimal transport problem true unknown data distribution latent variable model distribution show problem equivalently written terms probabilistic encoders constrained match posterior prior distributions latent space relaxed constrained optimization problem leads penalized optimal transport pot objective efficiently minimized using stochastic gradient descent sampling show pot wasserstein distance coincides objective heuristically employed adversarial autoencoders aae makhzani provides first theoretical justification aaes known authors also compare pot popular techniques like variational autoencoders vae kingma welling theoretical results include better understanding commonly observed blurriness images generated vaes establishing duality wasserstein gan arjovsky bottou pot wasserstein distance\",\"computing partition function important statistical inference task arising applications graphical models since computationally intractable approximate methods used resolve issue practice meanfield belief propagation arguably popular successful approaches variational type paper propose two new variational schemes coined gaugedmf gmf gaugedbp gbp improving respectively provide lower bounds partition function utilizing socalled gauge transformation modifies factors keeping partition function invariant moreover prove gmf gbp exact gms single loop special structure even though bare perform badly case extensive experiments complete gms relatively small size large upto variables confirm newly proposed algorithms outperform generalize\",\"short article revisits ideas introduced arxiv arxiv simple setup sheds lights connexions variational autoencoders vae generative adversarial networks gan minimum kantorovitch estimators mke\",\"deep generative models provide systematic way learn nonlinear data distributions set latent variables nonlinear generator function maps latent points input space nonlinearity generator imply latent space gives distorted view input space mild conditions show distortion characterized stochastic riemannian metric demonstrate distances interpolants significantly improved metric turn improves probability distributions sampling algorithms clustering latent space geometric analysis reveals current generators provide poor variance estimates propose new generator architecture vastly improved variance estimates results demonstrated convolutional fully connected variational autoencoders formalism easily generalize deep generative models\",\"propose general modeling inference framework composes probabilistic graphical models deep learning methods combines respective strengths model family augments graphical structure latent variables neural network observation models inference extend variational autoencoders use graphical model approximating distributions recognition networks output conjugate potentials components models learned simultaneously single objective giving scalable algorithm leverages stochastic variational inference natural gradients graphical model message passing reparameterization trick illustrate framework several example models application mouse behavioral phenotyping\",\"introduce new approach amortizing inference directed graphical models learning heuristic approximations stochastic inverses designed specifically use proposal distributions sequential monte carlo methods describe procedure constructing learning structured neural network represents inverse factorization graphical model resulting conditional density estimator takes input particular values observed random variables returns approximation distribution latent variables recognition model learned offline independent particular dataset prior performing inference output networks used automaticallylearned highquality proposal distributions accelerate sequential monte carlo across diverse range problem settings\",\"derive novel variational expectation maximization approach based truncated posterior distributions truncated distributions proportional exact posteriors within subsets discrete state space equal zero otherwise treatment distributions subsets variational parameters distinguishes approach previous variational approaches specific structure truncated distributions allows deriving novel mathematically grounded results turn used formulate novel efficient algorithms optimize parameters probabilistic generative models centrally find variational lower bounds correspond truncated distributions given concise efficiently computable expressions update equations model parameters remain standard form based findings show efficient easily applicable metaalgorithms formulated guarantee monotonic increase variational bound example applications derived framework provide novel theoretical results learning procedures latent variable models well mixture models furthermore show truncated variation naturally interpolates standard full posteriors based maximum aposteriori state map approach therefore regarded generalization popular hard approach towards similarly efficient method capture true posterior structure\",\"variational inference algorithms proven successful bayesian analysis large data settings recent advances using stochastic variational inference svi however methods largely studied independent exchangeable data settings develop svi algorithm learn parameters hidden markov models hmms timedependent data setting challenge applying stochastic optimization setting arises dependencies chain must broken consider minibatches observations propose algorithm harnesses memory decay chain adaptively bound errors arising edge effects demonstrate effectiveness algorithm synthetic experiments large genomics dataset batch algorithm computationally infeasible\",\"mean field variational bayes mfvb popular posterior approximation method due fast runtime largescale data sets however well known major failing mfvb underestimates uncertainty model variables sometimes severely provides information model variable covariance generalize linear response methods statistical physics deliver accurate uncertainty estimates model variablesboth individual variables coherently across variables call method linear response variational bayes lrvb mfvb posterior approximation exponential family lrvb simple analytic form even nonconjugate models indeed make assumptions form true posterior demonstrate accuracy scalability method range models simulated real data\",\"deep generative models provide powerful tools distributions complicated manifolds natural images many methods including generative adversarial networks gans difficult train part prone mode collapse means characterize modes true distribution address introduce veegan features reconstructor network reversing action generator mapping data noise training objective retains original asymptotic consistency guarantee gans interpreted novel autoencoder loss noise sharp contrast traditional autoencoder data points veegan require specifying loss function data rather representations standard normal assumption extensive set synthetic real world image datasets veegan indeed resists mode collapsing far greater extent recent gan variants produces realistic samples\",\"stein variational gradient descent svgd recently proposed particlebased bayesian inference method attracted lot interest due remarkable approximation ability particle efficiency compared traditional variational inference markov chain monte carlo methods however observed particles svgd tend collapse modes target distribution particle degeneracy phenomenon becomes severe higher dimensions theoretical analysis finds exists negative correlation dimensionality repulsive force svgd blamed phenomenon propose message passing svgd mpsvgd solve problem leveraging conditional independence structure probabilistic graphical models pgms mpsvgd converts original highdimensional global inference problem set local ones markov blanket lower dimensions experimental results show advantages preventing vanishing repulsive force highdimensional space svgd particle efficiency approximation flexibility inference methods graphical models\",\"report derive nonnegative series expansion jensenshannon divergence jsd two probability distributions series expansion shown useful numerical calculations jsd probability distributions nearly equal consequently small numerical errors dominate evaluation\",\"partition functions probability distributions important quantities model evaluation comparisons present new method compute partition functions complex multimodal distributions distributions often sampled using simulated tempering augments target space auxiliary inverse temperature variable method exploits multinomial probability law inverse temperatures provides estimates partition function terms simple quotient raoblackwellized marginal inverse temperature probability estimates updated sampling show method interesting connections several alternative popular methods offers significant advantages particular empirically find new method provides accurate estimates annealed importance sampling calculating partition functions large restricted boltzmann machines rbm moreover method sufficiently accurate track training validation loglikelihoods learning rbms minimal computational cost\",\"probabilistic graphical models key tool machine learning applications computing partition function normalizing constant fundamental task statistical inference generally computationally intractable leading extensive study approximation methods iterative variational methods popular successful family approaches however even state art variational methods return poor results fail converge difficult instances paper instead consider computing partition function via sequential summation variables develop robust approximate algorithms combining ideas minibucket elimination tensor network renormalization group methods statistical physics resulting convergencefree methods show good empirical performance synthetic realworld benchmark models even difficult instances\",\"discrete latent space models recently achieved performance par continuous counterparts deep variational inference still face various implementation challenges models offer opportunity better interpretation latent spaces well direct representation naturally discrete phenomena recent approaches propose train separately highdimensional prior models discrete latent data challenging task paper introduce latent data model discrete state markov chain allows fast endtoend training performance generative model assessed building management dataset publicly available electricity transformer dataset\",\"blackbox alpha bbalpha new approximate inference method based minimization alphadivergences bbalpha scales large datasets implemented using stochastic gradient descent bbalpha applied complex probabilistic models little effort since requires input likelihood function gradients gradients easily obtained using automatic differentiation changing divergence parameter alpha method able interpolate variational bayes alpha rightarrow algorithm similar expectation propagation alpha experiments probit regression neural network regression classification problems show bbalpha nonstandard settings alpha alpha usually produces better predictions alpha rightarrow alpha\",\"approximate bayesian computation abc framework performing likelihoodfree posterior inference simulation models stochastic variational inference svi appealing alternative inefficient sampling approaches commonly used abc however svi highly sensitive variance gradient estimators problem exacerbated approximating likelihood draw upon recent advances variance reduction likelihoodfree inference using deterministic simulations produce low variance gradient estimators variational lowerbound exploiting automatic differentiation libraries avoid nearly modelspecific derivations demonstrate performance three problems compare existing svi algorithms results demonstrate correctness efficiency algorithm\",\"note compares two recently published machine learning methods constructing flexible tractable families variational hiddenvariable posteriors first method called hierarchical variational models enriches inference model extra variable called auxiliary deep generative models enriches generative model instead conclude two methods mathematically equivalent\",\"datasets growing size complexity creating demand rich models quantification uncertainty bayesian methods excellent fit demand scaling bayesian inference challenge response challenge considerable recent work based varying assumptions model structure underlying computational resources importance asymptotic correctness result zoo ideas clear overarching principles paper seek identify unifying principles patterns intuitions scaling bayesian inference review existing work utilizing modern computing resources mcmc variational approximation techniques taxonomy ideas characterize general principles proven successful designing scalable inference procedures comment path forward\",\"empirically evaluate stochastic annealing strategy bayesian posterior optimization variational inference variational inference deterministic approach approximate posterior inference bayesian models typically nonconvex objective function locally optimized parameters approximating distribution investigate annealing method optimizing objective aim finding better local optimal solution compare deterministic annealing methods annealing show stochastic annealing provide clear improvement gmm hmm performance lda tends favor deterministic annealing methods\",\"deep generative models wildly successful learning coherent latent representations continuous data video audio however generative modeling discrete data arithmetic expressions molecular structures still poses significant challenges crucially stateoftheart methods often produce outputs valid make key observation frequently discrete data represented parse tree contextfree grammar propose variational autoencoder encodes decodes directly parse trees ensuring generated outputs always valid surprisingly show model often generate valid outputs also learns coherent latent space nearby points decode similar discrete outputs demonstrate effectiveness learned models showing improved performance bayesian optimization symbolic regression molecular synthesis\",\"variational inference provides powerful tool approximate probabilistic ference complex structured models typical variational inference methods however require use inference networks computationally tractable proba bility density functions largely limits design implementation vari ational inference methods consider wild variational inference methods require tractable density functions inference networks hence applied challenging cases example application treat stochastic gradient langevin dynamics sgld inference network use methods automatically adjust step sizes sgld yielding significant improvement handdesigned step size schemes\",\"propose simple algorithm train stochastic neural networks draw samples given target distributions probabilistic inference method based iteratively adjusting neural network parameters output changes along stein variational gradient direction liu wang maximally decreases divergence target distribution method works target distribution specified unnormalized density function train blackbox architectures differentiable terms parameters want adapt demonstrate method number applications including variational autoencoder vae expressive encoders model complex latent space structures hyperparameter learning mcmc samplers allows bayesian inference adaptively improve seeing data\",\"present novel model architecture leverages deep learning tools perform exact bayesian inference sets high dimensional complex observations model provably exchangeable meaning joint distribution observations invariant permutation property lies heart bayesian inference model require variational approximations train new samples generated conditional previous samples cost linear size conditioning set advantages architecture demonstrated learning tasks require generalisation short observed sequences modelling sequence variability conditional image generation fewshot learning anomaly detection\",\"introduce incremental variational inference apply latent dirichlet allocation lda incremental variational inference inspired incremental provides alternative stochastic variational inference incremental lda process massive document collections require set learning rate converges faster local optimum variational bound enjoys attractive property monotonically increasing study performance incremental lda large benchmark data sets introduce stochastic approximation incremental variational inference extends asynchronous distributed setting resulting distributed algorithm achieves comparable performance single host incremental variational inference significant speedup\",\"highlight pitfall applying stochastic variational inference general bayesian networks global random variables approximated exponential family distribution natural gradient steps commonly starting unit length step size averaged convergence useful insight scaling initial step sizes lost approximation factorizes across general bayesian network care must taken ensure practical convergence experimentally investigate much baby wellscaled steps thrown bath water exact gradients\",\"vector quantizedvariational autoencoders vqvae generative models based discrete latent representations data inputs mapped finite set learned embeddingsto generate new samples autoregressive prior distribution discrete states must trained separately prior generally complex leads slow generation work propose new model train prior encoderdecoder networks simultaneously build diffusion bridge continuous coded vector noninformative prior distribution latent discrete states given random functions continuous vectors show model competitive autoregressive prior miniimagenet cifar dataset efficient optimization sampling framework also extends standard vqvae enables endtoend training\",\"measuring divergence two distributions essential machine learning statistics various applications including binary classification change point detection twosample test furthermore era big data designing divergence measure interpretable handle highdimensional complex data becomes extremely important paper propose post selection inference psi framework divergence measure select set statistically significant features discriminate two distributions specifically employ additive variant maximum mean discrepancy mmd features introduce general hypothesis test psi novel mmd estimator using incomplete ustatistics asymptotically normal distribution mild assumptions gives high detection power psi also proposed analyzed theoretically synthetic realworld feature selection experiments show proposed framework successfully detect statistically significant features last propose sample selection framework analyzing different members generative adversarial networks gans family\",\"robustness outliers central issue realworld machine learning applications replacing model heavytailed one gaussian studentt standard approach robustification applied simple models paper based zellners optimization variational formulation bayesian inference propose outlierrobust pseudobayesian variational method replacing kullbackleibler divergence used data fitting robust divergence beta gammadivergences advantage approach superior complex models deep networks also handled theoretically prove deep networks relu activation functions emphinfluence function proposed method bounded unbounded ordinary variational inference implies proposed method robust input output outliers ordinary variational method experimentally demonstrate robust variational method outperforms ordinary variational inference regression classification deep networks\",\"standard interpretation importanceweighted autoencoders maximize tighter lower bound marginal likelihood standard evidence lower bound give alternate interpretation procedure optimizes standard variational lower bound using complex distribution formally derive result present tighter lower bound visualize implicit importanceweighted distribution\",\"recent efforts combining deep models probabilistic graphical models promising providing flexible models also easy interpret propose variational messagepassing algorithm variational inference models make three contributions first propose structured inference networks incorporate structure graphical model inference network variational autoencoders vae second establish conditions inference networks enable fast amortized inference similar vae finally derive variational message passing algorithm perform efficient naturalgradient inference retaining efficiency amortized inference simultaneously enabling structured amortized naturalgradient inference deep structured models method simplifies generalizes existing methods\",\"goal twosample tests assess whether two samples sim sim drawn distribution perhaps intriguingly one relatively unexplored method build twosample tests use binary classifiers particular construct dataset pairing examples positive label pairing examples negative label null hypothesis true classification accuracy binary classifier heldout subset dataset remain near chancelevel show classifier twosample tests cst learn suitable representation data fly return test statistics interpretable units simple null distribution predictive uncertainty allow interpret differ goal paper establish properties performance uses cst first analyze main theoretical properties second compare performance variety stateoftheart alternatives third propose use evaluate sample quality generative models intractable likelihoods generative adversarial networks gans fourth showcase novel application gans together cst causal discovery\",\"present derivation kullback leibler kldivergence also known relative entropy von mises fisher vmf distribution ddimensions\",\"stochastic variational inference svi paradigm combines variational inference natural gradients stochastic updates recently proposed largescale data analysis conjugate bayesian models demonstrated effective several problems paper studies family bayesian latent variable models two levels hidden variables without conjugacy requirements making several contributions context first observing svi improved structured variational approximation applicable general conditions previously thought requirement approximating variational distribution family prior resulting approach monte carlo structured svi mcssvi significantly extends scope svi enabling largescale learning nonconjugate models models latent gaussian variables propose hybrid algorithm using standard natural gradients shown improve stability convergence applications mixed effects models sparse gaussian processes probabilistic matrix factorization correlated topic models demonstrate generality approach advantages proposed algorithms\",\"propose secondorder hessian hessianfree based optimization method variational inference inspired gaussian backpropagation argue quasinewton optimization developed well accomplished generalizing gradient computation stochastic backpropagation via reparametrization trick lower complexity illustrative example apply approach problems bayesian logistic regression variational autoencoder vae additionally compute bounds estimator variance intractable expectations family lipschitz continuous function method practical scalable model free demonstrate method several realworld datasets provide comparisons stochastic gradient methods show substantial enhancement convergence rates\",\"computing partition function discrete graphical model fundamental inference challenge since computationally intractable variational approximations often used practice recently socalled gauge transformations used improve variational lower bounds paper propose new gaugevariational approach termed wmbeg combines gauge transformations weighted minibucket elimination wmbe method wmbeg provide upper lower bounds easier optimize prior gaugevariational algorithm show wmbeg strictly improves earlier wmbe approximation symmetric models including ising models magnetic field experimental results demonstrate effectiveness wmbeg even generic nonsymmetric models\",\"study mixtures factorizing probability distributions represented visible marginal distributions stochastic layered networks take perspective kernel transitions distributions gives unified picture distributed representations arising deep belief networks dbn networks without lateral connections describe combinatorial geometric properties set kernels products kernels realizable dbns network parameters vary describe explicit classes probability distributions including exponential families learned dbns use submodels bound maximal expected kullbackleibler approximation errors dbns depending number hidden layers units contain\",\"consider problem joint modelling metabolic signals gene expression systems biology applications propose approach based inputoutput factorial hidden markov models propose structured variational inference approach infer structure states model start classical free form structured variational mean field approach use expectation propagation approximate expectations needed variational loop show corresponds factored expectation constrained approximate inference validate model extensive simulations demonstrate applicability real world bacterial data set\",\"generative adversarial networks gans become widely popular framework generative modelling highdimensional datasets however training wellknown difficult work presents rigorous statistical analysis gans providing straightforward explanations common training pathologies vanishing gradients furthermore proposes new training objective kernel gans demonstrates practical effectiveness largescale realworld data sets key element analysis distinction training respect unknown data distribution empirical counterpart overcome issues gan training pursue idea smoothing jensenshannon divergence jsd incorporating noise input distributions discriminator show effectively leads empirical version jsd true generator densities replaced kernel density estimates leads kernel gans\",\"introduce local expectation gradients general purpose stochastic variational inference algorithm constructing stochastic gradients sampling variational distribution algorithm divides problem estimating stochastic gradients multiple variational parameters smaller subtasks subtask exploits intelligently information coming relevant part variational distribution achieved performing exact expectation single random variable mostly correlates variational parameter interest resulting raoblackwellized estimate low variance work efficiently continuous discrete random variables furthermore proposed algorithm interesting similarities gibbs sampling time unlike gibbs sampling trivially parallelized\",\"introduce novel generative formulation deep probabilistic models implementing soft constraints function dynamics particular develop flexible methodological framework modeled functions derivatives given order subject inequality equality constraints characterize posterior distribution model constraint parameters stochastic variational inference result proposed approach allows accurate scalable uncertainty quantification predictions parameters demonstrate application equality constraints challenging problem parameter inference ordinary differential equation models showcase application inequality constraints problem monotonic regression count data proposed approach extensively tested several experimental settings leading highly competitive results challenging modeling applications offering high expressiveness flexibility scalability\",\"propose new framework hamiltonian monte carlo hmc truncated probability distributions smooth underlying density functions traditional hmc requires computing gradient potential function associated target distribution therefore perform full power truncated distributions due lack continuity differentiability framework introduce sharp sigmoid factor density function approximate probability drop truncation boundary target potential function approximated new potential smoothly extends entire sample space hmc performed approximate potential method easy implement applies wide range problems also achieves comparable computational efficiency various sampling tasks compared baseline methods rbhmc also gives rise new approach bayesian inference constrained spaces\",\"stein variational gradient descent svgd deterministic sampling algorithm iteratively transports set particles approximate given distributions based efficient gradientbased update guarantees optimally decrease divergence within function space paper develops first theoretical analysis svgd discussing weak convergence properties showing asymptotic behavior captured gradient flow divergence functional new metric structure induced stein operator also provide number results stein operator steins identity using notion weak derivative including new proof distinguishability stein discrepancy weak conditions\",\"probability distributions produced crossentropy loss ordinal classification problems possess undesired properties propose straightforward technique constrain discrete ordinal probability distributions unimodal via use poisson binomial probability distributions evaluate approach context deep learning two large ordinal image datasets obtaining promising results\",\"propose novel algorithm solve expectation propagation relaxation bayesian inference continuousvariable graphical models contrast previous algorithms method provably convergent marrying convergent ideas opperwinther covariance decoupling techniques wipfnagarajan nickischseeger runs least order magnitude faster commonly used solver\",\"deep gaussian processes provide flexible approach probabilistic modelling data using either supervised unsupervised learning tractable inference approximations marginal likelihood model must made original approach approximate inference models used variational compression allow approximate variational marginalization hidden variables leading lower bound marginal likelihood model damianou lawrence paper extend idea nested variational compression resulting lower bound likelihood easily parallelized adapted stochastic variational inference\",\"build autoencoding sequential monte carlo aesmc method model proposal learning based maximizing lower bound log marginal likelihood broad family structured probabilistic models approach relies efficiency sequential monte carlo smc performing inference structured probabilistic models flexibility deep neural networks model complex conditional probability distributions develop additional theoretical insights introduce new training procedure improves model proposal learning demonstrate approach provides fast easytoimplement scalable means simultaneous model learning proposal adaptation deep generative models\",\"introduce new algorithm approximate inference combines reparametrization markov chain monte carlo variational methods construct flexible implicit variational distribution synthesized arbitrary markov chain monte carlo operation deterministic transformation optimized using reparametrization trick unlike current methods implicit variational inference method avoids computation log density ratios therefore easily applicable arbitrary continuous differentiable models demonstrate proposed algorithm fitting bananashaped distributions training variational autoencoders\",\"matrix generalized inverse gaussian mathcalmgig distribution arises naturally settings distribution symmetric positive semidefinite matrices certain key properties distribution effective ways sampling distribution carefully studied paper show mathcalmgig unimodal mode obtained solving algebraic riccati equation equation based property propose importance sampling method mathcalmgig mode proposal distribution matches target proposed sampling method efficient existing approaches use proposal distributions may mode far mathcalmgigs mode illustrate posterior distribution latent factor models probabilistic matrix factorization pmf marginalized one latent factor mathcalmgig distribution characterization leads novel collapsed monte carlo cmc inference algorithm latent factor models illustrate cmc lower log loss perplexity mcmc needs fewer samples\",\"investigate optimization two probabilistic generative models binary latent variables using novel variational approach approach distinguishes previous variational approaches using latent states variational parameters use efficient general purpose sampling procedures vary latent states investigate black box applicability resulting optimization procedure general purpose applicability samples drawn approximate marginal distributions considered generative model well models prior distribution variational sampling defined generic form directly executable given model proof concept apply novel procedure binary sparse coding model continuous observables basic sigmoid belief networks models binary observables numerical experiments verify investigated approach efficiently well effectively increases variational free energy objective without requiring additional analytical steps\",\"compute expected value kullbackleibler divergence various fundamental statistical models respect canonical priors probability simplex obtain closed formulas expected model approximation errors depending dimension models cardinalities sample spaces uniform prior expected divergence model containing uniform distribution bounded constant gamma models consider bound approached state space large models dimension grow fast dirichlet priors expected divergence bounded similar way concentration parameters take reasonable values results serve reference values complicated statistical models\",\"article describe model derivation implementation variational bayesian inference linear logistic regression without automatic relevance determination dual function acting tutorial derivation variational bayesian inference simple models well documenting providing brief examples matlaboctave functions implement inference functions freely available online\",\"two recently introduced criteria estimation generative models based reduction binary classification noisecontrastive estimation nce estimation procedure generative model trained able distinguish data samples noise samples generative adversarial networks gans pairs generator discriminator networks generator network learning generate samples attempting fool discriminator network believing samples real data estimation procedures use function drive learning naturally raises questions related well whether function related maximum likelihood estimation mle nce corresponds training internal data model belonging discriminator network using fixed generator network show variant nce dynamic generator network equivalent maximum likelihood estimation since pairing learned discriminator appropriate dynamically selected generator recovers mle one might expect reverse hold pairing learned generator certain discriminator however show recovering mle learned generator requires departing distinguishability game specifically expected gradient nce discriminator made match expected gradient mle one allowed use nonstationary noise distribution nce choice discriminator network make expected gradient gan generator match mle iii existing theory guarantee gans converge nonconvex case suggests key next step gan research determine whether gans converge modify training algorithm force convergence\",\"paper raises implicit manifold learning perspective generative adversarial networks gans studying support learned distribution modelled submanifold mathcalmtheta perfectly match mathcalmr support real data distribution show optimizing jensenshannon divergence forces mathcalmtheta perfectly match mathcalmr optimizing wasserstein distance hand comparing gradients jensenshannon divergence wasserstein distances primal forms conjecture wasserstein may enjoy desirable properties reduced mode collapse therefore interesting design new distances inherit best distances\",\"develop riemannian stein variational gradient descent rsvgd bayesian inference method generalizes stein variational gradient descent svgd riemann manifold benefits twofolds inference tasks euclidean spaces rsvgd advantage svgd utilizing information geometry inference tasks riemann manifolds rsvgd brings unique advantages svgd riemannian world appropriately transfer riemann manifolds conceive novel nontrivial techniques rsvgd required intrinsically different characteristics general riemann manifolds euclidean spaces also discover riemannian steins identity riemannian kernelized stein discrepancy experimental results show advantages svgd exploring distribution geometry advantages particleefficiency iterationeffectiveness approximation flexibility inference methods riemann manifolds\",\"extend stochastic gradient variational bayes perform posterior inference weights stickbreaking processes development allows define stickbreaking variational autoencoder sbvae bayesian nonparametric version variational autoencoder latent representation stochastic dimensionality experimentally demonstrate sbvae semisupervised variant learn highly discriminative latent representations often outperform gaussian vaes\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"8_variational_inference_models\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"8_variational_inference_models\"],\"textfont\":{\"size\":12},\"x\":[13.428562,13.538414,10.75092,13.491556,13.460528,13.431738,13.554899,13.529697,13.408148,12.924703,13.374367,13.598381,13.83046,13.560434,13.522304,12.746477,13.506679,13.325251,13.471496,13.670371,13.544817,13.539733,13.816535,12.749251,13.782421,13.451003,13.601239,13.540899,13.42975,13.23735,13.024005,13.591046,13.622438,13.56998,13.514521,13.127298,13.82068,13.113593,13.588422,13.375394,13.595946,13.586626,13.42699,13.347982,13.571535,13.389852,13.501189,13.285917,12.872828,13.736303,13.538459,13.449876,13.550857,13.246535,13.346919,13.339728,13.569899,13.849807,13.5945,13.385879,13.600771,12.795704,13.606766,13.558858,13.57804,13.329677,13.321031,13.101575,12.843909,10.668662,13.32551,13.129113,13.471475,12.558034,13.506874,13.816982,13.615224,13.5655985,13.559485,13.1079035,13.388009,13.355588],\"y\":[10.043161,9.370605,8.811371,9.355078,9.51096,9.270118,9.2637005,9.3545885,9.993805,9.957377,9.978551,9.2878895,8.910829,9.365813,9.420319,10.066843,10.3023405,9.975076,9.472963,9.120257,9.264966,10.373871,8.662636,9.975205,8.901143,10.150012,9.395709,10.364419,10.087774,9.812647,9.731756,9.265161,9.073038,9.288514,10.348871,9.166857,8.64904,9.100897,9.412062,10.003472,9.367209,9.315219,9.650481,9.208854,9.269435,10.031275,9.47555,9.803847,9.970087,9.078195,9.350623,10.065232,10.403964,9.445162,9.709213,9.735373,10.451994,8.607654,9.306174,9.439038,9.428842,10.0362,8.933498,10.40009,9.349061,9.479152,9.0524645,9.14386,9.974198,3.3114088,9.6894,9.831916,9.503154,8.823095,9.509168,8.660896,9.252631,10.43479,10.400797,9.150096,9.795734,9.477836],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"regression trees becoming increasingly popular omnibus predicting tools basis numerous modern statistical learning ensembles part popularity ability create regression prediction without ever specifying structure mean model however method implicitly assumes homogeneous variance across entire explanatoryvariable space unknown algorithm behaves faced heteroscedastic data study assess performance popular regressiontree algorithm singlevariable setting simple stepfunction model heteroscedasticity use simulation show locations splits hence ability accurately predict means adversely influenced change variance identify pruning algorithm main concern although effects splitting algorithm may meaningful applications\",\"propose original model inferring team strengths using markov random field used generate historical estimates offensive defensive strengths team time model designed applied sports soccer hockey contest outcomes take value limited discrete space perform inference using combination expectation maximization loopy belief propagation challenges working nonconvex optimization problem highdimensional parameter space discussed performance model demonstrated professional soccer data english premier league\",\"random forest missing data algorithms attractive approach dealing missing data desirable properties able handle mixed types missing data adaptive interactions nonlinearity potential scale big data settings currently many different imputation algorithms relatively little guidance efficacy motivated study performance using large diverse collection data sets performance various algorithms assessed different missing data mechanisms algorithms included proximity imputation fly imputation imputation utilizing multivariate unsupervised supervised splittingthe latter class representing generalization new promising imputation algorithm called missforest performance algorithms assessed ability impute data accurately findings reveal imputation generally robust performance improving increasing correlation performance good moderate high missingness even certain cases data missing random\",\"emergence mobile games caused paradigm shift videogame industry game developers disposal plethora information players thus take advantage reliable models accurately predict player behavior scale huge datasets churn prediction challenge common variety sectors particularly relevant mobile game industry player retention crucial successful monetization game article present approach predicting game abandon based survival ensembles method provides accurate predictions level player leave game accumulated playtime moment robust different data distributions applicable wide range response variables also allowing efficient parallelization algorithm makes model well suited perform realtime analyses churners even games millions daily active users\",\"aim create framework transfer learning using latent factor models learn dependence structure larger source dataset target dataset methodology motivated goal building riskassessment model surgery patients using institutional national surgical outcomes data national surgical outcomes data collected nsqip national surgery quality improvement program database housing almost million patients different hospitals build latent factor model hierarchical prior loadings matrix appropriately account different covariance structure data extend model handle complex relationships populations deriving scale mixture formulation using stickbreaking properties model provides transfer learning framework utilizes information source target data modeling underlying inherent differences\",\"study logistic modelbased active learning procedure binary classification problems adopt batch subject selection strategy modified sequential experimental design method moreover accompanying proposed subject selection scheme simultaneously conduct greedy variable selection procedure update classification model labeled training subjects proposed algorithm repeatedly performs subject variable selection steps prefixed stopping criterion reached numerical results show proposed procedure competitive performance smaller training size compact model comparing classifier trained variables full data set also apply proposed procedure wellknown wave data set breiman confirm performance method\",\"methods transfer learning try combine knowledge several related tasks domains improve performance test task inspired causal methodology relax usual covariate shift assumption assume holds true subset predictor variables conditional distribution target variable given subset predictors invariant tasks show assumption motivated ideas field causality focus problem domain generalization examples test task observed prove adversarial setting using subset prediction optimal domain generalization provide examples tasks sufficiently diverse estimator therefore outperforms pooling data even average examples test task available also provide method transfer knowledge training tasks exploit available features prediction however provide guarantees method introduce practical method allows automatic inference subset provide corresponding code present results synthetic data sets gene deletion data set\",\"accurate predictions typically obtained learning machines complex feature spaces induced kernels unfortunately decision rules hardly accessible humans cannot easily used gain insights application domain therefore one often resorts linear models combination variable selection thereby sacrificing predictive power presumptive interpretability introduce feature importance ranking measure firm retrospective analysis arbitrary learning machines allows achieve excellent predictive performance superior interpretation contrast standard raw feature weighting firm takes underlying correlation structure features account thereby able discover relevant features even appearance training data entirely prevented noise desirable properties firm investigated analytically illustrated simulations\",\"machinelearned models often described black boxes many realworld applications however models may sacrifice predictive power favour humaninterpretability case feature engineering becomes crucial task requires significant timeconsuming human effort whilst features inherently static representing properties cannot influenced age individual others capture characteristics could adjusted daily amount carbohydrates taken nonetheless model learned data prediction makes new instances irreversible assuming every instance static point located chosen feature space many circumstances however important understand model outputs certain prediction given instance adjustable features instance modified finally iii alter prediction mutated instance input back model paper present technique exploits internals treebased ensemble classifier offer recommendations transforming true negative instances positively predicted ones demonstrate validity approach using online advertising application first design random forest classifier effectively separates two types ads low negative high positive quality ads instances introduce algorithm provides recommendations aim transform low quality negative instance high quality one positive instance finally evaluate approach subset active inventory large network yahoo gemini\",\"rapid growth crowdsourcing platforms become easy relatively inexpensive collect dataset labeled multiple annotators short time however due lack control quality annotators abnormal annotators may affected position bias potentially degrade quality final consensus labels paper introduce statistical framework model detect annotators position bias order control false discovery rate fdr without prior knowledge amount biased annotators expected fraction false discoveries among discoveries high order assure discoveries indeed true replicable key technical development relies new knockoff filters adapted problem new algorithms based inverse scale space dynamics whose discretization potentially suitable large scale crowdsourcing data analysis studies supported experiments simulated examples realworld data proposed framework provides useful tool quantitatively studying annotators abnormal behavior crowdsourcing data arising machine learning sociology computer vision multimedia etc\",\"marketing analytics diverse field academic researchers practitioners coming range backgrounds including marketing expert systems statistics operations research paper provides integrative review boundary areas aim give researchers intelligent expert systems community opportunity gain broad view marketing analytics area provide starting point future interdisciplinary collaboration topics visualization segmentation class prediction featured links disciplines emphasized topics historical overview given starting initial work carrying present day recent innovations modern large complex big data sets described practical implementation advice given along directory open source routines implementing marketing analytics techniques\",\"arrival digital platforms revolutionized occupational health giving possibility occupational health services spsti acquire databases offer professionals new possibilities action however sector activity questioning development multidisciplinarity years arrival new tools sometimes seem quick solution study conducted precursor spsti terms development digital tools aims take stock methods impacts instrumental organizational transformations health professionals well members technical teams spsti question highlighting brakes levers well various possibilities accompaniment consider\",\"paper examines experimental perspective random forests increasingly used statistical method classification regression problems introduced leo breiman first aims confirming known sparse advice using random forests proposing complementary remarks standard problems well high dimensional ones number variables hugely exceeds sample size main contribution paper twofold provide insights behavior variable importance index based random forests addition propose investigate two classical issues variable selection first one find important variables interpretation second one restrictive try design good prediction model strategy involves ranking explanatory variables using random forests score importance stepwise ascending variable introduction strategy\",\"paper propose tackle problem reducing discrepancies multiple domains referred multisource domain adaptation consider target shift assumption domains aim solve classification problem output classes labels proportions differing across problem generally ignored vast majority papers domain adaptation papers nevertheless critical realworld applications theoretically show impact adaptation success address issue design method based optimal transport theory successfully used tackle adaptation problems machine learning method performs multisource adaptation target shift correction simultaneously learning class probabilities unlabeled target sample coupling allowing align two probability distributions experiments synthetic realworld data related satellite image segmentation task show superiority proposed method stateoftheart\",\"reducing user attrition churn broad challenge faced several industries mobile social games decreasing churn decisive increase player retention rise revenues churn prediction models allow understand player loyalty anticipate stop playing game thanks predictions several initiatives taken retain players likely churn survival analysis focuses predicting time occurrence certain event churn case classical methods like regressions could applied players left game challenge arises datasets incomplete churning information players still connect game called censored data problem nature churn censoring commonly dealt survival analysis techniques due inflexibility survival statistical algorithms accuracy achieved often poor contrast novel ensemble learning techniques increasingly popular variety scientific fields provide highclass prediction results work develop first time social games domain survival ensemble model provides comprehensive analysis together accurate prediction churn player predict probability churning function time permits distinguish various levels loyalty profiles additionally assess risk factors explain predicted player survival times results show churn prediction survival ensembles significantly improves accuracy robustness traditional analyses like cox regression\",\"infinitesimal jackknife recently applied random forest estimate prediction variance theorems verified traditional random forest framework uses classification regression trees cart bootstrap resampling however random forests using conditional inference trees subsampling found prone variable selection bias conduct simulation experiments using novel approach explore applicability random forests using variations resampling method base learner test data points simulated trained using random forest one hundred simulated training data sets using different combinations resampling base learners using trees instead traditional cart trees well using subsampling instead bootstrap sampling resulted much accurate estimation prediction variance using random forest variations incorporated open source software package programming language\",\"work analyze problem adoption mobile money pakistan using call detail records major telecom company input results highlight fact different sections society different patterns adoption digital financial services user mobility related features important one comes adopting using mobile money services\",\"many data mining applications collection sufficiently large datasets time consuming expensive hand industrial methods data collection create huge databases make difficult direct applications advanced machine learning algorithms address problems consider active learning may efficient either experimental design data filtering paper demonstrate using online evaluation opportunity provided challenge quite competitive results may produced using small percentage available data also present several alternative criteria may useful evaluation active learning processes author paper attended special presentation barcelona results wcci challenge discussed\",\"article proposes multinomial probit bayesian additive regression trees mpbart multinomial probit extension bart bayesian additive regression trees chipman mpbart flexible allow inclusion predictors describe observed units well available choice alternatives two simulation studies four real data examples show mpbart exhibits good predictive performance comparison discrete choice multiclass classification methods implement mpbart developed package mpbart available freely cran repositories\",\"currently machine learning plays important role lives individual activities numerous people accordingly become necessary design machine learning algorithms ensure discrimination biased views unfair treatment result decision making predictions made via machine learning work introduce novel empirical risk minimization erm framework supervised learning neutralized erm nerm ensures classifiers obtained guaranteed neutral respect viewpoint hypothesis specifically given viewpoint hypothesis nerm works find target hypothesis minimizes empirical risk simultaneously identifying target hypothesis neutral viewpoint hypothesis within nerm framework derive theoretical bound empirical generalization neutrality risks furthermore realization nerm linear classification derive maxmargin algorithm neutral support vector machine svm experimental results show neutral svm shows improved classification performance real datasets without sacrificing neutrality guarantee\",\"big data bring new opportunities modern society challenges data scientists one hand big data hold great promises discovering subtle population patterns heterogeneities possible smallscale data hand massive sample size high dimensionality big data introduce unique computational statistical challenges including scalability storage bottleneck noise accumulation spurious correlation incidental endogeneity measurement errors challenges distinguished require new computational statistical paradigm article give overviews salient features big data features impact paradigm change statistical computational methods well computing architectures also provide various new perspectives big data analysis computation particular emphasis viability sparsest solution highconfidence set point exogeneous assumptions statistical methods big data validated due incidental endogeneity lead wrong statistical inferences consequently wrong scientific conclusions\",\"sales forecasting plays prominent role business planning business strategy value importance advance information cornerstone planning activity wellset forecast goal guide saleforce efficiently paper cpu sales forecasting intel corporation multinational semiconductor industry considered past sale future booking exchange rates gross domestic product gdp forecasting seasonality indicators innovatively incorporated quantitative modeling benefit recent advances computation power software development millions models built upon multiple regressions time series analysis random forest boosting tree executed parallel models smaller validation errors selected form ensemble model better capture distinct characteristics forecasting models implemented lead time lines business level moving windows validation process automatically selected models closely represent current market condition weekly cadence forecasting schema allowed model response effectively market fluctuation generic variable importance analysis also developed increase model interpretability rather assuming fixed distribution nonparametric permutation variable importance analysis provided general framework across methods evaluate variable importance variable importance framework extend classification problem modifying mean absolute percentage errormape misclassify error please find demo code httpsgithubcomqxensembleforecastmethods\",\"inferring correct answers binary tasks based multiple noisy answers unsupervised manner emerged canonical question microtask crowdsourcing generally aggregating opinions graphon estimation one interested estimating edge intensities probabilities nodes using single snapshot graph realization recent literature exciting development within topics context crowdsourcing key intellectual challenge understand whether given task accurately denoised aggregating answers collected different tasks context graphon estimation precise information limits estimation algorithms remain interest paper utilize statistical reduction crowdsourcing graphon estimation advance stateofart challenges use concepts graphon estimation design algorithm achieves better performance majority voting scheme setup goes beyond rank one models considered literature use known explicit lower bounds crowdsourcing provide refined lower bounds graphon estimation\",\"characterize study variable importance vimp pairwise variable associations binary regression trees key component involves node mean squared error quantity refer maximal subtree theory naturally extends single trees ensembles trees applies methods like random forests useful importance values random forests used screen variables example used filter high throughput genomic data bioinformatics little theory exists properties\",\"boosting combines weak biased learners obtain effective learning algorithms classification prediction paper show connection boosting kernelbased methods highlighting theoretical practical applications context ell boosting start weak linear learner defined kernel show boosting learner equivalent estimation special boosting kernel depends well regression matrix noise variance hyperparameters number boosting iterations modeled continuous hyperparameter fit along parameters using standard techniques generalize boosting kernel broad new class boosting approaches general weak learners including based ell hinge vapnik losses approach allows fast hyperparameter tuning general class wide range applications including robust regression classification illustrate applications numerical examples synthetic real data\",\"possible perform linear regression datasets whose labels shuffled respect inputs explore question proposing several estimators recover weights noisy linear model labels shuffled unknown permutation show analog classical leastsquares estimator produces inconsistent estimates setting introduce estimator based selfmoments input features labels study regimes estimator excels generalize estimators setting partial ordering information available form experiments replicated independently result framework enables robust inference demonstrate experiments synthetic standard datasets able recover approximate weights using shuffled labels work demonstrates linear regression absence complete ordering information possible practical interest particularly experiments characterize populations particles flow cytometry\",\"work addresses various open questions theory active learning nonparametric classification contributions statistical algorithmic establish new minimaxrates active learning common textitnoise conditions rates display interesting transitions due interaction noise textitsmoothness margin present passive setting transitions previously conjectured remained unconfirmed present generic algorithmic strategy adaptivity unknown noise smoothness margin strategy achieves optimal rates many general situations furthermore unlike previous work avoid need textitadaptive confidence sets resulting strictly milder distributional requirements\",\"paper examines use residual bootstrap bias correction machine learning regression methods accounting bias important obstacle recent efforts develop statistical inference machine learning methods demonstrate empirically proposed bootstrap bias correction lead substantial improvements bias predictive accuracy context ensembles trees show correction approximated double cost training original ensemble without introducing additional variance method shown improve testset accuracy random forests example problems uci repository\",\"current work characterizes users vod streaming space userpersonas based tenure timeline temporal behavioral features absence explicit user profiles combination tenure timeline temporal characteristics caters business needs understanding evolution phases user behavior accounts age personas constructed work successfully represent dominant niche characterizations providing insightful maturation user behavior system two major highlights personas demonstration stability along tenure timelines population level exhibiting interesting migrations labels individual granularity clear interpretability user labels finally show tradeoff indispensable trio guarantees relevancescalabilityinterpretability using summary information personas ctr click rate predictive model proposed method uncovering latent personas consequent insights application information personas predictive models broadly applicable streaming based products\",\"work presents new classifier specifically designed fully interpretable technique determines probability class outcome based directly probability assignments measured training data accuracy predicted probability improved measuring probability estimates training data create series expansion refines predicted probability use work classify four standard datasets achieve accuracies comparable random forests technique interpretable design capable determining combinations features contribute particular classification probability individual cases well weightings combination features\",\"present new online boosting algorithm adapting weights boosted classifier yields closer approximation freund schapires adaboost algorithm previous online boosting algorithms also contribute new way deriving online algorithm ties together previous online boosting work assume weak hypotheses selected beforehand weights updated online boosting update rule derived minimizing adaboosts loss viewed incremental form equations show optimization computationally expensive however fast online approximation possible compare approximation error batch adaboost synthetic datasets generalization error face datasets mnist dataset\",\"consider problem learning forest nonlinear decision rules general loss functions standard methods employ boosted decision trees adaboost exponential loss friedmans gradient boosting general loss contrast traditional boosting algorithms treat tree learner black box method propose directly learns decision forests via fullycorrective regularized greedy search using underlying forest structure method achieves higher accuracy smaller models gradient boosting adaboost exponential loss many datasets\",\"multiplayer online battle arena moba games among played digital games world games teams players fight arena environments gameplay focused tactical combat mastering mobas requires extensive practice exemplified popular moba defence ancients dota paper present three datadriven measures spatiotemporal behavior dota zone changes distribution team members time series clustering via fuzzy approach present method obtaining accurate positional data dota investigate behavior varies across measures function skill level teams using four tiers novice professional players results indicate spatiotemporal behavior moba teams related team skill professional teams smaller withinteam distances conducting zone changes amateur teams temporal distribution withinteam distances professional highskilled teams also generally follows patterns distinct lower skill ranks\",\"objectives discussions fairness criminal justice risk assessments typically lack conceptual precision rhetoric often substitutes careful analysis paper seek clarify tradeoffs different kinds fairness fairness accuracy methods draw existing literatures criminology computer science statistics provide integrated examination fairness accuracy criminal justice risk assessments also provide empirical illustration using data arraignments results show least six kinds fairness incompatible one another accuracy conclusions except trivial cases impossible maximize accuracy fairness time impossible simultaneously satisfy kinds fairness practice major complication different base rates across different legally protected groups need consider challenging tradeoffs\",\"testament success theory random forests long outpaced application practice paper take step towards narrowing gap providing consistency result online random forests\",\"concept hierarchies formal concept analysis theoretically well grounded largely experimented methods rely line diagrams called galois lattices visualizing analysing objectattribute sets galois lattices visually seducing conceptually rich experts however present important drawbacks due concept oriented overall structure analysing show difficult non experts navigation cumbersome interaction poor scalability deep bottleneck visual interpretation even experts paper introduce semantic probes means overcome many problems extend usability application possibilities traditional fca visualization methods semantic probes visual user centred objects extract organize reduced galois subhierarchies simpler clearer provide better navigation support rich set interaction possibilities since probe driven subhierarchies limited users focus scalability control interpretation facilitated successful experiments several applications developed remaining problem finding compromise simplicity conceptual expressivity\",\"transfer learning aims improve learning target domain borrowing knowledge related different source domain reduce distribution shift source target domains recent methods focused exploring invariant representations similar distributions across domains however learning invariant knowledge existing methods assume labels source domain uncontaminated reality often access source data noisy labels paper first show label noise adversely affect learning invariant representations correcting label shift various transfer learning scenarios reduce adverse effects propose novel denoising conditional invariant component dcic framework provably ensures extracting invariant representations given examples noisy labels source domain unlabeled examples target domain estimating label distribution target domain bias experimental results synthetic realworld data verify effectiveness proposed method\",\"present first treebased regressor whose convergence rate depends intrinsic dimension data namely assouad dimension regressor uses rptree partitioning procedure simple randomized variant trees\",\"knearestneighbor knn ensembles exist despite efficacy approach regression classification outlier detection exist focus bagging features rather varying bagging observations unknown whether varying bagging observations improve prediction given recent studies topological data analysis varying may function like multiscale topological methods providing stability better prediction well increased ensemble diversity paper explores knn ensemble algorithms combining bagged features bagged observations varied understand contribute model fit specifically algorithms tested tweedie regression problems simulations real datasets results compared stateoftheart machine learning models including extreme learning machines random forest boosted regression morsesmale regression results simulations suggest gains varying beyond bagging features samples well robustness knn ensembles curse dimensionality knn regression ensembles perform favorably stateoftheart algorithms dramatically improve performance knn regression real dataset results suggest varying good strategy general particularly difficult tweedie regression problems knn regression ensembles often outperform stateoftheart methods results kvarying ensembles echo recent theoretical results topological data analysis multidimensional filter functions multiscale coverings provide stability performance gains singledimensional filters singlescale covering opens possibility leveraging multiscale neighborhoods multiple measures local geometry ensemble methods\",\"establish consistency algorithm mondrian forests randomized classification algorithm implemented online first amend original mondrian forest algorithm considers fixed lifetime parameter indeed fact parameter fixed hinders statistical consistency original procedure modified mondrian forest algorithm grows trees increasing lifetime parameters lambdan uses alternative updating rule allowing work also online fashion second provide theoretical analysis establishing simple conditions consistency theoretical analysis also exhibits surprising fact algorithm achieves minimax rate optimal rate estimation lipschitz regression function strong extension previous results arbitrary dimension\",\"tree ensembles random forests boosted trees renowned high prediction performance however interpretability critically limited due enormous complexity study present method make complex tree ensemble interpretable simplifying model specifically formalize simplification tree ensembles model selection problem given complex tree ensemble aim obtaining simplest representation essentially equivalent original one end derive bayesian model selection algorithm optimizes simplified model maintaining prediction performance numerical experiments several datasets showed complicated tree ensembles reasonably approximated interpretable\",\"ensemble regression trees become popular statistical tools estimation conditional mean given set predictors however quantile regression trees ensembles yet garnered much attention despite increasing popularity linear quantile regression model work proposes bayesian quantile additive regression trees model shows good predictive performance illustrated using simulation studies real data applications extension tackle binary classification problems also considered\",\"recently crowdsourcing emerged effective paradigm humanpowered large scale problem solving various domains however task requester usually limited amount budget thus desirable policy wisely allocate budget achieve better quality paper study principle information maximization active sampling strategies framework hodgerank approach based hodge decomposition pairwise ranking data multiple workers principle exhibits two scenarios active sampling fisher information maximization leads unsupervised sampling based sequential maximization graph algebraic connectivity without considering labels bayesian information maximization selects samples largest information gain prior posterior gives supervised sampling involving labels collected experiments show proposed methods boost sampling efficiency compared traditional sampling schemes thus valuable practical crowdsourcing experiments\",\"missing data expected issue large amounts data collected several imputation techniques proposed tackle problem beneath classical approaches mice application machine learning techniques tempting recently proposed missforest imputation method shown high imputation accuracy missing completely random scheme various missing rates core based random forest classification regression respectively paper study whether approach even enhanced methods stochastic gradient tree boosting method algorithm modified random forest procedures particular resampling strategies within random forest protocol suggested extensive simulation study analyze performances continuous categorical well mixedtype data therein missboopf combination stochastic gradient tree boosting method together parametrically bootstrapped random forest method appeared promising finally empirical analysis focusing credit information facebook data conducted\",\"paper present technique using bootstrap estimate operating characteristics variability certain types ensemble methods bootstrapping model require huge amount work training data set large fortunately many cases technique lets determine effect infinite resampling without actually refitting single model apply technique study metaparameter selection random forests demonstrate alternatives bootstrap aggregation considering sqrtd features split node number features produce improvements predictive accuracy\",\"offer novel view adaboost statistical setting propose bayesian model binary classification label noise modeled hierarchically using variational inference optimize dynamic evidence lower bound derive new boostinglike algorithm called viboost show close connections adaboost give experimental results four datasets\",\"modern scientific research massive datasets huge numbers observations frequently encountered facilitate computational process divideandconquer scheme often used analysis big data strategy full dataset first split several manageable segments final output averaged individual outputs segments despite popularity practice remains largely unknown whether distributive strategy provides valid theoretical inferences original data paper address fundamental issue distributed kernel regression dkr algorithmic feasibility measured generalization performance resulting estimator justify dkr uniform convergence rate needed bounding generalization error individual outputs brings new challenging issues big data setup mild conditions show proper number segments dkr leads estimator generalization consistent unknown regression function obtained results justify method dkr shed light feasibility using distributed algorithms processing big data promising preference method supported simulation real data examples\",\"crowdsourcing popular paradigm effectively collecting labels low cost dawidskene estimator widely used inferring true labels noisy labels provided nonexpert crowdsourcing workers however since estimator maximizes nonconvex loglikelihood function hard theoretically justify performance paper propose twostage efficient algorithm multiclass crowd labeling problems first stage uses spectral method obtain initial estimate parameters second stage refines estimation optimizing objective function dawidskene estimator via algorithm show algorithm achieves optimal convergence rate logarithmic factor conduct extensive experiments synthetic real datasets experimental results demonstrate proposed algorithm comparable accurate empirical approach outperforming several recently proposed methods\",\"item response theory irt models categorical response data widely used analysis educational data computerized adaptive testing psychological surveys however irt models rely assumption categories strictly ordered assumption ordering known priori assumptions impractical many realworld scenarios multiplechoice exams levels incorrectness distractor categories often unknown number results exist irt models unordered categorical data tend restrictive modeling assumptions lead poor data fitting performance practice furthermore existing unordered categorical models parameters difficult interpret work propose novel methodology unordered categorical irt call sprite short stochastic polytomous response item model analyzes ordered unordered categories offers interpretable outputs iii provides improved data fitting compared existing models compare sprite existing item response models demonstrate efficacy synthetic realworld educational datasets\",\"motivation work improve performance standard stacking approaches ensembles composed simple heterogeneous base models integration generation selection stages regression problems propose two extensions standard stacking approach first extension combine set standard stacking approaches ensemble ensembles using twostep ensemble learning regression setting second extension consists two parts initial part diversity mechanism injected original training data set systematically generating different training subsets partitions corresponding ensembles ensembles final part measuring quality different partitions ensembles maxmin rulebased selection algorithm used select appropriate ensemblepartition make final prediction show based experiments broad range data sets second extension performs better best standard stacking approaches good oracle databases best base model selected crossvalidation data set addition second extension performs better two stateoftheart ensemble methods regression good third stateoftheart ensemble method\",\"understanding player behavior fundamental game data science video games evolve players interact game able foresee player experience would help ensure successful game development particular game developers need evaluate beforehand impact ingame events simulation optimization events crucial increase player engagement maximize monetization present experimental analysis several methods forecast gamerelated variables two main aims obtain accurate predictions inapp purchases playtime operational production environment perform simulations ingame events order maximize sales playtime ultimate purpose take step towards datadriven development games results suggest even though performance traditional approaches arima still better outcomes stateoftheart techniques like deep learning promising deep learning comes wellsuited general model could used forecast variety time series different dynamic behaviors\",\"bagging device intended reducing prediction error learning algorithms simplest form bagging draws bootstrap samples training sample applies learning algorithm bootstrap sample averages resulting prediction rules extend definition bagging statistics statistical functionals study von mises expansion bagged statistical functionals show expansion related efronstein anova expansion raw unbagged functional basic observation bagged functional always smooth sense von mises expansion exists finite length resample size holds even raw functional rough unstable resample size acts smoothing parameter smaller means smoothing\",\"present new boosting algorithm motivated large margins theory boosting give experimental evidence new algorithm significantly robust label noise existing boosting algorithm\",\"new social economic activities massively exploit big data machine learning algorithms inference peoples lives applications include automatic curricula evaluation wage determination risk assessment credits loans recently many governments institutions raised concerns lack fairness equity ethics machine learning treat problems shown including sensitive features bias fairness gender race enough mitigate discrimination related features included instead including fairness objective function shown efficient present novel fair regression dimensionality reduction methods built previously proposed fair classification framework methods rely using hilbert schmidt independence criterion fairness term unlike previous approaches allows simplify problem use multiple sensitive variables simultaneously replacing linear formulation kernel functions allows methods deal nonlinear problems linear nonlinear formulations solution reduces solving simple matrix inversions generalized eigenvalue problems simplifies evaluation solutions different tradeoff values predictive error fairness terms illustrate usefulness proposed methods toy examples evaluate performance real world datasets predict income using gender andor race discrimination sensitive variables contraceptive method prediction demographic socioeconomic sensitive descriptors\",\"mapping forest resources carbon important improving forest management meeting objectives storing carbon preserving environment spaceborne remote sensing approaches considerable potential support forest height monitoring providing repeated observations high spatial resolution large areas study uses machine learning approach previously developed produce local maps forest parameters basal area height diameter etc aim paper present extension approach much larger scales french national coverage used gedi lidar mission reference height data satellite images sentinel sentinel alos palsa estimate forest height produce map france year height map derived volume aboveground biomass agb using allometric equations validation height map local maps als data shows accuracy close state art mean absolute error mae validation inventory plots representative french forests shows mae height estimates slightly better coniferous broadleaved forests volume agb maps derived height shows maes tonsha mha respectively results aggregated sylvoecoregion forest types owner species improved maes tonsha mha precision maps allows monitor forests locally well helping analyze forest resources carbon territorial scale specific types forests combining maps geolocated information administrative area species type owner protected areas environmental conditions etc height volume agb maps produced study made freely available\",\"tree ensembles random forest boosted trees renowned high prediction performance whereas interpretability critically limited paper propose post processing method improves model interpretability tree ensembles learning complex tree ensembles standard way approximate simpler model interpretable human obtain simpler model derive algorithm minimizing divergence complex ensemble synthetic experiment showed complicated tree ensemble approximated reasonably interpretable\",\"predictive models generalize well distributional shift often desirable sometimes crucial building robust reliable machine learning applications focus distributional shift arises causal inference observational data unsupervised domain adaptation pose problems prediction shift design popular methods overcoming distributional shift make unrealistic assumptions wellspecified model knowing policy gave rise observed data methods hindered need prespecified metric comparing observations poor asymptotic properties devise bound generalization error design shift incorporating representation learning sample reweighting based bound propose algorithmic framework require assumptions asymptotically consistent empirically study new framework using two synthetic datasets demonstrate effectiveness compared previous methods\",\"big data comes various ways types shapes forms sizes indeed almost areas science technology medicine public health economics business linguistics social science bombarded ever increasing flows data begging analyzed efficiently effectively paper propose rough idea possible taxonomy big data along commonly used tools handling particular category bigness dimensionality input space sample size usually main ingredients characterization data bigness specific statistical machine learning technique used handle particular big data set depend category falls within bigness taxonomy large small data sets instance require different set tools large small variety among tools discuss preprocessing standardization imputation projection regularization penalization compression reduction selection kernelization hybridization parallelization aggregation randomization replication sequentialization indeed important emphasize right away socalled free lunch theorem applies sense universally superior method outperforms methods categories bigness also important stress fact simplicity sense ockhams razor non plurality principle parsimony tends reign supreme comes massive data conclude comparison predictive performance commonly used methods data sets\",\"population migration valuable information leads proper decision urbanplanning strategy massive investment many fields instance intercity migration posterior evidence see governments constrain population works intercommunity immigration might prior evidence real estate price hike timely data also impossible compare city favorable people suppose cities release different new regulations could also compare customers different real estate development groups come probably unfortunately data available paper leveraging data generated positioning team didi propose novel approach timely monitoring population migration community scale provincial scale migration detected soon week could faster setting week statistical purpose monitoring system developed applied nation wide china observations derived system presented paper new method migration perception origin insight nowadays people mostly moving personal access point also known wifi hotspot assume ratio moving migration population constant analysis comparative population migration would feasible exact quantitative research would also done sample research model regression procedures processing data includes many steps eliminating impact pseudomigration instance pocket wifi secondhand traded router distinguishing moving population moving companies identifying shifting finger print clusters etc\",\"years ensemble methods become staple machine learning similarly generalized linear models glms become popular wide variety statistical inference tasks former shown enhance ofsample predictive power latter possess easy interpretability recently ensembles glms proposed possibility downside approach loses interpretability glms possess show minimum description length mdlmotivated compression inferred ensembles used recover interpretability without much downside performance illustrate number standard classification data sets\",\"one objectively measure performance individual offensive lineman nfl existing literature proposes various measures rely subjective assessments game film yet develop objective methodology evaluate performance using variety statistics related offensive linemans performance develop framework objectively analyze overall performance individual offensive lineman determine specific linemen overvalued undervalued relative salary identify eight players across nfl seasons considered overvalued undervalued corroborate results existing metrics based subjective evaluation best knowledge techniques set forth work utilized previous works evaluate performance nfl players position including offensive linemen\",\"processes governing lives use part automatic decision step based feature vector derived applicant algorithm decision power final outcome present simple idea gives power back applicant providing alternatives would make decision algorithm decide differently based formalization reminiscent methods used evasion attacks consists enumerating subspaces classifiers decides desired output implemented specific case decision forests ensemble methods based decision trees mapping problem iterative version enumerating kcliques\",\"decision trees algorithms use gain function select best split trees induction function crucial obtain trees high predictive accuracy gain functions suffer bias compares splits different arities quinlan proposed gain ratio information gain function fix bias paper present updated version gain ratio performs better tries fix gain ratios bias unbalanced trees splits low predictive interest\",\"article proposed several approaches post processing large ensemble prediction models rules results simulations show post processing methods considered promising used techniques developed estimation quantitative traits markers benchmark bostob housingdata set simulations cases produced models better prediction performance example ones produced random forest rulefit algorithms\",\"consider sequential decision making problems binary classification scenario learner takes active role repeatedly selecting samples action pool receives binary label selected alternatives problem motivated applications observations time consuming andor expensive resulting small samples goal identify best alternative highest response use bayesian logistic regression predict response alternative formulating problem markov decision process develop knowledgegradient type policy guide experiment maximizing expected value information labeling alternative provide finitetime analysis estimated error experiments benchmark uci datasets demonstrate effectiveness proposed method\",\"paper introduces develops novel variable importance score function context ensemble learning demonstrates appeal theoretically empirically proposed score function simple straightforward counterpart proposed context random forest avoiding permutations design computationally efficient random forest variable importance function like random forest variable importance function score handles regression classification seamlessly one distinct advantage proposed score fact offers natural cut zero positive scores indicating importance significance negative scores deemed indications insignificance extra advantage proposed score lies fact works well beyond ensemble trees seamlessly used base learners random subspace learning context examples simulated real demonstrate proposed score compete mostly favorably random forest score\",\"data analysis machine learning become integrative part modern scientific methodology offering automated procedures prediction phenomenon based past observations unraveling underlying patterns data providing insights problem yet caution avoid using machine learning blackbox tool rather consider methodology rational thought process entirely dependent problem study particular use algorithms ideally require reasonable understanding mechanisms properties limitations order better apprehend interpret results accordingly goal thesis provide indepth analysis random forests consistently calling question every part algorithm order shed new light learning capabilities inner workings interpretability first part work studies induction decision trees construction ensembles randomized trees motivating design purpose whenever possible contributions follow original complexity analysis random forests showing good computational performance scalability along indepth discussion implementation details contributed within scikitlearn second part work analyse discuss interpretability random forests eyes variable importance measures core contributions rests theoretical characterization mean decrease impurity variable importance measure prove derive properties case multiway totally randomized trees asymptotic conditions consequence work analysis demonstrates variable importances\",\"recursive partitioning approaches producing treelike models long standing staple predictive modeling last decade mostly sublearners within state art ensemble methods like boosting random forest however fundamental flaw partitioning splitting rule commonly used tree building methods precludes treating different types variables equally clearly manifests methods inability properly utilize categorical variables large number categories ubiquitous new age big data variables often informative current tree methods essentially leave choice either using exposing models severe overfitting propose conceptual framework splitting using leaveoneout loo cross validation selecting splitting variable performing regular split case following carts approach selected variable important consequence approach categorical variables many categories safely used tree building chosen contribute predictive power demonstrate extensive simulation real data analysis novel splitting approach significantly improves performance single tree models ensemble methods utilize trees importantly design algorithm loo splitting variable selection reasonable assumptions increase overall computational complexity compared cart twoclass classification regression tasks approach carries increased computational burden replacing ologn factor cart splitting rule search term\",\"causal random forests provide efficient estimates heterogeneous treatment effects however forest algorithms also wellknown blackbox nature therefore characterize input variables involved treatment effect heterogeneity strong practical limitation article develop new importance variable algorithm causal forests quantify impact input heterogeneity treatment effects proposed approach inspired drop relearn principle widely used regression problems importantly show handle forest retrain without confounding variable confounder involved treatment effect heterogeneity local centering step enforces consistency importance measure otherwise confounder also impacts heterogeneity introduce corrective term retrained causal forest recover consistency additionally experiments simulated semisynthetic real data show good performance importance measure outperforms competitors several test cases experiments also show approach efficiently extended groups variables providing key insights practice\",\"becoming increasingly important machine learning methods make predictions interpretable well accurate many practical applications interest features feature interactions relevant prediction task present novel method selective bayesian forest classifier strikes balance predictive power interpretability simultaneously performing classification feature selection feature interaction detection visualization builds parsimonious yet flexible models using treestructured bayesian networks samples ensemble models using markov chain monte carlo build feature selection dividing trees two groups according relevance outcome interest method performs competitively classification feature selection benchmarks low high dimensions includes visualization tool provides insight relevant features interactions\",\"paper describes structuring data constructing plots explore forest classification models interactively forest classifier example ensemble produced bagging multiple trees process bagging combining results multiple trees produces numerous diagnostics interactive graphics provide lot insight class structure high dimensions various aspects explored paper assess model complexity individual model contributions variable importance dimension reduction uncertainty prediction associated individual observations ideas applied random forest algorithm projection pursuit forest could broadly applied bagged ensembles interactive graphics built using ggplot plotly shiny packages\",\"uplift modeling aimed estimating incremental impact action individuals behavior useful various application domains targeted marketing advertisement campaigns personalized medicine medical treatments conventional methods uplift modeling require every instance jointly equipped two types labels taken action outcome however obtaining two labels instance time difficult expensive many realworld problems paper propose novel method uplift modeling applicable practical setting one type labels available instance show mean squared error bound proposed estimator demonstrate effectiveness experiments\",\"sampling replacement occurs many settings machine learning notably bagging ensemble technique validation scheme number unique original items bootstrap sample important role behaviour prediction models learned indeed uncontrived examples duplicate items effect purpose report present distribution number unique original items bootstrap sample clearly concisely view enabling machine learning researchers understand control quantity existing future resampling techniques describe key characteristics distribution along generalisation case items come distinct categories classification cases discuss normal limit conduct empirical investigation derive heuristic normal approximation permissible\",\"data collections become larger exploratory regression analysis becomes important challenging observations hierarchically clustered problem even challenging model selection mixed effect models produce misleading results nonlinear effects included model bauer cai machine learning method called boosted decision trees friedman good approach exploratory regression analysis real data sets detect predictors nonlinear interaction effects also accounting missing data propose extension boosted decision decision trees called metboost hierarchically clustered data works constraining structure tree across groups allowing terminal node means differ allows predictors split points lead different predictions within group approximates nonlinear group specific effects importantly metboost remains computationally feasible thousands observations hundreds predictors may contain missing values apply method predict math performance students schools data collected educational longitudinal study ingels allowing predictors unique effects school comparing results boosted decision trees metboost improved prediction performance results large simulation study show metboost improved variable selection performance improved prediction performance compared boosted decision trees group sizes small\",\"variable selection highdimensional linear models received lot attention lately mostly context lregularization part attraction variable selection effect parsimonious models obtained suitable interpretation terms predictive power however regularized linear models often slightly inferior machine learning procedures like tree ensembles tree ensembles hand lack usually formal way variable selection difficult visualize garrotestyle convex penalty trees ensembles particular random forests proposed penalty selects functional groups nodes trees could simple monotone functions individual predictor variables yields parsimonious function fit lends easily visualization interpretation predictive power maintained least level original tree ensemble key feature method tree ensemble fitted tuning parameter needs selected empirical performance demonstrated wide array datasets\",\"estimating strength dependency two variables fundamental exploratory analysis many applications data mining example nonlinear dependencies two continuous variables explored maximal information coefficient mic categorical variables dependent target class selected using gini gain random forests nonetheless dependency measures estimated finite samples interpretability quantification accuracy ranking dependencies become challenging dependency estimates equal variables independent cannot compared computed different sample size inflated chance variables categories paper propose framework adjust dependency measure estimates finite samples adjustments simple applicable dependency measure helpful improving interpretability quantifying dependency improving accuracy task ranking dependencies particular demonstrate approach enhances interpretability mic used proxy amount noise variables gain accuracy ranking variables splitting procedure random forests\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"9_forest_trees_data\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"9_forest_trees_data\"],\"textfont\":{\"size\":12},\"x\":[8.160513,8.989895,8.007801,8.754699,8.805926,9.084685,8.860327,8.169375,8.276316,8.751417,9.233035,9.132649,8.008266,8.878231,8.764941,8.0003195,9.220682,9.0714035,8.35297,8.759449,9.375643,9.232193,8.747139,8.0613785,8.684663,8.807567,9.1238,7.997802,8.792048,8.20568,8.613827,8.218385,8.783693,8.6608305,7.965634,8.243798,8.860746,8.4111395,8.060291,8.080214,8.12845,8.186675,8.712714,8.036739,7.9572325,8.720268,9.45567,8.71532,8.453374,8.142777,8.811728,7.966293,8.655144,8.72377,8.134039,8.127083,8.872995,9.299922,9.256895,8.171215,8.951203,8.184378,8.156207,8.06745,9.022147,8.018255,8.189527,8.173356,7.892197,8.115429,8.17145,8.472047,7.9458127,8.246979,8.127377,8.100847,8.50753],\"y\":[8.084204,8.800175,8.0334425,9.060103,7.514938,8.246082,7.566728,7.9542174,8.185358,7.551569,8.91926,8.901351,8.026114,7.59405,9.025522,8.017845,8.975544,8.239233,8.056212,7.368377,8.496031,9.074149,7.496225,8.0676155,8.04784,7.573108,8.228011,8.016882,9.042461,8.093881,8.038037,8.089308,8.999886,7.2480397,8.027477,8.16621,7.5633225,7.8200564,8.070454,7.957485,8.0624485,8.078288,7.543296,8.076104,7.9805603,8.018029,8.487966,7.5557866,8.223441,8.14729,9.063412,7.993445,8.023914,7.212728,8.174713,8.03138,7.621444,8.484298,9.033676,7.9853663,8.785653,8.100931,8.098296,8.040696,8.2231,8.026824,8.104552,8.1446085,7.9308977,8.055629,8.17491,8.278286,7.9403768,8.177082,8.030151,8.059108,8.134282],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"paper investigates theoretical foundations metric learning focused three key questions fully addressed prior work consider learning general lowdimensional lowrank metrics well sparse metrics develop upper lower minimaxbounds generalization error quantify sample complexity metric learning terms dimension feature space dimensionrank underlying metric also bound accuracy learned metric relative underlying true generative metric results involve novel mathematical approaches metric learning problem lso shed new light special case ordinal embedding aka nonmetric multidimensional scaling\",\"introduce new nearestprototype classifier prototype vector machine pvm arises combinatorial optimization problem cast variant set cover problem propose two algorithms approximating solution pvm selects relatively small number representative points used classification contains special case method compatible dissimilarity measure making amenable situations data embedded underlying feature space using noneuclidean metric desirable indeed demonstrate much studied zip code data pvm reap benefits problemspecific metric example pvm outperforms highly successful tangent distance retaining fewer half data points example highlights strengths pvm yielding lowerror highly interpretable model additionally apply pvm protein classification problem kernelbased distance used\",\"study safe screening metric learning distance metric learning optimize metric set triplets one defined pair class instances instance different class however number possible triplets quite huge even small dataset safe triplet screening identifies triplets safely removed optimization problem without losing optimality compared existing safe screening studies triplet screening particularly significant huge number possible triplets semidefinite constraint optimization derive several variants screening rules analyze relationships numerical experiments benchmark datasets demonstrate effectiveness safe triplet screening\",\"article derive concentration inequalities crossvalidation estimate generalization error stable predictors context risk assessment notion stability first introduced citedewa extended citekea citebe citekuniy characterize class predictors infinite dimension particular covers knearest neighbors rules bayesian algorithm citekea boosting general loss functions class predictors considered use formalism introduced citedud cover large variety crossvalidation procedures including leaveoneout crossvalidation kfold crossvalidation holdout crossvalidation split sample leaveupsilonout crossvalidation particular give simple rule choose crossvalidation depending stability class predictors special case uniform stability interesting consequence number elements test set required grow infinity consistency crossvalidation procedure special case particular interest leaveoneout crossvalidation emphasized\",\"data sets many features observations independent screening based univariate regression models leads computationally convenient variable selection method recent efforts shown case generalized linear models independent screening may suffice capture relevant features high probability even ultrahigh dimension unclear whether formal sure screening property attainable response rightcensored survival time propose computationally efficient independent screening method survival data viewed natural survival equivalent correlation screening state conditions method admits sure screening property within general class singleindex hazard rate models ultrahigh dimensional features iterative variant also described combines screening penalized regression order handle complex feature covariance structures methods evaluated simulation studies application real gene expression dataset\",\"propose new class metrics sets vectors functions used various stages data mining including exploratory data analysis learning result interpretation new distance functions unify generalize popular metrics jaccard bag distances sets manhattan distance vector spaces marczewskisteinhaus distance integrable functions prove new metrics complete show useful relationships fdivergences probability distributions extend approach structured objects concept hierarchies ontologies introduce informationtheoretic metrics directed acyclic graphs drawn according fixed probability distribution conduct empirical investigation demonstrate intuitive interpretation new metrics effectiveness realvalued highdimensional structured data extensive comparative evaluation demonstrates new metrics outperformed multiple similarity dissimilarity functions traditionally used data mining including minkowski family fractional family two fdivergences cosine distance two correlation coefficients finally argue new class metrics particularly appropriate rapid processing highdimensional structured data distancebased learning\",\"softmax representation probabilities categorical variables plays prominent role modern machine learning numerous applications areas large scale classification neural language modeling recommendation systems however softmax estimation expensive large scale inference high cost associated computing normalizing constant introduce efficient approximation softmax probabilities takes form rigorous lower bound exact probability bound expressed product pairwise probabilities leads scalable estimation based stochastic optimization allows perform doubly stochastic estimation subsampling training instances class labels show new bound interesting theoretical properties demonstrate use classification problems\",\"consider standard binary classification problem performance binary classifier based training data characterized excess risk study bahadurs type exponential bounds minimax accuracy confidence function based excess risk study quantity depends complexity class distributions characterized exponents entropies class regression functions class bayes classifiers corresponding distributions class also study dependence margin parameters classification problem\",\"highdimensional classification settings wish seek balance high power ensuring control desired loss function many settings points likely misclassified lie near decision boundary given classification method often uninformative points classified noisy exhibit strong signals paper introduce thresholding method parameterize problem determining points exhibit strong signals classified demonstrate empirical performance novel calibration method providing loss function control desired level well explore method assuages effect overfitting explore benefits error control thresholding method difficult highdimensional simulated settings finally show flexibility thresholding method applying method variety real data settings\",\"apply informationbased complexity analysis support vector machine svm algorithms goal comprehensive continuous algorithmic analysis algorithms involves complexity measures higher order operations certain optimizations considered primitive purposes measuring complexity consider classes information operators algorithms made scaled families investigate utility scaling complexities minimize error look division statistical learning information algorithmic components complexities applications support vector machine svm general machine learning algorithms give applications svm algorithms graded linear higher order components give example biomedical informatics\",\"introduce efficient method training linear ranking support vector machine method combines cutting plane optimization redblack tree based approach subgradient calculations omsmlogm time complexity number training examples average number nonzero features per example best previously known training algorithms achieve efficiency restricted special cases whereas proposed approach allows real valued utility scores training data experiments demonstrate superior scalability proposed approach compared fastest existing ranksvm implementations\",\"article carries large dimensional analysis standard regularized discriminant analysis classifiers designed assumption data arise gaussian mixture model different means covariances analysis relies fundamental results random matrix theory rmt number features cardinality training data within class grow large pace mild assumptions show asymptotic classification error approaches deterministic quantity depends means covariances associated class well problem dimensions result permits better understanding performance regularized discriminant analsysis practical large finite dimensions used determine preestimate optimal regularization parameter minimizes misclassification error probability despite theoretically valid gaussian data findings shown yield high accuracy predicting performances achieved real data sets drawn popular usps data base thereby making interesting connection theory practice\",\"introduce supersparse linear integer models slim tool create scoring systems binary classification derive theoretical bounds true risk slim scoring systems present experimental results show slim scoring systems accurate sparse interpretable classification models\",\"consider problem learning binary classifier training set positive unlabeled examples inductive transductive setting problem often referred emphpu learning differs standard supervised classification problem lack negative examples training set corresponds ubiquitous situation many applications information retrieval gene ranking identified set data interest sharing particular property wish automatically retrieve additional data sharing property among large easily available pool unlabeled data propose conceptually simple method akin bagging approach inductive transductive learning problems converting series supervised binary classification problems discriminating known positive examples random subsamples unlabeled set empirically demonstrate relevance method simulated real data performs least well existing methods faster\",\"kernelbased learning algorithms widely used machine learning problems make use similarity object pairs algorithms first embed data points alternative space inner product object pairs specifies distance embedding space applying kernel methods partially labeled datasets classical challenge regard requiring distances unlabeled pairs must somehow learnt using labeled data independent study summarize work lanckriet als work learning kernel matrix semidefinite programming used support vector machines svm algorithms transduction problem throughout report provide alternative explanations derivations analysis related work designed ease understanding original article\",\"many nonparametric regressors recently shown converge rates depend intrinsic dimension data regressors thus escape curse dimension highdimensional data low intrinsic dimension manifold show knn regression also adaptive intrinsic dimension particular rates local query depend way masses balls centered vary radius furthermore show simple way choose locally nearly achieve minimax rate terms unknown intrinsic dimension vicinity also establish minimax rate depend particular choice metric space distribution rather minimax rate holds metric space doubling measure\",\"often wish predict large number variables depend well observed variables structured prediction methods essentially combination classification graphical modeling combining ability graphical models compactly model multivariate data ability classification methods perform prediction using large sets input features tutorial describes conditional random fields popular probabilistic method structured prediction crfs seen wide application natural language processing computer vision bioinformatics describe methods inference parameter estimation crfs including practical issues implementing large scale crfs assume previous knowledge graphical modeling tutorial intended useful practitioners wide variety fields\",\"class schoenberg transformations embedding euclidean distances higher dimensional euclidean spaces presented derived theorems positive definite conditionally negative definite matrices original results arc lengths angles curvature transformations proposed visualized artificial data sets classical multidimensional scaling simple distancebased discriminant algorithm illustrates theory intimately connected gaussian kernels machine learning\",\"increasing body evidence suggesting exact nearest neighbour search highdimensional spaces affected curse dimensionality fundamental level necessarily mean true nearest neighbours based learning algorithms knn classifier analyse question number levels show answer different first main observation show consistency approximate nearest neighbour classifier however performance classifier high dimensions provably unstable second main observation point existing model statistical learning oblivious dimension domain every learning problem admits universally consistent deterministic reduction onedimensional case means borel isomorphism\",\"stability important aspect classification procedure unstable predictions potentially reduce users trust classification system also harm reproducibility scientific conclusions major goal work introduce novel concept classification instability decision boundary instability dbi incorporate generalization error standard selecting accurate stable classifier specifically implement twostage algorithm initially select subset classifiers whose estimated ges significantly different minimal estimated among candidate classifiers optimal classifier chosen one achieving minimal dbi among subset selected stage general selection principle applies linear nonlinear classifiers largemargin classifiers used prototypical example illustrate idea selection method shown consistent sense optimal classifier simultaneously achieves minimal minimal dbi various simulations real examples demonstrate advantage method several alternative approaches\",\"introduce new discriminant analysis method empirical discriminant analysis eda binary classification machine learning given dataset feature vectors method defines empirical feature map transforming training test data new data components gaussian empirical distributions map empirical version gaussian copula used probability mathematical finance purpose form feature mapped dataset close possible gaussian standard quadratic discriminants used classification discuss method general apply datasets computational biology\",\"one limiting factors using support vector machines svms large scale applications superlinear computational requirements terms number training samples address issue several approaches train svms many small chunks large data sets separately proposed literature far however almost approaches empirically investigated addition motivation always based computational requirements work consider localized svm approach based upon partition input space local svm derive general oracle inequality apply oracle inequality least squares regression using gaussian kernels deduce local learning rates essentially minimax optimal standard smoothness assumptions regression function gives first motivation using local svms based computational requirements theoretical predictions generalization performance introduce datadependent parameter selection method local svm approach show method achieves learning rates finally present larger scale experiments localized svm showing achieves essentially test performance global svm fraction computational requirements addition turns computational requirements local svms similar vanilla random chunk approach achieved test errors significantly better\",\"machine learning methods clustering classification rely distance function describe relationships datapoints complex datasets hard avoid making arbitrary choices defining distance function compare images one must choose spatial scale signals temporal scale right scale hard pin preferable results depend tightly exact value one picked topological data analysis seeks address issue focusing notion neighbourhood instead distance shown cases simpler solution available checked strongly distance relationships depend hyperparameter using dimensionality reduction variant dynamical multidimensional scaling mds formulated embeds datapoints curves resulting algorithm based concaveconvex procedure cccp provides simple efficient way visualizing changes invariances distance patterns hyperparameter varied variant analyze dependence multiple hyperparameters also presented cmds algorithm straightforward implement use extend provided illustrate possibilities cmds cmds applied several realworld data sets\",\"data science determining proximity observations critical many downstream analyses clustering information retrieval classification however underlying structure data probability space unclear function used compute similarity data points often arbitrarily chosen present novel concept proximity semblance uses empirical distribution across observations inform similarity pair advantage semblance lies distribution free formulation ability detect niche features placing greater emphasis similarity observation pairs fall outskirts data distribution opposed fall towards center prove semblance valid mercer kernel thus allowing principled use kernel based learning machines semblance applied data modality demonstrate consistently improved performance conventional methods simulations three real case studies different applications viz cell type classification using single cell rna sequencing selecting predictors positive return real estate investments image compression\",\"provide two main contributions pacbayesian theory domain adaptation objective learn source distribution wellperforming majority vote different related target distribution firstly propose improvement previous approach proposed germain relies novel distribution pseudodistance based disagreement averaging allowing derive new tighter domain adaptation bound target risk bound stands spirit common domain adaptation works derive second bound introduced germain brings new perspective domain adaptation deriving upper bound target risk distributions divergenceexpressed ratiocontrols tradeoff source error measure target voters disagreement discuss compare results obtain pacbayesian generalization bounds furthermore pacbayesian specialization linear classifiers infer two learning algorithms evaluate real data\",\"consider training probabilistic classifiers case large number classes number classes assumed large perform exact normalisation classes account consider simple approach directly approximates likelihood show simple approach works well toy problems competitive recently introduced alternative nonlikelihood based approximations furthermore relate approach simple ranking objective leads suggest specific setting optimal threshold ranking objective\",\"implemented several multilabel classification algorithms machine learning package mlr implemented methods binary relevance classifier chains nested stacking dependent binary relevance stacking used base learner accessible mlr moreover access multilabel classification versions randomforestsrc rferns methods easily compared different implemented multilabel performance measures resampling methods standardized mlr framework benchmark experiment several multilabel datasets performance different methods evaluated\",\"prove new fast learning rates onevsall multiclass plugin classifiers trained either exponentially strongly mixing data data generated converging drifting distribution two typical scenarios training data iid learning rates obtained multiclass version tsybakovs margin assumption type lownoise assumption depend number classes results general include previous result binaryclass plugin classifiers iid data special case contrast previous works least squares svms binaryclass setting results retain optimal learning rate iid case\",\"common method generalizing binary multiclass classification error correcting code ecc eccs may optimized number ways instance making orthogonal test two types orthogonal eccs seven different datasets using three types binary classifier compare three multiclass methods oneversustherest random eccs first type orthogonal ecc codes contain zeros admits fast simple method solving probabilities orthogonal eccs always accurate random eccs predicted recent literature improvments uncertainty coefficient range absolute improvements brier score unfortunately orthogonal eccs rarely accurate disparities worst methods paired logistic regression orthogonal eccs never beating methods paired svm losses less significant peaking relative absolute uncertainty coefficient brier scores orthogonal eccs always fastest five multiclass methods paired linear classifiers paired piecewise linear classifier whose classification speed depend number training samples classifications using orthogonal eccs always accurate methods also faster losses higher peaking absolute brier score gains speed ranged whether speed increase worth penalty accuracy depend application\",\"two proteins homologous common evolutionary origin binary classification problem identify proteins candidate set homologous particular native protein feature explanatory variables available classification various measures similarity proteins multiple classification problems type different native proteins respective candidate sets homologous proteins rare single candidate set giving highly unbalanced twoclass problem goal rank proteins candidate set according probability homologous sets native protein ideal classifier place homologous proteins head list approach uses ensemble models classifier ensemble assessment metrics given metric classifier combines models based subset available feature variables call phalanxes proposed ensemble phalanxes identifies strong diverse subsets feature variables second phase ensembling aggregates classifiers based diverse evaluation metrics overall result called ensemble phalanxes metrics provide robustness close distant homologues\",\"inspired importance diversity biological system built heterogeneous system could achieve goal architecture could summarized two basic steps first generate diverse set classification hypothesis using convolutional neural networks currently stateoftheart technique task among traditional innovative machine learning techniques optimally combine metanets family recently developed performing ensemble methods\",\"article large dimensional performance analysis kernel least squares support vector machines lssvms provided assumption twoclass gaussian mixture model input data building upon recent advances random matrix theory show dimension data number large lssvm decision function well approximated normally distributed random variable mean variance depend explicitly local behavior kernel function theoretical result applied mnist fashionmnist datasets despite nongaussianity exhibit convincingly close behavior importantly analysis provides deeper understanding mechanism play svmtype methods particular impact choice kernel function well theoretical limits separating high dimensional gaussian vectors\",\"project investigate idea reducing dimensionality datasets using borel isomorphism purpose subsequently applying supervised learning algorithms originally suggested supervisor pestov dagstuhl preprint consistent learning algorithm example knn retains universal consistency borel isomorphism applied series concrete examples borel isomorphisms reduce number dimensions dataset provided based multiplying data orthogonal matrices dimensionality reducing borel isomorphism applied test accuracy resulting classifier lower dimensional space various data sets working phoneme voice recognition dataset dimension classes phonemes show borel isomorphic reduction dimension leads minimal drop accuracy conclusion discuss prospects method\",\"supervised learning active research area numerous applications diverse fields data analytics computer vision speech audio processing image understanding cases loss functions used machine learning assume symmetric noise models seek estimate unknown function parameters however loss functions quantile quantile huber generalize symmetric ell huber losses asymmetric setting fixed quantile parameter paper propose jointly infer quantile parameter unknown function parameters asymmetric quantile huber quantile losses explore various properties quantile huber loss implement convexity certificate used check convexity quantile parameter loss convex respect parameter function prove biconvex function quantile parameters propose algorithm jointly estimate results synthetic real data demonstrate proposed approach automatically recover quantile parameter corresponding noise also provide improved recovery function parameters illustrate potential framework extend gradient boosting machines quantile losses automatically estimate quantile parameter iteration\",\"article derive concentration inequalities crossvalidation estimate generalization error empirical risk minimizers general setting prove sanitycheck bounds spirit citekr textquotedbllefttextitbounds showing worstcase error estimate much worse training error estimate textquotedblright general loss functions class predictors finite vcdimension considered closely follow formalism introduced citedud cover large variety crossvalidation procedures including leaveoneout crossvalidation fold crossvalidation holdout crossvalidation split sample leaveupsilonout crossvalidation particular focus proving consistency various crossvalidation procedures point interest crossvalidation procedure terms rate convergence estimation curve transition phases depending crossvalidation procedure percentage observations test sample gives simple rule choose crossvalidation interesting consequence size test sample required grow infinity consistency crossvalidation procedure\",\"rapid overlay chemical structures rocs standard tool calculation shape chemical color similarity rocs uses unweighted sums combine many aspects similarity yielding parameterfree models virtual screening report decompose rocs color force field color components color atom overlaps novel color similarity features weighted systemspecific manner machine learning algorithms crossvalidation experiments additional features significantly improve virtual screening performance roc auc scores relative standard rocs\",\"existing binary classification methods target optimization overall classification risk may fail serve realworld applications cancer diagnosis users concerned risk misclassifying one specific class neymanpearson paradigm introduced context novel statistical framework handling asymmetric type iii error priorities seeks classifiers minimal type error constrained type error user specified level article first attempt construct classifiers guaranteed theoretical performance paradigm highdimensional settings based fundamental neymanpearson lemma used plugin approach construct nptype classifiers naive bayes models proposed classifiers satisfy oracle inequalities natural paradigm counterparts oracle inequalities classical binary classification besides desirable theoretical properties also demonstrated numerical advantages prioritized error control via simulation real data studies\",\"regularized discriminant analysis rda proposed friedman widely popular classifier lacks interpretability impractical highdimensional data sets present interpretable computationally efficient classifier called highdimensional rda hdrda designed smallsample highdimensional setting hdrda show training observation regardless class contributes class covariance matrix resulting interpretable estimator borrows pooled sample covariance matrix moreover show hdrda equivalent classifier reducedfeature space dimension approximately equal training sample size result matrix operations employed hdrda computationally linear number features making classifier wellsuited highdimensional classification practice demonstrate hdrda often superior several sparse regularized classifiers terms classification accuracy three artificial six real highdimensional data sets also timing comparisons hdrda implementation sparsediscrim package standard rda formulation klar package demonstrate number features increases computational runtime hdrda drastically smaller rda\",\"paper concerned problems interaction screening nonlinear classification highdimensional setting propose twostep procedure iissqda first step innovated interaction screening iis approach based transforming original pdimensional feature vector proposed second step sparse quadratic discriminant analysis sqda proposed selecting important interactions main effects simultaneously conducting classification iis approach screens important interactions examining features instead twoway interactions order theory shows proposed method enjoys sure screening property interaction selection highdimensional setting growing exponentially sample size selection classification step establish sparse inequality estimated coefficient vector qda prove classification error procedure upperbounded oracle classification error plus smaller order term extensive simulation studies real data analysis show proposal compares favorably existing methods interaction selection highdimensional classification\",\"distance weighted discrimination dwd marginbased classifier interesting geometric motivation dwd originally proposed superior alternative support vector machine svm however dwd yet popular compared svm main reasons twofold first stateoftheart algorithm solving dwd based secondordercone programming socp svm quadratic programming problem much efficient solve second current statistical theory dwd mainly focuses linear dwd highdimensionlowsamplesize setting datapiling learning theory svm mainly focuses bayes risk consistency kernel svm fact bayes risk consistency dwd presented open problem original dwd paper work advance current understanding dwd computational theoretical perspectives propose novel efficient algorithm solving dwd algorithm several hundred times faster existing stateoftheart algorithm based socp addition algorithm handle generalized dwd socp algorithm works well special dwd generalized dwd furthermore consider natural kernel dwd reproducing kernel hilbert space establish bayes risk consistency kernel dwd compare dwd svm several benchmark data sets show two comparable classification accuracy dwd equipped new algorithm much faster compute svm\",\"classification problems datasets usually imbalanced noisy complex sampling algorithms make improvements linear sampling mechanism synthetic minority oversampling technique smote nevertheless linear oversampling several unavoidable drawbacks linear oversampling susceptible overfitting synthetic samples lack diversity rarely account original distribution characteristics informed nonlinear oversampling framework granular ball ingb new direction oversampling proposed paper uses granular balls simulate spatial distribution characteristics datasets informed entropy utilized optimize granularball space nonlinear oversampling performed following highdimensional sparsity isotropic gaussian distribution furthermore ingb good compatibility combined smotebased sampling algorithms improve performance also easily extended noisy imbalanced multiclassification problems mathematical model theoretical proof ingb given work extensive experiments demonstrate ingb outperforms traditional linear sampling frameworks algorithms oversampling complex datasets\",\"receiver operating characteristic roc analysis widely used evaluating diagnostic systems recent studies shown estimating area roc curve auc standard crossvalidation methods suffers large bias leavepairout lpo crossvalidation shown correct bias however lpo produces almost unbiased estimate auc provide ranking data needed plotting analyzing roc curve study propose new method called tournament leavepairout tlpo crossvalidation method extends lpo creating tournament pair comparisons produce ranking data tlpo preserves advantage lpo estimating auc also allows performing roc analyses shown using synthetic real world data tlpo reliable lpo auc estimation confirmed bias leaveoneout crossvalidation lowdimensional data case study roc analysis also evaluate reliably sensitivity specificity estimated tlpo roc curves\",\"numbers numerical vectors account large portion data however recently amount string data generated increased dramatically consequently classifying string data common problem many fields widely used approach problem convert strings numerical vectors using string kernels subsequently apply support vector machine works numerical vector space however nononetoone conversion involves loss information makes impossible evaluate using probability theory generalization error learning machine considering given data train test machine strings generated according probability laws study approach classification problem constructing classifier works set strings evaluate generalization error classifier theoretically probability theory strings required therefore first extend limit theorem asymptotic behavior consensus sequence strings counterpart mean numerical vectors demonstrated probability theory metric space strings developed one authors colleague previous study using obtained result demonstrate learning machine classifies strings asymptotically optimal manner furthermore demonstrate usefulness machine practical data analysis applying predicting proteinprotein interactions using amino acid sequences\",\"classification dissimilarity space become active research area since provides possibility learn data given form pairwise nonmetric dissimilarities otherwise would difficult cope selection prototypes key step creation space however despite previous efforts find good prototypes select best representation set remains open issue paper proposed scalable methods select set prototypes large datasets methods based genetic algorithms dissimilaritybased hashing two different unsupervised supervised scalable criteria unsupervised criterion based minimum spanning tree graph created prototypes nodes dissimilarities edges supervised criterion based counting matching labels objects closest prototypes suitability type algorithms analyzed specific case dissimilarity representations experimental results showed methods select good prototypes taking advantage large datasets low runtimes\",\"modern machine learning systems image classifiers rely heavily large scale data sets training data sets costly create thus practice small number freely available open source data sets widely used suggest examining geodiversity open data sets critical adopting data set use cases developing world analyze two large publicly available image data sets assess geodiversity find data sets appear exhibit observable amerocentric eurocentric representation bias analyze classifiers trained data sets assess impact training distributions find strong differences relative performance images different locales results emphasize need ensure georepresentation constructing data sets use developing world\",\"characteristics numerical patterns feature vector transform domain perturbation model differ significantly corresponding feature vector input domain differences caused perturbation techniques used transformation feature patterns degrade performance machine learning techniques transform domain paper proposed nonlinear parametric perturbation model transforms input feature patterns set elliptical patterns studied performance degradation issues associated random forest classification technique using input transform domain features compared linear transformation principal component analysis pca proposed method requires less statistical assumptions highly suitable applications data privacy security due difficulty inverting elliptical patterns transform domain input domain addition adopted flexible blockwise dimensionality reduction step proposed method accommodate possible highdimensional data modern applications evaluated empirical performance proposed method network intrusion data set biological data set compared results pca terms classification performance data privacy protection measured blind source separation attack signal interference ratio results confirmed superior performance proposed elliptical transformation\",\"concept refinement probability elicitation considered proper scoring rules taking directions axioms probability refinement clarified using hilbert space interpretation reformulated underlying data distribution setting connections maximal marginal diversity conditional entropy considered used derive measures provide arbitrarily tight bounds bayes error refinement also reformulated classifier output setting connections calibrated classifiers proper margin losses established\",\"finding statistically significant highorder interaction features predictive modeling important challenging task difficulty lies fact recent applications highdimensional covariates number possible highorder interaction features would extremely large identifying statistically significant features huge pool candidates would highly challenging computational statistical senses work problem consider two stage algorithm first select set highorder interaction features marginal screening make statistical inferences regression model fitted selected features statistical inferences called postselection inference psi receiving increasing attention literature one seminal recent advancements psi literature works lee authors presented algorithmic framework computing exact sampling distributions psi main challenge applying approach highorder interaction models cope fact psi general depends selected features also unselected features making hard apply extremely highdimensional highorder interaction models goal paper overcome difficulty introducing novel efficient method psi key idea exploit underlying tree structure among highorder interaction features develop pruning method tree enables quickly identify group unselected features guaranteed influence psi experimental results indicate proposed method allows reliably identify statistically significant highorder interaction features reasonable computational cost\",\"feature selection highdimensional data small proportion relevant features poses severe challenge standard statistical methods developed new approach harvest straightforward apply albeit somewhat computerintensive algorithm used prescreen large number features identify potentially useful basic idea evaluate feature context many random subsets features harvest predicated assumption irrelevant feature add real predictive value regardless features included subset motivated idea derived simple statistical test feature relevance empirical analyses simulations produced far indicate harvest algorithm highly effective predictive analytics science business\",\"novel linear classification method possesses merits support vector machine svm distanceweighted discrimination dwd proposed article proposed distanceweighted support vector machine method viewed hybrid svm dwd finds classification direction minimizing mainly dwd loss determines intercept term svm manner show method inheres merit dwd hence overcomes datapiling overfitting issue svm hand new method subject imbalanced data issue main advantage svm dwd uses unusual loss combines hinge loss svm dwd loss trick axillary hyperplane several theoretical properties including fisher consistency asymptotic normality dwsvm solution developed use simulated examples show new method compete dwd svm classification performance interpretability real data application establishes usefulness approach\",\"classification important topic statistics machine learning great potential many real applications paper investigate two popular large margin classification methods support vector machine svm distance weighted discrimination dwd two contexts highdimensional lowsample size data imbalanced data unified family classification machines flexible assortment machine flame proposed within dwd svm special cases flame family helps identify similarities differences svm dwd well known many classifiers overfit data highdimensional setting others sensitive imbalanced data class larger sample size overly influences classifier pushes decision boundary towards minority class svm resistant imbalanced data issue overfits highdimensional data sets showing undesired datapiling phenomena dwd method proposed improve svm highdimensional setting decision boundary sensitive imbalanced ratio sample sizes flame family helps understand intrinsic connection svm dwd improves methods providing better tradeoff sensitivity imbalanced data overfitting highdimensional data several asymptotic properties flame classifiers studied simulations real data applications investigated illustrate usefulness flame classifiers\",\"selecting important features nonlinear kernel spaces difficult challenge classification regression problems many features irrelevant kernel methods support vector machine kernel ridge regression sometimes perform poorly propose weighting features within kernel sparse set weights estimated conjunction original classification regression problem iterative algorithm knife alternates finding coefficients original problem finding feature weights kernel linearization addition slight modification knife yields efficient algorithm finding feature regularization paths paths features weight simulation results demonstrate utility knife kernel regression support vector machines variety kernels feature path realizations also reveal important nonlinear correlations among features prove useful determining subset significant variables results vowel recognition data parkinsons disease data microarray data also given\",\"fundamental question data analysis machine learning signal processing compare data points choice distance metric specifically challenging highdimensional data sets problem meaningfulness prominent euclidean distance images paper propose exploit property highdimensional data usually ignored structure stemming relationships coordinates specifically show organizing similar coordinates clusters exploited construction mahalanobis distance samples observable samples generated nonlinear transformation hidden variables mahalanobis distance allows recovery euclidean distances hidden spacewe illustrate advantage approach synthetic example discovery clusters correlated coordinates improves estimation principal directions samples method applied real data gene expression lung adenocarcinomas lung cancer using proposed metric found partition subjects risk groups good separation kaplanmeier survival plot\",\"propose simple kernel based nearest neighbor approach handwritten digit classification distance actually kernel defining similarity two images carefully study effects different number neighbors weight schemes report results nearest neighbors similar images vote test set error rate mnist database could reach close many advanced models\",\"consider classifiers highdimensional data strongly spiked eigenvalue sse model first show highdimensional data often sse model consider distancebased classifier using eigenstructures sse model apply noise reduction methodology estimation eigenvalues eigenvectors sse model create new distancebased classifier transforming data sse model nonsse model give simulation studies discuss performance new classifier finally demonstrate new classifier using microarray data sets\",\"scaled complex wishart distribution widely used model multilook full polarimetric sar data whose adequacy attested literature classification segmentation image analysis techniques depend model devised many employ type dissimilarity measure paper derive analytic expressions four stochastic distances relaxed scaled complex wishart distributions general form important particular cases using distances inequalities obtained lead new ways deriving bartlett revised wishart distances expressiveness four analytic distances assessed respect variation parameters distances used deriving new tests statistics proved asymptotic chisquare distribution adopting test size comparison criterion sensitivity study performed means monte carlo experiments suggesting bhattacharyya statistic outperforms others power tests also assessed applications actual data illustrate discrimination homogeneity identification capabilities distances\",\"paper generalizes important result pacbayesian literature binary classification case ensemble methods structured outputs prove generic version cbound upper bound risk models expressed weighted majority vote based first second statistical moments votes margin bound may advantageously applied complex outputs multiclass labels multilabel allow consider margin relaxations results open way develop new ensemble methods structured output prediction pacbayesian guarantees\",\"dimensionality reduction topic recent interest paper present classification constrained dimensionality reduction ccdr algorithm account label information algorithm account multiple classes well semisupervised setting present outofsample expressions labeled unlabeled data unlabeled data introduce method embedding new point preprocessing classifier labeled data introduce method improves embedding training phase using outofsample extension investigate classification performance using ccdr algorithm hyperspectral satellite imagery data demonstrate performance gain local global classifiers demonstrate improvement knearest neighbors algorithm performance present connection intrinsic dimension estimation optimal embedding dimension obtained using ccdr algorithm\",\"support vector machines svms naturally embody sparseness due use hinge loss functions however svms directly estimate conditional class probabilities paper propose study family coherence functions convex differentiable surrogates hinge function coherence function derived using maximumentropy principle characterized temperature parameter bridges hinge function logit function logistic regression limit coherence function zero temperature corresponds hinge function limit minimizer expected error minimizer expected error hinge loss refer use coherence function largemargin classification clearning present efficient coordinate descent algorithms training regularized cal clearning models\",\"security issues crucial number machine learning applications especially scenarios dealing human activity rather natural phenomena information ranking spam detection malware detection etc expected cases learning algorithms deal manipulated data aimed hampering decision making although previous work addressed handling malicious data context supervised learning little known behavior anomaly detection methods scenarios contribution analyze performance particular method online centroid anomaly detection presence adversarial noise analysis addresses following securityrelated issues formalization learning attack processes derivation optimal attack analysis efficiency constraints derive bounds effectiveness poisoning attack centroid anomaly different conditions bounded unbounded percentage traffic bounded false positive rate bounds show whereas poisoning attack effectively staged unconstrained case made arbitrarily difficult strict upper bound attackers gain external constraints properly used experimental evaluation carried real http exploit traces confirms tightness theoretical bounds practicality protection mechanisms\",\"article derive concentration inequalities crossvalidation estimate generalization error subagged estimators classification regressor general loss functions class predictors finite infinite vcdimension considered slightly generalize formalism introduced citedud cover large variety crossvalidation procedures including leaveoneout crossvalidation kfold crossvalidation holdout crossvalidation split sample leaveupsilonout crossvalidation bigskip noindent interesting consequence probability upper bound bounded minimum hoeffdingtype bound vapniktype bounds thus smaller even small learning set finally give simple rule subbag predictor bigskip\",\"sparse classifiers support vector machines svm efficient testphases classifier characterized subset samples called support vectors svs rest samples non svs influence classification result however advantage sparsity fully exploited training phases generally difficult know sample turns beforehand paper introduce new approach called safe sample screening enables identify subset nonsvs screen prior training phase approach different existing heuristic approaches sense screened samples guaranteed nonsvs optimal solution investigate advantage safe sample screening approach intensive numerical experiments demonstrate substantially decrease computational cost stateoftheart svm solvers libsvm current big data era believe safe sample screening would great practical importance since data size reduced without sacrificing optimality final solution\",\"modern data analyst must cope data encoded various forms vectors matrices strings graphs consequently statistical machine learning models tailored different data encodings important focus data encoded normalized vectors direction important magnitude specifically consider highdimensional vectors lie either surface unit hypersphere real projective plane data briefly review common mathematical models prevalent machine learning also outlining technical aspects software applications open mathematical challenges\",\"tomal introduced notion phalanxes context rareclass detection twoclass classification problems phalanx subset features work well classification tasks paper propose different class phalanxes application regression settings define regression phalanx subset features work well together prediction propose novel algorithm automatically chooses regression phalanxes highdimensional data sets using hierarchical clustering builds prediction model phalanx ensembling extensive simulation studies several reallife applications various areas including drug discovery chemical analysis spectra data microarray analysis climate projections show ensemble regression phalanxes improves prediction accuracy combined effective prediction methods like lasso random forests\",\"study problem interactively learning binary classifier using noisy labeling pairwise comparison oracles comparison oracle answers one given two instances likely positive learning oracles multiple applications obtaining direct labels harder pairwise comparisons easier algorithm leverage types oracles paper attempt characterize access easier comparison oracle helps improving label total query complexity show comparison oracle reduces learning problem learning threshold function present algorithm interactively queries label comparison oracles characterize query complexity tsybakov adversarial noise conditions comparison labeling oracles lower bounds show label total query complexity almost optimal\",\"paper novel approach coding nominal data proposed given nominal data rank form complex number assigned proposed method lose information attribute brings properties previously unknown approach based knew properties used classification analyzed example shows classification use coded nominal data numerical well coded nominal data effective classification uses numerical data\",\"study losses binary classification class probability estimation extend understanding margin losses general composite losses composition proper loss link function characterise margin losses proper composite losses explicitly show determine symmetric loss full half one partial losses introduce intrinsic parametrisation composite binary losses give complete characterisation relationship proper losses classification calibrated losses also consider question best surrogate binary loss introduce precise notion best show exist situations two convex surrogate losses incommensurable provide complete explicit characterisation convexity composite binary losses terms link function weight function associated proper loss make composite loss characterisation suggests new ways surrogate tuning finally appendix present new algorithmindependent results relationship properness convexity robustness misclassification noise binary losses show convex proper losses nonrobust misclassification noise\",\"present surrogate regret bounds arbitrary surrogate losses context binary classification labeldependent costs bounds relate classifiers risk assessed respect surrogate loss costsensitive classification risk two approaches surrogate regret bounds developed first direct generalization bartlett focus marginbased losses costinsensitive classification second adopts framework steinwart based calibration functions nontrivial surrogate regret bounds shown exist precisely surrogate loss satisfies calibration condition easily verified many common losses apply theory class uneven margin losses characterize losses properly calibrated uneven hinge squared error exponential sigmoid losses treated detail\",\"paper presents approach automation interpretable feature selection internet things analytics iota using machine learning techniques authors conducted survey different people involved different iota based application development tasks survey reveals feature selection time consuming niche skill demanding part entire workflow paper shows feature selection successfully automated without sacrificing decision making accuracy thereby reducing project completion time cost hiring expensive resources several pattern recognition principles state art soa techniques followed design overall approach proposed automation three data sets considered establish proofofconcept experimental results show proposed automation able reduce time feature selection days instead months would required absence automation reduction time achieved without sacrifice accuracy decision making process proposed method also compared multi layer perceptron mlp model state art works iota uses mlp based deep learning moreover feature selection method compared soa feature reduction technique namely principal component analysis pca variants results obtained show proposed method effective\",\"regularized logistic regression become workhorse data mining bioinformatics widely used many classification problems particularly ones many features however regularization typically selects many features socalled false positives unavoidable paper demonstrate analyze aggregation method sparse logistic regression high dimensions approach linearly combines estimators suitable set logistic models different underlying sparsity patterns balance predictive ability model interpretability numerical performance proposed aggregation method investigated using simulation studies also analyze published genomewide casecontrol dataset evaluate usefulness aggregation method multilocus association mapping\",\"support vector machines svms special kernel based methods belong successful learning methods since decade svms informally described kind regularized mestimators functions demonstrated usefulness many complicated reallife problems last years great part statistical research svms concentrated question design svms universally consistent statistically robust nonparametric classification nonparametric regression purposes many applications qualitative prior knowledge distribution unknown function estimated present prediction function good interpretability desired semiparametric model additive model interest paper mainly address question design svms choosing reproducing kernel hilbert space rkhs corresponding kernel obtain consistent statistically robust estimators additive models give explicit construction kernels thus rkhss leads combination lipschitz continuous loss function consistent statistically robust smvs additive models examples quantile regression based pinball loss function regression based epsiloninsensitive loss function classification based hinge loss function\",\"distance metric plays important role nearest neighbor classification usually euclidean distance metric assumed mahalanobis distance metric optimized improve performance paper study problem embedding arbitrary metric spaces euclidean space goal improve accuracy classifier propose solution appealing framework regularization reproducing kernel hilbert space prove representerlike theorem classification embedding function determined solving semidefinite program interesting connection softmargin linear binary support vector machine classifier although main focus paper present general theoretical framework metric embedding setting demonstrate performance proposed method benchmark datasets show performs better mahalanobis metric learning algorithm terms leaveoneout generalization errors\",\"thesis responds challenges using large number thousands features regression classification problems two situations high dimensional features arise one high dimensional measurements available example gene expression data produced microarray techniques computational reasons people may select small subset features modelling data looking relevant features predicting response based measure correlation response training data although used commonly procedure make response appear predictable actually chapter propose bayesian method avoid selection bias application naive bayes models mixture models high dimensional features also arise consider highorder interactions number parameters increase exponentially order considered chapter propose method compressing group parameters single one exploiting fact many predictor variables derived highorder interactions values training cases number compressed parameters may converged considering highest possible order apply compression method logistic sequence prediction models logistic classification models use simulated data real data test methods chapters\",\"last years many different performance measures introduced overcome weakness natural metric accuracy among matthews correlation coefficient recently gained popularity among researchers machine learning also several application fields bioinformatics nonetheless novel functions proposed literature show confusion entropy recently introduced classifier performance measure multiclass problems strong monotone relation multiclass generalization classical metric matthews correlation coefficient computational evidence support claim provided together outline theoretical explanation\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"10_classification_data_dwd\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"10_classification_data_dwd\"],\"textfont\":{\"size\":12},\"x\":[9.729904,9.534378,9.772772,10.265833,9.023551,9.7525425,9.511797,9.25154,9.050691,9.352751,9.397681,9.213044,9.004322,9.249128,9.712189,9.724478,9.150748,12.381687,9.683805,9.044849,9.151164,9.575499,9.731778,9.660187,8.942875,9.357816,9.222107,9.305546,9.236183,9.005137,8.953154,9.71683,9.56632,9.273923,10.296243,9.301384,9.218102,9.161334,9.659628,9.346941,8.491229,9.297955,9.365847,9.585743,9.542674,9.156692,9.123202,9.67097,9.160177,9.304178,9.050243,9.3225975,9.840618,9.715976,9.129945,10.000574,9.039379,9.763695,9.696937,9.171104,10.260226,9.611491,12.307655,8.963783,9.247682,9.328895,9.222317,9.109649,9.143522,9.081958,9.696565,9.718627,9.109129,9.448115,9.47526],\"y\":[7.017607,7.047544,7.0232763,7.2520766,6.405598,6.966125,8.120256,7.6867337,7.286662,7.4525237,7.820278,7.2359476,7.734664,7.6105375,7.2020726,7.0857077,6.434095,4.198543,7.0538054,7.3858485,7.1521506,7.486254,6.84223,6.940281,7.676517,7.7790127,7.513151,7.5923767,7.529936,6.4813976,6.45652,7.5236573,6.955741,7.8879337,7.2544928,6.6669693,7.652514,7.1723685,6.5472393,7.4049654,7.6179523,6.634484,7.543026,7.06756,7.019503,7.0495305,7.8014364,6.600458,6.739726,7.362304,7.298558,6.5877037,6.553405,6.940823,7.186322,6.354818,7.806567,6.787029,7.5071077,7.189967,7.263575,7.4058156,4.2517667,6.3557267,7.7700467,7.040026,7.7311215,7.7437,6.8989716,6.4905686,7.5458922,6.978999,6.476769,7.0521297,7.097149],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"traditionally practitioners initialize kmeans algorithm centers chosen uniformly random randomized initialization uneven weights kmeans recently used improve performance strategy cost runtime consider kmeans problem semisupervised information data prelabeled seek label rest according minimum cost solution extending kmeans algorithm analysis account labels derive improved theoretical bound expected cost observe improved performance simulated real data examples analysis provides theoretical justification roughly linear semisupervised clustering algorithm\",\"symbolic data analysis based special descriptions data symbolic objects descriptions preserve detailed information units clusters usual representations mean values special kind symbolic object representation frequency probability distributions modal values representation enables consider clustering process variables measurement types time paper clustering criterion function sos proposed representative cluster composed distributions variables values cluster corresponding leaders clustering method based result also shown corresponding agglomerative hierarchical method generalized wards formula holds methods compatible solving clustering optimization problem leaders method efficiently solves clustering problems large number units agglomerative method applied alone smaller data set could applied leaders obtained compatible nonhierarchical clustering method combination two compatible methods enables decide upon right number clusters basis corresponding dendrogram proposed methods applied different data sets paper results clustering ess data presented\",\"present technique clustering categorical data generating many dissimilarity matrices averaging begin demonstrating technique low dimensional categorical data comparing several techniques proposed give conditions method yield good results general method extends high dimensional categorical data equal lengths ensembling many choices explanatory variables context compare method two methods finally extend method high dimensional categorical data vectors unequal length using alignment techniques equalize lengths give examples show method continues provide good results particular better context genome sequences clusterings suggested phylogenetic trees\",\"identifying homogeneous subgroups variables challenging high dimensional data analysis highly correlated predictors propose new method called hexagonal operator regression shrinkage equality selection horses short simultaneously selects positively correlated variables identifies predictive clusters achieved via constrained leastsquares problem regularization consists linear combination penalty coefficients another penalty pairwise differences coefficients specification penalty function encourages grouping positively correlated predictors combined sparsity solution construct efficient algorithm implement horses procedure show via simulation proposed method outperforms variable selection methods terms prediction error parsimony technique demonstrated two data sets small data set analysis soil appalachia high dimensional data set near infrared nir spectroscopy study showing flexibility methodology\",\"introduce develop novel approach outlier detection based adaptation random subspace learning proposed method handles highdimension lowsample size traditional lowdimensional highsample size datasets essentially avoid computational bottleneck techniques like minimum covariance determinant mcd computing needed determinants associated measures much lower dimensional subspaces theoretical computational development approach reveal computationally efficient regularized methods highdimensional lowsample size often competes favorably existing methods far percentage correct outlier detection concerned\",\"paper consider clustering data assumed come one finitely many pointed convex polyhedral cones model referred union polyhedral cones uopc model similar union subspaces uos model data subspace generated unknown basis uopc model data cone assumed generated finite number unknown emphextreme raysto cluster data model consider several algorithms sparse subspace clustering nonnegative constraints lasso ncl least squares approximation lsa knearest neighbor knn algorithm arrive affinity data points spectral clustering applied resulting affinity matrix cluster data different polyhedral cones show average knn outperforms ncl lsa algorithm provide deterministic conditions correct clustering affinity measure cones shown long cones coherent long density data within cone exceeds threshold knn leads accurate clustering finally simulation results real datasets mnist yaleface datasets depict proposed algorithm works well real data indicating utility uopc model proposed algorithm\",\"clustering one important unsupervised problems machine learning statistics among many existing algorithms kernel kmeans drawn much research attention due ability find nonlinear cluster boundaries inherent simplicity two main approaches kernel kmeans svd kernel matrix convex relaxations despite attention kernel clustering received theoretical applied quarters much known robustness methods paper first introduce semidefinite programming relaxation kernel clustering problem prove suitable model specification ksvd sdp approaches consistent limit albeit sdp strongly consistent achieves exact recovery whereas ksvd weakly consistent fraction misclassified nodes vanish\",\"extremes play special role anomaly detection beyond inference simulation purposes probabilistic tools borrowed extreme value theory evt angular measure also used design novel statistical learning methods anomaly detectionranking paper proposes new algorithm based multivariate evt learn rank observations high dimensional space respect degree abnormality procedure relies original dimensionreduction technique extreme domain possibly produces sparse representation multivariate extremes allows gain insight dependence structure thereof escaping curse dimensionality representation output unsupervised methodology propose combined anomaly detection technique tailored nonextreme data performs linearly dimension almost linearly data odn log fits large scale problems approach paper novel evt never used multivariate version field anomaly detection illustrative experimental results provide strong empirical evidence relevance approach\",\"algorithm one many important tools field statistics often used imputing missing data widespread applications include common statistical tasks clustering clustering algorithm assumes parametric distribution clusters whose parameters estimated novel iterative procedure based theory maximum likelihood however one major drawback algorithm renders impractical especially working large datasets often requires several passes data convergence paper introduce new emstyle algorithm implements novel policy performing partial esteps call new algorithm emtau algorithm approximate traditional algorithm high accuracy fraction running time\",\"conceptual framework cluster analysis viewpoint padic geometry introduced describing space dendrograms datapoints relating moduli space padic riemannian spheres punctures using method recently applied murtagh method embeds dendrogram subtree bruhattits tree associated padic numbers goes back cornelissen padic geometry explaining definitions concept classifiers discussed context moduli spaces upper bounds number hidden vertices dendrograms given\",\"explore performance several automatic bandwidth selectors originally designed density gradient estimation databased procedures nonparametric modal clustering key tool obtain clustering density gradient estimators mean shift algorithm allows obtain partition data sample also whole space results simulation study suggest methods considered like cross validation plug bandwidth selectors useful cluster analysis via mean shift algorithm\",\"construct framework studying clustering algorithms includes two key ideas persistence functoriality first encodes idea output clustering scheme carry multiresolution structure second idea one able compare results clustering algorithms one varies data set example adding points applying functions show within framework one prove theorem analogous one kleinberg one obtains existence uniqueness theorem instead nonexistence result explore properties unique scheme stability convergence established\",\"describe many vantage points baire metric use clustering data use preprocessing structuring data order support search retrieval operations cases proceed directly clusters directly determine distances show hierarchical clustering read directly one pass data offer insights also practical implications precision data measurement mechanism treating multidimensional data including high dimensional data use random projections\",\"modes ridges probability density function behind observed data useful geometric features modeseeking clustering assigns cluster labels associating data samples nearest modes estimation density ridges enables find lowerdimensional structures hidden data key technical challenge modeseeking clustering density ridge estimation accurate estimation ratios first secondorder density derivatives density naive approach takes threestep approach first estimating data density computing derivatives finally taking ratios however threestep approach unreliable good density estimator necessarily mean good density derivative estimator division estimated density could significantly magnify estimation error cope problems propose novel estimator emphdensityderivativeratios proposed estimator involve density estimation rather emphdirectly approximates ratios density derivatives order moreover establish convergence rate proposed estimator based proposed estimator novel methods modeseeking clustering density ridge estimation developed respective convergence rates mode ridge underlying density also established finally experimentally demonstrate developed methods significantly outperform existing methods particularly relatively highdimensional data\",\"dendrograms used data analysis ultrametric spaces hence objects nonarchimedean geometry known exist padic representation dendrograms completed point infinity viewed subtrees bruhattits tree associated padic projective line implications certain moduli spaces known algebraic geometry padic parameter spaces families dendrograms stochastic classification also handled within framework end calculate topology hidden part dendrogram\",\"investigate role initialization stability kmeans clustering algorithm opposed papers consider actual kmeans algorithm ignore property getting stuck local optima interested actual clustering costs solution analyze different initializations lead local optimum lead different local optima enables prove reasonable select number clusters based stability scores\",\"many situations interest lies identifying clusters one might expect available variables carry information groups furthermore data quality outliers missing entries might present serious sometimes hardtoassess problem large complex datasets paper show small proportion atypical observations might serious adverse effects solutions found sparse clustering algorithm witten tibshirani propose robustification sparse kmeans algorithm based trimmed kmeans algorithm cuestaalbertos proposal also able handle datasets missing values illustrate use method microarray data cancer patients able identify strong biological clusters much reduced number genes simulation studies show outliers data robust sparse kmeans algorithm performs better competing methods terms selection features also identified clusters robust sparse kmeans algorithm implemented package rskc publicly available cran repository\",\"determination cluster centers generally depends scale use analyze data clustered inappropriate scale usually leads unreasonable cluster centers thus unreasonable results study first consider similarity elements data connectivity nodes undirected graph present concept connection center regard cluster center data based definition determination cluster centers assignment class simple natural effective one crucial finding cluster centers different scales obtained easily different powers similarity matrix change power small large leads dynamic evolution cluster centers local microscopic global microscopic process evolution number categories changes discontinuously means presented method automatically skip unreasonable number clusters suggest appropriate observation scales provide corresponding cluster results\",\"recent popularity graphical clustering methods increased focus information samples show learning cluster structure using edge features naturally simultaneously determines likely number clusters addresses data scale issues results particularly useful instances large number clusters labeled edges applications domain include image segmentation community discovery entity resolution model extension planted partition model solution uses results correlation clustering achieves partition olognclose loglikelihood true clustering\",\"logdensity gradient estimation fundamental statistical problem possesses various practical applications clustering measuring nongaussianity naive twostep approach first estimating density taking loggradient unreliable accurate density estimate necessarily lead accurate logdensity gradient estimate cope problem method directly estimate logdensity gradient without density estimation explored demonstrated work much better twostep method objective paper improve performance direct method multidimensional cases idea regard problem logdensity gradient estimation dimension task apply regularized multitask learning direct logdensity gradient estimator experimentally demonstrate usefulness proposed multitask method logdensity gradient estimation modeseeking clustering\",\"paper studies ordered weighted owl norm regularization sparse estimation problems strongly correlated variables prove sufficient conditions clustering based correlationcolinearity variables using owl norm socalled oscar particular case results extend previous ones oscar several ways squared error loss conditions hold general owl norm weaker assumptions also establish clustering conditions absolute error loss far know novel result furthermore characterize statistical performance owl norm regularization generative models certain clusters regression variables strongly even perfectly correlated variables different clusters uncorrelated show true pdimensional signal generating data involves clusters log samples suffice accurately estimate signal regardless number coefficients within clusters estimation ssparse signals completely independent variables requires many measurements words using owl pay price terms number measurements presence strongly correlated variables\",\"distributional distributionvalued data new type data arising several sources considered realizations distributional variables new set fuzzy cmeans algorithms data described distributional variables proposed algorithms use wasserstein distance distributions dissimilarity measures beside extension fuzzy cmeans algorithm distributional data considering decomposition squared wasserstein distance propose set algorithms using different automatic way compute weights associated variables well components globally clusterwise relevance weights computed clustering process introducing producttoone constraints relevance weights induce adaptive distances expressing importance variable component clustering process acting also variable selection method clustering tested proposed algorithms artificial realworld data results confirm proposed methods able better take account cluster structure data respect standard fuzzy cmeans nonadaptive distances\",\"propose method performs anomaly detection localisation within heterogeneous data using pairwise undirected mixed graphical model data mixture categorical quantitative variables model learned dataset supposed contain anomaly use model temporal data potentially data stream using version twosided cusum algorithm proposed decision statistic based conditional likelihood ratio computed variable given others results show function allows detect anomalies variable variable thus localise variables involved anomalies precisely univariate methods based simple marginals\",\"paper present novel method coclustering unsupervised learning approach aims discovering homogeneous groups data instances features grouping simultaneously proposed method uses entropy regularized optimal transport empirical measures defined data instances features order obtain estimated joint probability density function represented optimal coupling matrix matrix factorized obtain induced row columns partitions using multiscale representations approach justify method theoretically show solution regularized optimal transport seen variational inference perspective thus motivating use coclustering algorithm derived proposed method kernelized version based notion gromovwasserstein distance fast accurate determine automatically number row column clusters features vividly demonstrated extensive experimental evaluations\",\"distancebased hierarchical clustering methods widely used unsupervised data analysis authors take account uncertainty distance data incorporate statistical model uncertainty corruption noise pairwise distances investigate problem estimating unknown parameters measurements specifically focus single linkage hierarchical clustering slhc study geometry prove fairly reasonable conditions probability distribution governing measurements slhc equivalent maximum partial profile likelihood estimation mpple information contained data ignored time show direct evaluation slhc maximum likelihood estimation mle pairwise distances yields consistent estimator consequently full mle expected perform better slhc getting correct results ground truth metric\",\"clustering central approach unsupervised learning clustering applied fundamental analysis quantitatively compare clusterings comparisons crucial evaluation clustering methods well tasks consensus clustering often argued order establish baseline clustering similarity assessed context random ensemble clusterings prevailing assumption random clustering ensemble permutation model number sizes clusters fixed however assumption necessarily hold practice example multiple runs kmeans clustering returns clusterings fixed number clusters cluster size distribution varies greatly derive corrected variants two clustering similarity measures rand index mutual information context two random clustering ensembles number sizes clusters vary addition study impact onesided comparisons scenario reference clustering consequences different random models illustrated using synthetic examples handwriting recognition gene expression data demonstrate choice random model drastic impact ranking similar clustering pairs evaluation clustering method respect random baseline thus choice random clustering model carefully justified\",\"work robust clustering algorithm stationary time series proposed algorithm based use estimated spectral densities considered functional data basic characteristic stationary time series clustering purposes robust algorithm functional data applied set spectral densities trimming techniques restrictions scatter within groups reduce effect noise data help prevent identification spurious clusters procedure tested simulation study also applied real data set\",\"train statistical mixture model massive data set work show construct coresets mixtures gaussians coreset weighted subset data guarantees models fitting coreset also provide good fit original data set show perhaps surprisingly gaussian mixtures admit coresets size polynomial dimension number mixture components independent data set size hence one harness computationally intensive algorithms compute good approximation significantly smaller data set importantly coresets efficiently constructed distributed streaming settings impose restrictions data generating process results rely novel reduction statistical estimation problems computational geometry new combinatorial complexity results mixtures gaussians empirical evaluation several realworld datasets suggests coresetbased approach enables significant reduction trainingtime negligible approximation error\",\"paper formulate general terms approach prove strong consistency empirical risk minimisation inductive principle applied prototype distance based clustering approach motivated divisive informationtheoretic feature clustering model probabilistic space kullbackleibler divergence may regarded special case within clustering minimisation framework also propose clustering regularization restricting creation additional clusters significant essentially different comparing existing clusters\",\"investigations performed using clustering methods data mining timeseries data smart meters problem identify patterns trends energy usage profiles commercial industrial customers hour periods group similar profiles tested method energy usage data provided several power utilities results show accurate grouping accounts similar energy usage patterns potential method utilized energy efficiency programs\",\"several application domains highdimensional observations collected analysed search naturally occurring data clusters might provide insights nature problem paper describe new approach partitioning highdimensional data assumption within cluster data approximated well linear subspace estimated means principal component analysis pca proposed algorithm predictive subspace clustering psc partitions data clusters simultaneously estimating clusterwise pca parameters algorithm minimises objective function depends upon new measure influence pca models penalised version algorithm also described carrying simultaneous subspace clustering variable selection convergence psc discussed detail extensive simulation results comparisons competing methods presented comparative performance psc assessed six real gene expression data sets psc often provides stateofart results\",\"fundamental aim clustering algorithms partition data points consider tasks discovered partition allowed vary covariate space time one approach would use fragmentationcoagulation processes markov processes restricted linear tree structured covariate spaces define partitionvalued process arbitrary covariate space using gaussian processes use process construct multitask clustering model partitions datapoints similar way across multiple data sources time series model network data allows cluster assignments vary time describe sampling algorithms inference apply method defining cancer subtypes based different types cellular characteristics finding regulatory modules gene expression data multiple human populations discovering time varying community structure social network\",\"show objective function conventional kmeans clustering expressed frobenius norm difference data matrix low rank approximation data matrix short show kmeans clustering matrix factorization problem notes meant reference intended provide guided tour towards result often mentioned seldom made explicit literature\",\"many modern data mining applications concerned analysis datasets observations described paired highdimensional vectorial representations views typical examples found web mining genomics applications article present algorithm data clustering multiple views multiview predictive partitioning mvpp relies novel criterion predictive similarity data points assume within cluster dependence multivariate views modelled using twoblock partial least squares tbpls regression model performs dimensionality reduction particularly suitable highdimensional settings proposed mvpp algorithm partitions data withincluster predictive ability views maximised proposed objective function depends measure predictive influence points tbpls model derived extension press statistic commonly used ordinary least squares regression using simulated data compare performance mvpp competing multiview clustering methods rely upon geometric structures points ignore predictive relationship two views stateofart results obtained benchmark web mining datasets\",\"estimation density derivatives versatile tool statistical data analysis naive approach first estimate density compute derivative however twostep approach work well good density estimator necessarily mean good densityderivative estimator paper give direct method approximate density derivative without estimating density proposed estimator allows analytic computationally efficient approximation multidimensional highorder density derivatives ability hyperparameters chosen objectively crossvalidation show proposed densityderivative estimator useful improving accuracy nonparametric kldivergence estimation via metric learning practical superiority proposed method experimentally demonstrated change detection feature selection\",\"popular method selecting number clusters based stability arguments one chooses number clusters corresponding clustering results stable recent years series papers analyzed behavior method theoretical point view however results technical difficult interpret nonexperts paper give highlevel overview existing literature clustering stability addition presenting results slightly informal accessible way relate discuss different implications\",\"show dbscan estimate connected components lambdadensity level set lambda given iid samples unknown density characterize regularity level set boundaries using parameter beta analyze estimation error hausdorff metric data lies mathbbrd obtain rate widetildeonbeta matches known lower bounds logarithmic factors data lies embedded unknown ddimensional manifold mathbbrd obtain rate widetildeonbeta dcdot max beta finally provide adaptive parameter tuning order attain rates priori knowledge intrinsic dimension density beta\",\"standard clustering problems data points represented vectors stacking together one forms data matrix row column cluster structure paper consider class binary matrices arising many applications exhibit row column cluster structure goal exactly recover underlying row column clusters observing small fraction noisy entries first derive lower bound minimum number observations needed exact cluster recovery propose three algorithms different running time compare number observations needed successful cluster recovery analytical results show smooth timedata tradeoffs one gradually reduce computational complexity increasingly observations available\",\"investigate coresets succinct small summaries large data sets solutions found summary provably competitive solution found full data set provide overview stateoftheart coreset construction machine learning section present intuition behind theoretically sound framework construct coresets general problems apply kmeans clustering section summarize existing coreset construction algorithms variety machine learning problems maximum likelihood estimation mixture models bayesian nonparametric models principal component analysis regression general empirical risk minimization\",\"paper propose new fuzzy clustering algorithm based modeseeking framework given dataset mathbbrd define regions high density call cluster cores consider random walk neighborhood graph built top data points designed attracted high density regions strength attraction controlled temperature parameter beta membership point given cluster probability random walk hit corresponding cluster core many properties random walks hitting times commute distances etcdots shown enventually encode purely local information number data points grows show regularization introduced use cluster cores solves issue empirically show choice beta influences behavior algorithm small values beta result close hard modeseeking whereas beta close result similar output fuzzy spectral clustering finally demonstrate scalability approach providing fuzzy clustering protein configuration dataset containing million data points dimensions\",\"informationmaximization clustering learns probabilistic classifier unsupervised manner mutual information feature vectors cluster assignments maximized notable advantage approach involves continuous optimization model parameters substantially easier solve discrete optimization cluster assignments however existing methods still involve nonconvex optimization problems therefore finding good local optimal solution straightforward practice paper propose alternative informationmaximization clustering method based squaredloss variant mutual information novel approach gives clustering solution analytically computationally efficient way via kernel eigenvalue decomposition furthermore provide practical model selection procedure allows objectively optimize tuning parameters included kernel function experiments demonstrate usefulness proposed approach\",\"capturing dependence structure multivariate extreme events major concern many fields involving management risks stemming multiple sources portfolio monitoring insurance environmental risk management anomaly detection one convenient nonparametric characterization extremal dependence framework multivariate extreme value theory evt angular measure provides direct information probable directions extremes relative contribution featurecoordinate largest observations modeling angular measure high dimensional problems major challenge multivariate analysis rare events present paper proposes novel methodology aiming exhibiting sparsity pattern within dependence structure extremes done estimating amount mass spread angular measure representative sets directions corresponding specific subcones dimension reduction technique paves way towards scaling existing multivariate evt methods beyond nonasymptotic study providing theoretical validity framework method propose direct application first anomaly detection algorithm based multivariate evt algorithm builds sparse normal profile extreme behaviours confronted new possibly abnormal extreme observations illustrative experimental results provide strong empirical evidence relevance approach\",\"real data often contain anomalous cases also known outliers may spoil resulting analysis may also contain valuable information either case ability detect anomalies essential useful tool purpose robust statistics aims detect outliers first fitting majority data flagging data points deviate present overview several robust methods resulting graphical outlier detection tools discuss robust procedures univariate lowdimensional highdimensional data estimating location scatter linear regression principal component analysis classification clustering functional data analysis also challenging new topic cellwise outliers introduced\",\"one iteration standard kmeans lloyds algorithm standard gaussian mixture models gmms scales linearly number clusters data points data dimensionality study explore whether one iteration kmeans gmms scale sublinearly runtime improving clustering objective remains effective tool apply complexity reduction variational typically used make training generative models exponentially many hidden states tractable apply novel theoretical results truncated variational make tractable clustering algorithms efficient basic idea use partial variational estep reduces linear complexity mathcaloncd required full estep sublinear complexity main observation linear dependency reduced dependency much smaller parameter relates cluster neighborhood relations focus two versions partial variational clustering variational gmm scaling mathcalongd variational kmeans scaling mathcalongd per iteration empirical results show algorithms still require comparable numbers iterations improve clustering objective values kmeans data many clusters consequently observe reductions net computational demands two three orders magnitude generally results provide substantial empirical evidence favor clustering scale sublinearly\",\"introduce new unsupervised learning problem clustering widesense stationary ergodic stochastic processes covariancebased dissimilarity measure together asymptotically consistent algorithms designed clustering offline online datasets respectively also suggest formal criterion efficiency dissimilarity measures discuss approach improve efficiency clustering algorithms applied cluster particular type processes selfsimilar processes widesense stationary ergodic increments clustering synthetic data realworld data provided examples applications\",\"kernel methods popular clustering due generality discriminating power however show many kernel clustering criteria density biases theoretically explaining practically significant artifacts empirically observed past example provide conditions formally prove density mode isolation bias kernel kmeans common class kernels call breimans bias due similarity histogram mode isolation previously discovered breiman decision tree learning gini impurity also extend analysis popular kernel clustering methods averagenormalized cut dominant sets density biases take different forms example splitting isolated points cutbased criteria essentially sparsest subset bias opposite density mode bias findings suggest principled solution density biases kernel clustering directly address data inhomogeneity show density equalization implicitly achieved using either locally adaptive weights locally adaptive kernels moreover density equalization makes many popular kernel clustering objectives equivalent synthetic real data experiments illustrate density biases proposed solutions anticipate theoretical understanding kernel clustering limitations principled solutions important broad spectrum data analysis applications across disciplines\",\"improve current instabilitybased methods selection number clusters cluster analysis developing normalized cluster instability measure corrects distribution cluster sizes previously unaccounted driver cluster instability show normalized instability measure outperforms current instabilitybased measures across whole sequence possible especially overcomes limitations context large also compare first time modelbased modelfree approaches determine clusterinstability find performance comparable make method available rpackage verbcstab\",\"mean shift clustering finds modes data probability density identifying zero points density gradient since require fix number clusters advance mean shift popular clustering algorithm various application fields typical implementation mean shift first estimate density kernel density estimation compute gradient however since good density estimation necessarily imply accurate estimation density gradient indirect twostep approach reliable paper propose method directly estimate gradient logdensity without going density estimation proposed method gives global solution analytically thus computationally efficient develop meanshiftlike fixedpoint algorithm find modes density clustering mean shift one need set number clusters advance empirically show proposed clustering method works much better mean shift especially highdimensional data experimental results indicate proposed method outperforms existing clustering methods\",\"consider problem sparse clustering assumed subset features useful clustering purposes framework cosa method friedman meulman subsequently improved form sparse kmeans method witten tibshirani natural simpler hillclimbing approach introduced new method shown competitive two methods others\",\"consider problem clustering sequence multinomial observations way model selection criterion propose form penalty term model selection procedure approach subsumes conventional aic bic criteria also extends conventional criteria way applicable also sequence sparse multinomial observations even within cluster number multinomial trials may different different observations addition preliminary estimation step maximum likelihood estimation generally maximum estimation propose use reduced rank projection combination nonnegative factorization motivate approach showing model selection criterion preliminary estimation step yield consistent estimates simplifying assumptions also illustrate approach numerical experiments using real simulated data\",\"kernelbased kmeans clustering gained popularity due simplicity power implicit nonlinear representation data dominant concern memory requirement since memory scales square number data points provide new analysis class approximate kernel methods modest memory requirements propose specific onepass randomized kernel approximation followed standard kmeans transformed data analysis experiments suggest method accurate requiring drastically less memory standard kernel kmeans significantly less memory nystrom based approximations\",\"adjusted chance measures widely used compare partitionsclusterings data set particular adjusted rand index ari based paircounting adjusted mutual information ami based shannon information theory popular clustering community nonetheless open problem best application scenarios measure guidelines literature usage sparse result users often resort using generalized information theoretic measures based tsallis entropy shown link paircounting shannon measures paper aim bridge gap adjustment measures based paircounting measures based information theory solve key technical challenge analytically computing expected value variance generalized measures allows propose adjustments generalized measures reduce well known adjusted clustering comparison measures special cases using theory generalized measures able propose following guidelines using ari ami external validation indices ari used reference clustering large equal sized clusters ami used reference clustering unbalanced exist small clusters\",\"paper proposes original approach cluster multicomponent data sets including estimation number clusters construction minimal spanning tree prims algorithm assumption vertices approximately distributed according poisson distribution number clusters estimated thresholding prims trajectory corresponding cluster centroids computed order initialize generalized lloyds algorithm also known kmeans allows circumvent initialization problems results derived evaluating false positive rate cluster detection algorithm help approximations relevant euclidean spaces metrics used measuring similarity multidimensional data points based symmetrical divergences use informational divergences together proposed method leads better results compared clustering methods problem astrophysical data processing applications method multihyperspectral imagery domain satellite view paris image mars planet also presented order demonstrate usefulness divergences problem method informational divergence similarity measure compared method using classical metrics astrophysics application also compare method spectral clustering algorithms\",\"derive statistical model estimation dendrogram single linkage hierarchical clustering slhc takes account uncertainty noise corruption measurements separation data focus estimation hierarchy partitions afforded dendrogram rather heights latter concept estimating dendrogram structure introduced approximate maximum likelihood estimator mle dendrogram structure described ideas illustrated simple monte carlo simulation least small data sets suggests method outperforms slhc presence noise\",\"propose method estimating coefficients multivariate regression clustering structure response variables proposed method includes fusion penalty shrink difference fitted values responses cluster penalty simultaneous variable selection estimation method used grouping structure response variables known unknown clustering structure unknown method simultaneously estimate clusters response regression coefficients theoretical results presented penalized least squares case including asymptotic results allowing extend method setting responses binomial variables propose coordinate descent algorithm normal binomial likelihood easily extended generalized linear model glm settings simulations data examples business operations genomics presented show merits least squares binomial methods\",\"paper presents new approach nonparametric cluster analysis called adaptive weights clustering awc idea identify clustering structure checking different points different scales departure local homogeneity proposed procedure describes clustering structure terms weights wij measures degree local inhomogeneity two neighbor local clusters using statistical tests gap procedure starts local scale parameter locality grows factor step method fully adaptive require specify number clusters structure clustering results sensitive noise outliers procedure able recover different clusters sharp edges manifold structure method scalable computationally feasible intensive numerical study shows stateoftheart performance method various artificial examples applications text data theoretical study states optimal sensitivity awc local inhomogeneity\",\"goal data clustering partition data points groups minimize given objective function existing clustering algorithms treat data point vector many applications datum vector point pattern set points moreover many existing clustering methods require user specify number clusters available advance paper proposes new class models data clustering addresses setvalued data well unknown number clusters using dirichlet process mixture poisson random finite sets also develop efficient markov chain monte carlo posterior inference technique learn number clusters mixture parameters automatically data numerical studies presented demonstrate salient features new model particular capacity discover extremely unbalanced clusters data\",\"paper propose new method predict final destination vehicle trips based initial partial trajectories first review obtained clustering trajectories describes user behaviour explain model main traffic flow patterns mixture gaussian distributions yielded density based clustering locations produces data driven grid similar points within pattern present model used predict final destination new trajectory based first locations using two step procedure first assign new trajectory clusters mot likely belongs secondly use characteristics trajectories inside clusters predict final destination finally present experimental results methods classification trajectories final destination prediction datasets timestamped gpslocation taxi trips test methods two different datasets assess capacity method adapt automatically different subsets\",\"general approach anomaly detection novelty detection consists estimating high density regions minimum volume sets oneclass support vector machine ocsvm stateoftheart algorithm estimating regions high dimensional data yet suffers practical limitations applied limited number samples lead poor performance even picking best hyperparameters moreover solution ocsvm sensitive selection hyperparameters makes hard optimize unsupervised setting present new approach estimate sets using ocsvm different choice parameter controlling proportion outliers solution function ocsvm learnt training set desired probability mass obtained adjusting offset test set prevent overfitting models learnt different traintest splits aggregated reduce variance induced random splits approach makes possible tune hyperparameters automatically obtain nested set estimates experimental results show approach outperforms standard ocsvm formulation suffering less curse dimensionality kernel density estimates results actual data sets also presented\",\"propose novel method multiple clustering assumes coclustering structure partitions rows columns data matrix view new method applicable highdimensional data based nonparametric bayesian approach number views number featuresubject clusters inferred datadriven manner simultaneously model different distribution families gaussian poisson multinomial distributions cluster block makes method applicable datasets consisting numerical categorical variables biomedical data typically clustering solutions based variational inference mean field approximation apply proposed method synthetic real data show method outperforms multiple clustering methods recovering true cluster structures computation time finally apply method depression dataset true cluster structure available useful inferences drawn possible clustering structures data\",\"cluster analysis high dimensional data benefit properties high dimensionality informally expressed work focus analogous situation dimensionality moderate small relative massively sized set observations mathematically expressed dual spaces observations attributes point cloud observations attribute space point cloud attributes observation space paper begin summarizing various perspectives related methodologies used multivariate analytics draw establish efficient clustering processing pipeline partitioning hierarchical clustering\",\"mixture gaussians fit single curved heavytailed cluster report data contains many clusters produce appropriate clusterings introduce model warps latent mixture gaussians produce nonparametric cluster shapes possibly lowdimensional latent mixture model allows summarize properties highdimensional clusters density manifolds describing data number manifolds well shape dimension manifold automatically inferred derive simple inference scheme model analytically integrates mixture parameters warping function show model effective density estimation performs better infinite gaussian mixture models recovering true number clusters produces interpretable summaries highdimensional datasets\",\"propose novel nonparametric adaptive anomaly detection algorithm high dimensional data based ranksvm data points first ranked based scores derived nearest neighbor graphs npoint nominal data train ranksvm using ranked data testpoint declared anomaly alphafalse alarm level predicted score alphapercentile resulting anomaly detector shown asymptotically optimal adaptive false alarm rate alpha decision region converges alphapercentile level set unknown underlying density addition illustrate number synthetic realdata experiments statistical performance computational efficiency anomaly detector\",\"identifying set homogeneous clusters heterogeneous dataset one important classes problems statistical modeling realm unsupervised partitional clustering kmeans important algorithm technical report develop new kmeans variant called augmented kmeans hybrid kmeans logistic regression iteration logistic regression used predict current cluster labels cluster belonging probabilities used control subsequent reestimation cluster means observations cant firmly identified clusters excluded reestimation step valuable data exhibit many characteristics real datasets heterogeneity nonsphericity substantial overlap high scatter augmented kmeans frequently outperforms kmeans accurately classifying observations known clusters converging fewer iterations demonstrate simulated real datasets algorithm implemented python available report\",\"kernel methods obtain superb performance terms accuracy various machine learning tasks since effectively extract nonlinear relations however time complexity rather large especially clustering tasks paper define general class kernels easily approximated randomization kernels appear various applications particular traditional spectral clustering landmarkbased spectral clustering landmarkbased subspace clustering show data points clusters landmarks randomization procedure results algorithm complexity oknd furthermore bound error original clustering scheme randomization illustrate power framework propose new fast landmark subspace fls clustering algorithm experiments synthetic real datasets demonstrate superior performance fls accelerating subspace clustering marginal sacrifice accuracy\",\"present accelerated algorithm hierarchical density based clustering new algorithm improves upon hdbscan provided significant qualitative improvement popular dbscan algorithm accelerated hdbscan algorithm provides comparable performance dbscan supporting variable density clusters eliminating need difficult tune distance scale parameter makes accelerated hdbscan default choice density based clustering library available httpsgithubcomscikitlearncontribhdbscan\",\"density mathbb highdensity cluster connected component geq lambda lambda set highdensity clusters forms hierarchy called cluster tree present two procedures estimating cluster tree given samples first robust variant single linkage algorithm hierarchical clustering second based knearest neighbor graph samples give finitesample convergence rates algorithms also imply consistency derive lower bounds sample complexity cluster tree estimation finally study tree pruning procedure guarantees milder conditions usual remove clusters spurious recovering salient\",\"present methodology clustering objects described multivariate time series several sequences realvalued random variables clustering methodology leverages copulas distributions encoding dependence structure several random variables take fully account dependence information clustering need distance copulas work compare renowned distances distributions fisherrao geodesic distance related divergences optimal transport discuss advantages disadvantages applications methodology found clustering financial assets tutorial experiments implementation reproducible research found wwwdatagrapplecomtech\",\"paper propose pckid novel robust kernel function spectral clustering specifically designed handle incomplete data combining posterior distributions gaussian mixture models incomplete data different scales able learn kernel incomplete data depend critical hyperparameters unlike commonly used rbf kernel evaluate method perform experiments two real datasets pckid outperforms baseline methods fractions missing values cases outperforms baseline methods percentage points\",\"consumer demand response important research industry problem seeks categorize predict modify consumers energy consumption unfortunately traditional clustering methods resulted many hundreds clusters given consumer often associated several clusters making difficult classify consumers stable representative groups predict individual energy consumption patterns paper present shapebased approach better classifies predicts consumer energy consumption behavior household level method based dynamic time warping dtw seeks optimal alignment energy consumption patterns reflecting effect hidden patterns regular consumer behavior using real consumer hour load curves opower corporation method results reduction number representative groups improvement prediction accuracy measured dtw distance extend approach estimate electrical devices used hours\",\"datasets mixture numerical categorical attributes routinely encountered many application domains work examine approach clustering datasets using homogeneity analysis homogeneity analysis determines euclidean representation data analyzed leveraging large body tools techniques data euclidean representation experiments conducted part study suggest approach useful analysis exploration big datasets mixture numerical categorical attributes\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"11_clustering_clusters_data\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"11_clustering_clusters_data\"],\"textfont\":{\"size\":12},\"x\":[13.931585,13.969934,13.86162,13.504116,12.862305,13.418118,13.445632,13.369797,13.910679,14.124223,13.572741,13.96506,13.868462,13.574084,14.148928,13.935659,13.837411,13.994496,13.80954,13.586753,13.450611,14.000916,13.433263,13.571864,13.992974,13.918789,14.045728,13.940409,13.956529,14.087001,13.754333,13.820458,13.781047,13.878756,10.906986,14.016194,13.57226,13.823962,13.929691,13.956851,13.552322,13.346665,13.465637,13.936972,13.864359,13.4853115,13.983148,13.6158495,13.805784,13.859336,13.471326,13.968902,13.917767,14.076823,13.825498,13.935755,13.95704,14.094841,13.375646,13.866932,13.881393,13.932232,13.374763,13.968904,13.339008,13.61815,13.728068,14.057915,13.444046,14.092014,13.822496,13.735137],\"y\":[5.570248,5.51512,5.490045,5.2157855,5.9828835,5.5235176,5.6128182,6.562594,5.8480844,5.810423,5.8066616,5.4443207,5.379636,5.8105407,5.8618937,5.4745607,5.4388857,5.5443907,5.385588,5.807533,5.2323174,5.5355067,6.5759225,5.6535306,5.793974,5.536918,5.613203,6.122643,5.716799,5.4582524,5.296249,5.247292,5.414848,5.3118134,8.205317,5.438579,5.796144,5.464958,5.7970467,5.5046186,5.64037,6.553792,6.5203123,5.9480634,5.9343176,5.6631227,5.4443746,5.8456187,5.400573,5.3561735,5.6444807,5.536197,5.646481,5.919465,5.310059,5.8312564,5.569757,5.6993093,6.568987,5.397034,5.3793764,6.193993,6.5421753,5.5819488,5.583826,5.821454,5.754934,5.6116853,5.6511264,5.441671,5.5881286,5.723684],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"paper presents foundational theoretical results distributed parameter estimation undirected probabilistic graphical models introduces general condition composite likelihood decompositions models guarantees global consistency distributed estimators provided local estimators consistent\",\"present novel kway highdimensional graphical model called generalized root model grm explicitly models dependencies variable sets size standard pairwise graphical model model based taking kth root original sufficient statistics univariate exponential family positive sufficient statistics including poisson exponential distributions recent work square root graphical sqr models inouye restricted pairwise dependencieswe give conditions parameters needed normalization using radial conditionals similar pairwise case inouye particular show poisson grm restrictions parameters exponential grm restriction akin negative definiteness develop simple general learning algorithm based lregularized nodewise regressions also present general way numerically approximating log partition function associated derivatives grm univariate node conditionalsin contrast inouye provided algorithm estimating exponential sqr illustrate grm model word counts poisson grm show associated ksized variable sets finish discussing methods reducing parameter space various situations\",\"introduce randomized dependence coefficient rdc measure nonlinear dependence random variables arbitrary dimension based hirschfeldgebeleinrenyi maximum correlation coefficient rdc defined terms correlation random nonlinear copula projections invariant respect marginal distribution transformations low computational cost easy implement five lines code included end paper\",\"estimation dependencies multiple variables central problem analysis financial time series common approach express dependencies terms copula function typically copula function assumed constant may inaccurate covariates could large influence dependence structure data account bayesian framework estimation conditional copulas proposed framework parameters copula nonlinearly related arbitrary conditioning variables evaluate ability method predict timevarying dependencies several equities currencies observe consistent performance gains compared static copula models timevarying copula methods\",\"analyzing understanding structure complex relational data important many applications including analysis connectivity human brain networks prominent patterns different scales calling hierarchically structured model propose two nonparametric bayesian hierarchical network models based gibbs fragmentation tree priors demonstrate ability capture nested patterns simulated networks real networks demonstrate detection hierarchical structure show predictive performance par state art envision methods employed exploratory analysis large scale complex networks example model human brain connectivity\",\"bnlearn package includes several algorithms learning structure bayesian networks either discrete continuous variables constraintbased scorebased algorithms implemented use functionality provided snow package improve performance via parallel computing several network scores conditional independence algorithms available learning algorithms independent use advanced plotting options provided rgraphviz package\",\"introduce novel multivariate random process producing bernoulli outputs per dimension possibly formalize binary interactions various graphical structures used model opinion dynamics epidemics financial biological time series data etc call bernoulli autoregressive process bar bar process models discretetime vector random sequence scalar bernoulli processes autoregressive dynamics corresponds particular markov chain benefit autoregressive dynamics description ptimes transition matrix effective parameters dll two sparse matrices dimensions ptimes ptimes respectively parameterizing transitions additionally show bar process mixes rapidly proving mixing time olog hidden constant previous mixing time bound depends explicitly values chain parameters implicitly maximum allowed indegree node corresponding graph network nodes node indegree corresponds scalar bernoulli process generated bar provide greedy algorithm efficiently learn structure underlying directed graph sample complexity proportional mixing time bar process sample complexity proposed algorithm nearly orderoptimal log factor away informationtheoretic lower bound present simulation results illustrating performance algorithm various setups including model biological signaling network\",\"propose new high dimensional semiparametric principal component analysis pca method named copula component analysis coca semiparametric model assumes unspecified marginally monotone transformations distributions multivariate gaussian coca improves upon pca sparse pca three aspects robust modeling assumptions robust outliers data contamination iii scaleinvariant yields interpretable results prove coca estimators obtain fast estimation rates feature selection consistent dimension nearly exponentially large relative sample size careful experiments confirm coca outperforms sparse pca synthetic realworld datasets\",\"tick statistical learning library python particular emphasis timedependent models point processes tools generalized linear models survival analysis core library optimization module providing model computational classes solvers proximal operators regularization tick relies implementation stateoftheart optimization algorithms provide fast computations single node multicore setting source code documentation downloaded httpsgithubcomxdatainitiativetick\",\"proposed new statistical dependency measure called copula dependency coefficientcdc two sets variables based copula robust outliers easy implement powerful appropriate highdimensional variables properties important many applications experimental results show cdc detect dependence variables additive nonadditive models\",\"real world systems typically feature variety different dependency types topologies complicate model selection probabilistic graphical models introduce ensembleofforests model generalization ensembleoftrees model model enables structure learning markov random fields mrf multiple connected components arbitrary potentials present two approximate inference techniques model demonstrate performance synthetic data results suggest ensembleofforests approach accurately recover sparse possibly disconnected mrf topologies even presence nongaussian dependencies andor low sample size applied ensembleofforests model learn structure perturbed signaling networks immune cells found frequently exhibit nongaussian dependencies disconnected mrf topologies summary expect ensembleofforests model enable mrf structure learning high dimensional real world settings governed nontrivial dependencies\",\"propose method inferring conditional indepen dence graph cig highdimensional discretetime gaus sian vector random process finitelength observations approach rely parametric model autoregressive model vector random process rather assumes certain spectral smoothness proper ties proposed inference scheme compressive works sample sizes much smaller number scalar process components provide analytical conditions method correctly identify cig high probability\",\"propose novel graphical model selection gms scheme highdimensional stationary time series discrete time process method based natural generalization graphical lasso glasso introduced originally gms based iid samples estimates conditional independence graph cig time series finite length observation glasso time series defined solution lregularized maximum approximate likelihood problem solve optimization problem using alternating direction method multipliers admm approach nonparametric assume finite dimensional autoregressive parametric model observed process instead require process sufficiently smooth spectral domain gaussian processes characterize performance method theoretically deriving upper bound probability algorithm fails correctly identify cig numerical experiments demonstrate ability method recover correct cig limited amount samples\",\"introduce network maximal correlation nmc multivariate measure nonlinear association among random variables nmc defined via optimization infers transformations variables maximizing aggregate inner products transformed variables finite discrete jointly gaussian random variables characterize solution nmc optimization using basis expansion functions appropriate basis functions finite discrete variables propose algorithm based alternating conditional expectation determine nmc moreover propose distributed algorithm compute approximation nmc large dense graphs using graph partitioning finite discrete variables show probability discrepancy greater given level nmc nmc computed using empirical distributions decays exponentially fast sample size grows jointly gaussian variables show conditions nmc optimization instance maxcut problem illustrate application nmc inference graphical model bijective functions jointly gaussian variables finally show nmcs utility data application learning nonlinear dependencies among genes cancer dataset\",\"gaussian graphical models ggms popular tools studying network structures however many modern applications gene network discovery social interactions analysis often involve highdimensional noisy data outliers heavier tails gaussian distribution paper propose trimmed graphical lasso robust estimation sparse ggms method guards outliers implicit trimming mechanism akin popular least trimmed squares method used linear regression provide rigorous statistical analysis estimator highdimensional setting contrast existing approaches robust sparse ggms estimation lack statistical guarantees theoretical results complemented experiments simulated real gene expression data demonstrate value approach\",\"present novel approach estimating conditional probability tables based joint rather independent estimate conditional distributions belonging table derive exact analytical expressions estimators analyse properties analytically via simulation apply method estimation parameters bayesian network given structure network proposed approach better estimates joint distribution significantly improves classification performance respect traditional approaches\",\"study present multiclass graphical bayesian predictive classifier incorporates uncertainty model selection standard bayesian formalism class dependence structure underlying observed features represented set decomposable gaussian graphical models emphasis placed bayesian model averaging takes full account classspecific model uncertainty averaging posterior graph model probabilities explicit evaluation model probabilities well known infeasible address issue consider particle gibbs strategy olsson posterior sampling decomposable graphical models utilizes christmas tree algorithm olsson proposal kernel also derive strong hyper markov law call hyper normal wishart law allow perform resultant bayesian calculations locally proposed predictive graphical classifier reveals superior performance compared ordinary bayesian predictive rule account model uncertainty well number outofthebox classifiers\",\"bayesian graphical models useful tool understanding dependence relationships among many variables particularly situations external prior information highdimensional settings space possible graphs becomes enormous rendering even stateoftheart bayesian stochastic search computationally infeasible propose deterministic alternative estimate gaussian gaussian copula graphical models using expectation conditional maximization ecm algorithm extending approach bayesian variable selection graphical model estimation show ecm approach enables fast posterior exploration sequence mixture priors incorporate multiple sources information\",\"recent methods estimating sparse undirected graphs realvalued data high dimensional problems rely heavily assumption normality show use semiparametric gaussian copulaor nonparanormalfor high dimensional inference additive models extend linear models replacing linear functions set onedimensional smooth functions nonparanormal extends normal transforming variables smooth functions derive method estimating nonparanormal study methods theoretical properties show works well many examples\",\"networks capture intuition relationships world describe friendships facebook users interactions financial markets synapses connecting neurons brain networks richly structured cliques friends sectors stocks smorgasbord cell types govern neurons connect networks like social network friendships directly observed many cases indirect view network actions constituents understanding network mediates activity work focus problem latent network discovery case observable activity takes form mutuallyexcitatory point process known hawkes process build previous work taken bayesian approach problem specifying prior distributions latent network structure likelihood observed activity given network extend work proposing discretetime formulation developing computationally efficient stochastic variational inference svi algorithm allows scale approach long sequences observations demonstrate algorithm calcium imaging data used chalearn neural connectomics challenge\",\"understanding developing correlation measure detect general dependencies imperative statistics machine learning also crucial general scientific discovery big data age paper establish new framework generalizes distance correlation correlation measure recently proposed shown universally consistent dependence testing joint distributions finite moments multiscale graph correlation mgc utilizing characteristic functions incorporating nearest neighbor machinery formalize population version local distance correlations define optimal scale given dependency name optimal local correlation mgc new theoretical framework motivates theoretically sound sample mgc allows number desirable properties proved including universal consistency convergence almost unbiasedness sample version advantages mgc illustrated via comprehensive set simulations linear nonlinear univariate multivariate noisy dependencies loses almost power monotone dependencies achieving better performance general dependencies compared distance correlation popular methods\",\"present semiparametric spectral modeling complete larval drosophila mushroom body connectome motivated thorough exploratory data analysis network via gaussian mixture modeling gmm adjacency spectral embedding ase representation space introduce latent structure model lsm network modeling inference lsm generalization stochastic block model sbm special case random dot product graph rdpg latent position model amenable semiparametric gmm ase representation space resulting connectome code derived via semiparametric gmm composed ase captures latent connectome structure elucidates biologically relevant neuronal properties\",\"graphical models provide powerful tools uncover complicated patterns multivariate data commonly used bayesian statistics machine learning paper introduce package bdgraph performs bayesian structure learning general undirected graphical models decomposable nondecomposable continuous discrete mixed variables package efficiently implements recent improvements bayesian literature including mohammadi wit dobra mohammadi speed computations computationally intensive tasks implemented interfaced package parallel computing capabilities addition package contains several functions simulation visualization well several multivariate datasets taken literature used describe package capabilities paper includes brief overview statistical methods implemented package main part paper explains use package furthermore illustrate packages functionality real artificial examples\",\"recent years seen increasing popularity learning sparse emphchanges markov networks changes structure markov networks reflect alternations interactions random variables different regimes provide insights underlying system individual network structure complicated difficult learn overall change one network another simple intuition gave birth approach emphdirectly learns sparse changes without modelling learning individual possibly dense networks paper review direct learning method latest developments along line research\",\"tree structured graphical models powerful expressing long range hierarchical dependency among many variables widely applied different areas computer science statistics however existing methods parameter estimation inference structure learning mainly rely gaussian discrete assumptions restrictive many applications paper propose new nonparametric methods based reproducing kernel hilbert space embeddings distributions recover latent tree structures estimate parameters perform inference high dimensional continuous nongaussian variables usefulness proposed methods illustrated thorough numerical results\",\"paper consider sparse identifiable linear latent variable factor linear bayesian network models parsimonious analysis multivariate data propose computationally efficient method joint parameter model inference model comparison consists fully bayesian hierarchy sparse models using slab spike priors twocomponent deltafunction continuous mixtures nongaussian latent factors stochastic search ordering variables framework call slim sparse linear identifiable multivariate modeling validated benchmarked artificial real biological data sets slim closest spirit lingam shimizu differs substantially inference bayesian network structure learning model comparison experimentally slim performs equally well better lingam comparable computational complexity attribute mainly stochastic search strategy used parsimony sparsity identifiability explicit part model propose two extensions basic iid linear framework nonlinear dependence observed variables called snim sparse nonlinear identifiable multivariate modeling allowing correlations latent variables called cslim correlated slim temporal andor spatial data source code scripts available httpcogsysimmdtudkslim\",\"consider problem changepoint detection multivariate timeseries multivariate distribution observations supposed follow graphical model whose graph parameters affected abrupt changes throughout time demonstrate possible perform exact bayesian inference whenever one considers simple class undirected graphs called spanning trees possible structures able integrate graph segmentation spaces time combining classical dynamic programming algebraic results pertaining spanning trees particular show quantities posterior distributions changepoints posterior edge probabilities time efficiently obtained illustrate results synthetic experimental data arising biology neuroscience\",\"study graph estimation density estimation high dimensions using family density estimators based forest structured undirected graphical models density estimation assume true distribution corresponds forest rather form kernel density estimates bivariate univariate marginals apply kruskals algorithm estimate optimal forest held data prove oracle inequality excess risk resulting estimator relative risk best forest graph estimation consider problem estimating forests restricted tree sizes prove finding maximum weight spanning forest restricted tree size nphard develop approximation algorithm problem viewing tree size complexity parameter select forest using data splitting prove bounds excess risk structure selection consistency procedure experiments simulated data microarray data indicate methods practical alternative gaussian graphical models\",\"presence weak overall correlation may useful investigate correlation significantly substantially pronounced subpopulation two different testing procedures compared based rankings values two variables data set large number observations first maintains level gaussian copulas second adapts general alternatives sense number parameters used test grows analysis wine quality illustrates methods detect heterogeneity association chemical properties wine attributable mix different cultivars\",\"inductive probabilistic classification rule must generally obey principles bayesian predictive inference observed unobserved stochastic quantities jointly modeled parameter uncertainty fully acknowledged posterior predictive distribution several rules recently considered asymptotic behavior characterized assumption observed features variables used building classifier conditionally independent given simultaneous labeling training samples unknown origin extend theoretical results predictive classifiers acknowledging feature dependencies either graphical models sparser alternatives defined stratified graphical models also show experimentation synthetic real data predictive classifiers based stratified graphical models consistently best accuracy compared predictive classifiers based either conditionally independent features ordinary graphical models\",\"study problem estimating temporally varying coefficient varying structure vcvs graphical model underlying nonstationary time series data social states interacting individuals microarray expression profiles gene networks opposed iid data invariant model widely considered current literature structural estimation particular consider scenario model evolves piecewise constant fashion propose procedure minimizes socalled tesla loss temporally smoothed regularized regression allows jointly estimating partition boundaries vcvs model coefficient sparse precision matrix block partition highly scalable proximal gradient method proposed solve resultant convex optimization problem conditions sparsistent estimation convergence rate partition boundaries network structure established first time estimators\",\"consider structure discovery undirected graphical models observational data inferring likely structures examples complex task often requiring formulation priors sophisticated inference procedures popular methods rely estimating penalized maximum likelihood precision matrix however approaches structure recovery indirect consequence datafit term penalty difficult adapt domainspecific knowledge inference computationally demanding contrast may easier generate training samples data arise graphs desired structure properties propose leverage latter source information training data learn function parametrized neural network maps empirical covariance matrices estimated graph structures learning function brings two benefits implicitly models desired structure sparsity properties form suitable priors tailored specific problem edge structure discovery rather maximizing data likelihood applying framework find learnable graphdiscovery method trained synthetic data generalizes well identifying relevant edges synthetic real data completely unknown training time find genetics brain imaging simulation data obtain performance generally superior analytical methods\",\"directed networks pervasive nature engineered systems often underlying complex behavior observed biological systems microblogs social interactions web well global financial markets since structures often unobservable order facilitate network analytics one generally resorts approaches capitalizing measurable nodal processes infer unknown topology structural equation models sems capable incorporating exogenous inputs resolve inherent directional ambiguities however conventional sems assume full knowledge exogenous inputs may readily available practical settings present paper advocates novel sembased topology inference approach entails factorization threeway tensor constructed observed nodal data using wellknown parallel factor parafac decomposition turns secondorder piecewise stationary statistics exogenous variables suffice identify hidden topology capitalizing uniqueness properties inherent highorder tensor factorizations shown topology identification possible reasonably mild conditions addition facilitate realtime operation inference timevarying networks adaptive parafac tensor decomposition scheme tracks topologyrevealing tensor factors developed extensive tests simulated real stock quote data demonstrate merits novel tensorbased approach\",\"numerous social medical engineering biological challenges framed graphbased learning tasks propose new feature based approach network classification show dynamics network useful reveal patterns organization components underlying graph process takes place define generalized assortativities networks use generalized features across multiple time scales features turn suitable signatures discriminating different classes networks method evaluated empirically established network benchmarks also introduce new dataset human brain networks connectomes use evaluate method results reveal dynamics based features competitive often outperform state art accuracies\",\"propose method inferring conditional independence graph cig highdimensional gaussian vector time series discretetime process finitelength observation contrast existing approaches rely parametric process model autoregressive model observed random process instead require certain smoothness properties fourier domain process proposed inference scheme works even sample sizes much smaller number scalar process components true underlying cig sufficiently sparse theoretical performance analysis provides conditions guarantee probability proposed inference method deliver wrong cig prescribed value conditions imply lower bounds sample size new method consistent asymptotically numerical experiments validate theoretical performance analysis demonstrate superior performance scheme compared existing parametric approach case model mismatch\",\"propose novel class timevarying nonparanormal graphical models allows model high dimensional heavytailed systems evolution latent network structures model develop statistical tests presence edges locally fixed index value globally range values tests developed highdimensional regime robust model selection mistakes require commonly assumed minimum signal strength testing procedures based high dimensional debiasingfree moment estimator uses novel kernel smoothed kendalls tau correlation matrix input statistic estimator consistently estimates latent inverse pearson correlation matrix uniformly index variable kernel bandwidth rate convergence shown minimax optimal method supported thorough numerical simulations application neural imaging data set\",\"consider graphical model multivariate normal vector associated node underlying graph estimate graphical structure minimize loss function obtained regressing vector node remaining ones group penalty show proposed estimator computed fast convex optimization algorithm show sample size increases estimated regression coefficients correct graphical structure correctly estimated probability tending one extensive simulations show superiority proposed method comparable procedures apply technique two real datasets first one identify gene protein networks showing cancer cell lines second one reveal connections among different industries\",\"gaussian graphical models widely used represent conditional dependence among random variables paper propose novel estimator data arising group gaussian graphical models dependent motivating example modeling gene expression collected multiple tissues individual multivariate outcome affected dependencies acting level specific tissues also level whole body existing methods assume independence among graphs applicable case estimate multiple dependent graphs decompose problem two graphical layers systemic layer affects outcomes thereby induces cross graph dependence categoryspecific layer represents graphspecific variation propose graphical technique estimates layers jointly establish estimation consistency selection sparsistency proposed estimator confirm simulation method superior simple onestep method apply technique mouse genomics data obtain biologically plausible results\",\"bayesian networks bns graphical models useful representing highdimensional probability distributions great deal interest recent years nphard problem learning structure observed data typically one assigns score various structures search becomes optimization problem approached either deterministic stochastic methods paper walk space graphs modeling appearance disappearance edges birth death process compare novel approach popular metropolishastings search strategy give empirical evidence birth death process superior mixing properties\",\"investigate generic problem learning pairwise exponential family graphical models pairwise sufficient statistics defined global mapping function mercer kernels subclass pairwise graphical models allow flexibly capture complex interactions among variables beyond pairwise product propose two ellnorm penalized maximum likelihood estimators learn model parameters iid samples first one joint estimator estimates parameters simultaneously second one nodewise conditional estimator estimates parameters individually node estimators show proper conditions extra flexibility gained model comes almost cost statistical computational efficiency demonstrate advantages model stateoftheart methods synthetic real datasets\",\"structural equation models sems widely adopted inference causal interactions complex networks recent examples include unveiling topologies hidden causal networks processes spreading diseases rumors propagate appeal sems settings stems simplicity tractability since typically assume linear dependencies among observable variables acknowledging limitations inherent adopting linear models present paper advocates nonlinear sems account possible nonlinear dependencies among network nodes advocated approach leverages kernels powerful encompassing framework nonlinear modeling efficient estimator affordable tradeoffs put forth interestingly pursuit novel kernelbased approach yields convex regularized estimator promotes edge sparsity amenable proximalsplitting optimization methods end solvers complementary merits developed leveraging alternating direction method multipliers proximal gradient iterations experiments conducted simulated data demonstrate novel approach outperforms linear sems respect edge detection errors furthermore tests real gene expression dataset unveil interesting new edges revealed linear sems could shed light regulatory behavior human genes\",\"propose new class semiparametric exponential family graphical models analysis high dimensional mixed data different existing mixed graphical models allow nodewise conditional distributions semiparametric generalized linear models unspecified base measure functions thus one advantage method unnecessary specify type node method convenient apply practice proposed model consider problems parameter estimation hypothesis testing high dimensions particular propose symmetric pairwise score test presence single edge graph compared existing methods hypothesis tests approach takes account symmetry parameters inferential results invariant respect different parametrizations edge thorough numerical simulations real data example provided back results\",\"propose new method detecting changes markov network structure two sets samples instead naively fitting two markov network models separately two data sets figuring difference emphdirectly learn network structure change estimating ratio markov network models densityratio formulation naturally allows introduce sparsity network structure change highly contributes enhancing interpretability furthermore computation normalization term critical bottleneck naive approach remarkably mitigated also give dual formulation optimization problem reduces computation cost largescale markov networks experiments demonstrate usefulness method\",\"undirected graphical models known markov networks popular wide variety applications ranging statistical physics computational biology traditionally learning network structure done assumption chordality ensures efficient scoring methods used general nonchordal graphs intractable normalizing constants renders calculation bayesian scores difficult beyond smallscale systems recently surge interest towards use regularized pseudolikelihood methods structural learning largescale markov network models approach avoids assumption chordality currently available methods typically necessitate use tuning parameter adapt level regularization particular dataset optimized example crossvalidation introduce bayesian version pseudolikelihood scoring markov networks enables automatic regularization marginalization nuisance parameters model prove consistency resulting mpl estimator network structure via comparison pseudo information criterion identification mploptimal network prescanned graph space considered greedy hill climbing exact pseudoboolean optimization algorithms find reasonable sample sizes hill climbing approach often identifies networks negligible distance restricted global optimum using synthetic existing benchmark networks marginal pseudolikelihood method shown generally perform favorably recent popular inference methods markov networks\",\"many real world network problems often concern multivariate nodal attributes image textual multiview feature vectors nodes rather simple univariate nodal attributes existing graph estimation methods built gaussian graphical models covariance selection algorithms handle data neither theories developed around methods directly applied paper propose new principled framework estimating graphs multiattribute data instead estimating partial correlation current literature method estimates partial canonical correlations naturally accommodate complex nodal features computationally provide efficient algorithm utilizes multiattribute structure theoretically provide sufficient conditions guarantee consistent graph recovery extensive simulation studies demonstrate performance method various conditions furthermore provide illustrative applications uncovering gene regulatory networks gene protein profiles uncovering brain connectivity graph functional magnetic resonance imaging data\",\"develop square root graphical models sqr novel class parametric graphical models provides multivariate generalizations univariate exponential family distributions previous multivariate graphical models yang allow positive dependencies exponential poisson generalizations however many realworld datasets variables clearly positive dependencies example airport delay time new yorkmodeled exponential distributionis positively related delay time boston motivation give example model class derived univariate exponential distribution allows almost arbitrary positive negative dependencies mild condition parameter matrixa condition akin positive definiteness gaussian covariance matrix poisson generalization allows positive negative dependencies without constraints parameter values also develop parameter estimation methods using nodewise regressions ell regularization likelihood approximation methods using sampling finally demonstrate exponential generalization synthetic dataset realworld dataset airport delay times\",\"introduce truncated gaussian graphical model tggm novel framework designing statistical models nonlinear learning tggm gaussian graphical model ggm subset variables truncated nonnegative truncated variables assumed latent integrated induce marginal model show variables marginal model nongaussian distributed expected relations nonlinear use expectationmaximization break inference nonlinear model sequence tggm inference problems efficiently solved using properties numerical methods multivariate gaussian distributions use tggm design models nonlinear regression classification performances models demonstrated extensive benchmark datasets compared stateoftheart competing results\",\"propose communicationefficient distributed estimation inference methods transelliptical graphical model semiparametric extension elliptical distribution high dimensional regime detail proposed method distributes ddimensional data size generated transelliptical graphical model worker machines estimates latent precision matrix worker machine based data size nnm debiases local estimators worker machines send back master machine finally master machine aggregates debiased local estimators averaging hard thresholding show aggregated estimator attains statistical rate centralized estimator based data provided number machines satisfies lesssim minnlog ddsqrtnslog maximum number nonzero entries column latent precision matrix worth noting algorithm theory directly applied gaussian graphical models gaussian copula graphical models elliptical graphical models since special cases transelliptical graphical models thorough experiments synthetic data back theory\",\"graphical models commonly used tools modeling multivariate random variables exist many convenient multivariate distributions gaussian distribution continuous data mixed data presence discrete variables combination continuous discrete variables poses new challenges statistical modeling paper propose semiparametric model named latent gaussian copula model binary mixed data observed binary data assumed obtained dichotomizing latent variable satisfying gaussian copula distribution nonparanormal distribution latent gaussian model assumption latent variables multivariate gaussian special case proposed model novel rankbased approach proposed latent graph estimation latent principal component analysis theoretically proposed methods achieve rates convergence precision matrix estimation eigenvector estimation latent variables observed similar conditions consistency graph structure recovery feature selection leading eigenvectors established performance proposed methods numerically assessed simulation studies usage methods illustrated genetic dataset\",\"consider inference structure undirected graphical model exact bayesian framework specifically aim achieving inference closeform posteriors avoiding sampling step task would intractable without restriction considered graphs limit exploration mixtures spanning trees consider inference structure undirected graphical model bayesian framework avoid convergence issues highly demanding monte carlo sampling focus exact inference specifically aim achieving inference closeform posteriors avoiding sampling step aim restrict set considered graphs mixtures spanning trees investigate conditions priors tree structures parameters exact bayesian inference achieved conditions derive fast exact algorithm compute posterior probability edge belong tree model using algebraic result called matrixtree theorem show assumption made prevent approach perform well synthetic flow cytometry data\",\"high dimensions propose analyze aggregation estimator precision matrix gaussian graphical models estimator called graphical exponential screening ges linearly combines suitable set individual estimators different underlying graphs balances estimation error sparsity study risk aggregation estimator show comparable best estimator based single graph chosen oracle numerical performance method investigated using simulated real datasets comparison stateofart estimation procedures\",\"main contribution article new prior distribution directed acyclic graphs gives larger weight sparse graphs distribution intended structured bayesian networks structure given ordered block model nodes graph objects fall categories blocks blocks natural ordering presence relationship two objects denoted arrow object lower category object higher category models considered introduced kemp relational data extended multivariate data mansinghka prior graph structures presented explicit formula number nodes layer graph follow hoppe ewens urn model consider situation nodes graph represent random variables whose joint probability distribution factorises along dag describe monte carlo schemes finding optimal aposteriori structure given data matrix compare performance mansinghka also uniform prior\",\"challenging problem estimating highdimensional graphical models choose regularization parameter datadependent way standard techniques include kfold crossvalidation kcv akaike information criterion aic bayesian information criterion bic though methods work well lowdimensional problems suitable high dimensional settings paper present stars new stabilitybased method choosing regularization parameter high dimensional inference undirected graphs method clear interpretation use least amount regularization simultaneously makes graph sparse replicable random sampling interpretation requires essentially conditions mild conditions show stars partially sparsistent terms graph estimation high probability true edges included selected model even graph size diverges sample size empirically performance stars compared stateoftheart model selection procedures including kcv aic bic synthetic data real microarray dataset stars outperforms competing procedures\",\"paper proposes unified framework quantify local global inferential uncertainty high dimensional nonparanormal graphical models particular consider problems testing presence single edge constructing uniform confidence subgraph due presence unknown marginal transformations propose pseudo likelihood based inferential approach sharp contrast existing high dimensional score test method method free tuning parameters given initial estimator extends scope existing likelihood based inferential framework furthermore propose ustatistic multiplier bootstrap method construct confidence subgraph show constructed subgraph contained true graph probability greater given nominal level compared existing methods constructing confidence subgraphs method rely gaussian subgaussian assumptions theoretical properties proposed inferential methods verified thorough numerical experiments real data analysis\",\"theory graphical models matured three decades provide backbone several classes models used myriad applications genetic mapping diseases credit risk evaluation reliability computer security etc despite generic applicability wide adoptance constraints imposed undirected graphical models bayesian networks also recognized unnecessarily stringent certain circumstances observation led proposal several generalizations aim relaxed constraints models impose local contextspecific dependence structures consider additional class models termed stratified graphical models develop method bayesian learning models deriving analytical expression marginal likelihood data specific subclass decomposable stratified models nonreversible markov chain monte carlo approach used identify models highly supported posterior distribution model space method illustrated compared ordinary graphical models application several real synthetic datasets\",\"study problem learning latent variables gaussian graphical models existing methods problem assume precision matrix observed variables superposition sparse lowrank component paper focus estimation lowrank component encodes effect marginalization latent variables introduce fast proper learning algorithms problem contrast existing approaches algorithms manifestly nonconvex support efficacy via rigorous theoretical analysis show algorithms match best possible terms sample complexity achieving computational speedups existing methods complement theory several numerical experiments\",\"consider discrete graphical models markov respect graph propose two distributed marginal methods estimate maximum likelihood estimate canonical parameter model methods based relaxation marginal likelihood obtained considering density variables represented vertex neighborhood two methods differ size neighborhood show estimates consistent obtained larger neighborhood smaller asymptotic variance ones obtained smaller neighborhood\",\"model high dimensional data gaussian methods widely used since remain tractable yield parsimonious models imposing strong assumptions data vine copulas flexible combining arbitrary marginal distributions conditional bivariate copulas yet adaptability accompanied sharply increasing computational effort dimension increases approach proposed paper overcomes burden makes first step ultra high dimensional nongaussian dependence modeling using divideandconquer approach first apply gaussian methods split datasets feasibly small subsets second apply parsimonious flexible vine copulas thereon finally reconcile one joint model provide numerical results demonstrating feasibility approach moderate dimensions showcase ability estimate ultra high dimensional nongaussian dependence models thousands dimensions\",\"bayesian networks convenient graphical expressions high dimensional probability distributions representing complex relationships large number random variables employed extensively areas bioinformatics artificial intelligence diagnosis risk management recovery structure network data prime importance purposes modeling analysis prediction recovery algorithms literature assume either discrete continuous gaussian data general continuous data discretization usually employed often destroys structure one recover friedman goldszmidt suggest approach based minimum description length principle chooses discretization preserves information original data set however one difficult impossible implement even moderately sized networks paper provide extremely efficient search strategy allows one use friedman goldszmidt discretization practice\",\"propose methodology explore measure pairwise correlations exist variables dataset methodology leverages copulas encoding dependence two variables stateoftheart optimal transport providing relevant geometry copulas clustering summarizing main dependence patterns found variables clusters centers used parameterize novel dependence coefficient target forget specific dependence patterns finally illustrate benchmark methodology several datasets code numerical experiments available online reproducible research\",\"paper addresses problem scalable optimization lregularized conditional gaussian graphical models conditional gaussian graphical models generalize wellknown gaussian graphical models conditional distributions model output network influenced conditioning input variables highly scalable optimization methods exist sparse gaussian graphical model estimation stateoftheart methods conditional gaussian graphical models efficient enough importantly fail due memory constraints large problems paper propose new optimization procedure based newton method efficiently iterates two subproblems leading drastic improvement computation time compared previous methods extend method scale large problems memory constraints using block coordinate descent limit memory usage achieving fast convergence using synthetic genomic data show methods solve one million dimensional problems high accuracy little day single machine\",\"study problem learning sparse structure changes two markov networks rather fitting two markov networks separately two sets data figuring differences recent work proposed learn changes emphdirectly via estimating ratio two markov network models paper give sufficient conditions emphsuccessful change detection respect sample size dimension data number changed edges using unbounded density ratio model prove true sparse changes consistently identified omegad log fracmm omeganp exponentially decaying upperbound learning error sample complexity improved minnp omegad log fracmm boundedness density ratio model assumed theoretical guarantee applied wide range discretecontinuous markov networks\",\"network models popular modeling representing complex relationships dependencies observed variables data comes dynamic stochastic process single static network model cannot adequately capture transient dependencies gene regulatory dependencies throughout developmental cycle organism kolar proposed method based kernelsmoothing lpenalized logistic regression estimating timevarying networks nodal observations collected timeseries observational data paper establish conditions proposed method consistently recovers structure timevarying network work complements previous empirical findings providing sound theoretical guarantees proposed estimation procedure completeness include numerical simulations paper\",\"contagions spread popular news stories infectious diseases propagate cascades dynamic networks unobservable topologies however social signals product purchase time blog entry timestamps measurable implicitly depend underlying topology making possible track time interestingly network topologies often jump discrete states may account sudden changes observed signals present paper advocates switched dynamic structural equation model capture topologydependent cascade evolution well discrete states driving underlying topologies conditions proposed switched model identifiable established leveraging edge sparsity inherent social networks recursive ellnorm regularized leastsquares estimator put forth jointly track states network topologies efficient firstorder proximalgradient algorithm developed solve resulting optimization problem numerical experiments synthetic data real cascades measured span one year conducted test results corroborate efficacy advocated approach\",\"paper propose semiparametric approach named nonparanormal skeptic efficiently robustly estimating high dimensional undirected graphical models achieve modeling flexibility consider gaussian copula graphical models nonparanormal proposed liu achieve estimation robustness exploit nonparametric rankbased correlation coefficient estimators including spearmans rho kendalls tau high dimensional settings prove nonparanormal skeptic achieves optimal parametric rate convergence graph parameter estimation celebrating result suggests gaussian copula graphical models used safe replacement popular gaussian graphical models even data truly gaussian besides theoretical analysis also conduct thorough numerical simulations compare different estimators graph recovery performance ideal noisy settings proposed methods applied largescale genomic dataset illustrate empirical usefulness language software package huge implementing proposed methods available comprehensive archive network httpcran rprojectorg\",\"learn structure markov network two groups random variables joint observations since modelling learning full structure may hard learning links two groups directly may preferable option introduce novel concept called emphpartitioned ratio whose factorization directly associates markovian properties random variables across two groups simple oneshot convex optimization procedure proposed learning emphsparse factorizations partitioned ratio theoretically guaranteed recover correct intergroup structure mild conditions performance proposed method experimentally compared state art structure learning methods using roc curves real applications analyzing bipartisanship congress pairwise dnatimeseries alignments also reported\",\"consider two connected aspects maximum likelihood estimation parameter highdimensional discrete graphical models existence maximum likelihood estimate mle computation data sparse many zeros contingency table maximum likelihood estimate parameter may exist fienberg rinaldo shown mle exists iff data vector belongs face socalled marginal cone spanned rows design matrix model identifying faces highdimension challenging paper take local approach show one face albeit possibly smallest one identified looking collection marginal graphical models generated induced subgraphs giildotsk first contribution second contribution concerns composite maximum likelihood estimate dimension problem large estimating parameters given graphical model maximum likelihood challenging impossible traditional approach problem local use composite likelihood based local conditional likelihoods recent development components composite likelihood marginal likelihoods centred around first show estimates obtained consensus local conditional marginal likelihoods identical study asymptotic properties composite maximum likelihood estimate dimension model sample size infinity\",\"loglinear models popular workhorses analyzing contingency tables loglinear parameterization interaction model expressive direct parameterization based probabilities leading powerful way defining restrictions derived marginal conditional contextspecific independence however parameter estimation often simpler direct parameterization provided model enjoys certain decomposability properties introduce cyclical projection algorithm obtaining maximum likelihood estimates loglinear parameters arbitrary contextspecific graphical loglinear model needs satisfy criteria decomposability illustrate lifting restriction decomposability makes models expressive additional contextspecific independencies embedded real data identified also shown contextspecific graphical model correspond nonhierarchical loglinear parameterization concise interpretation observation pave way development nonhierarchical loglinear models largely neglected due believed lack interpretability\",\"inference learning graphical models wellstudied problems statistics machine learning found many applications science engineering however exact inference intractable general graphical models suggests problem seeking best approximation collection random variables within tractable family graphical models paper focus class planar ising models exact inference tractable using techniques statistical physics based techniques recent methods planarity testing planar embedding propose simple greedy algorithm learning best planar ising model approximate arbitrary collection binary random variables possibly sample data given set pairwise correlations among variables select planar graph optimal planar ising model defined graph best approximate set correlations demonstrate method simulations application modeling senate voting records\",\"study adaptive estimation copula correlation matrix sigma semiparametric elliptical copula model context correlations connected kendalls tau sine function transformation hence natural estimate sigma plugin estimator hatsigma kendalls tau statistic first obtain sharp bound operator norm hatsigmasigma study factor model sigma propose refined estimator widetildesigma fitting lowrank matrix plus diagonal matrix hatsigma using least squares nuclear norm penalty lowrank matrix bound operator norm hatsigmasigma serves scale penalty term obtain finite sample oracle inequalities widetildesigma also consider elementary factor copula model sigma propose closedform estimators estimation procedures entirely datadriven\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"12_graphical_models_model\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"12_graphical_models_model\"],\"textfont\":{\"size\":12},\"x\":[10.556412,10.619076,10.018042,10.043892,10.238171,10.625532,10.5335045,10.330803,12.385609,10.036489,10.226976,11.326437,11.335918,10.280721,10.155258,10.560737,10.655074,10.642458,10.378632,10.109087,9.994125,9.946903,10.620314,10.439978,10.693046,10.439884,10.626785,10.311107,10.002887,10.639421,10.21124,10.124559,10.359938,10.136494,11.326346,10.224277,10.143933,10.188466,10.602134,10.641372,10.195207,10.518241,10.446517,10.567165,9.981878,10.620612,10.6614,10.466946,10.418267,10.659139,10.363385,10.703274,10.295045,10.521539,10.60392,10.709581,10.546596,10.087799,10.544286,10.020341,10.603971,10.4606495,10.216295,10.371472,10.375356,10.4507265,10.540566,10.643993,10.771495,10.088297,10.459801],\"y\":[3.6212766,3.6487474,4.348331,4.342055,3.0034916,3.0343962,2.7914214,4.272837,7.9350023,4.3551373,3.2181017,8.623657,8.619092,3.4581518,3.5254312,3.3157248,3.317444,3.3011584,3.7891338,3.0783794,4.5805607,3.369544,3.3015761,2.9056764,3.631073,4.1889915,3.1496658,3.7321892,4.4801226,3.3651237,3.370663,3.1620936,2.6654096,3.0379524,8.622663,3.971977,3.468637,3.5454268,3.1814213,3.6045153,2.9118853,3.660513,2.9138987,2.9249399,3.3913183,3.6283715,3.9267344,3.890909,3.9081135,3.2058501,3.763878,2.8780289,3.6453516,3.6501386,3.3986335,4.0024924,3.6124756,4.3003283,3.2323022,4.3645573,3.9517276,2.918321,2.928375,2.616986,3.8022234,2.9085245,3.634839,3.584847,3.1120105,4.359024,3.799455],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"research manifold learning within density ridge estimation framework shown great potential recent work estimation denoising manifolds building intuitive welldefined notion principal curves surfaces however problem unwrapping unfolding manifolds received relatively little attention within density ridge approach despite integral part manifold learning general paper proposes two novel algorithms unwrapping manifolds based estimated principal curves surfaces one multidimensional manifolds respectively methods unwrapping founded realization principal curves principal surfaces inherent local maxima probability density function following observation coordinate systems follow shape manifold computed following integral curves gradient flow kernel density estimate manifold furthermore since integral curves gradient flow kernel density estimate inherently local propose stitch together local coordinate systems using parallel transport along manifold provide numerical experiments real synthetic data illustrates clear intuitive unwrapping results comparable stateoftheart manifold learning algorithms\",\"analyze performance class manifoldlearning algorithms find output minimizing quadratic form normalization constraints class consists locally linear embedding lle laplacian eigenmap local tangent space alignment ltsa hessian eigenmaps hlle diffusion maps present prove conditions manifold necessary success algorithms finite sample case limit case analyzed show simple manifolds necessary conditions violated hence algorithms cannot recover underlying manifolds finally present numerical results demonstrate claims\",\"study problem estimating data sparse approximation inverse covariance matrix estimating sparsity constrained inverse covariance matrix key component gaussian graphical model learning one numerically challenging address challenge developing new adaptive gradientbased method carefully combines gradient information adaptive stepscaling strategy results scalable highly competitive method algorithm like predecessors maximizes ellnorm penalized loglikelihood per iteration arithmetic complexity best methods class experiments reveal approach outperforms stateoftheart competitors often significantly large problems\",\"principal components analysis widely used technique dimension reduction characterization variability multivariate populations interest lies studying rotation principal components used effectively within responsepredictor set relationship context mode hunting specifically focusing patient rule induction method prim first develop fast version algorithm fastprim normality facilitates theoretical studies follow using basic geometrical arguments demonstrate rotation predictor space alone fact generate improved mode estimators simulation results used illustrate findings\",\"semiparametric nonlinear regression model presence latent variables introduced latent variables correspond unmodeled phenomena unmeasured agents complex networked system new formulation allows joint estimation certain nonlinearities system direct interactions measured variables effects unmodeled elements observed system particular form model adopted justified learning posed regularized empirical risk minimization leads classes structured convex optimization problems sparse plus lowrank flavor relations proposed model several common model paradigms robust principal component analysis pca vector autoregression var established particularly var setting lowrank contributions come broad trends exhibited time series details algorithm learning model presented experiments demonstrate performance model estimation algorithm simulated real data\",\"address structured covariance estimation elliptical distributions assuming covariance priori known belong given convex set set toeplitz banded matrices consider general method moments gmm optimization applied robust tylers scatter mestimator subject convex constraints unfortunately gmm turns nonconvex due objective instead propose new coca estimator convex relaxation efficiently solved prove relaxation tight unconstrained case finite number samples constrained case asymptotically illustrate advantages coca synthetic simulations structured compound gaussian distributions examples coca outperforms competing methods tylers estimator projection onto structure set\",\"paper autoassociative models proposed candidates generalization principal component analysis show models dedicated approximation dataset manifold word manifold refers topology properties structure approximating manifold built projection pursuit algorithm step algorithm dimension manifold incremented theoretical properties provided particular show step algorithm mean residuals norm increased moreover also established algorithm converges finite number steps particular autoassociative models exhibited compared classical pca neural networks models implementation aspects discussed show numerous cases optimization procedure required illustrations simulated real data presented\",\"reshef reshef recently published paper present method called maximal information coefficient mic detect forms statistical dependence pairs variables sample size goes infinity method praised also criticized lack power finite samples seek modify mic higher power detecting associations limited sample sizes present generalized mean information coefficient gmic generalization mic incorporates tuning parameter used modify complexity association favored measure define gmic prove maintains several key asymptotic properties mic increased power mic demonstrated using simulation eight different functional relationships sixty different noise levels results compared pearson correlation distance correlation mic simulation results suggest generally gmic slightly lower power distance correlation measure achieves higher power mic many forms underlying association functional relationships gmic surpasses statistics calculated preliminary results suggest choosing moderate value tuning parameter gmic yield test robust across underlying relationships gmic promising new method mitigates power issues suffered mic possible expense equitability nonetheless distance correlation simulations powerful many forms underlying relationships minimum work motivates consideration maximal informationbased nonparametric exploration mine methods statistical tests independence\",\"present robust alternative principal component analysis pca called elliptical component analysis eca analyzing high dimensional elliptically distributed data eca estimates eigenspace covariance matrix elliptical data cope heavytailed elliptical distributions multivariate rank statistic exploited modellevel consider two settings either leading eigenvectors covariance matrix nonsparse sparse methodologically propose eca procedures nonsparse sparse settings theoretically provide nonasymptotic asymptotic analyses quantifying theoretical performances eca nonsparse setting show ecas performance highly related effective rank covariance matrix sparse setting results twofold show sparse eca estimator based combinatoric program attains optimal rate convergence based recent developments estimating sparse leading eigenvectors show computationally efficient sparse eca estimator attains optimal rate convergence suboptimal scaling\",\"various problems data analysis statistical genetics call recovery columnsparse lowrank matrix noisy observations propose refactor simple variation classical truncated singular value decomposition tsvd algorithm contrast previous sparse principal component analysis pca algorithms algorithm provably reveal lowrank signal matrix better often significantly better widely used tsvd making algorithm choice whenever columnsparsity suspected empirically observe refactor consistently outperforms tsvd even underlying signal sparse suggesting generally safe use refactor instead tsvd pca algorithm extremely simple implement running time dominated runtime pca making practical standard principal component analysis\",\"multiple multivariate data sets derive conditions generalized canonical correlation analysis gcca improves classification performance projected datasets compared standard canonical correlation analysis cca using two data sets illustrate theoretical results simulations real data experiment\",\"independent component analysis ica popular method blind source separation bss diverse set applications biomedical signal processing video image analysis communications maximum likelihood optimal theoretical framework ica requires knowledge true underlying probability density function pdf latent sources many applications unknown ica algorithms cast framework often deviate theoretical optimality properties due poor estimation source pdf therefore accurate estimation source pdfs critical order avoid model mismatch poor ica performance paper propose new efficient ica algorithm based entropy maximization kernels icaemk uses global local measuring functions constraints dynamically estimate pdf sources reasonable complexity addition new algorithm performs optimization respect cost function gradient directions separately enabling parallel implementations multicore computers demonstrate superior performance icaemk competing ica algorithms using simulated well realworld data\",\"goal crossdomain object matching cdom find correspondence two sets objects different domains unsupervised way photo album summarization typical application cdom photos automatically aligned designed frame expressed cartesian coordinate system cdom usually formulated finding mapping objects one domain photos objects domain frame pairwise dependency maximized stateoftheart cdom method employs kernelbased dependency measure drawback kernel parameter needs determined manually paper propose alternative cdom methods naturally address model selection problem experiments image matching unpaired voice conversion photo album summarization tasks effectiveness proposed methods demonstrated\",\"consider robust covariance estimation group symmetry constraints nongaussian covariance estimation tyler scatter estimator multivariate generalized gaussian distribution methods usually involve nonconvex minimization problems recently shown underlying principle behind success extended form convexity geodesics manifold positive definite matrices modern approach improve estimation accuracy exploit prior knowledge via additional constraints restricting attention specific classes covariances adhere prior symmetry structures paper prove group symmetry constraints also geodesically convex therefore incorporated various nongaussian covariance estimators practical examples sets include circulant persymmetric complexquaternion proper structures provide simple numerical technique finding maximum likelihood estimates constraints demonstrate performance advantage using synthetic experiments\",\"introduce locally linear latent variable model lllvm probabilistic model nonlinear manifold discovery describes joint distribution observations manifold coordinates locally linear maps conditioned set neighbourhood relationships model allows straightforward variational optimisation posterior distribution coordinates locally linear maps latent space observation space given data thus lllvm encapsulates localgeometry preserving intuitions underlie nonprobabilistic methods locally linear embedding lle probabilistic semantics make easy evaluate quality hypothesised neighbourhood relationships select intrinsic dimensionality manifold construct outofsample extensions combine manifold model additional probabilistic models capture structure coordinates within manifold\",\"past decade techniques topological data analysis tda grown prominence describe shape data recent years increasing interest developing statistical methods particular hypothesis testing procedures tda statistical perspective persistence diagrams central multiscale topological descriptors data provided tda viewed random observations sampled population process context one earliest works hypothesis testing focuses twogroup permutationbased approach associated loss function defined terms withingroup pairwise bottleneck wasserstein distances persistence diagrams robinson turner however situations persistence diagrams large size number permutation test question gets computationally costly apply address limitation instead consider pairwise distances vectorized functional summaries persistence diagrams loss function present work explore utility betti function regard one simplest function summaries persistence diagrams introduce alternative vectorization method betti function based integration prove stability results respect wasserstein distance moreover propose new shuffling technique group labels increase power test several experimental studies synthetic real data show vectorized betti function leads competitive results compared baseline method involving wasserstein distances permutation test\",\"manifold learning dimensionality reduction techniques ubiquitous science engineering computationally expensive procedures applied large data sets similarities expensive compute date little work done investigate tradeoff computational resources quality learned representations present theoretical experimental explorations question particular consider laplacian eigenmaps embeddings based kernel matrix explore embeddings behave kernel matrix corrupted occlusion noise main theoretical result shows modest noise occlusion assumptions high probability recover good approximation laplacian eigenmaps embedding based uncorrupted kernel matrix results also show regularization aid approximation experimentally explore effects noise occlusion laplacian eigenmaps embeddings two realworld data sets one speech processing one neuroscience well synthetic data set\",\"linear dimensionality reduction methods cornerstone analyzing high dimensional data due simple geometric interpretations typically attractive computational properties methods capture many data features interest covariance dynamical structure correlation data sets inputoutput relationships margin data classes methods developed variety names motivations many fields perhaps result connections methods highlighted survey methods disparate literature optimization programs matrix manifolds discuss principal component analysis factor analysis linear multidimensional scaling fishers linear discriminant analysis canonical correlations analysis maximum autocorrelation factors slow feature analysis sufficient dimensionality reduction undercomplete independent component analysis linear regression distance metric learning optimization framework gives insight rarely discussed shortcomings wellknown methods suboptimality certain eigenvector solutions modern techniques optimization matrix manifolds enable generic linear dimensionality reduction solver accepts input data objective optimized returns output optimal lowdimensional projection data simple optimization framework allows straightforward generalizations novel variants classical methods demonstrate creating orthogonalprojection canonical correlations analysis broadly survey generic solver suggest linear dimensionality reduction move toward becoming blackbox objectiveagnostic numerical technology\",\"nongaussian component analysis ngca unsupervised linear dimension reduction method extracts lowdimensional nongaussian signals highdimensional data contaminated gaussian noise ngca regarded generalization projection pursuit independent component analysis ica multidimensional dependent nongaussian components indeed seminal approaches ngca based ica recently novel ngca approach called leastsquares ngca lsngca developed gives solution analytically leastsquares estimation logdensity gradients eigendecomposition however since prewhitening data involved lsngca performs unreliably data covariance matrix illconditioned often case highdimensional data analysis paper propose whiteningfree lsngca method experimentally demonstrate superiority\",\"density modeling notoriously difficult high dimensional data one approach problem search lower dimensional manifold captures main characteristics data recently gaussian process latent variable model gplvm successfully used find low dimensional manifolds variety complex data gplvm consists set points low dimensional latent space stochastic map observed space show interpreted density model observed space however gplvm trained density model therefore yields bad density estimates propose new training strategy obtain improved generalisation performance better density estimates comparative evaluations several benchmark data sets\",\"recently focus penalized loglikelihood covariance estimation sparse inverse covariance precision matrices penalty responsible inducing sparsity common choice convex norm however best estimator performance always achieved penalty natural sparsity promoting norm nonconvex penalty lack convexity deterred use sparse maximum likelihood estimation paper consider nonconvex penalized loglikelihood inverse covariance estimation present novel cyclic descent algorithm optimization convergence local minimizer proved highly nontrivial demonstrate via simulations reduced bias superior quality penalty compared penalty\",\"study sparse principal component analysis high dimensional vector autoregressive time series doubly asymptotic framework allows dimension scale series length treat transition matrix time series nuisance parameter directly apply sparse principal component analysis multivariate time series data independent provide explicit nonasymptotic rates convergence leading eigenvector estimation extend result principal subspace estimation analysis illustrates spectral norm transition matrix plays essential role determining final rates also characterize sufficient conditions sparse principal component analysis attains optimal parametric rate theoretical results backed thorough numerical studies\",\"contribution deals generalized symmetric fastica algorithm domain independent component analysis ica generalized symmetric version fastica shown potential achieve cramerrao bound crb allowing usage different nonlinearity functions parallel implementations oneunit fastica spite appealing property rigorous study asymptotic error generalized symmetric fastica algorithm still missing community fact existing results exhibit certain limitations ignoring impact data standardization asymptotic statistics based heuristic approach work aim filling blank first result contribution characterization limits generalized symmetric fastica shown algorithm optimizes function sum contrast functions used traditional oneunit fastica correction sign based characterization derive closedform analytic expression asymptotic covariance matrix generalized symmetric fastica estimator using method estimating equation mestimator\",\"principal component analysis pca popular perform dimension reduction selection number significant components essential often based practical heuristics depending application works proposed probabilistic approach able infer number significant components purpose paper introduces bayesian nonparametric principal component analysis bnppca proposed model projects observations onto random orthogonal basis assigned prior distribution defined stiefel manifold prior factor scores involves indian buffet process model uncertainty related number components parameters interest well nuisance parameters finally inferred within fully bayesian framework via monte carlo sampling study inconsistence marginal maximum posteriori estimator latent dimension carried new estimator subspace dimension proposed moreover sake statistical significance kolmogorovsmirnov test based posterior distribution principal components used refine estimate behaviour algorithm first studied various synthetic examples finally proposed bnp dimension reduction approach shown easily yet efficiently coupled clustering latent factor models within unique framework\",\"regularised canonical correlation analysis recently extended two sets variables multiblock method regularised generalised canonical correlation analysis rgcca sparse gcca sgcca proposed address issue variable selection however technical reasons variable selection offered sgcca restricted covariance link blocks tau one main contributions paper beyond covariance link propose extension sgcca full rgcca model tauin addition propose extension sgcca exploits structural relationships variables within blocks specifically propose algorithm allows structured sparsityinducing penalties included rgcca optimisation problem proposed multiblock method illustrated real threeblock highgrade glioma data set aim predict location brain tumours simulated data set aim illustrate methods ability reconstruct true underlying weight vectors\",\"spectral dimensionality reduction frequently used identify lowdimensional structure highdimensional data however learning manifolds especially streaming data computationally memory expensive paper argue stable manifold learned using fraction stream remaining stream mapped manifold significantly less costly manner identifying transition point manifold stable key step present error metrics allow identify transition point given stream quantitatively assessing quality manifold learned using isomap propose efficient mapping algorithm called sisomap used map new samples onto stable manifold describe experiments variety data sets show proposed approach computationally efficient without sacrificing accuracy\",\"provide way infer existence topological circularity highdimensional data sets mathbbrd projection mathbbr obtained fast manifold learning map function highdimensional dataset mathbbx particular choice positive real sigma known bandwidth parameter time also provide way estimate optimal bandwidth fast manifold learning setting minimization functions bandwidth also provide limit theorems characterize behavior proposed functions bandwidth\",\"present new method estimating multivariate secondorder stationary gaussian random field grf models based sparse precision matrix selection sps algorithm proposed davanloo estimating scalar grf models theoretical convergence rates estimated betweenresponse covariance matrix estimated parameters underlying spatial correlation function established numerical tests using simulated real datasets validate theoretical findings data segmentation used handle large data sets\",\"develop theory nonlinear dimensionality reduction nldr number nldr methods developed limited understanding methods work relationships limited basis using existing nldr theory deriving new algorithms provide novel framework analysis nldr via connection statistical theory linear smoothers allows understand existing methods derive new ones use connection smoothing show asymptotically existing nldr methods correspond discrete approximations solutions sets differential equations given boundary condition particular characterize many existing methods terms three limiting differential operators boundary conditions theory also provides way assert one method preferable another indeed show local tangent space alignment superior within class methods assume global coordinate chart defines isometric embedding manifold\",\"independent component analysis ica widely used bss method uniquely achieve source recovery subject scaling permutation ambiguities assumption statistical independence part latent sources independent vector analysis iva extends applicability ica jointly decomposing multiple datasets exploitation dependencies across datasets though ica iva algorithms cast maximum likelihood framework enable use available statistical information reality often deviate theoretical optimality properties due improper estimation probability density function pdf motivates development flexible ica iva algorithms closely adhere underlying statistical description data although attractive minimize assumptions important prior information data sparsity usually available incorporated ica model use additional information relax independence assumption resulting improvement overall separation performance therefore development unified mathematical framework take account statistical independence sparsity great interest work first introduce flexible ica algorithm uses effective pdf estimator accurately capture underlying statistical properties data discuss several techniques accurately estimate parameters multivariate generalized gaussian distribution integrate iva model finally provide mathematical framework enables direct control influence statistical independence sparsity use framework develop effective ica algorithm jointly exploit two forms diversity\",\"aim paper provide new method learning relationships data obtained independently unlike existing methods like matching proposed technique require contextual information provided dependency variables interest monotone therefore easily combined matching order exploit advantages methods technique described mix quantile matching deconvolution provide theoretical empirical validation\",\"sparse versions principal component analysis pca imposed simple yet powerful ways selecting relevant features highdimensional data unsupervised manner however several sparse principal components computed interpretation selected variables difficult since axis sparsity pattern interpreted separately overcome drawback propose bayesian procedure called globally sparse probabilistic pca gsppca allows obtain several sparse components sparsity pattern allows practitioner identify original variables relevant describe data end using roweis probabilistic interpretation pca gaussian prior loading matrix provide first exact computation marginal likelihood bayesian pca model avoid drawbacks discrete model selection simple relaxation framework presented allows find path models using variational expectationmaximization algorithm exact marginal likelihood maximized path approach illustrated real synthetic data sets particular using unlabeled microarray data gsppca infers much relevant gene subsets traditional sparse pca algorithms\",\"statistical dimensionality reduction common rely assumption high dimensional data tend concentrate near lower dimensional manifold rich literature approximating unknown manifold exploiting approximations clustering data compression prediction literature relies linear locally linear approximations article propose simple general alternative instead uses spheres approach refer spherelets develop spherical principal components analysis spca provide theory convergence rate global local spca showing spherelets provide lower covering numbers mses many manifolds results relative stateoftheart competitors show gains ability accurately approximate manifolds fewer components unlike competitors simply output lowerdimensional features approach projects data onto estimated manifold produce fitted values used model assessment cross validation methods illustrated applications multiple data sets\",\"propose novel method introducing structure existing machine learning techniques developing structurebased similarity distance measures learn structural information lowdimensional structure data captured solving nonlinear lowrank representation problem show lowrank representation kernelized closedform solution allows separation independent manifolds robust noise representation similarity observations based nonlinear structure computed incorporated existing feature transformations dimensionality reduction techniques machine learning methods experimental results synthetic real data sets show performance improvements clustering anomaly detection use structural similarity\",\"present procrustes measure novel measure based procrustes rotation enables quantitative comparison output manifoldbased embedding algorithms lle roweis saul isomap tenenbaum measure also serves natural tool choosing dimensionreduction parameters also present two novel dimensionreduction techniques attempt minimize suggested measure compare results techniques results existing algorithms finally suggest simple iterative method used improve output existing algorithms\",\"recent years manifold learning become increasingly popular tool performing nonlinear dimensionality reduction led development numerous algorithms varying degrees complexity aim recover man ifold geometry using either local global features data building laplacian eigenmap diffusionmaps framework propose new paradigm offers guarantee reasonable assumptions manifo learning algorithm preserve geometry data set approach based augmenting output embedding algorithms geometric informatio embodied riemannian metric manifold provide algorithm estimating riemannian metric data demonstrate possible application approach variety examples\",\"canonical correlation analysis cca widely used statistical tool well established theory favorable performance wide range machine learning problems however computing cca huge datasets slow since involves implementing decomposition singular value decomposition huge matrices paper introduce lcca iterative algorithm compute cca fast huge sparse datasets theory asymptotic convergence finite time accuracy lcca established experiments also show lcca outperform fast cca approximation schemes two real datasets\",\"reliable measures statistical dependence could useful tools learning independent features performing tasks like source separation using independent component analysis ica unfortunately many measures like mutual information hard estimate optimize directly propose learn independent features adversarial objectives optimize measures implicitly objectives compare samples joint distribution product marginals without need compute probability densities also propose two methods obtaining samples product marginals using either simple resampling trick separate parametric distribution experiments show strategy easily applied different types model architectures solve linear nonlinear ica problems\",\"many machine learning algorithms require precise estimates covariance matrices sample covariance matrix performs poorly highdimensional settings stimulated development alternative methods majority based factor models shrinkage recent work ledoit wolf extended shrinkage framework nonlinear shrinkage nls powerful covariance estimator based random matrix theory contribution shows contrary claims literature crossvalidation based covariance matrix estimation cvc yields comparable performance strongly reduced complexity runtime two real world data sets show cvc estimator yields superior results competing shrinkage factor based methods\",\"highdimensional data common genomics proteomics chemometrics often contains complicated correlation structures recently partial least squares pls sparse pls methods gained attention areas dimension reduction techniques context supervised data analysis introduce framework regularized pls solving relaxation simpls optimization problem penalties pls loadings vectors approach enjoys many advantages including flexibility general penalties easy interpretation results fast computation highdimensional settings also outline extensions methods leading novel methods nonnegative pls generalized pls adaption pls structured data demonstrate utility methods simulations case study proton nuclear magnetic resonance nmr spectroscopy data\",\"sparse generalized eigenvalue problem gep plays pivotal role large family highdimensional statistical models including sparse fishers discriminant analysis canonical correlation analysis sufficient dimension reduction sparse gep involves solving nonconvex optimization problem existing methods theory context specific statistical models special cases sparse gep require restrictive structural assumptions input matrices paper propose twostage computational framework solve sparse gep first stage solve convex relaxation sparse gep taking solution initial value exploit nonconvex optimization perspective propose truncated rayleigh flow method rifle estimate leading generalized eigenvector show rifle converges linearly solution optimal statistical rate convergence many statistical models theoretically method significantly improves upon existing literature eliminating structural assumptions input matrices stages achieve analysis involves two key ingredients new analysis gradient based method nonconvex objective functions finegrained characterization evolution sparsity patterns along solution path thorough numerical studies provided validate theoretical results\",\"independent component analysis ica technique unsupervised exploration multichannel data widely used observational sciences classical form ica relies modeling data linear mixture nongaussian independent sources problem seen likelihood maximization problem introduce picardo preconditioned lbfgs strategy set orthogonal matrices quickly separate super subgaussian signals returns set sources widely used fastica algorithm numerical experiments show method faster robust fastica real data\",\"present contribution suggests use multidimensional scaling mds algorithm visualization tool manifoldvalued elements visualization tool kind useful signal processing machine learning whenever learningadaptation algorithms insist highdimensional parameter manifolds\",\"statistical dependencies independent component analysis ica cannot remove often provide rich information beyond linear independent components would thus useful estimate dependency structure data models proposed usually concentrated higherorder correlations energy square correlations yet linear correlations fundamental informative form dependency many real data sets linear correlations usually completely removed ica related methods analyzed developing new methods explicitly allow linearly correlated components paper propose probabilistic model linear nongaussian components allowed linear energy correlations precision matrix linear components assumed randomly generated higherorder process explicitly parametrized parameter matrix estimation parameter matrix shown particularly simple using score matching objective function quadratic form using simulations artificial data demonstrate proposed method improves identifiability nongaussian components simultaneously learning correlation structure applications simulated complex cells natural image input well spectrograms natural audio data show method finds new kinds dependencies components\",\"learning low dimensional structure multidimensional data canonical problem machine learning one common approach suppose observed data close lowerdimensional smooth manifold rich variety manifold learning methods available allow mapping data points manifold however clear lack probabilistic methods allow learning manifold along generative distribution observed data best attempt gaussian process latent variable model gplvm identifiability issues lead poor performance solve issues proposing novel coulomb repulsive process corp locations points manifold inspired physical models electrostatic interactions among particles combining process prior mapping function yields novel electrostatic electrogp process focusing simple case onedimensional manifold develop efficient inference algorithms illustrate substantially improved performance variety experiments including filling missing frames video\",\"paper presents new framework manifold learning based sequence principal polynomials capture possibly nonlinear nature data proposed principal polynomial analysis ppa generalizes pca modeling directions maximal variance means curves instead straight lines contrarily previous approaches ppa reduces performing simple univariate regressions makes computationally feasible robust moreover ppa shows number interesting analytical properties first ppa volumepreserving map turn guarantees existence inverse second inverse obtained closed form invertibility important advantage learning methods permits understand identified features input domain data physical meaning moreover allows evaluate performance dimensionality reduction sensible inputdomain units volume preservation also allows easy computation information theoretic quantities reduction multiinformation transform third analytical nature ppa leads clear geometrical interpretation manifold allows computation frenetserret frames local features generalized curvatures point space fourth analytical jacobian allows computation metric induced data thus generalizing mahalanobis distance properties demonstrated theoretically illustrated experimentally performance ppa evaluated dimensionality redundancy reduction synthetic real datasets uci repository\",\"local linear embedding algorithm lle nonlinear dimensionreducing technique widely used due computational simplicity intuitive approach lle first linearly reconstructs input point nearest neighbors preserves neighborhood relations lowdimensional embedding show reconstruction weights computed lle capture highdimensional structure neighborhoods lowdimensional manifold structure consequently weight vectors highly sensitive noise moreover causes lle converge linear projection input opposed nonlinear embedding goal overcome problems propose compute weight vectors using lowdimensional neighborhood representation prove theoretically straightforward computationally simple modification lle reduces lles sensitivity noise modification also removes need regularization number neighbors larger dimension input present numerical examples demonstrating perturbation linear projection problems improved outputs using lowdimensional neighborhood representation\",\"canonical correlation analysis cca multivariate statistical technique finding linear relationship two sets variables kernel generalization cca named kernel cca proposed find nonlinear relations datasets despite wide usage one common limitation lack sparsity solution paper consider sparse kernel cca propose novel sparse kernel cca algorithm skcca algorithm based relationship kernel cca least squares sparsity dual transformations introduced penalizing ellnorm dual vectors experiments demonstrate algorithm performs well computing sparse dual transformations also alleviate overfitting problem kernel cca\",\"multitude methods perform multiset correlated component analysis mcca including require iterative solutions methods differ criterion optimize constraints placed solutions note focuses perhaps simplest version solved single step eigenvectors matrix covariance matrix concatenated data blockdiagonal note shows solution maximizes interset correlation isc without constraints also relates solution two step procedure first whitens dataset using pca performs additional pca concatenated whitened data solutions known although clear derivation simple implementation hard find short note aims remedy\",\"generalized canonical correlation analysis gcca aims finding latent lowdimensional common structure multiple views feature vectors different domains entities unlike principal component analysis pca handles single view gcca able integrate information different feature spaces focus maxvar gcca popular formulation recently gained renewed interest multilingual processing speech modeling classic maxvar gcca problem solved optimally via eigendecomposition matrix compounds whitened correlation matrices views solution serious scalability issues directly amenable incorporating pertinent structural constraints nonnegativity sparsity canonical components posit regularized maxvar gcca nonconvex optimization problem propose alternating optimization aobased algorithm handle algorithm alternates inexact solutions regularized least squares subproblem manifoldconstrained nonconvex subproblem thereby achieving substantial memory computational savings important benefit design easily handle structurepromoting regularization show algorithm globally converges critical point sublinear rate approaches global optimal solution linear rate regularization considered judiciously designed simulations largescale word embedding tasks employed showcase effectiveness proposed algorithm\",\"stochastic principal component analysis spca become popular dimensionality reduction strategy large highdimensional datasets derive simplified algorithm called lazy spca reduced computational complexity better suited largescale distributed computation prove spca lazy spca find approximations principal subspace pairwise distances samples lowerdimensional space invariant whether spca executed lazily empirical studies find downstream predictive performance identical methods superior random projections across range predictive models linear regression logistic lasso random forests largest experiment million samples lazy spca reduced hours computation hours overall lazy spca relies exclusively matrix multiplications besides operation small square matrix whose size depends target dimensionality\",\"independent component analysis ica aims decomposing observed random vector statistically independent variables deflationbased implementations popular oneunit fastica algorithm variants extract independent components one another novel method deflationary ica referred robustica put forward paper simple technique consists performing exact line search optimization kurtosis contrast function step size leading global maximum contrast along search direction found among roots fourthdegree polynomial polynomial rooting performed algebraically thus low cost iteration among practical benefits robustica avoid prewhitening deals real complexvalued mixtures possibly noncircular sources alike absence prewhitening improves asymptotic performance algorithm robust local extrema shows high convergence speed terms computational cost required reach given source extraction quality particularly short data records features demonstrated comparative numerical analysis synthetic data robusticas capabilities processing realworld data involving noncircular complex strongly supergaussian sources illustrated biomedical problem atrial activity extraction atrial fibrillation electrocardiograms ecgs outperforms alternative icabased technique\",\"consider stationary autoregressive processes coefficients restricted ellipsoid includes autoregressive processes absolutely summable coefficients provide consistency results different norms estimation processes using constrained penalized estimators application show weak form universal consistency simulations show directly including constraint estimation lead robust results\",\"given iid observations random vector highdimensional vector lowdimensional index variable study problem estimating conditional inverse covariance matrix omegaz exex mid zxex mid mid assumption set nonzero elements small depend index variable develop novel procedure combines ideas local constant smoothing group lasso estimating conditional inverse covariance matrix proximal iterative smoothing algorithm used solve corresponding convex optimization problems prove procedure recovers conditional independence assumptions distribution mid high probability result established developing uniform deviation bound highdimensional conditional covariance matrix population counterpart may independent interest furthermore develop pointwise confidence intervals individual elements conditional inverse covariance matrix perform extensive simulation studies demonstrate proposed procedure outperforms sensible competitors illustrate proposal stock price data set\",\"microwavebased breast cancer detection proposed complementary approach compensate drawbacks existing breast cancer detection techniques among existing microwave breast cancer detection methods machine learningtype algorithms recently become popular focus detecting existence breast tumours rather performing imaging identify exact tumour position key step machine learning approaches feature extraction one widely used feature extraction method principle component analysis pca however sensitive signal misalignment paper presents empirical mode decomposition emdbased feature extraction method robust misalignment experimental results involving clinical data sets combined numerically simulated tumour responses show combined features emd pca improve detection performance ensemble selectionbased classifier\",\"purpose sufficient dimension reduction sdr find lowdimensional subspace input features sufficient predicting output values paper propose novel distributionfree sdr method called sufficient component analysis sca computationally efficient existing methods method solution computed iteratively performing dependence estimation maximization dependence estimation analytically carried recentlyproposed leastsquares mutual information lsmi dependence maximization also analytically carried utilizing epanechnikov kernel largescale experiments realworld image classification audio tagging problems proposed method shown compare favorably existing dimension reduction approaches\",\"independent component analysis ica one basic tools data analysis aims find coordinate system components data independent popular ica methods use kurtosis metric nongaussianity maximize fastica jade however assumption fourthorder moment kurtosis may always satisfied practice one possible solution use thirdorder moment skewness instead kurtosis applied icasg ecoica paper present competitive approach ica based split generalized gaussian distribution sggd well adapted heavytailed well asymmetric data consequently obtain method works better classical approaches cases heavy tails nonsymmetric data endabstract\",\"independent component analysis ica powerful method blind source separation based assumption sources statistically independent though ica proven useful employed many applications complete statistical independence restrictive assumption practice additionally important prior information data sparsity usually available sparsity natural property data form diversity incorporated ica model relax independence assumption resulting improvement overall separation performance work propose new variant ica entropy bound minimization icaebma flexible yet parameterfree algorithmthrough direct exploitation sparsity using new sparseicaebm algorithm study synergy independence sparsity simulations synthetic well functional magnetic resonance imaging fmrilike data\",\"multivariate analysis mva comprises family wellknown methods feature extraction exploit correlations among input variables data representation one important property enjoyed methods uncorrelation among extracted features recently regularized versions mva methods appeared literature mainly goal gain interpretability solution cases solutions longer obtained closed manner frequent recur iteration two steps one orthogonal procrustes problem letter shows procrustes solution optimal perspective overall mva method proposes alternative approach based solution eigenvalue problem method ensures preservation several properties original methods notably uncorrelation extracted features demonstrated theoretically collection selected experiments\",\"ability many powerful machine learning algorithms deal large data sets without compromise often hampered computationally expensive linear algebra tasks calculating log determinant canonical example paper demonstrate optimality maximum entropy methods approximating calculations prove equivalence mean value constraints sample expectations big data limit covariance matrix eigenvalue distributions completely defined moment information reduction self entropy maximum entropy proposal distribution achieved adding moments reduces divergence proposal true eigenvalue distribution empirically verify results variety sparsesuite matrices establish best practices\",\"fastica algorithm one popular iterative algorithms domain linear independent component analysis despite success observed fastica occasionally yields outcomes correspond true solutions known demixing vectors ica problem outcomes commonly referred spurious solutions although fastica among extensively studied ica algorithms occurrence spurious solutions yet completely understood community contribution aim addressing issue first part work interested relationship demixing vectors local optimizers contrast function attractive unattractive fixed points fastica algorithm characterizations sets given inclusion relationship discovered second part investigate possible scenarios spurious solutions occur show certain bimodal gaussian mixtures distributions involved may exist spurious solutions attractive fixed points fastica case popular nonlinearities gauss tanh tend yield spurious solutions whereas kurtosis may give reliable results advices given practical choice nonlinearity function\",\"technical note considers problems blind sparse learning inference electrogram egm signals atrial fibrillation conditions first introduce mathematical model observed signals takes account multiple foci typically appearing inside heart propose reconstruction model based fixed dictionary discuss several alternatives choosing dictionary order obtain sparse solution takes account biological restrictions problem first alternative using lasso regularization followed postprocessing stage removes low amplitude coefficients violating refractory period characteristic cardiac cells alternative propose novel regularization term called cross products lasso cplasso able incorporate biological constraints directly optimization problem unfortunately resulting problem nonconvex show solved efficiently approximated way making use successive convex approximations sca finally spectral analysis performed clean activation sequence obtained sparse learning stage order estimate number latent foci frequencies simulations synthetic real data provided validate proposed approach\",\"maximum variance unfolding one main methods nonlinear dimensionality reduction study large sample limit providing specific rates convergence standard assumptions find consistent underlying submanifold isometric convex subset provide simple examples fails consistent\",\"paper propose new algorithm streaming principal component analysis limited memory small devices cannot store samples highdimensional regime streaming principal component analysis aims find kdimensional subspace explain variation ddimensional data points come memory sequentially order deal large large number samples streaming pca algorithms update current model using incoming sample dump information right away save memory however information contained previously streamed data could useful motivated idea develop new streaming pca algorithm called history pca achieves goal using obd memory bapprox block size algorithm converges much faster existing streaming pca algorithms changing number inner iterations memory usage reduced maintaining comparable convergence speed provide theoretical guarantees convergence algorithm along rate convergence also demonstrate synthetic real world data sets algorithm compares favorably stateoftheart streaming pca methods terms convergence speed performance\",\"slow feature analysis sfa method extracting slowly varying driving forces quickly varying nonstationary time series show possible sfa detect component even slower driving force envelope modulated sine wave shown depends circumstances like embedding dimension time series predictability base frequency whether driving force slower subcomponent detected observe phase transition one regime purpose work quantify influence various parameters phase transition conclude percieved slow sfa varies less fast switching one regime occurs perhaps showing similarity human perception\",\"matching datasets multiple modalities become important task data analysis existing methods often rely embedding transformation single modality without utilizing correspondence information often results suboptimal matching performance paper propose nonlinear manifold matching algorithm using shortestpath distance joint neighborhood selection specifically joint nearestneighbor graph built modalities shortestpath distance within modality calculated joint neighborhood graph followed embedding matching common lowdimensional euclidean space compared existing algorithms approach exhibits superior performance matching disparate datasets multiple modalities\",\"introduce approach based givens representation posterior inference statistical models orthogonal matrix parameters factor models probabilistic principal component analysis ppca show givens representation used develop practical methods transforming densities stiefel manifold densities subsets euclidean space show deal issues arising topology stiefel manifold inexpensively compute changeofmeasure terms introduce auxiliary parameter approach limits impact topological issues provide analysis methods numerical examples demonstrating effectiveness approach also discuss givens representation used define general classes distributions space orthogonal matrices give demonstrations several examples showing givens approach performs practice comparison methods\",\"inverse covariance matrix provides considerable insight understanding statistical models multivariate setting particular distribution variables assumed multivariate normal sparsity pattern inverse covariance matrix commonly referred precision matrix corresponds adjacency matrix representation gaussmarkov graph encodes conditional independence statements variables minimax results spectral norm previously established covariance matrices sparse banded sparse precision matrices establish minimax estimation bounds estimating banded precision matrices spectral norm results greatly improve upon existing bounds particular find minimax rate estimating banded precision matrices matches estimating banded covariance matrices key insight analysis able obtain barelynoisy estimates times subblocks precision matrix inverting slightly wider blocks empirical covariance matrix along diagonal theoretical results complemented experiments demonstrating sharpness bounds\",\"correlation matrices play key role many multivariate methods graphical model estimation factor analysis current stateoftheart estimating large correlation matrices focuses use pearsons sample correlation matrix although pearsons sample correlation matrix enjoys various good properties gaussian models effective estimator facing heavytailed distributions robust alternative han liu stat assoc advocated use transformed version kendalls tau sample correlation matrix estimating high dimensional latent generalized correlation matrix transelliptical distribution family elliptical copula transelliptical family assumes unspecified marginal monotone transformations data follow elliptical distribution paper study theoretical properties kendalls tau sample correlation matrix transformed version proposed han liu stat assoc estimating population kendalls tau correlation matrix latent pearsons correlation matrix spectral restricted spectral norms regard spectral norm highlight role effective rank quantifying rate convergence regard restricted spectral norm first time present sign subgaussian condition sufficient guarantee rankbased correlation matrix estimator attains fast rate convergence cases need moment condition\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"13_manifold_analysis_ica\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"13_manifold_analysis_ica\"],\"textfont\":{\"size\":12},\"x\":[12.591422,12.531585,10.929464,10.543964,11.103203,11.060096,12.548752,9.947871,10.852829,10.552842,10.549474,10.402711,9.9832115,11.054788,12.672458,10.228039,12.562933,10.523569,10.417888,12.673307,10.982806,10.703147,10.429556,12.804491,10.404343,12.594864,12.598077,10.983213,12.556754,10.394461,10.090074,10.605088,12.5714245,12.495518,12.551469,12.56721,10.692007,10.363116,10.772145,9.779119,10.871626,10.444494,12.56769,10.374696,12.613493,12.573156,12.533738,10.537015,10.405436,10.822063,10.806772,10.220274,11.068171,10.961572,10.078693,10.4530735,10.41852,10.408416,10.477281,10.838831,10.406989,10.147955,12.519132,10.572452,10.52043,10.002936,12.8352785,10.964673,10.092066,11.133423],\"y\":[4.1354384,4.0625267,5.343011,5.6042213,4.8383026,5.317479,4.155562,5.3373675,5.21408,5.119279,5.582978,5.272143,5.7535977,5.3133206,4.2044525,5.905096,4.103986,5.85343,5.281764,4.2125225,5.351381,5.0305295,5.227749,6.211893,5.5340247,4.099419,4.0910664,5.409949,4.0599537,5.2780952,5.649627,5.011246,4.1318936,4.1011877,4.055758,4.0975595,5.590417,5.315622,5.556715,5.6904016,5.3345246,5.2395267,4.0848923,5.3554454,4.1373672,4.1398125,4.0309143,5.593942,5.465299,5.57769,5.630487,5.309926,5.0462413,5.3515863,5.520162,5.361365,5.228758,5.24766,5.662026,5.600415,5.235781,5.6289544,4.091838,5.11186,5.1187534,5.7189083,6.1371455,5.3523116,4.50487,5.0713553],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"consider parametric exponential families dimension real line study variant textitboundary crossing probabilities coming multiarmed bandit literature case realvalued distributions form exponential family dimension formally result concentration inequality bounds probability mathcalbpsihat thetanthetastargeq ftnn thetastar parameter unknown target distribution hat thetan empirical parameter estimate built observations psi logpartition function exponential family mathcalbpsi corresponding bregman divergence perspective stochastic multiarmed bandits pay special attention case boundary function logarithmic enables analyze regret stateoftheart klucb klucbp strategies whose analysis left open generality indeed previous results hold case provide results arbitrary finite dimension thus considerably extending existing results perhaps surprisingly highlight proof techniques achieve strong results already existed three decades ago work lai apparently forgotten bandit community provide modern rewriting beautiful techniques believe useful beyond application stochastic multiarmed bandits\",\"consider bayesian optimization expensivetoevaluate blackbox objective function also access cheaper approximations objective general approximations arise applications reinforcement learning engineering natural sciences subject inherent unknown bias model discrepancy caused inadequate internal model deviates reality vary domain making utilization approximations nontrivial task present novel algorithm provides rigorous mathematical treatment uncertainties arising model discrepancies noisy observations optimization decisions rely value information analysis extends knowledge gradient factor setting multiple information sources vary cost sampling decision maximizes predicted benefit per unit cost conduct experimental evaluation demonstrates method consistently outperforms stateoftheart techniques finds designs considerably higher objective value additionally inflicts less cost exploration process\",\"present informationtheoretic framework solving global blackbox optimization problems also blackbox constraints particular interest efficiently solve problems decoupled constraints subsets objective constraint functions may evaluated independently example objective evaluated cpu constraints evaluated independently gpu problems require acquisition function separated contributions individual function evaluations develop one acquisition function call predictive entropy search constraints pesc pesc approximation expected information gain criterion compares favorably alternative approaches based improvement several synthetic realworld problems addition consider problems mix functions fast slow evaluate problems require balancing amount time spent metacomputation pesc actual evaluation target objective take bounded rationality approach develop partial update pesc trades accuracy speed propose method adaptively switching partial full updates pesc allows interpolate versions pesc efficient terms function evaluations efficient terms wallclock time overall demonstrate pesc effective algorithm provides promising direction towards unified solution constrained bayesian optimization\",\"address problem synthetic gene design using bayesian optimization main issue designing gene design space defined terms long strings characters different lengths renders optimization intractable propose threestep approach deal issue first use gaussian process model emulate behavior cell inputs model use set biologically meaningful gene features allows define optimal gene designs rules based model outputs define multitask acquisition function optimize simultaneously severals aspects interest finally define evaluation function allow rank sets candidate gene sequences coherent optimal design strategy illustrate performance approach real gene design experiment mammalian cells\",\"bayesian optimization recently emerged popular efficient tool global optimization hyperparameter tuning currently established bayesian optimization practice requires userdefined bounding box assumed contain optimizer however little known probed objective function difficult prescribe bounds work modify standard bayesian optimization framework principled way allow automatic resizing search space introduce two alternative methods compare two common synthetic benchmarking test functions well tasks tuning stochastic gradient descent optimizer multilayered perceptron convolutional neural network mnist\",\"consider mnkclassical problem controller activating sampling sequentially finite number geq populations specified unknown distributions time horizon time ldots controller wishes select population sample goal sampling population optimizes score function distribution maximizing expected sum outcomes minimizing variability define class textituniformly fast sampling policies show mild regularity conditions asymptotic lower bound expected total number suboptimal population activations provide sufficient conditions ucb policy asymptotically optimal since attains lower bound explicit solutions provided number examples interest including general score functionals unconstrained pareto distributions potentially infinite mean uniform distributions unknown support additional results bandits normal distributions also provided\",\"order achieve stateoftheart performance modern machine learning techniques require careful data preprocessing hyperparameter tuning moreover given ever increasing number machine learning models developed model selection becoming increasingly important automating selection tuning machine learning pipelines consisting data preprocessing methods machine learning models long one goals machine learning community paper tackle metalearning task combining ideas collaborative filtering bayesian optimization using probabilistic matrix factorization techniques acquisition functions bayesian optimization exploit experiments performed hundreds different datasets guide exploration space possible pipelines experiments show approach quickly identifies highperforming pipelines across wide range datasets significantly outperforming current stateoftheart\",\"important problem sequential decisionmaking uncertainty use limited data compute safe policy policy guaranteed perform least well given baseline strategy paper develop analyze new modelbased approach compute safe policy access inaccurate dynamics model system known accuracy guarantees proposed robust method uses inaccurate model directly minimize negative regret wrt baseline policy contrary existing approaches minimizing regret allows one improve baseline policy states accurate dynamics seamlessly fall back baseline policy otherwise show formulation nphard propose approximate algorithm empirical results several domains show even relatively simple approximate algorithm significantly outperform standard approaches\",\"tradeoff cost acquiring processing data uncertainty due lack data fundamental machine learning basic instance tradeoff problem deciding make noisy costly observations discretetime gaussian random walk minimise posterior variance plus observation costs present first proof simple policy observes posterior variance exceeds threshold optimal problem proof generalises wide range cost functions posterior variance result implies optimal policies linearquadraticgaussian control costly observations threshold structure also implies restless bandit problem observing multiple time series welldefined whittle index discuss computation index give closedform formulae compare performance associated index policy heuristic policies proof based new verification theorem demonstrates threshold structure markov decision processes relation binary sequences known mechanical words dynamics discontinuous nonlinear maps frequently arise physics control biology\",\"distributional approaches valuebased reinforcement learning model entire distribution returns rather expected values recently shown yield stateoftheart empirical performance demonstrated recently proposed algorithm based categorical distributional reinforcement learning cdrl bellemare however theoretical properties cdrl algorithms yet well understood paper introduce framework analyse cdrl algorithms establish importance projected distributional bellman operator distributional draw fundamental connections cdrl cramer distance give proof convergence samplebased categorical distributional reinforcement learning algorithms\",\"paper devoted regret lower bounds classical model stochastic multiarmed bandit wellknown result lai robbins extended burnetas katehakis established presence logarithmic bound consistent policies relax notion consistence exhibit generalisation logarithmic bound also show non existence logarithmic bound general case hannan consistency get results study variants popular upper confidence bounds ucb policies byproduct prove impossible design adaptive policy would select best two algorithms taking advantage properties environment\",\"provide yet another proof existence calibrated forecasters two merits first valid arbitrary finite number outcomes second short simple follows direct application blackwells approachability theorem carefully chosen vectorvalued payoff function convex target set proof captures essence existing proofs based approachability proof foster case binary outcomes highlights intrinsic connection approachability calibration\",\"inventory control unknown demand distribution considered emphasis placed case involving discrete nonperishable items focus adaptive policy every period uses much possible optimal newsvendor ordering quantity empirical distribution learned period policy assessed using regret criterion measures price paid ambiguity demand distribution periods guarantees latters separation critical newsvendor parameter betabhb constant upper bound regret found without prior information demand distribution show regret grow faster rate tepsilon epsilon view known lower bound almost best one could hope simulation studies involving along policies also conducted\",\"existing strategies finitearmed stochastic bandits mostly depend parameter scale must known advance sometimes form bound payoffs knowledge variance subgaussian parameter notable exceptions analysis gaussian bandits unknown mean variance cowan katehakis uniform distributions unknown support cowan katehakis results derived specialised cases generalised nonparametric setup learner knows bound kurtosis noise scale free measure extremity outliers\",\"renewed interest formulating integration inference problem motivated obtaining full distribution numerical error propagated subsequent computation current methods bayesian quadrature demonstrate impressive empirical performance lack theoretical analysis important challenge reconcile probabilistic integrators rigorous convergence guarantees paper present first probabilistic integrator admits theoretical treatment called frankwolfe bayesian quadrature fwbq fwbq convergence true value integral shown exponential posterior contraction rates proven superexponential simulations fwbq competitive stateoftheart methods outperforms alternatives based frankwolfe optimisation approach applied successfully quantify numerical error solution challenging model choice problem cellular biology\",\"paper consider problem stochastic optimization bandit feedback model generalize gpucb algorithm srinivas arbitrary kernels search spaces use notion localized chaining control supremum gaussian process provide novel optimization scheme based computation covering numbers theoretical bounds obtain cumulative regret generic present convergence rates gpucb algorithm finally algorithm shown empirically efficient natural competitors simple complex input spaces\",\"actorcritic methods solve reinforcement learning problems updating parameterized policy known actor direction increases estimate expected return known critic however existing actorcritic methods use values gradients critic update policy parameter paper propose novel actorcritic method called guide actorcritic gac gac firstly learns guide actor locally maximizes critic updates policy parameter based guide actor supervised learning main theoretical contributions two folds first show gac updates guide actor performing secondorder optimization action space curvature matrix based hessians critic second show deterministic policy gradient method special case gac hessians ignored experiments show method promising reinforcement learning method continuous controls\",\"consider novel stochastic multiarmed bandit problem called good arm identification gai good arm defined arm expected reward greater equal given threshold gai pureexploration problem single agent repeats process outputting arm soon identified good one confirming arms actually good objective gai minimize number samples process find gai faces new kind dilemma explorationexploitation dilemma confidence different difficulty best arm identification result efficient design algorithms gai quite different best arm identification derive lower bound sample complexity gai tight logarithmic factor mathrmolog fracdelta acceptance error rate delta also develop algorithm whose sample complexity almost matches lower bound also confirm experimentally proposed algorithm outperforms naive algorithms synthetic settings based conventional bandit problem clinical trial researches rheumatoid arthritis\",\"study problem allocating stocks dark pools propose analyze optimal approach allocations continuousvalued allocations allowed also propose modification case integervalued allocations possible extend previous work problem adversarial scenarios also improving results iid setup resulting algorithms efficient perform well simulations stochastic adversarial inputs\",\"modern supervised machine learning algorithms involve hyperparameters set running options setting hyperparameters default values software package manual configuration user configuring optimal predictive performance tuning procedure goal paper twofold firstly formalize problem tuning statistical point view define databased defaults suggest general measures quantifying tunability hyperparameters algorithms secondly conduct largescale benchmarking study based datasets openml platform six common machine learning algorithms apply measures assess tunability parameters results yield default values hyperparameters enable users decide whether worth conducting possibly time consuming tuning strategy focus important hyperparameters chose adequate hyperparameter spaces tuning\",\"typical applications bayesian optimization minimal assumptions made objective function optimized true even researchers prior information shape function respect one argument make case shape constraints often appropriate least two important application areas bayesian optimization hyperparameter tuning machine learning algorithms decision analysis utility functions describe methodology incorporating variety shape constraints within usual bayesian optimization framework present positive results simple applications suggest bayesian optimization shape constraints promising topic research\",\"paper studies deviations regret stochastic multiarmed bandit problem total number plays known beforehand agent audibert exhibit policy probability least regret policy order logn also shown property shared popular ucb policy auer work first answers open question extends negative result anytime policy second contribution paper design anytime robust policies specific multiarmed bandit problems restrictions put set possible distributions different arms\",\"bandit methods blackbox optimisation bayesian optimisation used variety applications including hyperparameter tuning experiment design recently emphmultifidelity methods garnered considerable attention since function evaluations become increasingly expensive applications multifidelity methods use cheap approximations function interest speed overall optimisation process however multifidelity methods assume finite number approximations many practical applications however continuous spectrum approximations might available instance tuning expensive neural network one might choose approximate cross validation performance using less data andor training iterations approximations best viewed arising continuous two dimensional space work develop bayesian optimisation method boca setting characterise theoretical properties show achieves better regret strategies ignore approximations boca outperforms several baselines synthetic real experiments\",\"propose practical extensions bayesian optimization solving dynamic problems model dynamic objective functions using spatiotemporal gaussian process priors capture instances functions time extensions bayesian optimization use information learnt model guide tracking temporally evolving minimum exploiting temporal correlations proposed method also determines make evaluations fast make evaluations induces appropriate budget steps based available information lastly evaluate technique synthetic realworld problems\",\"informationtheoretic bayesian optimisation techniques demonstrated stateoftheart performance tackling important global optimisation problems however current informationtheoretic approaches require many approximations implementation introduce oftenprohibitive computational overhead limit choice kernels available model objective develop fast informationtheoretic bayesian optimisation method fitbo avoids need sampling global minimiser thus significantly reducing computational overhead moreover comparison existing approaches method faces fewer constraints kernel choice enjoys merits dealing output space demonstrate empirically fitbo inherits performance associated informationtheoretic bayesian optimisation even faster simpler bayesian optimisation approaches expected improvement\",\"address online linear optimization problem actions forecaster represented binary vectors goal understand magnitude minimax regret worst possible set actions study problem three different assumptions feedback full information partial information models socalled semibandit bandit problems consider linfty ltype restrictions losses assigned adversary formulate general strategy using bregman projections top potentialbased gradient descent generalizes ones studied series papers gyorgy dani abernethy cesabianchi lugosi helmbold warmuth koolen uchiya kale audibert bubeck provide simple proofs recover previous results propose new upper bounds semibandit game moreover derive lower bounds three feedback assumptions exception bandit game upper lower bounds tight constant factor finally answer question asked koolen showing exponentially weighted average forecaster suboptimal linfty adversaries\",\"game theory finds nowadays broad range applications engineering machine learning however derivativefree expensive blackbox context algorithmic solutions available find game equilibria propose novel gaussianprocess based approach solving games context follow classical bayesian optimization framework sequential sampling decisions based acquisition functions two strategies proposed based either probability achieving equilibrium stepwise uncertainty reduction paradigm practical numerical aspects discussed order enhance scalability reduce computation time approach evaluated several synthetic game problems varying number players decision space dimensions show equilibria found reliably fraction cost terms blackbox evaluations compared classical derivativebased algorithms method available package gpgame available cran httpscranrprojectorgpackagegpgame\",\"study restless bandit associated extremely simple scalar kalman filter model discrete time certain assumptions prove problem indexable sense whittle index nondecreasing function relevant belief state spite long history problem appears first proof use results schurconvexity mechanical words particular binary strings intimately related palindromes\",\"paper index policies minimizing frequentist regret stochastic multiarmed bandit model inspired bayesian view problem main contribution prove bayesucb algorithm relies quantiles posterior distributions asymptotically optimal reward distributions belong onedimensional exponential family large class prior distributions also show bayesian literature gives new insight kind exploration rates could used frequentist ucbtype algorithms indeed approximations bayesian optimal solution finite horizon gittins indices provide justification klucb klucbh algorithms whose asymptotic optimality also established\",\"propose novel theoreticallygrounded acquisition function batch bayesian optimization informed insights distributionally ambiguous optimization acquisition function lower bound wellknown expected improvement function requires evaluation gaussian expectation multivariate piecewise affine function bound computed instead evaluating bestcase expectation probability distributions consistent mean variance original gaussian distribution unlike alternative approaches including expected improvement proposed acquisition function avoids multidimensional integrations entirely computed exactly even large batch sizes solution tractable convex optimization problem suggested acquisition function also optimized efficiently since first second derivative information calculated inexpensively byproducts acquisition function calculation derive various novel theorems ground work theoretically demonstrate superior performance via simple motivating examples benchmark functions realworld problems\",\"stochastic bandit problem goal maximize unknown function via sequence noisy evaluations typically observation noise assumed independent evaluation point satisfy tail bound uniformly domain restrictive assumption many applications work consider bandits heteroscedastic noise explicitly allow noise distribution depend evaluation point show leads new tradeoffs information regret taken account existing approaches like upper confidence bound algorithms ucb thompson sampling address shortcomings introduce frequentist regret analysis framework similar bayesian framework russo van roy prove new highprobability regret bound general possibly randomized policies depends quantity refer regretinformation ratio bound define frequentist version information directed sampling ids minimize regretinformation ratio possible action sampling distributions relies concentration inequalities online least squares regression separable hilbert spaces generalize case heteroscedastic noise formulate several variants ids linear reproducing kernel hilbert space response functions yielding novel algorithms bayesian optimization also prove frequentist regret bounds homoscedastic case recover known bounds ucb much better noise heteroscedastic empirically demonstrate linear setting heteroscedastic noise methods outperform ucb thompson sampling staying competitive noise homoscedastic\",\"unknown constraints arise many types expensive blackbox optimization problems several methods proposed recently performing bayesian optimization constraints based expected improvement heuristic however lead pathologies used constraints example case decoupled constraintsie one independently evaluate objective constraintsei encounter pathology prevents exploration additionally computing requires current best solution may exist none data collected far satisfy constraints contrast informationbased approaches suffer failure modes paper present new informationbased method called predictive entropy search constraints pesc analyze performance pesc show compares favorably eibased approaches synthetic benchmark problems well several realworld examples demonstrate pesc effective algorithm provides promising direction towards unified solution constrained bayesian optimization\",\"bayesian optimization modelbased approach gradientfree blackbox function optimization typically powered gaussian process whose algorithmic complexity cubic number evaluations hence gpbased cannot leverage large amounts past related function evaluations example warm start procedure develop multiple adaptive bayesian linear regression model scalable alternative whose complexity linear number observations multiple bayesian linear regression models coupled shared feedforward neural network learns joint representation transfers knowledge across machine learning problems\",\"propose framework modeling estimating state controlled dynamical systems agent affect system actions receives partial observations based framework propose predictive state representation random fourier features rffpsr key property rffpsrs state estimate represented conditional distribution future observations given future actions rffpsrs combine representation momentmatching kernel embedding local optimization achieve method enjoys several favorable qualities represent controlled environments affected actions efficient theoretically justified learning algorithm uses nonparametric representation expressive power represent continuous nonlinear dynamics provide detailed formulation theoretical analysis experimental evaluation demonstrates effectiveness method\",\"approximate bayesian computation abc using sequential monte carlo method provides comprehensive platform parameter estimation model selection sensitivity analysis differential equations however method like monte carlo methods incurs significant computational cost requires explicit numerical integration differential equations carry inference paper propose novel method circumventing requirement explicit integration using derivatives gaussian processes smooth observations parameters estimated evaluate methods using synthetic data generated model biological systems described ordinary delay differential equations upon comparing performance method existing abc techniques demonstrate produces comparably reliable parameter estimates significantly reduced execution time\",\"propose first fullyadaptive algorithm pure exploration linear banditsthe task find arm largest expected reward depends unknown parameter linearly existing methods partially entirely fix sequences arm selections observing rewards method adaptively changes arm selection strategy based past observations round show sample complexity matches achievable lower bound constant factor extreme case furthermore evaluate performance methods simulations based synthetic setting realworld data method shows vast improvement existing methods\",\"bayesian optimization effective methodology global optimization functions expensive evaluations relies querying distribution functions defined relatively cheap surrogate model accurate model distribution functions critical effectiveness approach typically fit using gaussian processes gps however since gps scale cubically number observations challenging handle objectives whose optimization requires many evaluations massively parallelizing optimization work explore use neural networks alternative gps model distributions functions show performing adaptive basis function regression neural network parametric form performs competitively stateoftheart gpbased approaches scales linearly number data rather cubically allows achieve previously intractable degree parallelism apply large scale hyperparameter optimization rapidly finding competitive models benchmark object recognition tasks using convolutional networks image caption generation using neural language models\",\"paper focus developing efficient sensitivity analysis methods computationally expensive objective function case minimization performed computationally expensive means evaluation takes significant amount time therefore main goal use small number function evaluations infer sensitivity information different parameters correspondingly consider optimization procedure adaptive experimental design reuse available function evaluations initial design points establish surrogate model called response surface sensitivity analysis performed lieu furthermore propose new local multivariate sensitivity measure example around optimal solution high dimensional problems corresponding objectiveoriented experimental design proposed order make generated surrogate better suitable accurate calculation proposed specific local sensitivity quantities addition demonstrate better performance gaussian radial basis function interpolator kriging cases relatively high dimensionality experimental design points numerical experiments demonstrate optimization procedure objectiveoriented experimental design behavior much better classical latin hypercube design addition performance kriging good gaussian rbf especially case high dimensional problems\",\"consider explorationexploitation tradeoff linear quadratic control problems state dynamics linear cost function quadratic states controls analyze regret thompson sampling aka posteriorsampling reinforcement learning frequentist setting parameters characterizing dynamics fixed despite empirical theoretical success wide range problems multiarmed bandit linear bandit show studying frequentist regret control problems need tradeoff frequency sampling optimistic parameters frequency switches control policy results overall regret significantly worse regret osqrtt achieved optimisminfaceofuncertainty algorithm control problems\",\"bayesian optimization methods useful optimizing functions expensive evaluate lack analytical expression whose evaluations contaminated noise methods rely probabilistic model objective function typically gaussian process upon acquisition function built function guides optimization process measures expected utility performing evaluation objective new point gps assume continous input variables case input variables take integer values one introduce extra approximations common approach round suggested variable value closest integer evaluation objective show lead problems optimization process describe principled approach account input variables integervalued illustrate synthetic real experiments utility approach significantly improves results standard methods problems involving integervalued variables\",\"classical approach inverse problems based optimization misfit function despite computational appeal approach suffers many shortcomings nonuniqueness solutions modeling prior knowledge etc bayesian formalism inverse problems avoids difficulties encountered optimization approach albeit increased computational cost work use information theoretic arguments cast bayesian inference problem terms optimization problem resulting scheme combines theoretical soundness fully bayesian inference computational efficiency simple optimization\",\"present glasses global optimisation lookahead stochastic simulation expectedloss search majority global optimisation approaches use myopic considering impact next function value nonmyopic approaches exist able consider handful future evaluations novel algorithm glasses permits consideration dozens evaluations future done approximating ideal lookahead loss function expensive evaluate cheaper alternative future steps algorithm simulated beforehand expectation propagation algorithm used compute expected value losswe show farhorizon planning thus enabled leads substantive performance gains empirical tests\",\"present mlrmbo flexible comprehensive toolbox modelbased optimization mbo also known bayesian optimization addresses problem expensive blackbox optimization approximating given objective function surrogate regression model designed single multiobjective optimization mixed continuous categorical conditional parameters additional features include multipoint batch proposal parallelization visualization logging errorhandling mlrmbo implemented modular fashion single components easily replaced adapted user specific use cases regression learner mlr toolbox machine learning used infill criteria infill optimizers easily exchangeable empirically demonstrate mlrmbo provides stateoftheart performance comparing different benchmark scenarios wide range optimizers including diceoptim rbayesianoptimization spot smac spearmint hyperopt\",\"propose novel bayesian optimization approach blackbox functions environmental variable whose value determines tradeoff evaluation cost fidelity evaluations use novel approach sampling support points allowing faster construction acquisition function allows achieve optimization lower overheads previous approaches implemented general class problem show approach effective synthetic real world benchmark problems\",\"simulated annealing popular method approaching solution global optimization problem existing results performance apply discrete combinatorial optimization optimization variables assume finite set possible values introduce new general formulation simulated annealing allows one guarantee finitetime performance optimization functions continuous variables results hold universally optimization problem bounded domain establish connection simulated annealing uptodate theory convergence markov chain monte carlo methods continuous domains work inspired concept finitetime learning known accuracy confidence developed statistical learning theory\",\"propose extension concept expected improvement criterion commonly used kriging based optimization extend complex kriging models models using derivatives target field application cfd problems objective function extremely expensive evaluate theory also used fields\",\"novel python framework bayesian optimization known gpflowopt introduced package based popular gpflow library gaussian processes leveraging benefits tensorflow including automatic differentiation parallelization gpu computations bayesian optimization design goals focus framework easy extend custom acquisition functions models framework thoroughly tested well documented provides scalability current released version gpflowopt includes standard singleobjective acquisition functions stateoftheart maxvalue entropy search well bayesian multiobjective approach finally permits easy use custom modeling strategies implemented gpflow\",\"introduce methodology efficiently computing lower bound empowerment allowing used unsupervised cost function policy learning realtime control empowerment channel capacity actions states maximises influence agent near future shown good model biological behaviour absence extrinsic goal empowerment also prohibitively hard compute especially nonlinear continuous spaces introduce efficient amortised method learning empowermentmaximising policies demonstrate algorithm reliably handle continuous dynamical systems using system dynamics learned raw data resulting policies consistently drive agents states use full potential\",\"many retailers today employ inventory management systems based reorder point policies rely assumption decreases product inventory levels result product sales unfortunately usually happens small random quantities product get lost stolen broken without record time passes consequence shoplifting usual retailers handling large varieties inexpensive products grocery stores turn time discrepancies lead stock freezing problems situations system believes stock reorder point actual stock zero replenishments sales occur motivated issues model interaction sales losses replenishments inventory levels dynamic bayesian network dbn inventory levels unobserved hidden variables wish estimate present expectationmaximization algorithm estimate parameters sale loss distributions relies solving onedimensional dynamic program estep solving two separate onedimensional nonlinear programs mstep\",\"exciting branch machine learning research focuses methods learning optimizing integrating unknown functions difficult costly evaluate popular bayesian approach problem uses gaussian process construct posterior distribution function interest given set observed measurements selects new points evaluate using statistics posterior extend methods exploit derivative information unknown function describe methods bayesian optimization bayesian quadrature settings first second derivatives may evaluated along function perform samplingbased inference order incorporate uncertainty hyperparameters show hyperparameter function uncertainty decrease much rapidly using derivative information moreover introduce techniques overcoming illconditioning issues plagued earlier methods gradientenhanced gaussian processes kriging illustrate efficacy methods using applications real simulated bayesian optimization quadrature problems show exploting derivatives provide substantial gains standard methods\",\"paper presents novel approach direct covariance function learning bayesian optimisation particular emphasis experimental design problems existing corpus condensed knowledge present method presented borrows techniques reproducing kernel banach space theory specifically mkernels leverages convert reweight existing covariance functions new problemspecific covariance functions key advantage approach rather relying user manually select hyperparameter tuning experimentation appropriate covariance function constructs covariance function specifically match problem hand technique demonstrated two realworld problems specifically alloy design shortpolymer fibre manufacturing well selected test function\",\"consider firm sells products periods without knowing demand function firm sequentially sets prices earn revenue learn underlying demand function simultaneously natural heuristic problem commonly used practice greedy iterative least squares gils time period gils estimates demand linear function price applying least squares set prior prices realized demands price maximizes revenue given estimated demand function used next time period performance measured regret expected revenue loss optimal oracle pricing policy demand function known recently den boer zwart keskin zeevi demonstrated gils suboptimal introduced algorithms integrate forced price dispersion gils achieve asymptotically optimal performance paper consider dynamic pricing problem datarich environment particular assume firm knows expected demand particular price historical data period setting price firm access extra information demand covariates may predictive demand prove setting gils achieves asymptotically optimal regret order logt also show following surprising result original dynamic pricing problem den boer zwart keskin zeevi inclusion set covariates gils potential demand covariates even though could carry information would make gils asymptotically optimal validate results via extensive numerical simulations synthetic real data sets\",\"consider problem sequentially making decisions rewarded successes failures predicted unknown relationship depends partially controllable vector attributes instance learner takes active role selecting samples instance pool goal maximize probability success either offline training online testing phases problem motivated realworld applications observations timeconsuming andor expensive develop knowledge gradient policy using online bayesian linear classifier guide experiment maximizing expected value information labeling alternative provide finitetime analysis estimated error show maximum likelihood estimator based produced policy consistent asymptotically normal also show knowledge gradient policy asymptotically optimal offline setting work extends knowledge gradient setting contextual bandits report results series experiments demonstrate efficiency\",\"scaling bayesian optimization high dimensions challenging task global optimization highdimensional acquisition function expensive often infeasible existing methods depend either limited active variables additive form objective function propose new method highdimensional bayesian optimization uses dropout strategy optimize subset variables iteration derive theoretical bounds regret show inform derivation algorithm demonstrate efficacy algorithms optimization two benchmark functions two realworld applications training cascade classifiers optimizing alloy composition\",\"methods decisiontheoretic online learning based hedge algorithm takes parameter called learning rate previous analyses learning rate carefully tuned obtain optimal worstcase performance leading suboptimal performance easy instances example exists action significantly better others propose new way setting learning rate adapts difficulty learning problem worst case procedure still guarantees optimal performance easy instances achieves much smaller regret particular adaptive method achieves constant regret probabilistic setting exists action average obtains strictly smaller loss actions also provide simulation study comparing approach existing methods\",\"chemical space large brute force searches new interesting molecules infeasible highthroughput virtual screening via computer cluster simulations speed discovery process collecting large amounts data parallel hundreds thousands parallel measurements bayesian optimization produce additional acceleration sequentially identifying useful simulations experiments performed next however current methods cannot scale large numbers parallel measurements massive libraries molecules currently used highthroughput screening propose scalable solution based parallel distributed implementation thompson sampling pdts show small scale problems pdts performs similarly parallel expected improvement batch version widely used heuristic additionally settings parallel scale pdts outperforms scalable baselines greedy search epsilongreedy approaches random search method results show pdts successful solution largescale parallel\",\"paper investigate capability universal kriging model singleobjective global optimization applied within efficient global optimization ego framework implemented combined ukego framework studied four variants methods firstorder polynomial secondorder polynomial blind kriging implementation oodace toolbox polynomialchaos kriging pck implementation ukego framework automatic trend function selection derived pck models works building surrogate model performing optimizations via expected improvement criteria kriging model lowest leaveoneout crossvalidation error next studied compared ukego variants standard ego using five synthetic test functions one aerodynamic problem results show proper choice trend function automatic feature selection improve optimization performance ukego relative ego results found pckego best variant robust performance compared rest ukego schemes however totalorder expansion used generate candidate trend function set highdimensional problems note test functions predetermined polynomial trend functions performed better pck indicating use automatic trend function selection always lead best quality solutions also found although variants globally accurate ordinary kriging still identify betteroptimized solutions due addition trend function helps optimizer locate global optimum\",\"thompson sampling demonstrated many complex bandit models however theoretical guarantees available parametric multiarmed bandit still limited bernoulli case extend proving asymptotic optimality algorithm using jeffreys prior dimensional exponential family bandits proof builds previous work also makes extensive use closed forms kullbackleibler divergence fisher information thus jeffreys prior available exponential family allow give finite time exponential concentration inequality posterior distributions exponential families may interest right moreover analysis covers distributions optimistic algorithm yet proposed including heavytailed exponential families\",\"popularity bayesian optimization methods efficient exploration parameter spaces lead series papers applying gaussian processes surrogates optimization functions however proposed approaches allow exploration parameter space occur sequentially often desirable simultaneously propose batches parameter values explore particularly case large parallel processing facilities available facilities could computational physical facets process optimized biological experiments many experimental set ups allow several samples simultaneously processed batch methods however require modeling interaction evaluations batch expensive complex scenarios investigate simple heuristic based estimate lipschitz constant captures important aspect interaction local repulsion negligible computational overhead resulting algorithm compares well running time much elaborate alternatives approach assumes function interest lipschitz continuous function wraploop around acquisition function used collect batches points certain size minimizing nonparallelizable computational effort speedup method respect previous approaches significant set computationally expensive experiments\",\"introduce means automating machine learning big data tasks performing scalable stochastic bayesian optimisation algorithm parameters hyperparameters often critical tuning algorithm parameters relied domain expertise experts along laborious handtuning brute search lengthy sampling runs background bayesian optimisation finding increasing use automating parameter tuning making algorithms accessible even nonexperts however state art bayesian optimisation incapable scaling large number evaluations algorithm performance required fit realistic models complex big data describe stochastic sparse bayesian optimisation strategy solve problem using many thousands noisy evaluations algorithm performance subsets data order effectively train algorithms big data provide comprehensive benchmarking possible sparsification strategies bayesian optimisation concluding nystrom approximation offers best scaling performance real tasks proposed algorithm demonstrates substantial improvement state art tuning parameters gaussian process time series prediction task real big data\",\"work presents pesmoc predictive entropy search multiobjective bayesian optimization constraints informationbased strategy simultaneous optimization multiple expensivetoevaluate blackbox functions presence several constraints pesmoc hence used solve wide range optimization problems iteratively pesmoc chooses input location evaluate objective functions constraints maximally reduce entropy pareto set corresponding optimization problem constraints considered pesmoc assumed similar properties objective functions typical bayesian optimization problems known expression prevents gradient computation evaluation considered expensive resulting observations may corrupted noise constraints arise plethora expensive blackbox optimization problems carry synthetic experiments illustrate effectiveness pesmoc sample objectives constraints gaussian process prior results obtained show pesmoc able provide better recommendations smaller number evaluations strategy based random search\",\"choosing bestperforming optimizers portfolio optimization algorithms usually difficult complex task gets even worse underlying functions unknown socalled blackbox problems function evaluations considered expensive case continuous singleobjective optimization problems exploratory landscape analysis ela sophisticated effective approach characterizing landscapes problems means numerical values actually performing optimization task advantageous unfortunately quite complicated compute multiple ela features simultaneously corresponding code spread across multiple platforms least across several packages within platforms article presents broad summary existing ela approaches introduces flacco rpackage featurebased landscape analysis continuous constrained optimization problems although functions neither solve optimization problem related algorithm selection problem asp offers easy access essential ingredient asp providing wide collection ela features single platform even within single package addition flacco provides multiple visualization techniques enhance understanding numerical features thereby make certain landscape properties comprehensible top introduce packages buildin well webhosted hence platformindependent graphical user interface gui facilitates usage package especially people familiar making convenient toolbox working towards algorithm selection continuous singleobjective optimization problems\",\"practical bayesian optimization must often search structures differing numbers parameters instance may wish search neural network architectures unknown number layers relate performance data gathered different architectures define new kernel conditional parameter spaces explicitly includes information parameters relevant given structure show kernel improves model quality bayesian optimization results several simpler baseline kernels\",\"bayesian optimization powerful tool finetuning hyperparameters wide variety machine learning models success machine learning led practitioners diverse realworld settings learn classifiers practical problems machine learning becomes commonplace bayesian optimization becomes attractive method practitioners automate process classifier hyperparameter tuning key observation data used tuning models settings often sensitive certain data genetic predisposition personal email statistics car accident history properly private may risk inferred bayesian optimization outputs address introduce methods releasing best hyperparameters classifier accuracy privately leveraging strong theoretical guarantees differential privacy known bayesian optimization convergence bounds prove assumption private quantities also nearoptimal finally even assumption satisfied use different smoothness guarantees protect privacy\",\"present pesmo bayesian method identifying pareto set multiobjective optimization problems functions expensive evaluate central idea pesmo choose evaluation points maximally reduce entropy posterior distribution pareto set critically pesmo multiobjective acquisition function decomposed sum objectivespecific acquisition functions enables algorithm used emphdecoupled scenarios objectives evaluated separately perhaps different costs decoupling capability also makes possible identify difficult objectives require evaluations pesmo also offers gains efficiency cost scales linearly number objectives comparison exponential cost methods compare pesmo related methods multiobjective bayesian optimization synthetic realworld problems results show pesmo produces better recommendations smaller number evaluations objectives decoupled evaluation lead improvements performance particularly number objectives large\",\"consider problem estimating expected value information knowledge gradient bayesian learning problems belief model nonlinear parameters goal maximize metric simultaneously learning unknown parameters nonlinear belief model guiding sequential experimentation process expensive overcome problem computing expected value experiment computationally intractable using sampled approximation helps guide experiments provide accurate estimate unknown parameters introduce resampling process allows sampled model adapt new information exploiting past experiments show theoretically method converges asymptotically true parameters simultaneously maximizing metric show empirically process exhibits rapid convergence yielding good results small number experiments\",\"bayesian optimization emerged last years effective approach optimizing blackbox functions direct queries objective expensive paper consider case direct access function possible information user preferences scenarios arise problems human preferences modeled tests recommender systems present new framework scenario call preferential bayesian optimization pbo allows find optimum latent function queried pairwise comparisons socalled duels pbo extends applicability standard ideas generalizes previous discrete dueling approaches modeling probability winner duel means gaussian process model bernoulli likelihood latent preference function used define family acquisition functions extend usual policies used illustrate benefits pbo variety experiments showing pbo needs drastically fewer comparisons finding optimum according experiments way modeling correlations pbo key obtaining advantage\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"14_optimization_bayesian_function\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"14_optimization_bayesian_function\"],\"textfont\":{\"size\":12},\"x\":[10.006388,10.681063,10.877111,11.063939,10.93506,10.037239,9.438679,10.105902,10.093537,10.170467,10.008764,9.96822,10.001431,10.000277,11.211435,10.011501,10.219215,10.003346,10.71235,9.4134865,10.973164,10.006479,10.197269,10.950035,10.828408,10.022483,10.858345,10.044459,10.013621,11.044869,10.037762,10.84271,11.016527,10.491087,11.17925,10.042631,11.044513,11.061306,10.062641,11.022009,11.308573,10.74473,10.862854,10.936944,11.177954,10.946247,11.001892,10.397117,10.011851,11.0697365,11.065205,9.968495,10.274191,11.056287,10.264672,11.125746,10.879018,10.008466,11.059631,9.458279,10.901523,10.881754,10.896466,10.82783,10.830084,10.453956,10.7886305,10.535778],\"y\":[11.156496,10.756359,10.681321,10.440137,10.500581,11.118548,8.605994,10.8740835,11.0023575,10.663031,11.138339,10.971437,10.468646,11.163858,10.197877,11.149412,10.854651,11.1620455,10.757992,8.580545,10.529999,11.149713,11.076724,10.600626,10.735165,11.126049,10.698696,11.108992,11.147046,10.511487,11.12511,10.729687,10.562306,10.305654,10.242637,11.117516,10.50163,10.356726,11.083845,10.501647,10.384075,10.74973,10.706742,10.632457,10.405462,10.535786,10.564193,10.5872345,10.284764,10.454716,10.429887,10.335966,10.792802,10.499509,10.83214,10.342442,10.523156,11.153276,10.450962,8.598078,10.674868,10.679528,10.523266,10.386471,10.738449,10.624792,10.794976,10.614042],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"introduce new class nonstationary kernels derive covariance functions novel family stochastic processes refer string gaussian processes string gps construct string gps allow multiple types local patterns data ensuring mild global regularity condition paper illustrate efficacy approach using synthetic data demonstrate model outperforms competing approaches well studied reallife datasets exhibit nonstationary features\",\"present scalable gaussian process model identifying characterizing smooth multidimensional changepoints automatically learning changes expressive covariance structure use random kitchen sink features flexibly define change surface combination expressive spectral mixture kernels capture complex statistical structure finally use novel methods additive nonseparable kernels scale model large datasets demonstrate model numerical real world data including large spatiotemporal disease dataset identify previously unknown heterogeneous changes space time\",\"renewable distributed energy resources ders penetrate power grid accelerating speed essential operators accurate solar photovoltaic energy forecasting efficient operations planning generally observed weather data applied solar generation forecasting model practice energy forecasting based forecasted weather data paper study uncertainty weather forecasting commonly used weather variables presented forecasted weather data six days ahead compared observed data results analysis quantified statistical metrics addition influential weather predictors energy forecasting model selected performance historical observed weather data errors assessed using solar generation forecasting model finally sensitivity test performed identify influential weather variables whose accurate values significantly improve results energy forecasting\",\"separating short jobs long known technique improve scheduling performance paper describe method developed accurately predicting runtimes classes jobs enable separation method uses fact runtimes represented mixture overlapping gaussian distributions order train cart classifier provide prediction threshold separates short jobs long jobs determined evaluation classifier maximize prediction accuracy results indicate overall accuracy data set used study sensitivity specificity\",\"paper deals inference prediction multiple correlated time series one also choice using candidate pool contemporaneous predictors target series starting structural model timeseries bayesian tools used model fitting prediction feature selection thus extending recent work along lines univariate case bayesian paradigm multivariate setting helps model avoid overfitting well capture correlations among multiple time series various state components model provides needed flexibility choose different set components available predictors target series cyclical component model handle large variations short term may caused external shocks run extensive simulations investigate properties estimation accuracy performance forecasting run empirical study onestepahead prediction max log return portfolio stocks involve four leading financial institutions simulation studies extensive empirical study confirm multivariate model outperforms three benchmark models viz model treats target series independent autoregressive integrated moving average model regression arimax multivariate arimax marimax model\",\"nowadays unprecedented penetration renewable distributed energy resources ders necessity efficient energy forecasting model demanding generally forecasting models trained using observed weather data trained models applied energy forecasting using forecasted weather data study performance several commonly used forecasting methods presence weather predictors uncertainty assessed compared accordingly observed forecasted weather data collected influential predictors solar generation forecasting model selected using several measures using observed forecasted weather data analysis uncertainty weather variables represented mae bootstrapping energy forecasting model trained using observed weather data finally performance several commonly used forecasting methods solar energy forecasting simulated compared real case study\",\"paper develop new sequential regression modeling approach data streams data streams commonly found around retail enterprise sales data continuously collected every day demand forecasting model important outcome data needs continuously updated new incoming data main challenge modeling arises high dimensional sparsity need adaptive use prior knowledge andor structural changes system proposed approach addresses challenges incorporating adaptive lpenalty inertia terms loss function thus called inertial regularization selection irs former term performs model selection handle first challenge latter shown address last two challenges recursive estimation algorithm developed shown outperform commonly used statespace models kalman filters experimental studies real data\",\"paper introduce novel online time series forecasting model refer pmgp filter show model equivalent gaussian process regression advantage online forecasting online learning hyperparameters constant rather cubic time complexity constant rather squared memory requirement number observations without resorting approximations moreover proposed model expressive family covariance functions implied latent process namely spectral matern kernels recently proven capable approximating arbitrarily well translationinvariant covariance function benefit approach compared competing models demonstrated using experiments several reallife datasets\",\"deep learning methods attempt learn generic features unsupervised fashion large unlabelled data set generic features perform well best hand crafted features learning problem makes use data provide definition generic features characterize possible learn provide methods closely related autoencoder deep belief network deep learning order use notion deficiency illustrate value studying certain general learning problems\",\"vector autoregressive var model powerful tool modeling complex time series exploited many fields however fitting high dimensional var model poses unique challenges one hand dimensionality caused modeling large number time series higher order autoregressive processes usually much higher time series length hand temporal dependence structure var model gives rise extra theoretical challenges high dimensions one popular approach assume transition matrix sparse fit var model using least squares method lassotype penalty manuscript propose alternative way estimating var model main idea via exploiting temporal dependence structure formulate estimating problem linear program instant advantage proposed approach lassotype estimators estimation equation decomposed multiple subequations accordingly efficiently solved parallel fashion addition method brings new theoretical insights var model analysis far theoretical results developed high dimensions song bickel kock callot mainly pose assumptions design matrix formulated regression problems conditions indirect transition matrices transparent contrast results show operator norm transition matrices plays important role estimation accuracy provide explicit rates convergence estimation prediction addition provide thorough experiments synthetic realworld equity data show empirical advantages method lassotype estimators parameter estimation forecasting\",\"gaussian process model vectorvalued function shown useful multioutput prediction existing method model reformulate matrixvariate gaussian distribution multivariate normal distribution although effective many cases reformulation always workable difficult apply distributions matrixvariate distributions transformed respective multivariate distributions case matrixvariate studentt distribution paper propose unified framework used introduce novel multivariate studentt process regression model mvtpr multioutput prediction also reformulate multivariate gaussian process regression mvgpr overcomes limitations existing methods mvgpr mvtpr closedform expressions marginal likelihoods predictive distributions unified framework thus adopt optimization approaches used conventional gpr usefulness proposed methods illustrated several simulated real data examples particular verify empirically mvtpr superiority datasets considered including air quality prediction bike rent prediction last proposed methods shown produce profitable investment strategies stock markets\",\"present novel algorithm westfallyoung light detecting patterns itemsets subgraphs statistically significantly enriched one two classes method corrects rigorously multiple hypothesis testing correlations patterns westfallyoung permutation procedure empirically estimates null distribution pattern frequencies class via permutations experiments westfallyoung light dramatically outperforms current stateoftheart approach terms runtime memory efficiency popular realworld benchmark datasets pattern mining key efficiency unlike existing methods algorithm neither needs solve underlying frequent itemset mining problem anew permutation needs store occurrence list frequent patterns westfallyoung light opens door significant pattern mining large datasets previously led prohibitive runtime memory costs\",\"address problem detecting changes multivariate datastreams investigate intrinsic difficulty changedetection methods face data dimension scales particular consider general approach changes detected comparing distribution loglikelihood datastream different time windows despite fact approach constitutes frame several changedetection methods effectiveness data dimension scales never investigated indeed goal paper show magnitude change naturally measured symmetric kullbackleibler divergence pre postchange distributions detectability change given magnitude worsens data dimension increases problem refer emphdetectability loss due linear relationship variance loglikelihood data dimension analytically derive detectability loss gaussiandistributed datastreams empirically demonstrate problem holds also realworld datasets harmful even low datadimensions say\",\"discovering statistically significant patterns databases important challenging problem main obstacle problem difficulty taking account selection bias bias arising fact patterns selected extremely large number candidates databases paper introduce new approach predictive pattern mining problems address selection bias issue approach built recently popularized statistical inference framework called selective inference selective inference statistical inferences statistical hypothesis testing conducted based sampling distributions conditional selection event selection event characterized tractable way statistical inferences made without minding selection bias issue however pattern mining problems difficult characterize entire selection process mining algorithms main contribution paper solve challenging problem class predictive pattern mining problems introducing novel algorithmic framework demonstrate approach useful finding statistically significant patterns databases\",\"interpretation deep learning models challenge due size complexity often opaque internal state addition many systems image classifiers operate lowlevel features rather highlevel concepts address challenges introduce concept activation vectors cavs provide interpretation neural nets internal state terms humanfriendly concepts key idea view highdimensional internal state neural net aid obstacle show use cavs part technique testing cavs tcav uses directional derivatives quantify degree userdefined concept important classification resultfor example sensitive prediction zebra presence stripes using domain image classification testing ground describe cavs may used explore hypotheses generate insights standard image classification network well medical application\",\"gaussian graphical model graphical representation dependence structure gaussian random vector recognized powerful tool different applied fields bioinformatics errorcontrol codes speech language information retrieval others gaussian graphical model selection statistical problem identify gaussian graphical model sample given size different approaches gaussian graphical model selection suggested literature one based considering family individual conditional independence tests application approach leads construction variety multiple testing statistical procedures gaussian graphical model selection important characteristic procedures error rate given sample size existing literature great attention paid control error rates incorrect edge inclusion type error however graphical model selection also important take account error rates incorrect edge exclusion type error deal issue consider graphical model selection problem framework multiple decision theory quality statistical procedures measured risk function additive losses additive losses allow types errors taken account construct tests neyman structure individual hypotheses combine obtain multiple decision statistical procedure show obtained procedure optimal sense minimizes linear combination expected numbers type type errors class unbiased multiple decision procedures\",\"paper study predictive pattern mining problems goal construct predictive model based subset predictive patterns database main contribution introduce novel method called safe pattern pruning spp class predictive pattern mining problems spp method allows efficiently find superset predictive patterns database needed optimal predictive model advantage spp method existing boostingtype method former find superset single search database latter requires multiple searches spp method inspired recent development safe feature screening order extend idea safe feature screening predictive pattern mining derive novel pruning rule called safe pattern pruning spp rule used searching tree defined among patterns database spp rule property node corresponding pattern database pruned spp rule guaranteed patterns corresponding descendant nodes never needed optimal predictive model apply spp method graph mining itemset mining problems demonstrate computational advantage\",\"identifying changes generative process sequential data known changepoint detection become increasingly important topic wide variety fields recently developed approach call exact online bayesian changepoint detection exo shown reasonable results efficient computation real time updates method based textitforward recursive messagepassing algorithm however detected changepoints methods unstable propose new algorithm called lagged exact online bayesian changepoint detection lexo improves accuracy stability detection incorporating elltime lags inference new algorithm adds recursive textitbackward step forward exo computational complexity linear number added lags estimation parameters associated regimes also developed simulation studies three common changepoint models show detected changepoints lexo much stable parameter estimates lexo considerably lower mse exo illustrate applicability methods two real world data examples comparing exo lexo\",\"study problem detecting change points cps characterized subset dimensions multidimensional sequence method detecting cps formulated twostage method one selecting relevant dimensions another selecting cps difficult properly control false detection probability detection methods selection bias stage must properly corrected main contribution paper formulate detection problem selective inference problem show exact nonasymptotic inference possible class detection methods demonstrate performances proposed selective inference framework numerical simulations application motivating medical data analysis problem\",\"time series analysis used understand predict dynamic processes including evolving demands business weather markets biological rhythms exponential smoothing used domains obtain simple interpretable models time series forecast future values despite popularity exponential smoothing fails dramatically presence outliers large amounts noise underlying time series changes propose flexible model time series analysis using exponential smoothing cells overlapping time windows approach detect remove outliers denoise data fill missing observations provide meaningful forecasts challenging situations contrast classic exponential smoothing solves nonconvex optimization problem smoothing parameters initial state proposed approach requires solving single structured convex optimization problem recent developments efficient convex optimization largescale dynamic models make approach tractable illustrate new capabilities using synthetic examples use approach analyze forecast noisy realworld time series code approach experiments publicly available\",\"statistical downscaling global climate models gcms allows researchers study local climate change effects decades future wide range statistical models applied downscaling gcms recent advances machine learning explored paper compare four fundamental statistical methods bias correction spatial disaggregation bcsd ordinary least squares elasticnet support vector machine three advanced machine learning methods multitask sparse structure learning mssl bcsd coupled mssl convolutional neural networks downscale daily precipitation northeast united states metrics evaluate methods ability capture daily anomalies large scale climate shifts extremes analyzed find linear methods led bcsd consistently outperform nonlinear approaches direct application stateoftheart machine learning methods statistical downscaling provide improvements simpler longstanding approaches\",\"soil moisture active passive smap mission delivered valuable sensing surface soil moisture since however short time span irregular revisit schedule utilizing stateoftheart timeseries deep learning neural network long shortterm memory lstm created system predicts smap level soil moisture data atmospheric forcing modelsimulated moisture static physiographic attributes inputs system removes bias model simulations improves predicted moisture climatology achieving small test rootmeansquared error high correlation coefficient continental united states including forested southeast first application lstm hydrology show proposed network avoids overfitting robust temporal spatial extrapolation tests lstm generalizes well across regions distinct climates physiography high fidelity smap lstm shows great potential hindcasting data assimilation weather forecasting\",\"provide selfcontained proof theorem relating probabilistic coherence forecasts nondomination rival forecasts respect proper scoring rule theorem appears new closely related results achieved investigators\",\"time series forecasting widely used multitude domains paper present four models predict stock price using spx index input time series data martingale ordinary linear models require strongest assumption stationarity use baseline models generalized linear model requires lesser assumptions unable outperform martingale empirical testing rnn model performs best comparing two models update input lstm instantaneously also beat martingale addition introduce online batch algorithm discrepancy measure inform readers newest research time series predicting method doesnt require stationarity non mixing assumptions time series data finally apply forecasting practice introduce basic trading strategies create win win zero sum situations\",\"power supply renewable resources global rise forecasted renewable generation surpass types generation foreseeable future increased generation renewable resources mainly solar wind exposes power grid vulnerabilities conceivably due variable generation thus highlighting importance accurate forecasting methods paper proposes twostage dayahead solar forecasting method breaks forecasting linear nonlinear parts determines subsequent forecasts accordingly improves accuracy obtained results reduce error resulted nonstationarity historical solar radiation data data processing approach including preprocess postprocess levels integrated proposed method numerical simulations three test days different weather conditions exhibit effectiveness proposed twostage model\",\"modern aircraft may require order thousands custom shims fill gaps structural components airframe arise due manufacturing tolerances adding across large structures shims necessary eliminate gaps maintain structural performance minimize pulldown forces required bring aircraft engineering nominal configuration peak aerodynamic efficiency gap filling timeconsuming process involving either expensive byhand inspection computations vast quantities measurement data increasingly sophisticated metrology equipment either case amounts significant delays production much time spent critical path aircraft assembly work presents alternative strategy predictive shimming based machine learning sparse sensing first learn gap distributions historical data design optimized sparse sensing strategies streamline data collection processing new approach based assumption patterns exist shim distributions across aircraft may mined used reduce burden data collection processing future aircraft specifically robust principal component analysis used extract lowdimensional patterns gap measurements rejecting outliers next optimized sparse sensors obtained informative dimensions new aircraft lowdimensional principal components demonstrate success proposed approach called pixel identification despite uncertainty sensor technology pixidust historical production data representative boeing commercial aircraft algorithm successfully predicts shim gaps within desired measurement tolerance using laser scan points typically required results crossvalidated\",\"gaussian graphical models ggms probabilistic tools choice analyzing conditional dependencies variables complex systems finding changepoints structural evolution ggm therefore essential detecting anomalies underlying system modeled ggm order detect structural anomalies ggm consider problem estimating changes precision matrix corresponding gaussian distribution take twostep approach solving problem estimating background precision matrix using system observations past without anomalies estimating foreground precision matrix using sliding temporal window anomaly monitoring primary contribution estimating foreground precision using novel contrastive inverse covariance estimation procedure order accurately learn structural changes ggm maximize penalized loglikelihood penalty norm difference foreground precision estimated already learned background precision modify alternating direction method multipliers admm algorithm sparse inverse covariance estimation perform contrastive estimation foreground precision matrix results simulated ggm data show significant improvement precision recall detecting structural changes ggm compared noncontrastive sliding window baseline\",\"article introduces new algorithm reconstructing epsilonmachines data well decisional states defined internal states system lead decision based userprovided utility payoff function utility function encodes priori knowledge external system quantifies bad make mistakes intrinsic underlying structure system modeled epsilonmachine causal states decisional states form partition lowerlevel causal states defined according higherlevel users knowledge complex systems perspective decisional states thus emerging patterns corresponding utility function transitions decisional states correspond events lead change decision new remapf algorithm estimates epsilonmachine decisional states data application examples given hidden model reconstruction cellular automata filtering edge detection images\",\"properties data frequently seen vary depending sampled situations usually changes along time evolution owing environmental effects one way analyze data find invariances representative features kept constant changes aim paper identify one feature namely interactions dependencies among variables common across multiple datasets collected different conditions end propose common substructure learning cssl framework based graphical gaussian model present simple learning algorithm based dual augmented lagrangian alternating direction method multipliers confirm performance cssl existing techniques finding unchanging dependency structures multiple datasets numerical simulations synthetic data real world application anomaly detection automobile sensors\",\"unsupervised discovery latent representations addition useful density modeling visualisation exploratory data analysis also increasingly important learning features relevant discriminative tasks autoencoders particular proven effective way learn latent codes reflect meaningful variations data continuing challenge however guiding autoencoder toward representations useful particular tasks complementary challenge find codes invariant irrelevant transformations data common way introducing problemspecific guidance autoencoders incorporation parametric component ties latent representation label information work argue preferable approach relies instead nonparametric guidance mechanism conceptually ensures exists function predict label information without explicitly instantiating function superiority guidance mechanism confirmed two datasets particular approach able incorporate invariance information lighting elevation etc small norb object recognition dataset yields stateoftheart performance single layer nonconvolutional network\",\"latent feature modeling allows capturing latent structure responsible generating observed properties set objects often used make predictions either new values interest missing information original data well perform data exploratory analysis however although extensive literature latent feature models homogeneous datasets attributes describe object continuous discrete nature lack work latent feature modeling heterogeneous databases paper introduce general bayesian nonparametric latent feature model suitable heterogeneous datasets attributes describing object either discrete continuous mixed variables proposed model presents several important properties first accounts heterogeneous data keeping properties conjugate models allow infer model linear time respect number objects attributes second bayesian nonparametric nature allows automatically infer model complexity data number features necessary capture latent structure data third latent features model binaryvalued variables easing interpretability obtained latent features data exploratory analysis show flexibility proposed model solving prediction data analysis tasks several realworld datasets moreover software package glfm publicly available researcher use improve\",\"work develop fast saliency detection method applied differentiable image classifier train masking model manipulate scores classifier masking salient parts input image model generalises well unseen images requires single forward pass perform saliency detection therefore suitable use realtime systems test approach cifar imagenet datasets show produced saliency maps easily interpretable sharp free artifacts suggest new metric saliency test method imagenet object localisation task achieve results outperforming weakly supervised methods\",\"study problem estimating parameters regression model set observations consisting response predictor response assumed related predictor via regression model unknown parameters often models parameters estimated assumed constant consider general scenario parameters allowed evolve time natural assumption many applications model dynamics via linear update equation additive noise often used wide range engineering applications particularly wellknown widely used kalman filter system state seeks estimate maps parameter values derive approximate algorithm estimate mean variance parameter estimates online fashion generic regression model algorithm turns equivalent extended kalman filter specialize algorithm multivariate exponential family distribution obtain generalization generalized linear model glm common regression models encountered practice logistic exponential multinomial observations modeled exponential family distribution results used easily obtain algorithms online mean variance parameter estimation regression models context timedependent parameters lastly propose use algorithms contextual multiarmed bandit scenario far model parameters assumed static observations univariate gaussian bernoulli restrictions relaxed using algorithms described combine thompson sampling show resulting performance simulation\",\"challenging task modeling multivariate time series propose new class models use dependent matern processes capture underlying structure data explain interdependencies predict unknown values although similar models proposed econometric statistics machine learning literature approach several advantages distinguish existing methods flexible provide high prediction accuracy yet complexity controlled avoid overfitting interpretability separates blackbox methods finally computational efficiency makes scalable highdimensional time series paper use several simulated real data sets illustrate advantages also briefly discuss extensions model\",\"present vectorspace markov random fields vsmrfs novel class undirected graphical models variable belong arbitrary vector space vsmrfs generalize recent line work scalarvalued uniparameter exponential family mixed graphical models thereby greatly broadening class exponential families available allowing multinomial dirichlet distributions specifically vsmrfs joint graphical model distributions nodeconditional distributions belong generic exponential families general vector space domains also present sparsistent mestimator learning class mrfs recovers correct set edges high probability validate approach via set synthetic data experiments well realworld case study four million foods popular diet tracking app myfitnesspal results demonstrate algorithm performs well empirically vsmrfs capable capturing highlighting interesting structure complex realworld data code algorithm open source publicly available\",\"waggle dance honeybees perform astonishing way communicating location food source years discovery researchers still use manual labeling watching hours dance videos detect different transitions dance components thus extracting information regarding distance direction food source propose automated process monitor segment different components honeybee waggle dance process highly accurate runs realtime use shared information multiple dances\",\"use covariance kernels ubiquitous field spatial statistics kernels allow data mapped highdimensional feature spaces thus extend simple linear additive methods nonlinear methods higher order interactions however recently strong reliance limited class stationary kernels matern squared exponential limiting expressiveness modelling approaches recent machine learning research focused spectral representations model arbitrary stationary kernels introduced general representations include classes nonstationary kernels paper exploit connections fourier feature representations gaussian processes neural networks generalise previous approaches develop simple efficient framework learn arbitrarily complex nonstationary kernel functions directly data taking care avoid overfitting using stateoftheart methods deep learning highlight broad array kernel classes could created within framework apply time series dataset remote sensing problem involving land surface temperature eastern africa show without increasing computational storage complexity nonstationary kernels used improve generalisation performance provide interpretable results\",\"abtesting popular technique web companies since makes possible accurately predict impact modification simplicity random split across users one critical aspects abtest duration important reliably compute confidence intervals associated metric interest know stop test paper define clean mathematical framework model abtest process propose three algorithms based bootstrapping central limit theorem compute reliable confidence intervals extend metrics common probabilities success apply absolute relative increments used comparison metrics including number occurrences particular event clickthrough rate implying ratio\",\"widespread need techniques discover structure time series data recently introduced techniques automatic bayesian covariance discovery abcd provide way find structure within single time series searching space covariance kernels generated using simple grammar abcd identify broad class temporal patterns difficult extend brittle practice paper shows extend abcd formulating terms probabilistic program synthesis key technical ideas represent models using abstract syntax trees domainspecific probabilistic language represent time series model prior likelihood search strategy using probabilistic programs sufficiently expressive language final probabilistic program written lines probabilistic code venture paper demonstrates application time series clustering involves nonparametric extension abcd experiments interpolation extrapolation realworld econometric data improvements accuracy nonparametric standard regression baselines\",\"propose work new family kernels variablelength time series work builds upon vector autoregressive var model multivariate stochastic processes given multivariate time series consider likelihood function pthetax different parameters theta var model features describe compare two time series form product features pthetax pthetax integrated wrt theta using matrix normalinverse wishart prior among properties kernel easily computed dimension time series much larger lengths considered time series also generalized time series taking values arbitrary state spaces long state space endowed kernel kappa case kernel function gram matrices produced kappa observations subsequences observations enumerated describe computationally efficient implementation generalization uses lowrank matrix factorization techniques kernels compared known kernels using set benchmark classification tasks carried support vector machines\",\"uncertainty analysis form probabilistic forecasting significantly improve decision making processes smart power grid better integrating renewable energy sources wind whereas point forecasting provides single expected value probabilistic forecasts provide information form quantiles prediction intervals full predictive densities paper analyzes effectiveness novel approach nonparametric probabilistic forecasting wind power combines smooth approximation pinball loss function neural network architecture weighting initialization scheme prevent quantile cross problem numerical case study conducted using publicly available wind data global energy forecasting competition multiple quantiles estimated form prediction intervals evaluated using quantile score reliability measures benchmark models persistence climatology distributions multiple quantile regression support vector quantile regression used comparison results demonstrate proposed approach leads improved performance preventing problem overlapping quantile estimates\",\"propose paper differentiable learning loss time series building upon celebrated dynamic time warping dtw discrepancy unlike euclidean distance dtw compare time series variable size robust shifts dilatations across time dimension compute dtw one typically solves minimalcost alignment problem two time series using dynamic programming work takes advantage smoothed formulation dtw called softdtw computes softminimum alignment costs show paper softdtw differentiable loss function value gradient computed quadratic timespace complexity dtw quadratic time linear space complexity show regularization particularly well suited average cluster time series dtw geometry task proposal significantly outperforms existing baselines next propose tune parameters machine outputs time series minimizing fit groundtruth labels softdtw sense\",\"point forecasting univariate time series challenging problem extensive work conducted however nonparametric probabilistic forecasting time series form quantiles prediction intervals even challenging problem effort expand possible forecasting paradigms devise explore extrapolationbased approach applied probabilistic forecasting present novel quantile fourier neural network nonparametric probabilistic forecasting univariate time series multistep predictions provided form composite quantiles using time input model effectively form extrapolation based nonlinear quantile regression applied forecasting experiments conducted eight real world datasets demonstrate variety periodic aperiodic patterns nine naive advanced methods used benchmarks including quantile regression neural network support vector quantile regression sarima exponential smoothing obtained empirical results validate effectiveness proposed method providing high quality accurate probabilistic predictions\",\"paper discusses package implements pattern sequence based forecasting psf algorithm developed univariate time series forecasting algorithm successfully applied many different fields psf algorithm consists two major parts clustering prediction clustering part includes selection optimum number clusters labels time series data reference clusters prediction part includes functions like optimum window size selection specific patterns prediction future values reference past pattern sequences psf package consists various functions implement psf algorithm also contains function automates functions obtain optimized prediction results aim package promote psf algorithm ease implementation minimum efforts paper describes functions psf package syntax also provides simple example usage finally usefulness package discussed comparing autoarima ets wellknown time series forecasting functions available cran repository\",\"exploration hydrocarbon resources highly complicated expensive process various geological geochemical geophysical factors developed combined together highly significant design seismic data acquisition survey locate exploratory wells since incorrect imprecise locations lead waste time money operation objective study locate highpotential oil gas field sheet ahwaz including oil fields reduce time costs exploration production processes regard maps developed using gis functions factors including minimum maximum total organic carbon toc yield potential hydrocarbons production tmax peak production index oxygen index hydrogen index well presence proximity high residual bouguer gravity anomalies proximity anticline axis faults topography curvature maps obtained asmari formation subsurface contours model integrate maps study employed artificial neural network adaptive neurofuzzy inference system anfis methods results obtained model validation demonstrated neural network rms kappa trained better models anfis predicts potential areas accurately however method failed predict oil fields wrongly predict areas potential zones\",\"using proper model characterize time series crucial making accurate predictions work use timevarying autoregressive process tvar describe nonstationary time series model mixture multiple stable autoregressive processes introduce new model selection technique based gap statistics learn appropriate number filters needed model time series define new distance measure stable filters draw reference curve used measure much adding new filter improves performance model choose number filters maximum gap reference curve end propose new method order generate uniform random stable filters root domain numerical results provided demonstrating performance proposed approach\",\"changepoints abrupt variations generative parameters data sequence online detection changepoints useful modelling prediction time series application areas finance biometrics robotics frequentist methods yielded online filtering prediction techniques bayesian papers focused retrospective segmentation problem examine case model parameters changepoint independent derive online algorithm exact inference recent changepoint compute probability distribution length current run time since last changepoint using simple messagepassing algorithm implementation highly modular algorithm may applied variety types data illustrate modularity demonstrating algorithm three different realworld data sets\",\"present method conditional time series forecasting based adaptation recent deep convolutional wavenet architecture proposed network contains stacks dilated convolutions allow access broad range history forecasting relu activation function conditioning performed applying multiple convolutional filters parallel separate time series allows fast processing data exploitation correlation structure multivariate time series test analyze performance convolutional network unconditionally well conditionally financial time series forecasting using volatility index cboe interest rate several exchange rates extensively compare performance wellknown autoregressive model longshort term memory network show convolutional network wellsuited regressiontype problems able effectively learn dependencies series without need long historical time series timeefficient easy implement alternative recurrenttype networks tends outperform linear recurrent models\",\"paper presents technique reducedorder markov modeling compact representation timeseries data work symbolic dynamicsbased tools used infer approximate generative markov model timeseries data first symbolized partitioning continuous measurement space signal discrete sequential data modeled using symbolic dynamics proposed approach size temporal memory symbol sequence estimated spectral properties resulting stochastic matrix corresponding firstorder markov model symbol sequence hierarchical clustering used represent states corresponding fullstate markov model construct reducedorder size markov model nondeterministic algebraic structure subsequently parameters reducedorder markov model identified original model making use bayesian inference rule final model selected using informationtheoretic criteria proposed concept elucidated validated two different data sets examples first example analyzes set pressure data swirlstabilized combustor controlled protocols used induce flame instabilities variations complexity derived markov model represent system operating condition changes stable unstable combustion regime second example data set taken nasas data repository prognostics bearings rotating shafts show even small statespace reducedorder models able achieve comparable performance proposed approach provides flexibility selection final model representation learning\",\"deep convolutional neural networks cnns based approaches stateoftheart various computer vision tasks including face recognition considerable research effort currently directed towards improving deep cnns focusing powerful model architectures better learning techniques however studies systematically exploring strengths weaknesses existing deep models face recognition still relatively scarce literature paper try fill gap study effects different covariates verification performance four recent deep cnn models using labeled faces wild lfw dataset specifically investigate influence covariates related image quality blur jpeg compression occlusion noise image brightness contrast missing pixels model characteristics cnn architecture color information descriptor computation analyze impact face verification performance alexnet vggface googlenet squeezenet based comprehensive rigorous experimentation identify strengths weaknesses deep learning models present key areas potential future research results indicate high levels noise blur missing pixels brightness detrimental effect verification performance models whereas impact contrast changes compression artifacts limited found descriptor computation strategy color information significant influence performance\",\"present new approach estimating interdependence industries economy applying data science solutions exploiting interfirm buyerseller network data show problem estimating interdependence industries similar problem uncovering latent block structure network science literature estimate underlying structure greater accuracy propose extension sparse block model incorporates node textual information unbounded number industries interactions among latter task accomplished extending wellknown chinese restaurant process two dimensions inference based collapsed gibbs sampling model evaluated synthetic realworld datasets show proposed model improves predictive accuracy successfully provides satisfactory solution motivated problem also discuss issues affect future performance approach\",\"paper presents novel datadriven technique based spatiotemporal pattern network stpn energypower prediction complex dynamical systems built symbolic dynamic filtering stpn framework used capture individual system characteristics also pairwise causal dependencies among different subsystems quantifying causal dependency mutual information based metric presented energy prediction approach subsequently proposed based stpn framework validating proposed scheme two case studies presented one involving wind turbine power prediction supply side energy using western wind integration data set generated national renewable energy laboratory nrel identifying spatiotemporal characteristics residential electric energy disaggregation demand side energy using building america data set nrel exploring temporal features energy disaggregation context convex programming techniques beyond stpn framework developed applied achieve improved disaggregation performance\",\"scale data growing every day reducing dimensionality aka sketching highdimensional data emerged task paramount importance relevant issues address context include sheer volume data may consist categorical samples typically streaming format acquisition possibly missing entries cope challenges present paper develops novel categorical subspace learning approach unravel latent structure three prominent categorical bilinear models namely probit tobit logit deterministic probit tobit models treat data quantized values analogvalued process lying lowdimensional subspace probabilistic logit model relies low dimensionality data loglikelihood ratios leveraging low intrinsic dimensionality sought models rank regularized maximumlikelihood estimator devised solved recursively via alternating majorizationminimization sketch highdimensional categorical data fly resultant procedure alternates sketching new incomplete datum refining latent subspace leading lightweight firstorder algorithms highly parallelizable tasks per iteration extra degree freedom quantization thresholds also learned jointly along subspace enhance predictive power sought models performance subspace iterates analyzed infinite finite data streams former asymptotic convergence stationary point set batch estimator established latter sublinear regret bounds derived empirical cost simulated tests synthetic realworld datasets corroborate merits novel schemes realtime movie recommendation chessgame classification\",\"linear autoregressive models serve basic representations discrete time stochastic processes different attempts made provide nonlinear versions basic autoregressive process including different versions based kernel methods motivated powerful framework hilbert space embeddings distributions paper apply methodology kernel embedding autoregressive process order provide nonlinear version autoregressive process shows increased performance linear model highly complex time series use method proposed onestep ahead forecasting different timeseries compare performance nonlinear methods\",\"paper proposes hierarchical feature extractor nonstationary streaming time series based concept switching observable markov chain models slow timescale nonstationary behaviors considered mixture quasistationary fast timescale segments exhibited complex dynamical systems idea model unique stationary characteristic without priori knowledge number possible unique characteristics lower logical level capture transitions one lowlevel model another higher level context concepts recently developed symbolic dynamic filtering sdf extended build online algorithm suited handling quasistationary data lower level nonstationary behavior higher level without priori knowledge key observation made study rate change data likelihood seems better indicator change data characteristics compared traditional methods mostly consider data likelihood change detection algorithm minimizes model complexity captures data likelihood efficacy demonstration comparative evaluation proposed algorithm performed using time series data simulated systems exhibit nonlinear dynamics discuss results show proposed hierarchical sdf algorithm identify underlying features significantly high degree accuracy even noisy conditions algorithm demonstrated perform better baseline hierarchical dirichlet processhidden markov models hdphmm low computational complexity algorithm makes suitable onboard real time operations\",\"paper investigate link state space models gaussian processes time series modeling forecasting particular several widely used state space models transformed continuous time form corresponding gaussian process kernels derived experimen tal results demonstrate derived kernels correct appropriate gaussian process regression experiment real world dataset shows modeling identical statespace models proposed kernels considered connection allows researchers look models different angle facilitate sharing ideas two different modeling approaches\",\"present new method forecasting systems multiple interrelated time series method learns forecast models together discovering leading indicators within system serve good predictors improving forecast accuracy cluster structure predictive tasks around method based classical linear vector autoregressive model var links discovery leading indicators inferring sparse graphs granger causality formulate new constrained optimisation problem promote desired sparse structures across models sharing information amongst learning tasks multitask manner propose algorithm solving problem document battery synthetic realdata experiments advantages new method baseline var models well stateoftheart sparse var learning methods\",\"propose framework general probabilistic multistep time series regression specifically exploit expressiveness temporal nature sequencetosequence neural networks recurrent convolutional structures nonparametric nature quantile regression efficiency direct multihorizon forecasting new training scheme forkingsequences designed sequential nets boost stability performance show approach accommodates temporal static covariates learning across multiple related series shifting seasonality future planned event spikes coldstarts real life largescale forecasting performance framework demonstrated application predict future demand items sold amazoncom public probabilistic forecasting competition predict electricity price load\",\"analyzing multivariate time series data important predict future events changes complex systems finance manufacturing administrative decisions expressiveness power gaussian process regression methods significantly improved compositional covariance structures paper present new model naturally handles multiple time series placing indian buffet process ibp prior presence shared kernels selective covariance structure decomposition allows exploiting shared parameters set multiple selected time series also investigate welldefinedness models infinite latent components introduced present pragmatic search algorithm explores larger structure space efficiently experiments conducted five realworld data sets demonstrate new model outperforms existing methods term structure discoveries predictive performances\",\"yield curve forecasting important problem finance work explore use gaussian processes conjunction dynamic modeling strategy much like kalman filter model yield curve gaussian processes successfully applied model functional data variety applications gaussian process used model yield curve hyperparameters gaussian process model updated algorithm receives yield curve data yield curve data typically available time series frequency one day compare existing methods forecast yield curve proposed method results study showed competing method multivariate time series method performed well forecasting yields short term structure region yield curve gaussian processes perform well medium long term structure regions yield curve accuracy long term structure region yield curve important practical implications gaussian process framework yields uncertainty probability estimates directly contrast competing methods analysts frequently interested information study proposed method applied yield curve forecasting however applied model high frequency time series data data streams domains\",\"paper present method determine global horizontal irradiance ghi power measurements one systems located neighborhood method completely unsupervised based physical model plant precise assessment solar irradiance pivotal forecast electric power generated photovoltaic plants however onground measurements expensive generally performed small mediumsized plants satellitebased services represent valid alternative site measurements spacetime resolution limited results two case studies located switzerland presented performance proposed method assessing ghi compared free commercial satellite services results show presented method generally better satellitebased services especially high temporal resolutions\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"15_series_time_forecasting\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"15_series_time_forecasting\"],\"textfont\":{\"size\":12},\"x\":[10.481541,10.601124,9.475325,9.215999,9.816763,9.484709,9.902514,10.141545,9.343755,9.889257,10.171444,9.11747,9.703416,9.128551,9.361236,10.933195,9.082089,9.662867,9.578226,9.713915,10.473979,10.450879,10.276088,9.692195,9.462141,10.069454,10.826051,9.668983,10.470979,9.42949,11.379775,9.405336,9.799365,9.93772,9.658264,9.161787,10.451472,9.756736,9.880323,9.946862,9.526794,9.809951,9.652599,9.646698,9.818671,9.774875,9.663514,9.719315,9.933323,9.390718,12.676104,9.482349,9.843719,10.106224,9.664858,10.177441,9.82551,9.699765,9.933378,9.768945,9.459564,9.878314],\"y\":[9.458825,9.3205185,9.78326,8.322504,9.609987,9.74753,9.6120205,9.634725,6.6370497,9.623196,9.557451,8.739808,9.148377,8.733028,6.4823575,9.104159,8.797535,9.136256,9.063821,9.605699,9.408934,9.393571,9.61408,9.551719,9.788908,8.81993,9.156869,9.16074,9.312013,6.6071725,8.999479,6.441527,9.2983055,9.617415,7.4136157,8.727523,9.359294,8.976169,9.571589,9.595661,9.7597885,9.613575,9.657299,9.5745535,9.20453,9.410585,9.126838,9.6467495,9.126881,6.445034,6.657649,9.769211,8.817908,9.620437,9.171753,9.644078,9.645219,9.674787,9.625431,9.480632,9.788462,9.05564],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"question determine number independent latent factors topics mixture models latent dirichlet allocation lda great practical importance applications exact number topics unknown depends application size data set bayesian nonparametric methods avoid problem topic number selection impracticably slow large sample sizes subject local optima develop guaranteed procedure topic number recovery necessitate learning models latent parameters beforehand procedure relies adapting results random matrix theory performance topic number recovery procedure superior hlda nonparametric method also discuss implications results sample complexity accuracy popular spectral learning algorithms lda results procedure extended spectral learning algorithms exchangeable mixture models well hidden markov models\",\"topic models popular modeling discrete data texts images videos links provide efficient way discover hidden structuressemantics massive data one core problems field posterior inference individual data instances problem particularly important streaming environments often intractable paper investigate use frankwolfe algorithm recovering sparse solutions posterior inference detailed elucidation theoretical practical aspects exhibits many interesting properties beneficial topic modeling employ design fast methods including mlfw learning latent dirichlet allocation lda large scales extensive experiments show reach predictiveness level mlfw perform tens thousand times faster existing stateoftheart methods learning lda massivestreaming data\",\"rising topic computational journalism enhance diversity news served subscribers foster exploration behavior news reading despite success preference learning personalized news recommendation overexploitation causes filter bubble isolates readers opposing viewpoints hurts longterm user experiences lack serendipity since news providers recommend neither opposite diversified opinions unpopularity articles surely predicted bet articles whose forecasts clickthrough rate involve high variability risks high estimation errors uncertainties propose novel bayesian model uncertaintyaware scoring ranking news articles bayesian binary classifier models probability success defined news click betadistributed random variable conditional vector context user features article features contextual features posterior contextual coefficients computed efficiently using lowrank version laplaces method via thin singular value decomposition efficiencies personalized targeting exceptional articles chosen subscriber test period evaluated realworld news datasets proposed estimator slightly outperformed existing training scoring algorithms terms efficiency identifying successful outliers\",\"nonparametric mixture models based dirichlet process elegant alternative finite models number underlying components unknown inference models slow existing attempts parallelize inference models relied introducing approximations lead inaccuracies posterior estimate paper describe auxiliary variable representations dirichlet process hierarchical dirichlet process allow sample true posterior distributed manner show approach allows scalable inference without deterioration estimate quality accompanies existing methods\",\"supervised topic models help clinical researchers find interpretable cooccurence patterns count data relevant diagnostics however standard formulations supervised latent dirichlet allocation two problems first documents many words labels influence labels negligible second due conditional independence assumptions graphical model impact supervised labels learned topicword probabilities often minimal leading poor predictions heldout data investigate penalized optimization methods training slda produce interpretable topicword parameters useful heldout predictions using recognition networks speedup inference report preliminary results synthetic data predicting successful antidepressant medication given patients diagnostic history\",\"among easiest ways find meaningful structure discrete data latent dirichlet allocation lda related component models applied widely simple computationally fast scalable interpretable admit nonparametric priors currently popular field network modeling relatively little work taken uncertainty data seriously bayesian sense component models introduced field recently treating node bag outgoing links introduce alternative interaction component model communities icmc whole network bag links stemming different components former finds disassortative assortative structure alternative assumes assortativity finds communitylike structures like earlier methods motivated physics dirichlet process priors efficient implementation models highly scalable demonstrated social network lastfm web site nodes million links\",\"document travel may expect short snippets document also travel introduce general framework incorporating types invariances discriminative classifier framework imagines data drawn slice levy process slice levy process earlier point time obtain additional pseudoexamples used train classifier show scheme two desirable properties preserves bayes decision boundary equivalent fitting generative model limit rewind time back construction captures popular schemes gaussian feature noising dropout training well admitting new generalizations\",\"using nonparametric methods increasingly explored bayesian hierarchical modeling way increase model flexibility although field shows lot promise inference many models including hierachical dirichlet processes hdp remain prohibitively slow one promising path forward exploit submodularity inherent indian buffet process ibp derive nearoptimal solutions polynomial time work present brief tutorial bayesian nonparametric methods especially applied topic modeling show comparison different nonparametric models current stateoftheart parametric model latent dirichlet allocation lda\",\"present sparse treebased listbased density estimation methods binarycategorical data density estimation models higher dimensional analogies variable bin width histograms leaf tree list density constant similar flat density within bin histogram histograms however cannot easily visualized two dimensions whereas models accuracy histograms fades dimensions increase whereas models priors help generalization models sparse unlike highdimensional fixedbin histograms present three generative modeling methods first one allows user specify preferred number leaves tree within bayesian prior second method allows user specify preferred number branches within prior third method returns density lists rather trees allows user specify preferred number rules length rules within prior new approaches often yield better balance sparsity accuracy density estimates methods task present application crime analysis estimate unusual type modus operandi house breakin\",\"many modern data analysis problems involve inferences streaming data however streaming data easily amenable standard probabilistic modeling approaches assume condition finite data develop population variational bayes new approach using bayesian modeling analyze streams data approximates new type distribution population posterior combines notion population distribution data bayesian inference probabilistic model study method latent dirichlet allocation dirichlet process mixtures several largescale data sets\",\"inference latent feature models bayesian nonparametric setting generally difficult especially high dimensional settings usually requires proposing features prior distribution special cases integration tractable sample new feature assignments according predictive likelihood present novel method accelerate mixing latent variable model inference proposing feature locations based data opposed prior first introduce accelerated feature proposal mechanism show valid mcmc algorithm posterior inference next propose approximate inference strategy perform accelerated inference parallel twostage algorithm combines two approaches provides computationally attractive method quickly reach local convergence posterior distribution model allowing exploit parallelization\",\"present wrightfisher indian buffet process wfibp probabilistic model timedependent data assumed generated unknown number latent features model suitable prior bayesian nonparametric feature allocation models features underlying observed data exhibit dependency structure time specifically establish new framework generating dependent indian buffet processes poisson random field model population genetics used way constructing dependent beta processes inference model complex describe sophisticated markov chain monte carlo algorithm exact posterior simulation apply construction develop nonparametric focused topic model collections timestamped text documents test full corpus nips papers published\",\"timevarying mixture densities occur many scenarios example distributions keywords appear publications may evolve year year video frame features associated multiple targets may evolve sequence models realistically cater phenomenon must exhibit two important properties underlying mixture densities must unknown number mixtures must smoothness constraints place adjacent mixture densities traditional hierarchical dirichlet process hdp may suited first property certainly second due random measure lower hierarchies sampled independent hence facilitate temporal correlations overcome shortcomings proposed new smoothed hierarchical dirichlet process shdp key novelty model place temporal constraint amongst nearby discrete measures form symmetric kullbackleibler divergence fixed bound although constraint place involves single scalar value nonetheless allows flexibility corresponding successive measures remarkably also led infer model within stickbreaking process traditional beta distribution used stickbreaking replaced new constraint calculated present inference algorithm elaborate solutions experiment using nips keywords shown desirable effect model\",\"new geometricallymotivated algorithm nonnegative matrix factorization developed applied discovery latent topics text image document corpora algorithm based robustly finding clustering extreme points empirical crossdocument wordfrequencies correspond novel words unique topic contrast related approaches based solving nonconvex optimization problems using suboptimal approximations locallyoptimal methods heuristics new algorithm convex polynomial complexity competitive qualitative quantitative performance compared current stateoftheart approaches synthetic realworld datasets\",\"dirichlet process mixture dpm ubiquitous flexible bayesian nonparametric statistical model however full probabilistic inference model analytically intractable computationally intensive techniques gibbs sampling required result dpmbased methods considerable potential restricted applications computational resources time inference plentiful example would practical digital signal processing embedded hardware computational resources serious premium develop simplified yet statistically rigorous approximate maximum aposteriori map inference algorithms dpms algorithm simple kmeans clustering performs experiments well gibbs sampling requiring fraction computational effort unlike related small variance asymptotics algorithm nondegenerate inherits rich get richer property dirichlet process also retains nondegenerate closedform likelihood enables standard tools crossvalidation used wellposed approximation map solution probabilistic dpm model\",\"introduce novel approach estimating latent dirichlet allocation lda parameters collapsed gibbs samples cgs leveraging full conditional distributions latent variable assignments efficiently average multiple samples little computational cost drawing single additional collapsed gibbs sample approach understood adapting soft clustering methodology collapsed variational bayes cvb cgs parameter estimation order get best techniques estimators straightforwardly applied output existing implementation cgs including modern accelerated variants perform extensive empirical comparisons estimators standard collapsed inference algorithms realworld data unsupervised lda priorlda supervised variant lda multilabel classification results show consistent advantage approach traditional cgs experimental conditions cvb inference majority conditions broadly results highlight importance averaging multiple samples lda parameter estimation use efficient computational techniques\",\"indian buffet process based models elegant way discovering underlying features within data set inference models slow inferring underlying features using markov chain monte carlo either relies uncollapsed representation leads poor mixing collapsed representation leads quadratic increase computational complexity existing attempts distributing inference introduced additional approximation within inference procedure paper present novel algorithm perform asymptotically exact parallel markov chain monte carlo inference indian buffet process models take advantage fact features conditionally independent betabernoulli process conditional independence partition features two parts one part containing finitely many instantiated features part containing infinite tail uninstantiated features finite partition parallel inference simple given instantiation features infinite tail performing uncollapsed mcmc leads poor mixing hence collapse features resulting hybrid sampler parallel produces samples asymptotically true posterior\",\"propose dirichlet process mixtures generalized linear models dpglm new method nonparametric regression accommodates continuous categorical inputs responses modeled generalized linear model prove conditions asymptotic unbiasedness dpglm regression mean function estimate also give examples conditions hold including models compactly supported continuous distributions model continuous covariates categorical response empirically analyze properties dpglm provides better results existing dirichlet process mixture regression models evaluate dpglm several data sets comparing modern methods nonparametric regression like cart bayesian trees gaussian processes compared existing techniques dpglm provides single model corresponding inference algorithms performs well many regression settings\",\"single stationary topic model latent dirichlet allocation inappropriate modeling corpora span long time periods popularity topics likely change time number models incorporate time proposed general either exhibit limited forms temporal variation require computationally expensive inference methods paper propose nonparametric topics time nptot model timevarying topics allows unbounded number topics exible distribution temporal variations topics popularity develop collapsed gibbs sampler proposed model compare existing models synthetic real document sets\",\"tree structures ubiquitous data across many domains many datasets naturally modelled unobserved tree structures paper first review theory random fragmentation processes bertoin number existing methods modelling trees including popular nested chinese restaurant process ncrp define general class probability distributions trees dirichlet fragmentation process dfp novel combination theory dirichlet processes random fragmentation processes dfp presents stickbreaking construction relates ncrp way dirichlet process relates chinese restaurant process furthermore develop novel hierarchical mixture model dfp empirically compare new model similar models machine learning experiments show dfp mixture model convincingly better existing stateoftheart approaches hierarchical clustering density modelling\",\"propose new algorithms topic modeling number topics unknown approach relies analysis concentration mass angular geometry topic simplex convex polytope constructed taking convex hull vertices representing latent topics algorithms shown practice accuracy comparable gibbs sampler terms topic estimation requires number topics given moreover one fastest among several state art parametric techniques statistical consistency estimator established conditions\",\"describe new method visualizing topics distributions terms automatically extracted large text corpora using latent variable models method finds significant ngrams related topic used help understand interpret underlying distribution compared usual visualization simply lists probable topical terms multiword expressions provide better intuitive impression topic approach based language model arbitrary length expressions develop new methodology based nested permutation tests find significant phrases show method outperforms standard use chi likelihood ratio tests illustrate topic presentations corpora scientific abstracts news articles\",\"many practical modeling problems involve discrete data best represented draws multinomial categorical distributions example nucleotides dna sequence childrens names given state year text documents commonly modeled multinomial distributions cases expect form dependency draws nucleotide one position dna strand may depend preceding nucleotides childrens names highly correlated year year topics text may correlated dynamic dependencies naturally captured typical dirichletmultinomial formulation leverage logistic stickbreaking representation recent innovations polyagamma augmentation reformulate multinomial distribution terms latent variables jointly gaussian likelihoods enabling take advantage host bayesian inference techniques gaussian models minimal overhead\",\"document going derive equations needed implement variational bayes estimation parameters simplified probabilistic linear discriminant analysis splda model used adapt splda one database another development data implement fully bayesian recipe approach similar bishops ppca\",\"modern vehicles equipped increasingly complex sensors sensors generate large volumes data provide opportunities modeling analysis interested exploiting data learn aspects behaviors road network associated individual drivers dataset collected standard vehicle used commute work personal trips hidden markov model hmm trained gps position orientation data utilized compress large amount position information small amount road segment states state set observations car signals associated quantized modeled draws hierarchical dirichlet process hdp inference topic distributions carried using hdp splitmerge sampling algorithm topic distributions joint quantized car signals characterize driving situation respective road state novel manner demonstrate sparsity personal road network driver conjunction hierarchical topic model allows data driven predictions destinations well likely road conditions\",\"one core problems statistical models estimation posterior distribution topic models problem posterior inference individual texts particularly important especially dealing data streams often intractable worst case consequence existing methods posterior inference approximate guarantee neither quality convergence rate paper introduce provably fast algorithm namely online maximum posteriori estimation ope posterior inference topic models ope attractive properties existing inference approaches including theoretical guarantees quality fast rate convergence local maximalstationary point inference problem discussions ope general hence easily employed wide range contexts finally employ ope design three methods learning latent dirichlet allocation text streams large corpora extensive experiments demonstrate superior behaviors ope new learning methods\",\"novel dynamic bayesian nonparametric topic model anomaly detection video proposed paper batch online gibbs samplers developed inference paper introduces new abnormality measure decision making proposed method evaluated synthetic real data comparison nondynamic model shows superiority proposed dynamic one terms classification performance anomaly detection\",\"bayesian models mix multiple dirichlet prior parameters called multidirichlet priors paper gaining popularity inferring mixing weights parameters mixed prior distributions seems tricky sums dirichlet parameters complicate joint distribution model parameters paper shows novel auxiliary variable scheme helps simplify inference models involving hierarchical mds mdps using scheme easy derive fully collapsed inference schemes allow efficient inference\",\"present novel scalable bayesian approach modelling occurrence pairs symbols drawn large vocabulary observed pairs assumed generated simple popularity based selection process followed censoring using preference function basing inference wellfounded principle variational bounding using new siteindependent bounds show scalable inference procedure obtained large data sets state art results presented realworld movie viewing data\",\"histogram method powerful nonparametric approach estimating probability density function continuous variable construction histogram compared parametric approaches demands large number observations capture underlying density function thus suitable analyzing sparse data set collection units small size data paper employing probabilistic topic model develop novel bayesian approach alleviating sparsity problem conventional histogram estimation method estimates units density function mixture basis histograms number bins basis well heights determined automatically estimation procedure performed using fast easytoimplement collapsed gibbs sampling apply proposed method synthetic data showing performs well\",\"topic models bayesian models frequently used capture latent structure certain corpora documents images data element corpus instance item collection scientific articles regarded convex combination small number vectors corresponding topics components weights assumed dirichlet prior distribution standard approach towards approximating posterior use variational inference algorithms particular mean field approximation show approach suffers instability produce misleading conclusions namely certain regimes model parameters variational inference outputs nontrivial decomposition topics however parameter values data contain actual information true decomposition hence output algorithm uncorrelated true topic decomposition among consequences estimated posterior mean significantly wrong estimated bayesian credible regions achieve nominal coverage discuss instability remedied accurate mean field approximations\",\"full length article draft version problem number topics topic modeling discussed proposed idea renyi tsallis entropy used identification optimal number large textual collections also report results numerical experiments semantic stability topic models shows semantic stability play important role problem topic number calculation renyi tsallis entropy based thermodynamics approach\",\"topic models one popular methods learning representations text major challenge change topic model requires mathematically deriving new inference algorithm promising approach address problem autoencoding variational bayes aevb proven diffi cult apply topic models practice present knowledge first effective aevb based inference method latent dirichlet allocation lda call autoencoded variational inference topic model avitm model tackles problems caused aevb dirichlet prior component collapsing find avitm matches traditional methods accuracy much better inference time indeed inference network find unnecessary pay computational cost running variational optimization test data avitm black box readily applied new topic models dramatic illustration present new topic model called prodlda replaces mixture model lda product experts changing one line code lda find prodlda yields much interpretable topics even lda trained via collapsed gibbs sampling\",\"introduce supervised latent dirichlet allocation slda statistical model labelled documents model accommodates variety response types derive approximate maximumlikelihood procedure parameter estimation relies variational methods handle intractable posterior expectations prediction problems motivate research use fitted model predict response values new documents test slda two realworld problems movie ratings predicted reviews political tone amendments senate based amendment text illustrate benefits slda versus modern regularized regression well versus unsupervised lda analysis followed separate regression\",\"paper presents algorithm unsupervised learning latent variable models unlabeled sets data base technique spectral decomposition providing technique proves robust theory practice also describe use algorithm learn parameters two well known text mining models single topic model latent dirichlet allocation providing cases efficient technique retrieve parameters feed algorithm compare results algorithm existing algorithms synthetic data provide examples applications real world text corpora single topic model lda obtaining meaningful results\",\"paper proposes novel dynamic hierarchical dirichlet process topic model considers dependence successive observations conventional posterior inference algorithms kind models require processing whole data several passes computationally intractable massive sequential data design batch online inference algorithms based gibbs sampling proposed model allows process sequential data incrementally updating model new observation model applied abnormal behaviour detection video sequences new abnormality measure proposed decision making proposed method compared method based non dynamic hierarchical dirichlet process also derive online gibbs sampler abnormality measure results synthetic real data show consideration dynamics topic model improves classification performance abnormal behaviour detection\",\"large scale online inference problems update strategy critical performance derive adaptive scan gibbs sampler optimizes update frequency selecting optimum minibatch size demonstrate performance adaptive batchsize gibbs sampler comparing collapsed gibbs sampler bayesian lasso dirichlet process mixture models dpmm latent dirichlet allocation lda graphical models\",\"investigate class feature allocation models generalize indian buffet process parameterized gibbstype random measures two existing classes contained special cases original twoparameter indian buffet process corresponding dirichlet process stable threeparameter indian buffet process corresponding pitmanyor process asymptotic behavior gibbstype partitions power laws holding number latent clusters translates analogous characteristics class gibbstype feature allocation models despite containing several different distinct subclasses properties gibbstype partitions allow develop blackbox procedure posterior inference within subclass models numerical experiments compare contrast subclasses highlight utility varying powerlaw behaviors latent features\",\"generating user interpretable multiclass predictions data rich environments many classes explanatory covariates daunting task introduce diagonal orthant latent dirichlet allocation dolda supervised topic model multiclass classification handle many classes well many covariates handle many classes use recently proposed diagonal orthant probit model johndrow together efficient horseshoe prior variable selectionshrinkage carvalho propose computationally efficient parallel gibbs sampler new model important advantage dolda learned topics directly connected individual classes without need reference class evaluate models predictive accuracy two datasets demonstrate doldas advantage interpreting generated predictions\",\"document going derive equations needed implement variational bayes ivector extractor used extract longer ivectors reducing risk overfittig adapt ivector extractor database another scarce development data work based patrick kennys joint factor analysis christopher bishops variational principal components\",\"semisupervised unsupervised systems provide operators invaluable support tremendously reduce operators load light necessity process large volumes video data provide autonomous decisions work proposes new learning algorithms activity analysis video activities behaviours described dynamic topic model two novel learning algorithms based expectation maximisation approach variational bayes inference proposed theoretical derivations posterior model parameters given designed learning algorithms compared gibbs sampling inference scheme introduced earlier literature detailed comparison learning algorithms presented real video data also propose anomaly localisation procedure elegantly embedded topic modeling framework proposed framework applied number areas including transportation systems security surveillance\",\"nonnegative matrix factorization nmf popular tool data exploration bayesian nmf promises also characterize uncertainty factorization unfortunately current inference approaches mcmc mix slowly tend get stuck single modes introduce novel approach using rapidlyexploring random trees rrts asymptotically cover regions high posterior density placed principled bayesian framework via online extension nonparametric variational inference experiments real synthetic data obtain greater coverage posterior higher elbo values standard nmf inference approaches\",\"emergence thriving development social networks huge number short texts accumulated need processed inferring latent topics collected short texts useful understanding hidden structure predicting new contents unlike conventional topic models latent dirichlet allocation lda biterm topic model btm recently proposed short texts overcome sparseness documentlevel word cooccurrences directly modeling generation process word pairs stochastic inference algorithms based collapsed gibbs sampling cgs collapsed variational inference proposed btm however either require large computational complexity rely crude estimation work develop stochastic divergence minimization inference algorithm btm estimate latent topics accurately scalable way experiments demonstrate superiority proposed algorithm compared existing inference algorithms\",\"propose geometric algorithm topic learning inference built convex geometry topics arising latent dirichlet allocation lda model nonparametric extensions end study optimization geometric loss function surrogate ldas likelihood method involves fast optimization based weighted clustering procedure augmented geometric corrections overcomes computational statistical inefficiencies encountered techniques based gibbs sampling variational inference achieving accuracy comparable gibbs sampler topic estimates produced method shown statistically consistent conditions algorithm evaluated extensive experiments simulated real data\",\"describe simple efficient procedure approximating levy measure textgammaalpha random variable use approximation derive finite sumrepresentation converges almost surely fergusons representation dirichlet process based arrivals homogeneous poisson process compare efficiency approximation several well known approximations dirichlet process demonstrate substantial improvement\",\"present discrete infinite logistic normal distribution diln bayesian nonparametric prior mixed membership models diln generalization hierarchical dirichlet process hdp models correlation structure weights atoms group level derive representation diln normalized collection gammadistributed random variables study statistical properties consider applications topic modeling derive variational inference algorithm approximate posterior inference study empirical performance diln topic model four corpora comparing performance hdp correlated topic model ctm deal largescale data sets also develop online inference algorithm diln compare online hdp online lda nature magazine contains approximately articles\",\"present nested chinese restaurant process ncrp stochastic process assigns probability distributions infinitelydeep infinitelybranching trees show stochastic process used prior distribution bayesian nonparametric model document collections specifically present application information retrieval documents modeled paths random tree preferential attachment dynamics ncrp leads clustering documents according sharing topics multiple levels abstraction given corpus documents posterior inference algorithm finds approximation posterior distribution trees topics allocations words levels tree demonstrate algorithm collections scientific abstracts several journals model exemplifies recent trend statistical machine learningthe use bayesian nonparametric methods infer distributions flexible data structures\",\"introduce new approach topic modeling supervised survival analysis specifically build recent work unsupervised topic modeling socalled anchor words providing supervision elasticnet regularized cox proportional hazards model short anchor word present document provides strong indication document partially specific topic example seeing gallstones document fairly certain document partially medicine proposed method alternates learning topic model learning survival model find local minimum block convex optimization problem apply proposed approach predicting long patients pancreatitis admitted intensive care unit icu stay icu approach accurate best variety baselines interpretable baselines\",\"nonnegative matrix factorization nmf technique finding latent representations data method applied corpora construct topic models however nmf likelihood assumptions often violated real document corpora present double parametric bootstrap test evaluating fit nmfbased topic model based duality divergence poisson maximum likelihood estimation test correctly identifies whether topic model based nmf approach yields reliable results simulated real data\",\"theory bayesian nonparametric bnp models well suited streaming data scenarios due ability adapt model complexity observed data unfortunately benefits fully realized practice existing inference algorithms either applicable streaming applications extensible bnp models special case dirichlet processes streaming inference considered however growing interest flexible bnp models building class normalized random measures nrms work within general framework present streaming variational inference algorithm nrm mixture models algorithm based assumed density filtering adf leading straightforwardly expectation propagation largescale batch inference well demonstrate efficacy algorithm clustering documents large streaming text corpora\",\"robust bayesian models appealing alternatives standard models providing protection data contains outliers departures model assumptions historically robust models mostly developed casebycase basis examples include robust linear regression robust mixture models bursty topic models paper develop general approach robust bayesian modeling show turn existing bayesian model robust model develop generic strategy computing use method study robust variants several models including linear regression poisson regression logistic regression probabilistic topic models discuss connections methods existing approaches especially empirical bayes jamesstein estimation\",\"bayesian nonnegative matrix factorization nmf promising approach understanding uncertainty structure matrix data however large volume applied work optimizes traditional nonbayesian nmf objectives fail provide principled understanding nonidentifiability inherent nmf issue ideally addressed bayesian approach despite suitability current bayesian nmf approaches failed gain popularity applied setting sacrifice flexibility modeling tractable computation tend get stuck local modes require many thousands samples meaningful uncertainty estimates address issues particlebased variational approach bayesian nmf requires joint likelihood differentiable tractability uses novel initialization technique identify multiple modes posterior allows domain experts inspect small set factorizations faithfully represent posterior introduce employ class likelihood prior distributions nmf formulate bayesian model using popular nonbayesian nmf objectives several real datasets obtain better particle approximations bayesian nmf posterior less time baselines demonstrate significant role multimodality plays nmfrelated tasks\",\"dynamic topic models dtms effective discovering topics capturing evolution trends time series data posterior inference dtms existing methods batch algorithms scan full dataset update model make inexact variational approximations meanfield assumptions due lack scalable inference algorithm despite usefulness dtms captured large topic dynamics paper fills research void presents fast parallelizable inference algorithm using gibbs sampling stochastic gradient langevin dynamics make unwarranted assumptions also present metropolishastings based sampler topic assignments word token distributed environment algorithm requires little communication workers sampling almost embarrassingly parallel scales largescale applications able learn largest dynamic topic model knowledge learned dynamics topics million documents less half hour empirical results show algorithm orders magnitude faster baselines also achieves lower perplexity\",\"develop nested hierarchical dirichlet process nhdp hierarchical topic modeling nhdp generalization nested chinese restaurant process ncrp allows word follow path topic node according documentspecific distribution shared tree alleviates rigid singlepath formulation ncrp allowing document easily express thematic borrowings random effect demonstrate algorithm million documents new york times\",\"bayesian mixture models widely applied unsupervised learning exploratory data analysis markov chain monte carlo based gibbs sampling splitmerge moves widely used inference models however methods restricted limited types transitions suffer torpid mixing low accept rates even problems modest size propose method considers broader range transitions close equilibrium exploiting multiple chains parallel using past states adaptively inform proposal distribution method significantly improves gibbs splitmerge sampling quantified using convergence diagnostics acceptance rates adaptive mcmc methods use past states inform proposal distribution given rise many ingenious sampling schemes continuous problems present work seen important first step bringing benefits partitionbased problems\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"16_topic_dirichlet_inference\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"16_topic_dirichlet_inference\"],\"textfont\":{\"size\":12},\"x\":[14.485531,14.52883,14.473202,13.996081,14.57307,14.439395,14.298609,14.130861,14.144007,14.062783,13.774178,14.042514,14.068981,12.151382,13.984764,14.388676,13.896784,13.967729,14.551918,14.227013,14.575282,14.584457,14.176342,14.1809635,14.481405,14.527465,14.278788,13.964264,14.133037,14.138346,14.547367,14.558749,14.528052,14.53791,14.558053,14.258198,14.237914,14.00469,14.503278,14.143154,14.3444195,12.574413,14.562855,14.537983,14.019121,14.233283,14.245264,14.596006,12.611205,14.067589,14.356642,12.551201,14.561203,14.549305,13.943392,14.161054],\"y\":[7.4606934,7.4674063,7.439095,7.2245154,7.4871173,7.414367,7.483262,7.3260956,6.9601917,7.2575326,7.4336324,7.3218765,7.1293144,5.4924088,7.200347,7.480287,7.3451424,7.146811,7.486821,6.8322787,7.4952083,7.50275,7.468936,7.6366043,7.446805,7.4694858,7.2727327,7.3979096,7.2972913,7.18603,7.511479,7.481209,7.4744043,7.529881,7.503567,7.26886,7.3431334,7.3087277,7.453769,7.690501,7.3444586,6.2459526,7.4900866,7.4461637,7.295823,7.3636093,6.9671545,7.463524,6.230027,7.2068176,7.419243,6.2292175,7.4784474,7.433569,7.2559886,7.2727013],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"propose new method discovering causal relationships temporal data based notion causal compression end adopt pearlian graph setting directed information information theoretic tool quantifying causality introduce chain rule directed information use motivate causal sparsity show two applications proposed method causal time series segmentation selects time points capturing incoming outgoing causal flow time points belonging different signals causal bipartite graph recovery prove modelling causality adopted setup requires estimating copula density data distribution thus depend marginals evaluate method time resolved gene expression data\",\"classical approaches granger causality detection repose upon linear time series assumptions many interactions neuroscience economics applications nonlinear develop approach nonlinear granger causality detection using multilayer perceptrons input network past time lags series output future value single series sufficient condition granger noncausality setting outgoing weights input data past lags series first hidden layer zero estimation utilize group lasso penalty shrink groups input weights zero also propose hierarchical penalty simultaneous granger causality lag estimation validate approach simulated data sparse linear autoregressive model sparse nonlinear lorenz model\",\"article contains detailed proofs additional examples related uai submission learning sparse causal models nphard describes fci algorithm method sound complete causal model discovery presence latent confounders andor selection bias worst case polynomial complexity order number independence tests sparse graphs nodes bounded node degree algorithm adaptation wellknown fci algorithm spirtes also sound complete worst case complexity exponential\",\"provide theoretical empirical evidence type asymmetry causes effects present related via linear models contaminated additive nongaussian noise assuming causes effects distribution show distribution residuals linear fit anticausal direction closer gaussian distribution residuals causal direction gaussianization effect characterized reduction magnitude highorder cumulants increment differential entropy residuals problem nonlinear causal inference addressed performing embedding expanded feature space relation causes effects assumed linear effectiveness method discriminate causes effects based type asymmetry illustrated variety experiments using different measures gaussianity proposed method shown competitive stateoftheart techniques causal inference\",\"given data sampled number variables one often interested underlying causal relationships form directed acyclic graph general case without interventions variables possible identify graph markov equivalence class however situations one find true causal graph observational data example structural equation models additive noise nonlinear edge functions current methods achieving rely nonparametric independence tests one problems null hypothesis independence one would like get evidence take different approach work using penalized likelihood score model selection practically feasible many settings advantage yielding natural ranking candidate models making smoothness assumptions probability density space prove consistency penalized maximum likelihood estimator also present empirical results simulated scenarios real twodimensional data sets causeeffect pairs obtain similar results stateoftheart methods\",\"interpretability prediction mechanisms respect underlying prediction problem often unclear several studies focused developing prediction models meaningful parameters causal relationships predictors actual prediction considered connect underlying causal structure data generation process causal structure prediction mechanism achieve propose framework identifies feature greatest causal influence prediction estimates necessary causal intervention feature desired prediction obtained general concept framework restrictions regarding data linearity however focus implementation linear data framework applicability evaluated using artificial data demonstrated using realworld data\",\"one fundamental problems causal inference estimation causal effect variables confounded difficult observational study one direct evidence confounders adjusted introduce novel approach estimating causal effects exploits observational conditional independencies suggest weak paths unknown causal graph widely used faithfulness condition spirtes relaxed allow varying degrees path cancellations imply conditional independencies rule existence confounding causal paths outcome posterior distribution bounds average causal effect via linear programming approach bayesian inference claim approach used regular practice along default tools observational studies\",\"important topic systems biology developing statistical methods automatically find causal relations gene regulatory networks prior knowledge causal connectivity many methods developed time series data however discovery methods based steadystate data often necessary preferable since obtaining time series data expensive andor infeasible many biological systems conventional approach causal bayesian networks however estimation bayesian networks illposed many cases cannot uniquely identify underlying causal network gives large class equivalent causal networks cannot distinguished based data distribution propose new discovery algorithm uniquely identifying underlying causal network genes best knowledge proposed method first algorithm learning gene networks based fully identifiable causal model called lingam compare algorithm competing algorithms using artificiallygenerated data although definitely better test based real microarray gene expression data\",\"paper addresses problem inferring sparse causal networks modeled multivariate autoregressive mar processes conditions derived group lasso glasso procedure consistently estimates sparse network structure key condition involves false connection score particular show consistent recovery possible even number observations network far less number parameters describing network provided false connection score less one false connection score also demonstrated useful metric recovery nonasymptotic regimes conditions suggest modified glasso procedure tends improve false connection score reduce chances reversing direction causal influence computational experiments real network based electrocorticogram ecog simulation study demonstrate effectiveness approach\",\"estimation optimal treatment regimes considerable interest precision medicine work propose causal knearest neighbor method estimate optimal treatment regime method roots framework causal inference estimates causal treatment effects within nearest neighborhood although method simple possesses nice theoretical properties show causal knearest neighbor regime universally consistent causal knearest neighbor regime eventually learn optimal treatment regime sample size increases also establish convergence rate however causal knearest neighbor regime may suffer curse dimensionality performance deteriorates dimensionality increases alleviate problem develop adaptive causal knearest neighbor method perform metric selection variable selection simultaneously performance proposed methods illustrated simulation studies analysis chronic depression clinical trial\",\"new causal discovery method structural agnostic modeling sam presented paper leveraging conditional independencies distributional asymmetries sam aims find underlying causal structure observational data approach based game different players estimating variable distribution conditionally others neural net adversary aimed discriminating generated data original data learning criterion combining distribution estimation sparsity acyclicity constraints used enforce optimization graph structure parameters stochastic gradient descent sam extensively experimentally validated synthetic real data\",\"information geometric causal inference igci new approach distinguish cause effect two variables based independence assumption input distribution causal mechanism phrased terms orthogonality information space describe two intuitive reinterpretations approach makes igci accessible broader audience moreover show described independence related hypothesis unsupervised learning semisupervised learning works predicting cause effect vice versa\",\"paper considers problem estimating structure multiple related directed acyclic graph dag models building recent developments exact estimation dags using integer linear programming ilp present ilp approach joint estimation multiple dags require vertices dag share common ordering furthermore allow also potentially unknown dependency structure dags results presented simulated data fmri data obtained multiple subjects\",\"interested learning causal relationships pairs random variables purely observational data effectively address task stateoftheart relies strong assumptions regarding mechanisms mapping causes effects invertibility existence additive noise hold limited situations contrary short paper proposes learn perform causal inference directly data without need feature engineering particular pose causality kernel mean embedding classification problem inputs samples arbitrary probability distributions pairs random variables labels types causal relationships validate performance method synthetic realworld data stateoftheart moreover submitted algorithm chalearns fast causation coefficient challenge competition fastest code prize ranked third overall leaderboard\",\"propose method infer causal structures containing discrete continuous variables idea select causal hypotheses conditional density every variable given causes becomes smooth define family smooth densities conditional densities second order exponential models maximizing conditional entropy subject first second statistical moments variables take values proper subsets conditionals induce different families joint distributions even markovequivalent graphs consider case one binary one realvalued variable method distinguish cause effect using example describe sometimes causal hypothesis must rejected peffectcause pcause share algorithmic information untypical chosen independently way method spirit faithfulnessbased causal inference also rejects nongeneric mutual adjustments among dagparameters\",\"present causal generative neural networks cgnns learn functional causal models observational data cgnns leverage conditional independencies distributional asymmetries discover bivariate multivariate causal structures cgnns make assumption regarding lack confounders learn differentiable generative model data using backpropagation extensive experiments show good performances comparatively state art observational causal discovery simulated real data respect causeeffect inference vstructure identification multivariate causal discovery\",\"paper frames causal structure estimation machine learning task idea treat indicators causal relationships variables labels exploit available data variables interest provide features labelling task background scientific knowledge available interventional data provide labels causal relationships remainder treated unlabelled illustrate key ideas develop distancebased approach based bivariate histograms within manifold regularization framework present empirical results three different biological data sets including examples causal effects verified experimental intervention together demonstrate efficacy general nature approach well simplicity users point view\",\"causal inference using observational data challenging especially bivariate case minimum description length principle link postulate independence generating mechanisms cause effect given cause quantile regression based theory develop bivariate quantile causal discovery bqcd new method distinguish cause effect assuming confounding selection bias feedback uses multiple quantile levels instead conditional mean bqcd adaptive additive also multiplicative even locationscale generating mechanisms illustrate effectiveness approach perform extensive empirical comparison synthetic real datasets study shows bqcd robust across different implementations method quantile regression computationally efficient compares favorably stateoftheart methods\",\"machine learning science discovering statistical dependencies data use dependencies perform predictions last decade machine learning made spectacular progress surpassing human performance complex tasks object recognition car driving computer gaming however central role prediction machine learning avoids progress towards generalpurpose artificial intelligence one way forward argue causal inference fundamental component human intelligence yet ignored learning algorithms causal inference problem uncovering causeeffect relationships variables data generating system causal structures provide understanding systems behave changing unseen environments turn knowledge causal dynamics allows answer questions describing potential responses system hypothetical manipulations interventions thus understanding cause effect one step machine learning towards machine reasoning machine intelligence currently available causal inference algorithms operate specific regimes rely assumptions difficult verify practice thesis advances art causal inference three different ways first develop framework study statistical dependence based copulas random features second build framework interpret problem causal inference task distribution classification yielding family novel causal inference algorithms third discover causal structures convolutional neural network features using algorithms algorithms presented thesis scalable exhibit strong theoretical guarantees achieve stateoftheart performance variety realworld benchmarks\",\"consider learning possible causal direction two observed variables presence latent confounding variables several existing methods shown consistently estimate causal direction assuming linear type nonlinear relationship latent confounders however estimation results could distorted either assumption actually violated paper first propose new linear nongaussian acyclic structural equation model individualspecific effects allows latent confounders considered propose empirical bayesian approach estimating possible causal direction using new model demonstrate effectiveness method using artificial realworld data\",\"consider fundamental problem inferring causal direction two univariate numeric random variables observational data twovariable case especially difficult solve since possible use standard conditional independence tests variables tackle problem follow information theoretic approach based kolmogorov complexity use minimum description length mdl principle provide practical solution particular propose compression scheme encode local global functional relations using mdlbased regression infer causes case shorter describe function inverse direction addition introduce slope efficient lineartime algorithm thorough empirical evaluation synthetic real world data show outperforms state art wide margin\",\"many statistical methods proposed estimate causal models classical situations fewer variables observations number variables number observations however modern datasets including gene expression data need highdimensional causal modeling challenging situations orders magnitude variables observations paper propose method find exogenous variables linear nongaussian causal model requires much smaller sample sizes conventional methods works even key idea identify variables exogenous based nongaussianity instead estimating entire structure model exogenous variables work triggers activate causal chain model identification leads efficient experimental designs better understanding causal mechanism present experiments artificial data realworld gene expression data evaluate method\",\"notion causality used many situations dealing uncertainty consider problem whether causality identified given data set generated discrete random variables rather continuous ones particular nonbinary data thus far known causality identified except rare cases paper present necessary sufficient condition integer modular acyclic additive noise iman two variables addition relate bivariate multivariate causal identifiability explicit manner develop practical algorithm find order variables parent sets demonstrate performance applications artificial data real world body motion data comparisons conventional methods\",\"introduce new approach functional causal modeling observational data called causal generative neural networks cgnn cgnn leverages power neural networks learn generative model joint distribution observed variables minimizing maximum mean discrepancy generated observed data approximate learning criterion proposed scale computational cost approach linear complexity number observations performance cgnn studied throughout three experiments firstly cgnn applied causeeffect inference task identify best causal hypothesis xrightarrow yrightarrow secondly cgnn applied problem identifying vstructures conditional independences thirdly cgnn applied multivariate functional causal modeling given skeleton describing direct dependences set random variables textbfx ldots cgnn orients edges skeleton uncover directed acyclic causal graph describing causal structure random variables three tasks cgnn extensively assessed artificial realworld data comparing favorably stateoftheart finally cgnn extended handle case confounders latent variables involved overall causal model\",\"consider learning causal ordering variables linear nongaussian acyclic model called lingam several existing methods shown consistently estimate causal ordering assuming model assumptions correct estimation results could distorted assumptions actually violated paper propose new algorithm learning causal orders robust one typical violation model assumptions latent confounders key idea detect latent confounders testing independence estimated external influences find subsets parcels include variables affected latent confounders demonstrate effectiveness method using artificial data simulated brain imaging data\",\"classical approaches granger causality detection assume linear dynamics many interactions realworld applications like neuroscience genomics inherently nonlinear cases using linear models may lead inconsistent estimation granger causal interactions propose class nonlinear methods applying structured multilayer perceptrons mlps recurrent neural networks rnns combined sparsityinducing penalties weights encouraging specific sets weights zeroin particular use convex grouplasso penaltieswe extract granger causal structure contrast traditional approaches framework naturally enables efficiently capture longrange dependencies series either via rnns automatic lag selection mlp show neural granger causality methods outperform stateoftheart nonlinear granger causality methods dream challenge data data consists nonlinear gene expression regulation time courses limited number time points successes show challenging dataset provide powerful example deep learning useful cases beyond prediction large datasets likewise illustrate methods detecting nonlinear interactions human motion capture dataset\",\"study model one target variable correlated vector xxxd predictor variables potential causes describe method infers extent statistical dependences due influence extent due hidden common cause confounder method relies concentration measure results large dimensions independence assumption stating absence confounding vector regression coefficients describing influence typically generic orientation relative eigenspaces covariance matrix special case scalar confounder show confounding typically spoils generic orientation characteristic way used quantitatively estimate amount confounding\",\"learning causal effect observational data straightforward possible without assumptions hidden common causes treatment outcome cannot blocked measurements one possibility use instrumental variable principle possible assumptions discover whether variable structurally instrumental target causal effect rightarrow current frameworks somewhat lacking general assumptions instrumental variable discovery problem challenging variable tested instrument isolation groups different variables might require different conditions considered instrument moreover identification constraints might hard detect statistically paper give theoretical characterization instrumental variable discovery highlighting identifiability problems solutions need nongaussianity assumptions fit within existing methods\",\"controlled interventions provide direct source information learning causal effects particular doseresponse curve learned varying treatment level observing corresponding outcomes however interventions expensive timeconsuming observational data treatment controlled known mechanism sometimes available strong assumptions observational data allows estimation doseresponse curves estimating curves nonparametrically hard sample sizes controlled interventions may small observational case large number measured confounders may need marginalized paper introduce hierarchical gaussian process prior constructs distribution doseresponse curve learning observational data reshapes distribution nonparametric affine transform learned controlled interventions function composition different sources shown speedup learning demonstrate thorough sensitivity analysis application modeling effect therapy cognitive skills premature infants\",\"consider problem learning fair decision systems complex scenarios sensitive attribute might affect decision along fair unfair pathways introduce causal approach disregard effects along unfair pathways simplifies generalizes previous literature method corrects observations adversely affected sensitive attribute uses form decision avoids disregarding fair information require often intractable computation pathspecific effect leverage recent developments deep learning approximate inference achieve solution widely applicable complex nonlinear scenarios\",\"paper consider problem fair statistical inference involving outcome variables examples include classification regression problems estimating treatment effects randomized trials observational data issue fairness arises problems covariates treatments sensitive sense potential creating discrimination paper argue presence discrimination formalized sensible way presence effect sensitive covariate outcome along certain causal pathways view generalizes pearl fair outcome model learned solving constrained optimization problem discuss number complications arise classical statistical inference due view provide workarounds based recent work causal semiparametric inference\",\"study optimal covariate balance causal inferences observational data rich covariates complex relationships necessitate flexible modeling neural networks standard approaches propensity weighting matchingbalancing fail settings due miscalibrated propensity nets inappropriate covariate representations respectively propose new method based adversarial training weighting discriminator network effectively addresses methodological gap demonstrated new theoretical characterizations method well empirical results using fully connected architectures learn complex relationships convolutional architectures handle image confounders showing new method enable strong causal analyses challenging settings\",\"inferring causal structure set random variables finite sample joint distribution important problem science recently methods using additive noise models suggested approach case continuous variables many situations however variables interest discrete even finitely many states work extend notion additive noise models cases prove whenever joint distribution probxy admits model one direction yfxn independent admit reversed model xgytilde tilde independent long model chosen generic way based deliberations propose efficient new algorithm able distinguish cause effect finite sample discrete variables extensive experimental study show algorithm works synthetic real data sets\",\"causal inference relies structure graph often directed acyclic graph dag different graphs may result different causal inference statements different intervention distributions quantify differences propose pre distance dags structural intervention distance sid sid based graphical criterion quantifies closeness two dags terms corresponding causal inference statements therefore wellsuited evaluating graphs used computing interventions instead dags also possible compare cpdags completed partially directed acyclic graphs represent markov equivalence classes since differs significantly popular structural hamming distance shd sid constitutes valuable additional measure discuss properties distance provide efficient implementation software code available first authors homepage package construction\",\"many decisions healthcare business policy domains made without support rigorous evidence due cost complexity performing randomized experiments using observational data answer causal questions risky subjects receive different treatments also differ ways affect outcomes many causal inference methods developed mitigate biases however way know method might produce best estimate treatment effect given study analogy crossvalidation estimates prediction error predictive models applied given dataset propose synthvalidation procedure estimates estimation error causal inference methods applied given dataset synthvalidation use observed data estimate generative distributions known treatment effects apply causal inference method datasets sampled distributions compare effect estimates known effects estimate error using simulations show using synthvalidation select causal inference method study lowers expected estimation error relative consistently using single method\",\"widely applied approach causal inference nonexperimental time series often referred linear granger causal analysis regress present past interpret regression matrix hatb causally however unmeasured time series influences approach lead wrong causal conclusions distinct one would draw one additional information paper take different approach assume together hidden forms first order vector autoregressive var process transition matrix argue valid interpret causally instead hatb examine conditions important parts identifiable almost identifiable essentially sufficient conditions nongaussian independent noise influence present two estimation algorithms tailored towards conditions respectively evaluate synthetic realworld data discuss check model using\",\"consider problem learning causal directed acyclic graphs observational joint distribution one use graphs predict outcome interventional experiments data often available show observational distribution follows structural equation model additive noise structure directed acyclic graph becomes identifiable distribution mild conditions constitutes interesting alternative traditional methods assume faithfulness identify markov equivalence class graph thus leaving edges undirected provide practical algorithms finitely many samples resit regression subsequent independence test two methods based independence score prove resit correct population setting provide empirical evaluation\",\"structural equation models bayesian networks widely used analyze causal relations continuous variables frameworks linear acyclic models typically used model datagenerating process variables recently shown use nongaussianity identifies full structure linear acyclic model causal ordering variables connection strengths without using prior knowledge network structure case conventional methods however existing estimation methods based iterative search algorithms may converge correct solution finite number steps paper propose new direct method estimate causal ordering connection strengths based nongaussianity contrast previous methods algorithm requires algorithmic parameters guaranteed converge right solution within small fixed number steps data strictly follows model\",\"discovery nonlinear causal relationship additive nongaussian noise models attracted considerable attention recently high flexibility paper propose novel causal inference algorithm called leastsquares independence regression lsir lsir learns additive noise model minimization estimator squaredloss mutual information inputs residuals notable advantage lsir existing approaches tuning parameters kernel width regularization parameter naturally optimized crossvalidation allowing avoid overfitting datadependent fashion experiments realworld datasets show lsir compares favorably stateoftheart causal inference method\",\"consider learn causal ordering variables linear nongaussian acyclic model called lingam several existing methods shown consistently estimate causal ordering assuming model assumptions correct estimation results could distorted assumptions actually violated paper propose new algorithm learning causal orders robust one typical violation model assumptions latent confounders demonstrate effectiveness method using artificial data\",\"present new methods estimate causal effects retrospectively micro data assistance machine learning ensemble approach overcomes two important limitations conventional methods like regression modeling matching ambiguity pertinent retrospective counterfactuals potential misspecification overfitting otherwise biasprone inefficient use large identifying covariate set estimation causal effects method targets analysis toward well defined retrospective intervention effect rie based hypothetical population interventions applies machine learning ensemble allows data guide controlled fashion use large identifying covariate set illustrate analysis policy options reducing excombatant recidivism colombia\",\"linear nongaussian structural equation model called lingam identifiable model exploratory causal analysis previous methods estimate causal ordering variables connection strengths based single dataset however many application domains data obtained different conditions multiple datasets obtained rather single dataset paper present new method jointly estimate multiple lingams assumption models share causal ordering may different connection strengths differently distributed variables simulations new method estimates models accurately estimating separately\",\"consider problem structure learning bowfree acyclic path diagrams baps baps viewed generalization linear gaussian dag models allow certain hidden variables present first method problem using greedy scorebased search algorithm also prove necessary sufficient conditions distributional equivalence baps used algorithmic proach compute nearly equivalent model structures allows infer lower bounds causal effects also present applications real simulated datasets using publicly available rpackage\",\"describe method inferring linear causal relations among multidimensional variables idea use asymmetry distributions cause effect occurs covariance matrix cause structure matrix mapping cause effect independently chosen method works stochastic deterministic causal relations provided dimensionality sufficiently high experiments enough applicable gaussian well nongaussian data\",\"causal inference deals identifying random variables cause control random variables recent advances topic causal inference based tools statistical estimation machine learning resulted practical algorithms causal inference causal inference potential significant impact medical research prevention control diseases identifying factors impact economic changes name however promising applications causal inference often ones involve sensitive personal data users need kept private medical records personal finances etc therefore need development causal inference methods preserve data privacy study problem inferring causality using current popular causal inference framework additive noise model anm simultaneously ensuring privacy users framework provides differential privacy guarantees variety anm variants run extensive experiments demonstrate techniques practical easy implement\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"17_causal_variables_data\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"17_causal_variables_data\"],\"textfont\":{\"size\":12},\"x\":[6.605995,6.7515874,6.5365243,6.567575,6.5165586,6.7223296,6.670933,6.743001,6.6444345,7.3253064,6.5211267,6.5937963,9.923473,6.6253424,6.514673,6.6748695,6.650139,6.6940975,6.7660003,6.618435,6.51536,6.6934233,6.5249934,6.6759176,6.5941896,6.879736,6.6256633,6.717949,7.200713,6.7328405,6.721694,6.694285,6.496603,6.514813,7.008528,6.6284547,6.508073,6.530575,6.595183,6.5844173,6.7710214,6.5665236,6.4713345,6.502218,6.638747,6.734655],\"y\":[5.597753,5.632411,5.553867,5.7198777,5.5684156,5.8361855,5.7956834,5.616006,5.5633245,6.2313495,5.622189,5.729147,3.3456721,5.738876,5.6695986,5.718248,5.728961,5.831621,5.7991104,5.7971973,5.6745863,5.727923,5.6824155,5.7292957,5.7821403,5.6346974,5.758352,5.8530216,6.163593,5.826803,5.8658137,5.7717853,5.646856,5.5843616,6.0387425,5.747485,5.576509,5.7297387,5.739399,5.7966475,5.876622,5.7567286,5.568367,5.7085776,5.76814,5.691202],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"proceedings nips workshop machine learning developing world held long beach california usa december\",\"lactate threshold considered essential parameter assessing performance elite recreational runners prescribing training intensities endurance sports however measurement blood lactate concentration requires expensive equipment extraction blood samples inconvenient frequent monitoring furthermore recreational runners access routine assessment physical fitness aforementioned equipment able calculate lactate threshold without resorting expensive specialized centre therefore main objective study create intelligent system capable estimating lactate threshold recreational athletes participating endurance running sports solution proposed based machine learning system models lactate evolution using recurrent neural networks includes proposal standardization temporal axis well modification stratified sampling method results show proposed system accurately estimates lactate threshold athletes correlation experimentally measured lactate threshold high moreover behaviour test dataset good training set meaning generalization power model high therefore study machine learning based system proposed alternative traditional invasive lactate threshold measurement tests recreational runners\",\"introduce semiparametric bayesian model survival analysis model centred parametric baseline hazard uses gaussian process model variations away nonparametrically well dependence covariates opposed many methods survival analysis framework impose unnecessary constraints hazard rate survival function furthermore model handles left right interval censoring mechanisms common survival analysis propose mcmc algorithm perform inference approximation scheme based random fourier features make computations faster report experimental results synthetic real data showing model performs better competing models cox proportional hazards anovaddp random survival forests\",\"mobile technologies offer opportunities higher resolution monitoring health conditions opportunity seems particular promise psychiatry diagnoses often rely retrospective subjective recall mood states however getting actionable information rather complex time series challenging present implications clinical care largely hypothetical research demonstrates well chosen cohorts bipolar disorder borderline personality disorder control modern methods possible objectively learn identify distinctive behaviour short periods reports effectively separate cohorts participants bipolar disorder borderline personality disorder healthy volunteers completed daily mood ratings using bespoke smartphone app year signaturebased machine learning model used classify participants basis interrelationship different mood items assessed predict subsequent mood signature methodology significantly superior earlier statistical approaches applied data distinguishing participant three groups clearly placing original groups basis reports subsequent mood ratings correctly predicted greater accuracy groups prediction mood accurate healthy volunteers compared bipolar disorder borderline personality disorder\",\"daytime hypoglycemia accurately predicted achieve normoglycemia avoid disastrous situations hypoglycemia abnormally low blood glucose level divided daytime hypoglycemia nocturnal hypoglycemia many studies hypoglycemia prevention deal nocturnal hypoglycemia paper propose new predictor variables predict daytime hypoglycemia using continuous glucose monitoring cgm data apply classification regression tree cart prediction method independent variables prediction model rate decrease peak absolute level decision point evaluation results showed model able detect almost hypoglycemic events min advance higher existing methods similar conditions proposed method might achieve realtime prediction well embedded monitoring device\",\"date instability prognostic predictors sparse high dimensional model hinders clinical adoption received little attention stable prediction often overlooked favour performance yet stability prevails key adopting models critical areas healthcare study proposes stabilization scheme detecting higher order feature correlations using linear model basis prediction achieve feature stability regularising latent correlation features latent higher order correlation among features modelled using autoencoder network stability enhanced combining recent technique uses feature graph augmenting external unlabelled data training autoencoder network experiments conducted heart failure cohort australian hospital stability measured using consistency index feature subsets signaltonoise ratio model parameters methods demonstrated significant improvement feature stability model estimation stability compared baselines\",\"random survival forests rsf powerful method risk prediction rightcensored outcomes biomedical research rsf use logrank split criterion form ensemble survival trees common approach evaluate prediction accuracy rsf model harrells concordance index survival data index conceptually strategy implies split criterion rsf different evaluation criterion interest discrepancy overcome using harrells node splitting evaluation compare difference two split criteria analytically simulation studies respect preference unbalanced splits termed endcut preference ecp specifically show logrank statistic stronger ecp compared index simulation studies help two medical data sets demonstrate accuracy rsf predictions measured harrells improved logrank statistic replaced index node splitting especially true situations censoring rate fraction informative continuous predictor variables high conversely logrank splitting preferable noisy scenarios cbased logrank splitting implemented rpackage ranger recommend harrells split criterion use smaller scale clinical studies logrank split criterion use largescale omics studies\",\"biological systems often modelled different levels abstraction depending particular aimsresources study different models often provide qualitatively concordant predictions specific parametrisations generally unclear whether model predictions quantitatively agreement whether agreement holds different parametrisations present generally applicable statistical machine learning methodology automatically reconcile predictions different models across abstraction levels approach based defining correction map random function modifies output model order match statistics output different model system use two biological examples give proofofprinciple demonstration methodology discuss advantages potential applications\",\"many complex diseases wide variety ways individual manifest disease challenge personalized medicine develop tools accurately predict trajectory individuals disease turn enable clinicians optimize treatments represent individuals disease trajectory continuousvalued continuoustime function describing severity disease time propose hierarchical latent variable model individualizes predictions disease trajectories model shares statistical strength across observations different resolutionsthe population subpopulation individual level describe algorithm learning population subpopulation parameters offline online procedure dynamically learning individualspecific parameters finally validate model task predicting course interstitial lung disease leading cause death among patients autoimmune disease scleroderma compare approach stateoftheart demonstrate significant improvements predictive accuracy\",\"vision precision medicine use individual patient characteristics inform personalized treatment plan leads best healthcare possible patient mobile technologies important role play vision offer means monitor patients health status realtime subsequently deliver interventions dose needed dynamic treatment regimes formalize individualized treatment plans sequences decision rules one per stage clinical intervention map current patient information recommended treatment however existing methods estimating optimal dynamic treatment regimes designed small number fixed decision points occurring coarse timescale propose new reinforcement learning method estimating optimal treatment regime applicable data collected using mobile technologies outpatient setting proposed method accommodates indefinite time horizon minutebyminute decision making common mobile health applications show proposed estimators consistent asymptotically normal mild conditions proposed methods applied estimate optimal dynamic treatment regime controlling blood glucose levels patients type diabetes\",\"research interpretability machine learning systems focuses development rigorous notion interpretability suggest better understanding deficiencies intuitive notion interpretability needed well show visualization enables also impedes intuitive interpretability presupposes two levels technical preinterpretation dimensionality reduction regularization furthermore argue use positive concepts emulate distributed semantic structure machine learning models introduces significant human bias model consequence suggest intuitive interpretability needed singular representations internal model states avoided\",\"accurate reliable predictions infectious disease dynamics valuable public health organizations plan interventions decrease prevent disease transmission great variety models developed task using different model structures covariates targets prediction experience shown performance models varies tend better worse different seasons different points within season ensemble methods combine multiple models obtain single prediction leverages strengths model considered range ensemble methods form predictive density target interest weighted sum predictive densities component models simplest case equal weight assigned component model complex case weights vary region prediction target week season predictions made measure component model uncertainty recent observations disease incidence applied methods predict measures influenza season timing severity united states national regional levels using three component models trained models retrospective predictions seasons evaluated models prospective outofsample performance five subsequent influenza seasons test phase ensemble methods showed overall performance similar best component models offered consistent performance across seasons component models ensemble methods offer potential deliver reliable predictions public health decision makers\",\"assessing heterogeneous treatment effects become growing interest advancing precision medicine individualized treatment effects ite play critical role endeavor concerning experimental data collected randomized trials put forward method termed random forests interaction trees rfit estimating ite basis interaction trees end first propose smooth sigmoid surrogate sss method alternative greedy search speed tree construction rfit outperforms traditional separate regression approach estimating ite furthermore standard errors estimated ite via rfit obtained infinitesimal jackknife method assess illustrate use rfit via simulation analysis data acupuncture headache trial\",\"scenario realtime monitoring hospital patients highquality inference patients health status using information available clinical covariates lab tests essential enable successful medical interventions improve patient outcomes developing computational framework learn observational largescale electronic health records ehrs make accurate realtime predictions critical step work develop explore bayesian nonparametric model based gaussian process regression hospital patient monitoring propose medgp statistical framework incorporates clinical lab covariates supports rich reference data set relationships observed covariates may inferred exploited highquality inference patient state time develop highly structured sparse kernel enable tractable computation tens thousands time points estimating correlations among clinical covariates patients periodicity patient observations medgp number benefits current methods including requiring alignment time series data quantifying confidence regions predictions iii exploiting vast rich database patients inferring interpretable relationships among clinical covariates evaluate compare results medgp task online prediction three patient subgroups two medical data sets across patients found medgp improves online prediction baseline methods nearly covariates across different disease subgroups studies publicly available code httpsgithubcombeehivemedgp\",\"inpatient care large share total health care spending making analysis inpatient utilization patterns important part understanding drives health care spending growth common features inpatient utilization measures include zero inflation overdispersion skewness complicate statistical modeling mixture modeling popular approach accommodate features health care utilization data work add nonparametric clustering component models fully bayesian model framework allows unknown number mixing components data determine number mixture components apply modeling framework data hospital lengths stay patients lung cancer find distinct subgroups patients differences means variances hospital days health treatment covariates relationships covariates length stay\",\"decision makers doctors judges make crucial decisions recommending treatments patients granting bails defendants daily basis decisions typically involve weighting potential benefits taking action costs involved work aim automate task learning costeffective interpretable actionable treatment regimes formulate problem learning decision list sequence ifthenelse rules maps characteristics subjects diagnostic test results patients treatments propose novel objective construct decision list maximizes outcomes population minimizes overall costs model problem learning list markov decision process mdp employ variant upper confidence bound trees uct strategy leverages customized checks pruning search space effectively experimental results real world observational data capturing treatment recommendations asthma patients demonstrate effectiveness approach\",\"technique formal concept analysis applied dataset describing traits rodents goal identifying zoonotic disease carriersor species carrying infections spillover cause human disease concepts identified among species together provide rulesofthumb intrinsic biological features rodents carry zoonotic diseases offer utility better targeting field surveillance efforts search novel disease carriers wild\",\"consider problem constructing diffusion operators high dimensional data address counterfactual functions individualized treatment effectiveness propose construct new diffusion metric captures local geometry directions variance resulting diffusion metric used define localized filtration answer counterfactual questions pointwise particularly situations drug trials individual patients outcomes cannot studied long term taking taking medication validate model synthetic real world clinical trials create individualized notions benefit treatment\",\"recurrent major mood episodes subsyndromal mood instability cause substantial disability patients bipolar disorder early identification mood episodes enabling timely mood stabilisation important clinical goal recent technological advances allow prospective reporting mood real time enabling accurate efficient data capture complex nature data streams combination challenge deriving meaning missing data mean pose significant analytic challenge signature method derived stochastic analysis ability capture important properties complex ordered time series data explore whether onset episodes mania depression identified using selfreported mood data\",\"quick accurate medical diagnosis crucial successful treatment disease using machine learning algorithms built two models predict hematologic disease based laboratory blood test results one predictive model used available blood test parameters reduced set usually measured upon patient admittance models produced good results prediction accuracy considering list five probable diseases considering probable disease models differ significantly indicates reduced set parameters contains relevant fingerprint disease expanding utility model general practitioners use indicating information blood test results physicians recognize clinical test showed accuracy predictive models par ability hematology specialists study first show machine learning predictive model based blood tests alone successfully applied predict hematologic diseases could open unprecedented possibilities medical diagnosis\",\"estimation individual treatment effect observational data complicated due challenges confounding selection bias useful inferential framework address counterfactual potential outcomes model takes hypothetical stance asking individual received treatments making use random forests within counterfactual framework estimate individual treatment effects directly modeling response find accurate estimation individual treatment effects possible even complex heterogeneous settings type approach plays important role accuracy methods designed adaptive confounding used parallel outofsample estimation best one method found especially promising counterfactual synthetic forests illustrate new methodology applying large comparative effectiveness trial project aware order explore role drug use plays sexual risk analysis reveals important connections risky behavior drug usage sexual risk\",\"realizations stochastic process often observed temporal data functional data growing interests classification dynamic functional data basic feature functional data functional data infinite dimensions highly correlated essential issue classifying dynamic functional data effectively reduce dimension explore dynamic feature however statistical methods dynamic data classification directly used rich dynamic features data propose use second order ordinary differential equation ode model dynamic process principal differential analysis estimate constant timevarying parameters ode examine differential dynamic properties dynamic system across different conditions including stability transientresponse determine dynamic systems maintain functions performance broad range random internal external perturbations use parameters ode features classifiers proof principle proposed methods applied classifying normal abnormal qrs complexes electrocardiogram ecg data analysis great clinical values diagnosis cardiovascular diseases show odebased classification methods qrs complex classification outperform currently widely used neural networks fourier expansion coefficients functional data features expect dynamic modelbased classification methods may open new avenue functional data classification\",\"develop model using deep learning techniques natural language processing unstructured text medical records predict hospitalwide day unplanned readmission cstatistic model constructed allow physicians interpret significant features prediction\",\"predicting individual risk clinical event using complete patient history still major challenge personalized medicine among methods developed compute individual dynamic predictions joint models assets using available information accounting dropout however restricted small number longitudinal predictors objective propose innovative alternative solution predict event probability using possibly large number longitudinal predictors developed dynforest extension competingrisk random survival forests handles endogenous longitudinal predictors node tree timedependent predictors translated timefixed features using mixed models used candidates splitting subjects two subgroups individual event probability estimated tree aalenjohansen estimator leaf subject classified according hisher history predictors final individual prediction given average treespecific individual event probabilities carried simulation study demonstrate performances dynforest small dimensional context comparison joint models large dimensional context comparison regression calibration method ignores informative dropout also applied dynforest predict individual probability dementia elderly according repeated measures cognitive functional vascular neurodegeneration markers quantify importance type markers prediction dementia implemented package dynforest methodology provides novel appropriate solution prediction events number longitudinal endogenous predictors\",\"healthcare applications temporal variables encode movement health status longitudinal patient evolution often accompanied rich structured information demographics diagnostics medical exam data however current methods jointly optimize structured covariates time series feature extraction process present shortfuse method boosts accuracy deep learning models time series explicitly modeling temporal interactions dependencies structured covariates shortfuse introduces hybrid convolutional lstm cells incorporate covariates via weights shared across temporal domain shortfuse outperforms competing models two biomedical applications forecasting osteoarthritisrelated cartilage degeneration predicting surgical outcomes cerebral palsy patients matching exceeding accuracy models use features engineered domain experts\",\"paper presents systematic review stateoftheart approaches identify patient cohorts using electronic health records gives comprehensive overview commonly detected phenotypes underlying data sets special attention given preprocessing input data different modeling approaches literature review confirms natural language processing promising approach electronic phenotyping however accessibility lack natural language process standards medical texts remain challenge future research develop standards investigate machine learning approaches best suited type medical data\",\"growing interest applying machine learning methods electronic medical records emr across different institutions however emr quality vary widely work investigated impact disparity performance three advanced machine learning algorithms logistic regression multilayer perceptron recurrent neural network emr disparity emulated using different permutations emr collected childrens hospital los angeles chla pediatric intensive care unit picu cardiothoracic intensive care unit cticu algorithms trained using patients picu predict inicu mortality patients held set picu cticu patients disparate patient populations picu cticu provide estimate generalization errors across different icus quantified evaluated generalization algorithms varying emr size input types fidelity data\",\"disease classification crucial element biomedical research recent studies demonstrated machine learning techniques support vector machine svm modeling produce similar improved predictive capabilities comparison traditional method logistic regression addition found social network metrics provide useful predictive information disease modeling study combine simulated social network metrics svm predict diabetes sample data behavioral risk factor surveillance system dataset logistic regression outperformed svm roc index models without graph metrics respectively svm polynomial kernel roc index models without graph metrics respectively although perform well logistic regression results consistent previous studies utilizing svm classify diabetes\",\"proceedings nips symposium interpretable machine learning held long beach california usa december\",\"given two possible treatments may exist subgroups benefit greater one treatment problem relevant field marketing treatments may correspond different ways selling product similarly relevant field public policy treatments may correspond specific government programs finally personalized medicine field wholly devoted understanding subgroups individuals benefit particular medical treatments present computationally fast treebased method abtree treatment effect differentiation unlike methods abtree specifically produces decision rules optimal treatment assignment perindividual basis treatment choices selected maximizing overall occurrence desired binary outcome conditional set covariates poster present methodology tree growth pruning show performance results applied simulated data well real data\",\"introduce mixture model censored durations cmix develop maximum likelihood inference joint estimation time distributions latent regression parameters model consider highdimensional setting datasets containing large number biomedical covariates therefore penalize negative loglikelihood elasticnet leads sparse parameterization model inference achieved using efficient quasinewton expectation maximization qnem algorithm provide convergence properties propose score assessing patients risk early adverse event statistical performance method examined extensive monte carlo simulation study finally illustrated three genetic datasets highdimensional covariates show approach outperforms stateoftheart namely cure cox proportional hazards models task terms cindex auct\",\"tremendous interest precision medicine means improve patient outcomes tailoring treatment individual characteristics individualized treatment rule formalizes precision medicine map patient information recommended treatment treatment rule defined optimal maximizes mean scalar outcome population interest symptom reduction however clinical intervention scientists often must balance multiple possibly competing outcomes symptom reduction risk adverse event one approach precision medicine setting elicit composite outcome balances competing outcomes unfortunately eliciting composite outcome directly patients difficult without highquality instrument expertderived composite outcome may account heterogeneity patient preferences propose new paradigm study precision medicine using observational data relies solely assumption clinicians approximately imperfectly making decisions maximize individual patient utility estimated composite outcomes subsequently used construct estimator individualized treatment rule maximizes mean patientspecific composite outcomes estimated composite outcomes estimated optimal individualized treatment rule provide new insights patient preference heterogeneity clinician behavior value precision medicine given domain derive inference procedures proposed estimators mild conditions demonstrate finite sample performance suite simulation experiments illustrative application data study bipolar depression\",\"communitybased question answering cqa sites play important role addressing health information needs however significant number posted questions remain unanswered automatically answering posted questions provide useful source information online health communities study developed algorithm automatically answer healthrelated questions based past questions answers also aimed understand information embedded within online health content good features identifying valid answers proposed algorithm uses information retrieval techniques identify candidate answers resolved order rank candidates implemented semisupervised leaning algorithm extracts best answer question assessed approach curated corpus yahoo answers compared rulebased string similarity baseline dataset semisupervised learning algorithm accuracy umlsbased healthrelated features used model enhance algorithms performance proximately reasonably high rate accuracy obtained given data considerably noisy important features distinguishing valid answer invalid answer include text length number stop words contained test question distance test question questions corpus well number overlapping healthrelated terms questions overall automated system based historical pairs shown effective according data set case study developed general use health care domain also applied cqa sites\",\"prediction disease onset patient survey lifestyle data quickly becoming important tool diagnosing disease progresses study data national health nutrition examination survey nhanes questionnaire used predict onset type diabetes ensemble model using output five classification algorithms developed predict onset diabetes based features ensemble model auc indicating high performance\",\"preterm births occur alarming rate preemies higher risk infant mortality developmental retardation longterm disabilities predicting preterm birth difficult even experienced clinicians welldesigned clinical study thus far reaches modest sensitivity specificity take different approach exploiting databases normal hospital operations aims twofold derive easytouse interpretable prediction rule quantified uncertainties construct accurate classifiers preterm birth prediction approach automatically generate select hundreds thousands possible predictors using stabilityaware techniques derived large database women simplified prediction rule items sensitivity specificity\",\"predicting individuals risk experiencing future clinical outcome statistical task important consequences practicing clinicians public health experts modern observational databases electronic health records ehrs provide alternative longitudinal cohort studies traditionally used construct risk models bringing opportunities challenges large sample sizes detailed covariate histories enable use sophisticated machine learning techniques uncover complex associations interactions observational databases often messy high levels missing data incomplete patient followup paper propose adaptation wellknown naive bayes machine learning approach classification timetoevent outcomes subject censoring compare predictive performance method cox proportional hazards model commonly used risk prediction healthcare populations illustrate application prediction cardiovascular risk using ehr dataset large midwest integrated healthcare system\",\"datasets containing large samples timetoevent data arising several small heterogeneous groups commonly encountered statistics presents problems cannot pooled directly due heterogeneity analyzed individually small sample size bayesian nonparametric modelling approaches used model datasets given ability flexibly share information across groups paper compare three popular bayesian nonparametric methods modelling survival functions heterogeneous groups specifically first compare modelling accuracy dirichlet process hierarchical dirichlet process nested dirichlet process simulated datasets different sizes group survival curves differ shape expectation compare models realworld injury dataset\",\"given functional data survival process timedependent covariates derive smooth convex representation nonparametric loglikelihood functional obtain functional gradient devise generic gradient boosting procedure estimating hazard function nonparametrically illustrative implementation procedure using regression trees described show recover unknown hazard generic estimator consistent model correctly specified alternatively oracle inequality demonstrated treebased models avoid overfitting boosting employs several regularization devices one stepsize restriction rationale somewhat mysterious viewpoint consistency work brings clarity issue revealing stepsize restriction mechanism preventing curvature risk derailing convergence\",\"devising course treatment patient doctors often little quantitative evidence base decisions beyond medical education published clinical trials stanford health care alone millions electronic medical records emrs recently leveraged inform better treatment recommendations data present unique challenge highdimensional observational goal make personalized treatment recommendations based outcomes past patients similar new patient propose analyze three methods estimating heterogeneous treatment effects using observational data methods perform well simulations using wide variety treatment effect functions present results applying two promising methods data sprint data analysis challenge large randomized trial treatment high blood pressure\",\"calls arms build interpretable models express wellfounded discomfort machine learning software agent even know loan decide qualifies one indeed ought cautious injecting machine learning anything else matter applications may significant risk causing social harm however claims stakeholders wont accept provide sufficient foundation proposed field study field interpretable machine learning advance must ask following questions precisely wont various stakeholders accept want desiderata reasonable feasible order answer questions well give realworld problems respective stakeholders greater consideration\",\"proceedings nips workshop interpretable machine learning complex systems held barcelona spain december\",\"patent lawsuits costly timeconsuming ability forecast patent litigation time litigation allows companies better allocate budget time managing patent portfolios develop predictive models estimating likelihood litigation patents expected time litigation based textual nontextual features work focuses improving stateoftheart relying different set features employing sophisticated algorithms realistic data rate patent litigations low consequently makes problem difficult initial model predicting likelihood modified capture timetolitigation perspective\",\"kernel induced random survival forests kirsf statistical learning algorithm aims improve prediction accuracy survival data random survival forests rsf cumulative hazard function predicted individual test set prediction error estimated using harrells concordance index index harrell cindex interpreted misclassification probability depend single fixed time evaluation cindex also specifically accounts censoring utilizing kernel functions kirsf achieves better results rsf many situations report show incorporate kernel functions rsf test performance kirsf compare method rsf find kirsfs performance better rsf many occasions\",\"portable wearable wireless electrocardiogram ecg systems potential used pointofcare cardiovascular disease diagnostic systems wearable wireless ecg systems require automatic detection cardiovascular disease even primary care automation ecg diagnostic systems improve efficiency ecg diagnosis reduce minimal training requirement local healthcare workers however fully automatic myocardial infarction disease detection algorithms well developed paper presents novel automatic classification algorithm using second order ordinary differential equation ode time varying coefficients simultaneously captures morphological dynamic feature highly correlated ecg signals effectively estimating unobserved state variables parameters second order ode accuracy classification significantly improved estimated time varying coefficients second order ode used input support vector machine svm classification proposed method applied ptb diagnostic ecg database within physionet overall sensitivity specificity classification accuracy lead ecgs binary classifications respectively also found even using one lead ecg signals reach accuracy high multiclass classification challenging task developed ode approach lead ecgs coupled multiclass svm reached accuracy classifying subgroups healthy controls\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"18_treatment_disease_patient\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"18_treatment_disease_patient\"],\"textfont\":{\"size\":12},\"x\":[8.241926,8.449055,8.193266,8.187321,8.3742285,8.297738,8.107808,8.367227,8.2096815,7.593313,8.179236,8.206224,7.4228086,8.098041,8.168519,7.617368,8.562187,7.537925,8.260875,8.362461,7.4094753,8.465866,8.124124,8.103337,8.111753,8.108681,8.103291,8.589196,8.205217,7.476264,8.144588,7.553987,8.099584,8.36627,8.168262,8.096833,8.15906,8.231061,7.539808,8.208558,8.189876,9.6860285,8.186198,8.484578,8.142025],\"y\":[7.375017,6.1236644,6.501663,6.0314965,6.2118254,6.188953,6.5708885,5.011122,6.35812,6.3118024,7.375587,6.405033,6.4162946,6.3322573,6.4713798,6.392109,6.328162,6.3208704,5.975542,6.2536435,6.412655,6.101818,6.312255,6.3838806,6.322697,6.3055096,6.2943754,6.343208,7.386678,6.4393396,6.426842,6.327364,6.3952208,6.2741537,6.4450283,6.3203974,6.531583,6.5276318,6.3471446,7.3891964,7.382423,9.485431,6.6802225,6.0544586,6.4964767],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"unsupervised image segmentation aims clustering set pixels image spatially homogeneous regions introduce class bayesian nonparametric models address problem models based combination pottslike spatial smoothness component prior partitions used control number size clusters class models flexible enough include standard potts model recent pottsdirichlet process model citeorbanz importantly prior partitions introduced control global clustering structure possible penalize small large clusters necessary bayesian computation carried using original generalized swendsenwang algorithm experiments demonstrate method competitive terms rand index compared popular image segmentation methods meanshift recent alternative bayesian nonparametric models\",\"heavytailed distributions widely used robust mixture modelling due possessing thick tails computationally tractable subclass stable distributions subgaussian alphastable distribution received much interest literature introduce type expectation maximization algorithm estimates parameters mixture subgaussian stable distributions comparative study presence wellknown mixture models performed show robustness performance mixture subgaussian alphastable distributions modelling simulated synthetic real data\",\"compare two statistical models three binary random variables one mixture model product mixtures model called restricted boltzmann machine although two models study look different parametrizations show represent set distributions interior probability simplex equal closure give semialgebraic description model terms six binomial inequalities obtain closed form expressions maximum likelihood estimates briefly discuss extensions larger models\",\"introduce pitman yor diffusion tree pydt hierarchical clustering generalization dirichlet diffusion tree neal removes restriction binary branching structure generative process described shown result exchangeable distribution data points prove theoretical properties model present two inference methods collapsed mcmc sampler allows model uncertainty tree structures computationally efficient greedy bayesian search algorithm algorithms use message passing tree structure utility model algorithms demonstrated synthetic real world data continuous binary\",\"introduce new bayesian model hierarchical clustering based prior trees called kingmans coalescent develop novel greedy sequential monte carlo inferences operate bottomup agglomerative fashion show experimentally superiority algorithms others demonstrate approach document clustering phylolinguistics\",\"neymanscott classic example estimation problem partiallyconsistent posterior standard estimation methods tend produce inconsistent results past attempts create consistent estimators neymanscott led adhoc solutions estimators satisfy representation invariance restrictions choice prior present simple construction generalpurpose bayes estimator invariant representation satisfies consistency neymanscott nondegenerate prior argue good attributes estimator due intrinsic properties generalise beyond neymanscott well\",\"propose bayesian nonparametric mixture model prediction information extraction tasks efficient inference scheme models categoricalvalued time series exhibit dynamics multiple underlying patterns user behavior traces simplify idea capturing patterns hierarchical hidden markov models hhmms extend existing approaches additional representation structural information empirical results based synthetic real world data indicate results easily interpretable model excels segmentation prediction performance successfully identifies generating patterns used effective prediction future observations\",\"recent years rank aggregation received significant attention machine learning community goal problem combine partially revealed preferences objects large population single relatively consistent ordering objects however many cases might want single ranking instead opt individual rankings study version problem known collaborative ranking problem assume individual users provide pairwise preferences example purchasing one item another preferences wish obtain rankings items users opportunity explore results interesting connection standard matrix completion problem provide theoretical justification nuclear norm regularized optimization procedure provide highdimensional scaling results show error estimating user preferences behaves number observations increase\",\"multivariate normal density monotonic function distance mean ellipsoidal shape due underlying euclidean metric suggest replace metric locally adaptive smoothly changing riemannian metric favors regions high local density resulting locally adaptive normal distribution land generalization normal distribution manifold setting data assumed lie near potentially lowdimensional manifold embedded mathbbrd land parametric depending mean covariance maximum entropy distribution given metric underlying metric however nonparametric develop maximum likelihood algorithm infer distribution parameters relies combination gradient descent monte carlo integration extend land mixture models provide corresponding algorithm demonstrate efficiency land fit nontrivial probability distributions synthetic data eeg measurements human sleep\",\"paper variable selection clustering estimation unsupervised highdimensional setting approach based fitting constrained gaussian mixture models learn number clusters set relevant variables using generalized bayesian posterior sparsity inducing prior prove sparsity oracle inequality shows procedure selects optimal parameters procedure implemented using metropolishastings algorithm based clusteringoriented greedy proposal makes convergence posterior fast\",\"motivated generating personalized recommendations using ordinal preference data study question learning mixture multinomial logit mnl model parameterized class distributions permutations partial ordinal preference data pairwise comparisons despite long standing importance across disciplines including social choice operations research revenue management little known question case single mnl models mixture computationally statistically tractable learning pairwise comparisons feasible however even learning mixture two mnl components infeasible general given state affairs seek conditions feasible learn mixture model computationally statistically efficient manner present sufficient condition well efficient algorithm learning mixed mnl models partial preferencescomparisons data particular mixture mnl components objects learnt using samples whose size scales polynomially concretely rnlog rll model parameters sufficiently incoherent algorithm two phases first learn pairwise marginals component using tensor decomposition second learn model parameters component using rank centrality introduced negahban process proving results obtain generalization existing analysis tensor decomposition realistic regime partial information sample available\",\"hierarchical learning models mixture models bayesian networks widely employed unsupervised learning tasks clustering analysis consist observable hidden variables represent given data hidden generation process respectively pointed conventional statistical analysis applicable models redundancy latent variable produces singularities parameter space recent years method based algebraic geometry allowed analyze accuracy predicting observable variables using bayesian estimation however analyze latent variables sufficiently studied even though one main issues unsupervised learning determine accurately latent variable estimated previous study proposed method used range latent variable redundant compared model generating data present paper extends method situation latent variables redundant dimensions formulate new error functions derive asymptotic forms calculation error functions demonstrated twolayered bayesian networks\",\"show kmeans lloyds algorithm obtained special case truncated variational approximations applied gaussian mixture models gmm isotropic gaussians contrast standard way relate kmeans gmms provided derivation shows required consider gaussians small variances limit case zero variances number consequences directly follow approach kmeans shown increase free energy associated truncated distributions free energy directly reformulated terms kmeans objective kmeans generalizations directly derived considering closest closest etc cluster addition closest one embedding kmeans free energy framework allows theoretical interpretations kmeans generalizations literature general truncated variational provides natural rigorous quantitative link kmeanslike clustering gmm clustering algorithms may relevant future theoretical empirical studies\",\"given set pairwise comparisons classical ranking problem computes single ranking best represents preferences users paper study problem inferring individual preferences arising context making personalized recommendations particular assume users types users type provide similar pairwise comparisons items according bradleyterry model propose efficient algorithm accurately estimates individual preferences almost users max nlog log pairwise comparisons per type near optimal sample complexity grows logarithmically algorithm three steps first user compute emphnetwin vector projection binommdimensional vector pairwise comparisons onto mdimensional linear subspace second cluster users based netwin vectors third estimate single preference cluster separately netwin vectors much less noisy high dimensional vectors pairwise comparisons clustering accurate projection confirmed numerical experiments moreover show cluster approximately correct maximum likelihood estimation bradleyterry model still close true preference\",\"bayesian nonparametrics class probabilistic models model size inferred data recently developed methodology field smallvariance asymptotic analysis mathematical technique deriving learning algorithms capture much flexibility bayesian nonparametric inference algorithms simpler implement less computationally expensive past work smallvariance analysis bayesian nonparametric inference algorithms exclusively considered batch models trained single static dataset incapable capturing time evolution latent structure data work presents smallvariance analysis maximum posteriori filtering problem temporally varying mixture model markov dependence structure captures temporally evolving clusters within dataset two clustering algorithms result analysis dmeans iterative clustering algorithm linearly separable spherical clusters sdmeans spectral clustering algorithm derived kernelized relaxed version clustering problem empirical results experiments demonstrate advantages using dmeans sdmeans contemporary clustering algorithms terms computational cost clustering accuracy\",\"work consider problem detecting anomalous spatiotemporal behavior videos approach learn normative multiframe pixel joint distribution detect deviations using likelihood based approach due extreme lack available training samples relative dimension distribution use mean covariance approach consider methods learning spatiotemporal covariance lowsample regime approach estimate covariance using parameter reduction sparse models first method considered representation covariance sum kronecker products greenewald found accurate approximation setting propose learning algorithms relevant problem consider sparse multiresolution model choi apply kronecker product methods parameter reduction well introducing modifications enhanced efficiency greater applicability spatiotemporal covariance matrices apply methods detection crowd behavior anomalies university minnesota crowd anomaly dataset achieve competitive results\",\"observations organized groups commonalties exist amongst dependent random measures ideal choice modeling one propositions dependent random measures atoms posterior distribution shared amongst groups hence groups borrow information normalized dependent random measures prior independent increments applied derive appropriate exchangeable probability partition function eppf subsequently also deduce inference algorithm given mixture model likelihood provide necessary derivation solution framework demonstration used mixture gaussians likelihood combination dependent structure constructed linear combinations crms experiments show superior performance using framework inferred values including mixing weights number clusters respond appropriately number completely random measure used\",\"hierarchical probabilistic models mixture models used cluster analysis models two types variables observable latent cluster analysis latent variable estimated expected additional information improve accuracy estimation latent variable many proposed learning methods able use additional data include semisupervised learning transfer learning however statistical point view complex probabilistic model encompasses initial additional data might less accurate due higherdimensional parameter present paper presents theoretical analysis accuracy model clarifies factor greatest effect accuracy advantages obtaining additional data disadvantages increasing complexity\",\"hierarchical probabilistic models gaussian mixture models widely used unsupervised learning tasks models consist observable latent variables represent observable data underlying datageneration process respectively unsupervised learning tasks cluster analysis regarded estimations latent variables based observable ones estimation latent variables semisupervised learning labels observed precise unsupervised one concerns clarify effect labeled data however sufficient theoretical analysis accuracy estimation latent variables previous study distributionbased error function formulated asymptotic form calculated unsupervised learning generative models shown estimation latent variables bayes method accurate maximumlikelihood method present paper reveals asymptotic forms error function bayesian semisupervised learning discriminative generative models results show generative model uses given data performs better model well specified\",\"paper generalized multivariate studentt mixture model developed classification clustering low probability intercept radar waveforms low probability intercept radar signal characterized pulse compression waveform either frequencymodulated phasemodulated proposed model classify cluster different modulation types linear frequency modulation non linear frequency modulation polyphase barker polyphase frank zadoff codes classification method focuses introduction new prior distribution model hyperparameters gives possibility handle sensitivity mixture models initialization allow less restrictive modeling data inference processed variational bayes method bayesian treatment adopted model learning supervised classification clustering moreover novel prior distribution wellknown probability distribution deterministic stochastic methods employed estimate expectations numerical experiments show proposed method less sensitive initialization provides accurate results previous state art mixture models\",\"classical mixture gaussians model related kmeans via smallvariance asymptotics covariances gaussians tend zero negative loglikelihood mixture gaussians model approaches kmeans objective algorithm approaches kmeans algorithm kulis jordan used observation obtain novel kmeanslike algorithm gibbs sampler dirichlet process mixture instead consider applying smallvariance asymptotics directly posterior bayesian nonparametric models framework independent specific bayesian inference algorithm major advantage generalizes immediately range models beyond mixture illustrate apply framework feature learning setting beta process indian buffet process provide appropriate bayesian nonparametric prior obtain novel objective function goes beyond clustering learn penalize new groupings relax mutual exclusivity exhaustivity assumptions clustering demonstrate several algorithms scalable simple implement empirical results demonstrate benefits new framework\",\"present convex approach probabilistic segmentation modeling time series data approach builds upon recent advances multivariate total variation regularization seeks learn separate set parameters distribution observations time point additional penalty encourages parameters remain constant time propose efficient optimization methods solving resulting large optimization problems twostage procedure estimating recurring clusters models based upon kernel density estimation finally show number realworld segmentation tasks resulting methods often perform well better existing latent variable models substantially easier train\",\"propose probabilistic model aggregate answers respondents answering multiplechoice questions model assume everyone access information assume consensus answer correct instead infers probable world state even minority vote respondent modeled receiving signal contingent actual world state using signal determine answer predict answers given others incorporating respondents predictions others answers model infers latent parameters corresponding prior world states probability different signals received possible world states including counterfactual ones unlike probabilistic models aggregation model applies single multiple questions case estimates respondents expertise model shows good performance compared number probabilistic models data seven studies covering different types expertise\",\"gaussian mixture models gmm found many applications density estimation data clustering however model adapt well curved strongly nonlinear data recently appeared improvement called acagmm active curve axis gaussian mixture model fits gaussians along curves using emlike expectation maximization approach using ideas standing behind acagmm build alternative active function model clustering advantages acagmm particular naturally defined arbitrary dimensions enables easy adaptation clustering complicated datasets along predefined family functions moreover need external methods determine number clusters automatically reduces number groups online\",\"define beta diffusion tree random tree structure set leaves defines collection overlapping subsets objects known feature allocation generative process tree structure defined terms particles representing objects diffusing continuous space analogously dirichlet diffusion tree neal defines tree structure partitions nonoverlapping subsets objects unlike dirichlet diffusion tree multiple copies particle may exist diffuse along multiple branches beta diffusion tree object may therefore belong multiple subsets particles demonstrate build hierarchicallyclustered factor analysis model beta diffusion tree perform inference random tree structures markov chain monte carlo algorithm conclude several numerical experiments missing data problems data sets gene expression microarrays international development statistics intranational socioeconomic measurements\",\"analyzing underlying structure multiple timesequences provides insights understanding social networks human activities work present emphbayesian nonparametric poisson process allocation banppa latentfunction model timesequences automatically infers number latent functions model intensity sequence infinite mixture latent functions obtained using function drawn gaussian process show technical challenge inference mixture models unidentifiability weights latent functions propose cope issue regulating volume latent function within variational inference algorithm algorithm computationally efficient scales well large data sets demonstrate usefulness proposed model experiments synthetic realworld data sets\",\"present new sequential monte carlo sampler coalescent based bayesian hierarchical clustering model appropriate modeling noniid data offers substantial reduction computational cost compared original sampler without resorting approximations also propose quadratic complexity approximation practice shows almost loss performance compared counterpart show byproduct formulation obtain greedy algorithm exhibits performance improvement greedy algorithms particularly small data sets order exploit correlation structure data describe incorporate gaussian process priors model flexible way model noniid data results artificial real data show significant improvements closely related approaches\",\"reciprocating interactions represent central feature human exchanges target various recent experiments healthy participants psychiatric populations engaging dyads multiround exchanges repeated trust task behaviour exchanges involves complexities related agents preference equity partner beliefs partners appetite equity beliefs partners model partner agents may also plan different numbers steps future providing computationally precise account behaviour essential step towards understanding underlies choices natural framework interactive partially observable markov decision process ipomdp however various complexities make ipomdps inordinately computationally challenging show approximate solution multiround trust task using variant montecarlo tree search algorithm demonstrate algorithm efficient effective therefore used invert observations behavioural choices use generated behaviour elucidate richness sophistication interactive inference\",\"paper considers statistical estimation problems probability distribution observed random variable invariant respect actions finite topological group shown distribution must satisfy restricted finite mixture representation specialized case distributions sphere invariant actions finite spherical symmetry group mathcal groupinvariant extension von mises fisher vmf distribution obtained mathcal ginvariant vmf parameterized location scale parameters specify distributions mean orientation concentration mean respectively using restricted finite mixture representation parameters estimated using expectation maximization maximum likelihood estimation algorithm illustrated problem mean crystal orientation estimation spherically symmetric group associated crystal form cubic octahedral hexahedral simulations experiments establish advantages extended vmf emml estimator data acquired electron backscatter diffraction ebsd microscopy polycrystalline nickel alloy sample\",\"hierarchical parametric models consisting observable latent variables widely used unsupervised learning tasks example mixture model representative hierarchical model clustering statistical point view models regular singular due distribution data regular case models identifiability onetoone relation probability density function model expression parameter fisher information matrix positive definite estimation accuracy observable latent variables studied singular case hand models identifiable fisher matrix positive definite conventional statistical analysis based inverse fisher matrix applicable recently algebraic geometrical analysis developed used elucidate bayes estimation observable variables present paper applies analysis latentvariable estimation determines theoretical performance results clarify behavior convergence posterior distribution found posterior observablevariable estimation different one latentvariable estimation difference markov chain monte carlo method based parameter latent variable cannot construct desired posterior distribution\",\"propose probabilistic modeling framework learning dynamic patterns collective behaviors social agents developing profiles different behavioral groups using data collected multiple information sources proposed model based hierarchical bayesian process observation finite mixture set latent groups mixture proportions group probabilities drawn randomly group associated distributions finite set outcomes moreover time evolves structure groups also changes model change group structure hidden markov model hmm fixed transition probability present efficient inference method based tensor decompositions expectationmaximization algorithm parameter estimation\",\"paper scale mixture normal distributions model developed classification clustering data outliers missing values classification method based mixture model focuses introduction latent variables gives possibility handle sensitivity model outliers allow less restrictive modelling missing data inference processed variational bayesian approximation bayesian treatment adopted model learning supervised classification clustering\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"19_mixture_model_models\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"19_mixture_model_models\"],\"textfont\":{\"size\":12},\"x\":[13.964351,13.9379635,13.787772,14.225531,14.11768,13.540668,13.872676,13.04644,13.892194,13.951646,13.069356,13.858155,13.937062,13.061849,13.957094,13.471127,13.952642,13.828638,13.840624,13.932154,13.940575,13.7821045,14.078604,13.937906,14.23641,13.897067,13.996399,12.999655,13.919932,13.850748,13.948176,13.8803835,13.80355],\"y\":[6.285503,6.329675,7.0946326,6.6930714,6.535464,6.910028,6.841271,7.090772,6.4445972,6.316963,7.0823073,6.4923162,6.268679,7.089897,6.3219366,6.755993,7.0035906,6.5632677,6.565334,6.3746777,6.4128366,6.6295943,7.108765,6.2988296,6.746512,6.9795313,6.37997,7.037053,6.8237944,6.5770936,6.9346137,6.408065,6.668645],\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"shapes\":[{\"line\":{\"color\":\"#CFD8DC\",\"width\":2},\"type\":\"line\",\"x0\":11.143020820617675,\"x1\":11.143020820617675,\"y0\":1.552674812078476,\"y1\":14.177737474441528},{\"line\":{\"color\":\"#9E9E9E\",\"width\":2},\"type\":\"line\",\"x0\":5.500634288787841,\"x1\":16.78540735244751,\"y0\":7.865206143260002,\"y1\":7.865206143260002}],\"annotations\":[{\"showarrow\":false,\"text\":\"D1\",\"x\":5.500634288787841,\"y\":7.865206143260002,\"yshift\":10},{\"showarrow\":false,\"text\":\"D2\",\"x\":11.143020820617675,\"xshift\":10,\"y\":14.177737474441528}],\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eDocuments and Topics\\u003c\\u002fb\\u003e\",\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":750,\"xaxis\":{\"visible\":false},\"yaxis\":{\"visible\":false}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('868988f9-29f2-458f-b420-a49f78c12e59');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 101
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_barchart()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:49.992826Z",
          "iopub.status.idle": "2025-05-13T21:01:49.993255Z",
          "shell.execute_reply.started": "2025-05-13T21:01:49.993019Z",
          "shell.execute_reply": "2025-05-13T21:01:49.993035Z"
        },
        "id": "WG1_eJYacYLy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "outputId": "bbc2b887-59a3-4576-e0bf-917dd341e17a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"30e5ac39-980d-4125-b703-746d8f7426f4\" class=\"plotly-graph-div\" style=\"height:500px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"30e5ac39-980d-4125-b703-746d8f7426f4\")) {                    Plotly.newPlot(                        \"30e5ac39-980d-4125-b703-746d8f7426f4\",                        [{\"marker\":{\"color\":\"#D55E00\"},\"orientation\":\"h\",\"x\":[0.02214922943017521,0.025543046835055847,0.02559014006265311,0.028486590829885416,0.04098626274999652],\"y\":[\"models  \",\"processes  \",\"model  \",\"process  \",\"gaussian  \"],\"type\":\"bar\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"marker\":{\"color\":\"#0072B2\"},\"orientation\":\"h\",\"x\":[0.02387081271662058,0.025180198812647936,0.026082073785749598,0.027532395201076802,0.04432528328039532],\"y\":[\"sparse  \",\"data  \",\"algorithm  \",\"dictionary  \",\"matrix  \"],\"type\":\"bar\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"marker\":{\"color\":\"#CC79A7\"},\"orientation\":\"h\",\"x\":[0.019757040832342528,0.021619851821558362,0.027803339043283064,0.037064679800245345,0.04095522874169191],\"y\":[\"regularization  \",\"group  \",\"sparse  \",\"regression  \",\"lasso  \"],\"type\":\"bar\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"marker\":{\"color\":\"#E69F00\"},\"orientation\":\"h\",\"x\":[0.017771723873168048,0.019042444213442714,0.020754198090980305,0.029205164561014352,0.045779781538429674],\"y\":[\"reproducing  \",\"test  \",\"kernels  \",\"learning  \",\"kernel  \"],\"type\":\"bar\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"marker\":{\"color\":\"#56B4E9\"},\"orientation\":\"h\",\"x\":[0.02711997813628191,0.027595742740816036,0.03043779586197969,0.041280211124344726,0.059800191157804106],\"y\":[\"clustering  \",\"vertex  \",\"network  \",\"graphs  \",\"graph  \"],\"type\":\"bar\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"},{\"marker\":{\"color\":\"#009E73\"},\"orientation\":\"h\",\"x\":[0.019265258912513513,0.02106676136575277,0.021173655708844188,0.029871259182400873,0.03336378766514229],\"y\":[\"analysis  \",\"method  \",\"kernel  \",\"data  \",\"brain  \"],\"type\":\"bar\",\"xaxis\":\"x6\",\"yaxis\":\"y6\"},{\"marker\":{\"color\":\"#F0E442\"},\"orientation\":\"h\",\"x\":[0.01732388600693761,0.01763867899841201,0.019261781750440396,0.020566310076086655,0.022458622269874648],\"y\":[\"problem  \",\"statistical  \",\"algorithm  \",\"problems  \",\"algorithms  \"],\"type\":\"bar\",\"xaxis\":\"x7\",\"yaxis\":\"y7\"},{\"marker\":{\"color\":\"#D55E00\"},\"orientation\":\"h\",\"x\":[0.02026398140347138,0.02178004527837581,0.022036924163704657,0.02261684322064425,0.032284556191981666],\"y\":[\"markov  \",\"algorithm  \",\"monte  \",\"carlo  \",\"sampling  \"],\"type\":\"bar\",\"xaxis\":\"x8\",\"yaxis\":\"y8\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.175],\"showgrid\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.6000000000000001,1.0],\"showgrid\":true},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.275,0.45],\"showgrid\":true},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.6000000000000001,1.0],\"showgrid\":true},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.55,0.7250000000000001],\"showgrid\":true},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.6000000000000001,1.0],\"showgrid\":true},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.825,1.0],\"showgrid\":true},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.6000000000000001,1.0],\"showgrid\":true},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.0,0.175],\"showgrid\":true},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.0,0.4],\"showgrid\":true},\"xaxis6\":{\"anchor\":\"y6\",\"domain\":[0.275,0.45],\"showgrid\":true},\"yaxis6\":{\"anchor\":\"x6\",\"domain\":[0.0,0.4],\"showgrid\":true},\"xaxis7\":{\"anchor\":\"y7\",\"domain\":[0.55,0.7250000000000001],\"showgrid\":true},\"yaxis7\":{\"anchor\":\"x7\",\"domain\":[0.0,0.4],\"showgrid\":true},\"xaxis8\":{\"anchor\":\"y8\",\"domain\":[0.825,1.0],\"showgrid\":true},\"yaxis8\":{\"anchor\":\"x8\",\"domain\":[0.0,0.4],\"showgrid\":true},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 0\",\"x\":0.0875,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 1\",\"x\":0.36250000000000004,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 2\",\"x\":0.6375000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 3\",\"x\":0.9125,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 4\",\"x\":0.0875,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.4,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 5\",\"x\":0.36250000000000004,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.4,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 6\",\"x\":0.6375000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.4,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 7\",\"x\":0.9125,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.4,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"Topic Word Scores\",\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"showlegend\":false,\"width\":1000,\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('30e5ac39-980d-4125-b703-746d8f7426f4');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 102
    },
    {
      "cell_type": "code",
      "source": [
        "keybert_labels = [label[0][0].split(\"\\n\")[0] for label in topic_model.get_topics(full=True)[\"KeyBERT\"].values()]\n",
        "print(keybert_labels)\n",
        "\n",
        "# Get document info\n",
        "document_info = topic_model.get_document_info(abstracts)\n",
        "document_info[\"KeyBERT\"] = document_info[\"KeyBERT\"].apply(lambda x: x[0])\n",
        "all_labels = document_info[\"KeyBERT\"]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:49.994874Z",
          "iopub.status.idle": "2025-05-13T21:01:49.995342Z",
          "shell.execute_reply.started": "2025-05-13T21:01:49.995049Z",
          "shell.execute_reply": "2025-05-13T21:01:49.995064Z"
        },
        "id": "AOx0V1vscYLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f5494ad-a563-440d-e2b0-e7c6685f9030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['gaussian', 'algorithms', 'lasso', 'kernels', 'graphs', 'fmri', 'algorithms', 'sgmcmc', 'variational', 'ensembles', 'classifiers', 'clustering', 'graphical', 'pca', 'optimisation', 'forecasting', 'topics', 'causal', 'clinical', 'bayesian']\n"
          ]
        }
      ],
      "execution_count": 103
    },
    {
      "cell_type": "code",
      "source": [
        "df_plot = pd.DataFrame({\n",
        "    \"x1\": [point[0] for point in reduced_embeddings],\n",
        "    \"x2\": [point[1] for point in reduced_embeddings],\n",
        "    \"docs\": abstracts,\n",
        "    \"label\": all_labels\n",
        "})\n",
        "df_plot[\"docs_short\"] = df_plot[\"docs\"].str[:100] + \"...\"\n",
        "df_plot.head(10)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:49.996974Z",
          "iopub.status.idle": "2025-05-13T21:01:49.997312Z",
          "shell.execute_reply.started": "2025-05-13T21:01:49.997164Z",
          "shell.execute_reply": "2025-05-13T21:01:49.997178Z"
        },
        "id": "TYwwre3XcYLy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "89f16e9f-11b2-4198-e90b-2305d7d36fb0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         x1        x2                                               docs  \\\n",
              "0 -0.657060  4.779006  consider problem binary classification one par...   \n",
              "1 -1.518607  3.958432  distance metric plays important role nearest n...   \n",
              "2 -3.145614  6.410346  dendrograms used data analysis ultrametric spa...   \n",
              "3 -3.203511  6.571202  conceptual framework cluster analysis viewpoin...   \n",
              "4 -4.546526  5.217755  present analyse three online algorithms learni...   \n",
              "5 -2.277890  4.325052  recent years kernel density estimation exploit...   \n",
              "6 -0.375601  3.811363  thesis responds challenges using large number ...   \n",
              "7 -3.622922  2.599043  simulated annealing popular method approaching...   \n",
              "8 -4.468881  6.162722  present nested chinese restaurant process ncrp...   \n",
              "9 -2.742345  3.328779  provide selfcontained proof theorem relating p...   \n",
              "\n",
              "          label                                         docs_short  \n",
              "0         lasso  consider problem binary classification one par...  \n",
              "1   classifiers  distance metric plays important role nearest n...  \n",
              "2    clustering  dendrograms used data analysis ultrametric spa...  \n",
              "3    clustering  conceptual framework cluster analysis viewpoin...  \n",
              "4        sgmcmc  present analyse three online algorithms learni...  \n",
              "5       kernels  recent years kernel density estimation exploit...  \n",
              "6   classifiers  thesis responds challenges using large number ...  \n",
              "7  optimisation  simulated annealing popular method approaching...  \n",
              "8        topics  present nested chinese restaurant process ncrp...  \n",
              "9   forecasting  provide selfcontained proof theorem relating p...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8b5f4f0b-3a23-499f-8e4d-84046a8e978f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>docs</th>\n",
              "      <th>label</th>\n",
              "      <th>docs_short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.657060</td>\n",
              "      <td>4.779006</td>\n",
              "      <td>consider problem binary classification one par...</td>\n",
              "      <td>lasso</td>\n",
              "      <td>consider problem binary classification one par...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.518607</td>\n",
              "      <td>3.958432</td>\n",
              "      <td>distance metric plays important role nearest n...</td>\n",
              "      <td>classifiers</td>\n",
              "      <td>distance metric plays important role nearest n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-3.145614</td>\n",
              "      <td>6.410346</td>\n",
              "      <td>dendrograms used data analysis ultrametric spa...</td>\n",
              "      <td>clustering</td>\n",
              "      <td>dendrograms used data analysis ultrametric spa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-3.203511</td>\n",
              "      <td>6.571202</td>\n",
              "      <td>conceptual framework cluster analysis viewpoin...</td>\n",
              "      <td>clustering</td>\n",
              "      <td>conceptual framework cluster analysis viewpoin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-4.546526</td>\n",
              "      <td>5.217755</td>\n",
              "      <td>present analyse three online algorithms learni...</td>\n",
              "      <td>sgmcmc</td>\n",
              "      <td>present analyse three online algorithms learni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-2.277890</td>\n",
              "      <td>4.325052</td>\n",
              "      <td>recent years kernel density estimation exploit...</td>\n",
              "      <td>kernels</td>\n",
              "      <td>recent years kernel density estimation exploit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.375601</td>\n",
              "      <td>3.811363</td>\n",
              "      <td>thesis responds challenges using large number ...</td>\n",
              "      <td>classifiers</td>\n",
              "      <td>thesis responds challenges using large number ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-3.622922</td>\n",
              "      <td>2.599043</td>\n",
              "      <td>simulated annealing popular method approaching...</td>\n",
              "      <td>optimisation</td>\n",
              "      <td>simulated annealing popular method approaching...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-4.468881</td>\n",
              "      <td>6.162722</td>\n",
              "      <td>present nested chinese restaurant process ncrp...</td>\n",
              "      <td>topics</td>\n",
              "      <td>present nested chinese restaurant process ncrp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-2.742345</td>\n",
              "      <td>3.328779</td>\n",
              "      <td>provide selfcontained proof theorem relating p...</td>\n",
              "      <td>forecasting</td>\n",
              "      <td>provide selfcontained proof theorem relating p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b5f4f0b-3a23-499f-8e4d-84046a8e978f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8b5f4f0b-3a23-499f-8e4d-84046a8e978f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8b5f4f0b-3a23-499f-8e4d-84046a8e978f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8a2b4930-e102-45e5-bde9-3aba1363b021\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8a2b4930-e102-45e5-bde9-3aba1363b021')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8a2b4930-e102-45e5-bde9-3aba1363b021 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_plot",
              "summary": "{\n  \"name\": \"df_plot\",\n  \"rows\": 1601,\n  \"fields\": [\n    {\n      \"column\": \"x1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1601,\n        \"samples\": [\n          -1.599982738494873,\n          0.8505657911300659,\n          -2.602396011352539\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"x2\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1601,\n        \"samples\": [\n          4.268630027770996,\n          5.366730213165283,\n          4.1065521240234375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"docs\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1601,\n        \"samples\": [\n          \"propose likelihood ratio based inferential framework high dimensional semiparametric generalized linear models framework addresses variety challenging problems high dimensional data analysis including incomplete data selection bias heterogeneous multitask learning work three main contributions develop regularized statistical chromatography approach infer parameter interest proposed semiparametric generalized linear model without need estimating unknown base measure function propose new framework construct postregularization confidence regions tests low dimensional components high dimensional parameters unlike existing postregularization inferential methods approach based novel directional likelihood particular framework naturally handles generic regularized estimators nonconvex penalty functions used infer least false parameters misspecified models iii develop new concentration inequalities normal approximation results ustatistics unbounded kernels independent interest demonstrate consequences general theory using example missing data problem extensive simulation studies real data analysis provided illustrate proposed approach\",\n          \"manuscript consider problem jointly estimating multiple graphical models high dimensions assume data collected subjects consists possibly dependent observations graphical models subjects vary assumed change smoothly corresponding measure closeness subjects propose kernel based method jointly estimating graphical models theoretically double asymptotic framework dimension increase provide explicit rate convergence parameter estimation characterizes strength one borrow across different individuals impact data dependence parameter estimation empirically experiments synthetic real resting state functional magnetic resonance imaging rsfmri data illustrate effectiveness proposed method\",\n          \"propose novel algebraic framework treating probability distributions represented cumulants mean covariance matrix example consider unsupervised learning problem finding subspace several probability distributions agree instead minimizing objective function involving estimated cumulants show treating cumulants elements polynomial ring directly solve problem lower computational cost higher accuracy moreover algebraic viewpoint probability distributions allows invoke theory algebraic geometry demonstrate compact proof identifiability criterion\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 19,\n        \"samples\": [\n          \"lasso\",\n          \"optimisation\",\n          \"algorithms\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"docs_short\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1596,\n        \"samples\": [\n          \"problem finding overlapping communities networks gained much attention recently optimizationbased ap...\",\n          \"propose paper differentiable learning loss time series building upon celebrated dynamic time warping...\",\n          \"estimating state dynamical system series noisecorrupted observations fundamental many areas science ...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "execution_count": 104
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(df_plot, x=\"x1\", y=\"x2\", color=\"label\", hover_data=[\"docs_short\"])\n",
        "fig.update_layout(\n",
        "    title=f\"Category: {category_desired}\",\n",
        "    title_font_size=20\n",
        ")\n",
        "print(\"Embedding Model:\", topic_model.embedding_model)\n",
        "print(\"\\nDimensionality Reduction Model:\", topic_model.umap_model)\n",
        "print(\"\\nClustering Model:\", topic_model.hdbscan_model)\n",
        "print(\"\\nRepresentation Model(s):\", topic_model.representation_model)\n",
        "fig.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:49.99818Z",
          "iopub.status.idle": "2025-05-13T21:01:49.998424Z",
          "shell.execute_reply.started": "2025-05-13T21:01:49.99831Z",
          "shell.execute_reply": "2025-05-13T21:01:49.998321Z"
        },
        "id": "AC8gDAUGcYLy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "a7ebc7d3-08ee-48b3-e947-bac8c441ab87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Model: <bertopic.backend._sentencetransformers.SentenceTransformerBackend object at 0x7e062c7b3e10>\n",
            "\n",
            "Dimensionality Reduction Model: UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0, n_neighbors=10, tqdm_kwds={'bar_format': '{desc}: {percentage:3.0f}%| {bar} {n_fmt}/{total_fmt} [{elapsed}]', 'desc': 'Epochs completed', 'disable': True})\n",
            "\n",
            "Clustering Model: KMeans(n_clusters=20)\n",
            "\n",
            "Representation Model(s): {'KeyBERT': KeyBERTInspired(), 'MMR': MaximalMarginalRelevance(diversity=0.3)}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"91eb554c-fe05-4da3-97a1-672f94035a06\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"91eb554c-fe05-4da3-97a1-672f94035a06\")) {                    Plotly.newPlot(                        \"91eb554c-fe05-4da3-97a1-672f94035a06\",                        [{\"customdata\":[[\"consider problem binary classification one particular cost choose classify observation present simpl...\"],[\"propose new sparsitysmoothness penalty highdimensional generalized additive models combination spars...\"],[\"consider empirical risk minimization problem linear supervised learning regularization structured sp...\"],[\"present extension sparse pca sparse dictionary learning sparsity patterns dictionary elements struct...\"],[\"minimum description length mdl principle states optimal model given data set compresses best due pra...\"],[\"propose novel algorithm greedy forward feature selection regularized leastsquares rls regression cla...\"],[\"supervised linear feature extraction achieved fitting reduced rank multivariate model paper studies ...\"],[\"sharply characterize performance different penalization schemes problem selecting relevant variables...\"],[\"study problem learning sparse linear regression vector additional conditions structure sparsity patt...\"],[\"group lasso penalized regression method used regression problems covariates partitioned groups promo...\"],[\"performance lasso well understood assumptions standard linear model homoscedastic noise however seve...\"],[\"propose novel application simultaneous orthogonal matching pursuit somp procedure sparsistant variab...\"],[\"investigate learning rate multiple kernel leaning mkl elasticnet regularization consists ellregulari...\"],[\"present two sets theoretical results grouped lasso overlap jacob obozinski vert linear regression se...\"],[\"theoretically investigate convergence rate support consistency correctly identifying subset nonzero ...\"],[\"nonparametric methods widely applicable statistical inference problems since rely modeling assumptio...\"],[\"compare risk ridge regression simple variant ordinary least squares one simply projects data onto fi...\"],[\"number recent work studied effectiveness feature selection using lasso known restricted isometry pro...\"],[\"sparse bayesian learning sbl gaussian scale mixtures gsms used model sparsityinducing priors realize...\"],[\"concave regularization methods provide natural procedures sparse recovery however difficult analyze ...\"],[\"performance orthogonal matching pursuit omp variable selection analyzed random designs contrasted de...\"],[\"propose scalable efficient statistically motivated computational framework graphical lasso friedman ...\"],[\"popular cubic smoothing spline estimate regression function arises minimizer penalized sum squares s...\"],[\"paper develops general theoretical framework analyze structured sparse recovery problems using notat...\"],[\"existing methods sparse channel estimation typically provide estimate computed solution maximizing o...\"],[\"multitask sparse feature learning aims improve generalization performance exploiting shared features...\"],[\"model selection crucial issue machinelearning wide variety penalisation methods possibly data depend...\"],[\"propose approach multivariate nonparametric regression generalizes reduced rank regression linear mo...\"],[\"shown aictype criteria asymptotically efficient selectors tuning parameter nonconcave penalized regr...\"],[\"introduce new approach variable selection called predictive correlation screening predictor design p...\"],[\"popular sparse estimation methods based ellrelaxation lasso dantzig selector require knowledge varia...\"],[\"paper propose study family sparsityinducing penalty functions since penalty functions related kineti...\"],[\"introduce application group lasso design experiments note trying explain experimental design group l...\"],[\"introduce computationally effective algorithm linear model selection consisting three steps screenin...\"],[\"paper study nonconvex penalization using bernstein functions since bernstein function concave nonsmo...\"],[\"multitask learning shown significantly enhance performance multiple related learning tasks variety s...\"],[\"recent paper shown lasso algorithm exhibits nearideal behavior following sense suppose eta satisfies...\"],[\"lasso computationally efficient regression regularization procedure produce sparse estimators number...\"],[\"present supervisedlearning algorithm graph data set graphs arbitrary twicedifferentiable loss functi...\"],[\"describe simple efficient permutation based procedure selecting penalty parameter lasso procedure in...\"],[\"study novel splinelike basis name falling factorial basis bearing many similarities classic truncate...\"],[\"propose new two stage algorithm ling large scale regression problems ling risk well known ridge regr...\"],[\"propose loco algorithm largescale ridge regression distributes features across workers cluster impor...\"],[\"significant attention given minimizing penalized least squares criterion estimating sparse solutions...\"],[\"distributed learning probabilistic models multiple data repositories minimum communication increasin...\"],[\"consider task fitting regression model involving interactions among potentially large set covariates...\"],[\"paper introduce new optimization formulation sparse regression compressed sensing called clot combin...\"],[\"article considers problem multigroup classification setting number variables larger number observati...\"],[\"recent studies literature paid much attention sparsity linear classification tasks one motivation im...\"],[\"propose robust inferential procedure assessing uncertainties parameter estimation highdimensional li...\"],[\"consider problem multivariate regression setting relevant predictors could shared among different re...\"],[\"structured sparsity recently emerged statistics machine learning signal processing promising paradig...\"],[\"introduce gamsel generalized additive model selection penalized likelihood approach fitting sparse g...\"],[\"taking account highorder interactions among covariates valuable many practical regression problems h...\"],[\"factorized information criterion fic recently developed information criterion based novel model sele...\"],[\"linear regression models depend directly design matrix properties techniques efficiently estimate mo...\"],[\"study propose automatic learning method variables selection based lasso epidemiology context one aim...\"],[\"order identify important variables involved making optimal treatment decision proposed penalized lea...\"],[\"paper study nonconvex penalization using bernstein functions whose firstorder derivatives completely...\"],[\"life sciences experts generally use empirical knowledge recode variables choose interactions perform...\"],[\"propose framework perform streaming covariance selection approach employs regularization constraints...\"],[\"compute approximate solutions regularized linear regression using regularization also known lasso in...\"],[\"investigate properties estimators obtained minimization uprocesses lasso penalty highdimensional set...\"],[\"compressed sensing order recover sparse nearly sparse vector possibly noisy measurements popular app...\"],[\"present method variable selection sparse generalized additive model method doesnt assume specific fu...\"],[\"problem learning sparse model conceptually interpreted process identifying active featuressamples op...\"],[\"generalised degrees freedom gdf defined jasa represent sensitivity model fits perturbations data com...\"],[\"consider problem robustifying highdimensional structured estimation robust techniques key realworld ...\"],[\"propose nonconvex estimator joint multivariate regression precision matrix estimation high dimension...\"],[\"paper study problem recovering group sparse vector small number linear measurements past common appr...\"],[\"deep learning methods multitask neural networks recently applied ligandbased virtual screening drug ...\"],[\"consider learning highdimensional multiresponse linear models structured parameters exploiting noise...\"],[\"additive nonparametric regression models provide attractive tool variable selection high dimensions ...\"],[\"extend adaptive regression spline model incorporating saturation natural requirement function extend...\"],[\"propose communicationefficient distributed estimation method sparse linear discriminant analysis lda...\"],[\"stochastic gradient descent sgd algorithm widely used statistical estimation largescale data due com...\"],[\"paper combine two important extensions ordinary least squares regression regularization optimal scal...\"],[\"propose method finding alternate features missing lasso optimal solution ordinary lasso problem one ...\"],[\"paper consider problem learning highdimensional tensor regression problems lowrank structure one cor...\"],[\"focus maximum regularization parameter anisotropic totalvariation denoising corresponds minimum valu...\"],[\"tuning parameter selection critical importance kernel ridge regression date data driven tuning metho...\"],[\"regression models increasingly built using datasets follow design experiment instead data gathered a...\"],[\"subset selection multiple linear regression aims choose subset candidate explanatory variables trade...\"],[\"forward regression statistical model selection estimation procedure inductively selects covariates a...\"],[\"study additive models built trend filtering additive models whose components regularized discrete to...\"],[\"partial least squares pls methods heavily exploited analyse association two blocs data powerful appr...\"],[\"consider problem estimating regression function common situation number features small interpretabil...\"],[\"propose data aggregationbased algorithm monotonic convergence global optimum generalized version lno...\"],[\"paper deals problem largescale linear supervised learning settings large number continuous features ...\"],[\"highdimensional andor nonparametric regression problems regularization penalization used control mod...\"],[\"many applied settings empirical economics involve simultaneous estimation large number parameters pa...\"],[\"least absolute shrinkage selection operator lasso method adapted recently networkstructured datasets...\"],[\"temporal group lasso example multitask regularized regression approach prediction response variables...\"],[\"boosting gradient descent algorithms one popular method machine learning paper novel boostingtype al...\"],[\"address problem defining group sparse formulation principal components analysis pca equivalent formu...\"],[\"sparse mapping key methodology many highdimensional scientific problems multiple tasks share set rel...\"],[\"develop new method called discriminated hub graphical lasso dhgl based hub graphical lasso hgl provi...\"],[\"many distributed learning problems heterogeneous loading computing machines may harm overall perform...\"],[\"propose nuclear norm penalty alternative ridge penalty regularized multinomial regression convex rel...\"],[\"sparsity learning known grouping structure received considerable attention due wide modern applicati...\"],[\"given observation highdimensional ornsteinuhlenbeck process continuous time proceed inference drift ...\"],[\"consider problem estimating regression function common situation number features small interpretabil...\"],[\"although various distributed machine learning schemes proposed recently pure linear models fully non...\"],[\"graphical lasso popular method learning structure undirected graphical model based regularization te...\"],[\"sparse alphanorm regularization many datarich applications marketing economics alphanorm contrast la...\"],[\"consistency doubly robust estimators relies consistent estimation least one two nuisance regression ...\"],[\"multiple linear regression setting propose general framework termed weighted orthogonal components r...\"],[\"propose bayesian regression method accounts multiway interactions arbitrary orders among predictor v...\"],[\"inferring predictive maps multiple input multiple output variables tasks innumerable applications da...\"],[\"machine learning data mining linear models widely used model response parametric linear functions pr...\"],[\"sparse regularization ell regularization quite powerful widely used strategy high dimensional learni...\"],[\"subset selection multiple linear regression aims construct regression model minimizes errors selecti...\"],[\"paper provides estimation inference methods conditional average treatment effects cate characterized...\"],[\"consider multitask learning simultaneously learns related prediction tasks improve generalization pe...\"],[\"convex sparsityinducing regularizations ubiquitous highdimensional machine learning solving resultin...\"]],\"hovertemplate\":\"label=lasso\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"lasso\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"lasso\",\"showlegend\":true,\"x\":[-0.6570604,-0.7178993,-1.0335921,-0.30175686,-0.5596965,-0.89441514,-1.0964985,-0.853024,-0.617106,-0.51784766,-0.49521765,-0.59823626,-0.8261026,-0.42725563,-0.8303255,-1.0918784,-0.9490922,-0.44473228,-0.39116734,-0.5659205,-0.4790767,-0.44314748,-1.1978313,-0.4417019,-0.3917391,-0.6154342,-1.0692348,-0.9698395,-1.1742797,-0.29779375,-0.6118934,-0.9559551,-0.44296348,-0.5678302,-0.76713574,-0.6098339,-0.44893616,-0.5102036,-0.6210734,-0.45455924,-1.0500782,-1.1597736,-1.2668315,-0.6638328,-1.4259751,-0.46206892,-0.40298772,-0.42361084,-0.47140726,-1.3669302,-0.96772027,-0.5084294,-1.0276783,-0.45723674,-4.080101,-1.1076729,-0.44389167,-1.1079881,-0.94518507,-0.50605416,-0.8453384,-0.5567782,-0.62591636,-0.44362748,-0.96720326,-0.7529918,-1.1819501,-0.4982907,-1.0049334,-0.41988137,-0.125329,-1.4425603,-1.0250583,-1.0785897,-1.2285984,-1.5085058,-0.66784483,-0.45039806,-1.146481,-0.75900686,-1.1106535,-1.2528958,-1.0914998,-0.92445314,-1.1113774,-0.9008841,-0.9845758,-1.2505399,-0.78534335,-1.0411397,-1.0497836,-0.50343955,-0.4543311,-0.65916723,-0.38621053,-0.43418926,-0.43828502,-1.3444498,-0.90917176,-0.48612446,-1.0313302,-1.081701,-1.2715396,-0.5598543,-0.64476347,-1.313976,-0.9494203,-4.0137897,-0.41693527,-1.0089213,-0.58890975,-0.9597898,-1.0117639,-0.7800031,-1.2235893],\"xaxis\":\"x\",\"y\":[4.7790065,5.3861375,4.784302,5.6335874,4.9881406,5.0178046,5.2889867,4.700833,5.224401,5.085552,5.22158,4.5549636,5.118199,5.125546,5.1693993,4.7771688,5.392875,5.1391935,5.6346498,5.533127,5.538063,4.9173784,5.035981,5.6061387,5.6635313,5.152883,4.452613,4.8940597,4.654043,4.4304314,5.3532124,5.31275,5.191617,4.730976,5.503532,4.67207,5.176158,5.0055027,5.4610043,4.875088,5.408719,5.3733754,5.423055,5.5966387,5.4694467,4.777797,5.581645,5.16488,5.0976295,4.5629334,5.3843412,5.412739,5.005256,4.4109135,5.6830893,5.4308095,4.606606,4.4391813,5.338569,4.4762363,5.1746793,5.0983033,4.7212734,5.5330434,4.9062943,4.1000586,4.154916,4.974324,5.6723833,5.549939,4.239519,5.7606688,5.0027194,4.999812,5.6694202,4.9597583,4.9934754,4.962573,5.7222342,5.3047633,4.4195633,4.3494954,4.7139163,4.731393,5.1351457,5.4639378,4.7573795,4.8148746,5.20353,4.8344016,4.4931307,5.365086,4.8169966,5.117375,5.5044975,5.214114,4.926321,5.4050655,4.731924,5.1959496,5.0276346,4.7530236,5.545339,5.0959,4.9994726,4.509593,5.336904,5.669913,3.609156,5.198567,5.0633235,4.7113647,4.4097786,4.8826957,4.8429103],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"distance metric plays important role nearest neighbor classification usually euclidean distance metr...\"],[\"thesis responds challenges using large number thousands features regression classification problems ...\"],[\"dimensionality reduction topic recent interest paper present classification constrained dimensionali...\"],[\"selecting important features nonlinear kernel spaces difficult challenge classification regression p...\"],[\"introduce new nearestprototype classifier prototype vector machine pvm arises combinatorial optimiza...\"],[\"study losses binary classification class probability estimation extend understanding margin losses g...\"],[\"security issues crucial number machine learning applications especially scenarios dealing human acti...\"],[\"class schoenberg transformations embedding euclidean distances higher dimensional euclidean spaces p...\"],[\"introduce efficient method training linear ranking support vector machine method combines cutting pl...\"],[\"support vector machines svms special kernel based methods belong successful learning methods since d...\"],[\"last years many different performance measures introduced overcome weakness natural metric accuracy ...\"],[\"propose simple kernel based nearest neighbor approach handwritten digit classification distance actu...\"],[\"present surrogate regret bounds arbitrary surrogate losses context binary classification labeldepend...\"],[\"consider problem learning binary classifier training set positive unlabeled examples inductive trans...\"],[\"article derive concentration inequalities crossvalidation estimate generalization error empirical ri...\"],[\"often wish predict large number variables depend well observed variables structured prediction metho...\"],[\"article derive concentration inequalities crossvalidation estimate generalization error stable predi...\"],[\"article derive concentration inequalities crossvalidation estimate generalization error subagged est...\"],[\"data sets many features observations independent screening based univariate regression models leads ...\"],[\"many nonparametric regressors recently shown converge rates depend intrinsic dimension data regresso...\"],[\"increasing body evidence suggesting exact nearest neighbour search highdimensional spaces affected c...\"],[\"consider standard binary classification problem performance binary classifier based training data ch...\"],[\"introduce new discriminant analysis method empirical discriminant analysis eda binary classification...\"],[\"support vector machines svms naturally embody sparseness due use hinge loss functions however svms d...\"],[\"apply informationbased complexity analysis support vector machine svm algorithms goal comprehensive ...\"],[\"concept refinement probability elicitation considered proper scoring rules taking directions axioms ...\"],[\"scaled complex wishart distribution widely used model multilook full polarimetric sar data whose ade...\"],[\"introduce supersparse linear integer models slim tool create scoring systems binary classification d...\"],[\"project investigate idea reducing dimensionality datasets using borel isomorphism purpose subsequent...\"],[\"novel linear classification method possesses merits support vector machine svm distanceweighted disc...\"],[\"classification important topic statistics machine learning great potential many real applications pa...\"],[\"machine learning methods clustering classification rely distance function describe relationships dat...\"],[\"sparse classifiers support vector machines svm efficient testphases classifier characterized subset ...\"],[\"numbers numerical vectors account large portion data however recently amount string data generated i...\"],[\"paper generalizes important result pacbayesian literature binary classification case ensemble method...\"],[\"prove new fast learning rates onevsall multiclass plugin classifiers trained either exponentially st...\"],[\"regularized logistic regression become workhorse data mining bioinformatics widely used many classif...\"],[\"paper concerned problems interaction screening nonlinear classification highdimensional setting prop...\"],[\"finding statistically significant highorder interaction features predictive modeling important chall...\"],[\"one limiting factors using support vector machines svms large scale applications superlinear computa...\"],[\"existing binary classification methods target optimization overall classification risk may fail serv...\"],[\"distance weighted discrimination dwd marginbased classifier interesting geometric motivation dwd ori...\"],[\"supervised learning active research area numerous applications diverse fields data analytics compute...\"],[\"paper novel approach coding nominal data proposed given nominal data rank form complex number assign...\"],[\"regularized discriminant analysis rda proposed friedman widely popular classifier lacks interpretabi...\"],[\"propose new class metrics sets vectors functions used various stages data mining including explorato...\"],[\"modern data analyst must cope data encoded various forms vectors matrices strings graphs consequentl...\"],[\"rapid overlay chemical structures rocs standard tool calculation shape chemical color similarity roc...\"],[\"consider training probabilistic classifiers case large number classes number classes assumed large p...\"],[\"softmax representation probabilities categorical variables plays prominent role modern machine learn...\"],[\"article large dimensional performance analysis kernel least squares support vector machines lssvms p...\"],[\"stability important aspect classification procedure unstable predictions potentially reduce users tr...\"],[\"inspired importance diversity biological system built heterogeneous system could achieve goal archit...\"],[\"implemented several multilabel classification algorithms machine learning package mlr implemented me...\"],[\"study problem interactively learning binary classifier using noisy labeling pairwise comparison orac...\"],[\"highdimensional classification settings wish seek balance high power ensuring control desired loss f...\"],[\"two proteins homologous common evolutionary origin binary classification problem identify proteins c...\"],[\"tomal introduced notion phalanxes context rareclass detection twoclass classification problems phala...\"],[\"paper presents approach automation interpretable feature selection internet things analytics iota us...\"],[\"provide two main contributions pacbayesian theory domain adaptation objective learn source distribut...\"],[\"fundamental question data analysis machine learning signal processing compare data points choice dis...\"],[\"paper investigates theoretical foundations metric learning focused three key questions fully address...\"],[\"kernelbased learning algorithms widely used machine learning problems make use similarity object pai...\"],[\"feature selection highdimensional data small proportion relevant features poses severe challenge sta...\"],[\"characteristics numerical patterns feature vector transform domain perturbation model differ signifi...\"],[\"consider classifiers highdimensional data strongly spiked eigenvalue sse model first show highdimens...\"],[\"article carries large dimensional analysis standard regularized discriminant analysis classifiers de...\"],[\"modern machine learning systems image classifiers rely heavily large scale data sets training data s...\"],[\"classification dissimilarity space become active research area since provides possibility learn data...\"],[\"common method generalizing binary multiclass classification error correcting code ecc eccs may optim...\"],[\"receiver operating characteristic roc analysis widely used evaluating diagnostic systems recent stud...\"],[\"study safe screening metric learning distance metric learning optimize metric set triplets one defin...\"],[\"data science determining proximity observations critical many downstream analyses clustering informa...\"],[\"classification problems datasets usually imbalanced noisy complex sampling algorithms make improveme...\"]],\"hovertemplate\":\"label=classifiers\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"classifiers\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"classifiers\",\"showlegend\":true,\"x\":[-1.5186067,-0.37560064,-1.7506086,-0.72540843,-1.3497646,-1.4489741,-1.3454894,-2.2736106,-1.4350325,-1.7108372,-1.1748328,-1.5745463,-1.4718589,-1.6017945,-1.2657253,-0.4519379,-1.2803552,-1.392633,-0.1539833,-1.6189708,-1.5332891,-1.4989558,-1.0900395,-1.5803996,-1.0633777,-1.5382464,-2.4320536,-0.99666053,-1.4013652,-1.1769314,-1.0384207,-2.7212536,-1.3960925,-0.95262545,-1.4672353,-1.4298096,-0.24366222,-0.4268808,-0.41686964,-1.5161388,-1.1943736,-1.2848067,-1.6082618,-1.0472454,-1.0686764,-1.3080355,-2.1655397,-1.0401753,-1.5787073,-1.8873211,-1.9055741,-1.2205524,-0.267722,-1.504982,-1.593273,-1.2996323,-0.30647442,-0.012599041,-0.72912586,-1.7593955,-2.658997,-1.4925262,-1.5720903,-0.51696104,-0.80682576,-0.9628816,-1.1593733,-1.578009,-1.3174944,-1.2958925,-1.1304592,-1.3456473,-1.2814597,-1.1352518],\"xaxis\":\"x\",\"y\":[3.9584322,3.8113635,5.795802,3.985563,3.7790215,3.5166368,3.4204948,5.822498,3.5667608,3.9694755,3.5227,3.9656448,3.5363684,3.1773205,3.9065359,3.7077703,3.9379609,3.932599,3.9576373,4.152894,3.9778624,3.4552772,3.6850412,3.9226217,3.5662875,3.233465,5.4180145,3.1499238,3.8765328,3.5110343,3.3769257,6.2237177,3.814436,3.5074382,3.2472572,3.5273442,3.7854908,3.923581,3.769124,3.8174143,3.3783479,3.5879736,3.6472971,3.6205266,3.6612444,3.8058758,5.9705477,3.5380461,3.4982142,3.3819149,4.081808,3.2968748,3.4649892,3.3440785,3.175812,3.391788,3.5418942,3.7166002,3.5595593,3.0029256,6.1632113,3.9923875,3.9303985,3.7268248,3.6424856,3.6226144,3.8962216,3.6622257,3.6539485,3.500356,3.5336797,4.015991,3.8379824,2.9509842],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"dendrograms used data analysis ultrametric spaces hence objects nonarchimedean geometry known exist ...\"],[\"conceptual framework cluster analysis viewpoint padic geometry introduced describing space dendrogra...\"],[\"construct framework studying clustering algorithms includes two key ideas persistence functoriality ...\"],[\"investigate role initialization stability kmeans clustering algorithm opposed papers consider actual...\"],[\"paper proposes original approach cluster multicomponent data sets including estimation number cluste...\"],[\"paper formulate general terms approach prove strong consistency empirical risk minimisation inductiv...\"],[\"popular method selecting number clusters based stability arguments one chooses number clusters corre...\"],[\"describe many vantage points baire metric use clustering data use preprocessing structuring data ord...\"],[\"informationmaximization clustering learns probabilistic classifier unsupervised manner mutual inform...\"],[\"many situations interest lies identifying clusters one might expect available variables carry inform...\"],[\"many modern data mining applications concerned analysis datasets observations described paired highd...\"],[\"several application domains highdimensional observations collected analysed search naturally occurri...\"],[\"mixture gaussians fit single curved heavytailed cluster report data contains many clusters produce a...\"],[\"identifying homogeneous subgroups variables challenging high dimensional data analysis highly correl...\"],[\"fundamental aim clustering algorithms partition data points consider tasks discovered partition allo...\"],[\"standard clustering problems data points represented vectors stacking together one forms data matrix...\"],[\"explore performance several automatic bandwidth selectors originally designed density gradient estim...\"],[\"consider problem clustering sequence multinomial observations way model selection criterion propose ...\"],[\"mean shift clustering finds modes data probability density identifying zero points density gradient ...\"],[\"propose novel nonparametric adaptive anomaly detection algorithm high dimensional data based ranksvm...\"],[\"density mathbb highdensity cluster connected component geq lambda lambda set highdensity clusters fo...\"],[\"paper propose new fuzzy clustering algorithm based modeseeking framework given dataset mathbbrd defi...\"],[\"estimation density derivatives versatile tool statistical data analysis naive approach first estimat...\"],[\"paper studies ordered weighted owl norm regularization sparse estimation problems strongly correlate...\"],[\"introduce develop novel approach outlier detection based adaptation random subspace learning propose...\"],[\"present technique clustering categorical data generating many dissimilarity matrices averaging begin...\"],[\"capturing dependence structure multivariate extreme events major concern many fields involving manag...\"],[\"symbolic data analysis based special descriptions data symbolic objects descriptions preserve detail...\"],[\"logdensity gradient estimation fundamental statistical problem possesses various practical applicati...\"],[\"general approach anomaly detection novelty detection consists estimating high density regions minimu...\"],[\"propose novel method multiple clustering assumes coclustering structure partitions rows columns data...\"],[\"kernel methods obtain superb performance terms accuracy various machine learning tasks since effecti...\"],[\"distancebased hierarchical clustering methods widely used unsupervised data analysis authors take ac...\"],[\"derive statistical model estimation dendrogram single linkage hierarchical clustering slhc takes acc...\"],[\"adjusted chance measures widely used compare partitionsclusterings data set particular adjusted rand...\"],[\"show objective function conventional kmeans clustering expressed frobenius norm difference data matr...\"],[\"traditionally practitioners initialize kmeans algorithm centers chosen uniformly random randomized i...\"],[\"consider problem sparse clustering assumed subset features useful clustering purposes framework cosa...\"],[\"investigations performed using clustering methods data mining timeseries data smart meters problem i...\"],[\"extremes play special role anomaly detection beyond inference simulation purposes probabilistic tool...\"],[\"present methodology clustering objects described multivariate time series several sequences realvalu...\"],[\"distributional distributionvalued data new type data arising several sources considered realizations...\"],[\"recent popularity graphical clustering methods increased focus information samples show learning clu...\"],[\"paper propose new method predict final destination vehicle trips based initial partial trajectories ...\"],[\"clustering one important unsupervised problems machine learning statistics among many existing algor...\"],[\"propose method performs anomaly detection localisation within heterogeneous data using pairwise undi...\"],[\"datasets mixture numerical categorical attributes routinely encountered many application domains wor...\"],[\"improve current instabilitybased methods selection number clusters cluster analysis developing norma...\"],[\"kernelbased kmeans clustering gained popularity due simplicity power implicit nonlinear representati...\"],[\"paper consider clustering data assumed come one finitely many pointed convex polyhedral cones model ...\"],[\"determination cluster centers generally depends scale use analyze data clustered inappropriate scale...\"],[\"clustering central approach unsupervised learning clustering applied fundamental analysis quantitati...\"],[\"consumer demand response important research industry problem seeks categorize predict modify consume...\"],[\"work robust clustering algorithm stationary time series proposed algorithm based use estimated spect...\"],[\"paper propose pckid novel robust kernel function spectral clustering specifically designed handle in...\"],[\"show dbscan estimate connected components lambdadensity level set lambda given iid samples unknown d...\"],[\"goal data clustering partition data points groups minimize given objective function existing cluster...\"],[\"investigate coresets succinct small summaries large data sets solutions found summary provably compe...\"],[\"train statistical mixture model massive data set work show construct coresets mixtures gaussians cor...\"],[\"cluster analysis high dimensional data benefit properties high dimensionality informally expressed w...\"],[\"kernel methods popular clustering due generality discriminating power however show many kernel clust...\"],[\"paper present novel method coclustering unsupervised learning approach aims discovering homogeneous ...\"],[\"present accelerated algorithm hierarchical density based clustering new algorithm improves upon hdbs...\"],[\"identifying set homogeneous clusters heterogeneous dataset one important classes problems statistica...\"],[\"modes ridges probability density function behind observed data useful geometric features modeseeking...\"],[\"propose method estimating coefficients multivariate regression clustering structure response variabl...\"],[\"real data often contain anomalous cases also known outliers may spoil resulting analysis may also co...\"],[\"paper presents new approach nonparametric cluster analysis called adaptive weights clustering awc id...\"],[\"one iteration standard kmeans lloyds algorithm standard gaussian mixture models gmms scales linearly...\"],[\"algorithm one many important tools field statistics often used imputing missing data widespread appl...\"],[\"introduce new unsupervised learning problem clustering widesense stationary ergodic stochastic proce...\"]],\"hovertemplate\":\"label=clustering\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"clustering\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"clustering\",\"showlegend\":true,\"x\":[-3.1456141,-3.2035108,-3.31594,-3.3259501,-3.267476,-3.2809505,-3.4012756,-3.1928988,-2.976243,-3.232972,-3.2032883,-3.190394,-3.4124413,-0.66412824,-3.2296658,-3.2204025,-3.1506634,-3.3026223,-3.1754675,-2.061399,-3.31177,-3.292438,-2.6827621,-1.4163969,-1.2715232,-3.313594,-2.0205758,-3.3117416,-3.1273143,-2.0687935,-3.3048851,-2.514449,-3.4514928,-3.47525,-3.338172,-3.187296,-3.3056169,-3.3362277,-3.21343,-2.0786126,-3.2734873,-3.325029,-3.3035388,-3.345442,-2.9304345,-2.171443,-3.3899047,-3.3315544,-2.8462918,-2.8947291,-3.3706272,-3.3615613,-3.223461,-3.2658918,-2.8263178,-3.0718124,-3.3567882,-3.3326623,-3.4200892,-3.228724,-2.9473937,-3.1402435,-3.1442652,-3.352255,-3.1053867,-3.136612,-2.0416052,-3.4262645,-3.4321396,-3.3920317,-3.3651123],\"xaxis\":\"x\",\"y\":[6.410346,6.571202,6.933715,6.929758,6.792601,6.872608,7.0016227,6.781454,6.7575254,6.8734813,6.816629,6.879123,6.6028585,4.919653,6.943989,6.899728,6.4960194,6.948088,6.5385857,4.847738,7.0995502,6.9054174,4.9721875,6.3386846,6.2204747,6.9028654,4.8337545,6.877314,6.468758,4.8202667,6.813815,6.465777,6.8337746,6.6138096,6.881072,6.9334316,6.851689,6.9918294,6.6753354,4.858548,6.72768,6.8289003,7.1162486,6.7423816,6.6311164,4.898474,6.8281097,6.990734,6.5346427,6.734492,6.992254,6.9958553,6.5524464,6.7189817,6.454235,6.435634,6.8518925,6.8284445,6.347525,6.8366737,6.544976,6.6283245,6.544332,6.8667784,6.445536,6.7415094,4.9300756,6.9359574,6.777309,6.6482463,6.6380167],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"present analyse three online algorithms learning discrete hidden markov models hmms compare baldicha...\"],[\"distributions permutations arise applications ranging multiobject tracking ranking instances difficu...\"],[\"propose restricted collapsed draw rcd sampler general markov chain monte carlo sampler simultaneous ...\"],[\"asymptotic pseudotrajectory approach stochastic approximation benaim hofbauer sorin extended asynchr...\"],[\"consider problem adaptive stratified sampling monte carlo integration differentiable function given ...\"],[\"many random processes simulated output deterministic model accepting random inputs model usually des...\"],[\"expectation propagation provides framework approximate inference model consideration latent gaussian...\"],[\"consider problem adaptive stratified sampling monte carlo integration noisy function given finite bu...\"],[\"dasgupta shulman showed tworound variant algorithm learn mixture gaussian distributions near optimal...\"],[\"optimal transportation distances fundamental family parameterized distances histograms despite appea...\"],[\"present new algorithms compute mean set empirical probability measures optimal transport metric mean...\"],[\"paper addresses problem filtering statespace model standard approaches filtering assume probabilisti...\"],[\"probabilistic models often parameters translated scaled permuted otherwise transformed without chang...\"],[\"paper demonstrate tempering markov chain monte carlo samplers bayesian models recursively subsamplin...\"],[\"present nonparametric prior reversible markov chains use completely random measures specifically gam...\"],[\"focus interpolation method referred bayesian reconstruction paper whereas standard interpolation met...\"],[\"applying standard markov chain monte carlo mcmc algorithms large data sets computationally expensive...\"],[\"propose novel sampling framework inference probabilistic models active learning approach converges q...\"],[\"probabilistic programming languages simplify development machine learning techniques inference suffi...\"],[\"recently theoretical guarantees obtained matrix completion nonuniform sampling regime particular sam...\"],[\"annealed importance sampling ais common algorithm estimate partition functions useful stochastic mod...\"],[\"approximate bayesian computation abc likelihoodfree monte carlo methods abc methods use comparison s...\"],[\"infinite hidden markov models ihmms attractive nonparametric generalization classical hidden markov ...\"],[\"range fields including geosciences molecular biology robotics computer vision one encounters problem...\"],[\"models complex systems often formalized sequential software simulators computationally intensive pro...\"],[\"large matrix factorisation problems develop distributed markov chain monte carlo mcmc method based s...\"],[\"propose kernel hamiltonian monte carlo kmc gradientfree adaptive mcmc algorithm based hamiltonian mo...\"],[\"modern scale data brought new challenges bayesian inference particular conventional mcmc algorithms ...\"],[\"given family probability measures space probability measures hilbert space goal paper highlight one ...\"],[\"factorial hidden markov models fhmms powerful tools modeling sequential data learning fhmms yields c...\"],[\"adaptive monte carlo schemes developed last years usually seek ensure ergodicity sampling process li...\"],[\"paper consider statistical problem learning linear model noisy samples existing work focused approxi...\"],[\"recent decades seen interest prediction problems bayesian methodology used ubiquitously sampling app...\"],[\"consider continuous time markovian processes populations individual agents interact stochastically a...\"],[\"consider problem transforming samples one continuous source distribution samples another target dist...\"],[\"introduce multivariate stochastic volatility model asset returns imposes restrictions structure vola...\"],[\"regular particle filter algorithm sequential monte carlo smc methods initial weights traditionally d...\"],[\"learning hidden markov model hmm sequen tial observations often complemented realvalued summary resp...\"],[\"recently stochastic gradient markov chain monte carlo sgmcmc methods proposed scaling monte carlo co...\"],[\"propose segmented ihmm sihmm hierarchical infinite hidden markov model ihmm supports simple efficien...\"],[\"contrastive divergence algorithm achieved notable success training energybased models including rest...\"],[\"estimation normalizing constants fundamental step probabilistic model comparison sequential monte ca...\"],[\"article present elitist particle filter based evolutionary strategies epfes efficient approach nonli...\"],[\"propose datadriven coarsegraining formulation context equilibrium statistical mechanics contrast exi...\"],[\"monte carlo sampling algorithms extremely widelyused technique estimate expectations functions espec...\"],[\"probabilistic programming languages represent complex data intermingled models lines code efficient ...\"],[\"hamiltonian monte carlo hmc exploits hamiltonian dynamics construct efficient proposals markov chain...\"],[\"study probability measures induced set functions constraints measures arise variety realworld settin...\"],[\"factorial hidden markov models fhmms powerful models sequential data scale well long sequences propo...\"],[\"manifold markov chain monte carlo algorithms introduced sample effectively challenging target densit...\"],[\"develop automated variational inference method bayesian structured prediction problems gaussian proc...\"],[\"hamiltonian monte carlo hmc popular markov chain monte carlo mcmc algorithm generates proposals metr...\"],[\"study statistical inference distributionally robust solution methods stochastic optimization problem...\"],[\"importance sampling widely used machine learning statistics power limited restriction using simple p...\"],[\"recent advances bayesian learning largescale data witnessed emergence stochastic gradient mcmc algor...\"],[\"ability track moving vehicle crucial importance numerous applications task often approached importan...\"],[\"increasing availability vehicle gps data created potentially transformative opportunities traffic ma...\"],[\"introduce novel approach parallelizing mcmc inference models spatially determined conditional indepe...\"],[\"common problem disciplines applied statistics research astrostatistics estimating posterior distribu...\"],[\"propose novel method semisupervised learning ssl based datadriven distributionally robust optimizati...\"],[\"discuss bayesian formulation coarsegraining pdes coefficients material parameters exhibit random fin...\"],[\"study dual volume sampling method selecting columns short wide matrix probability selection proporti...\"],[\"propose novel adaptive importance sampling algorithm incorporates stein variational gradient decent ...\"],[\"langevin diffusion commonly used tool sampling given distribution work establish target density log ...\"],[\"ability compare two degenerate probability distributions two probability distributions supported two...\"],[\"stochastic gradient mcmc sgmcmc algorithms proven useful scaling bayesian inference large datasets a...\"],[\"stochastic gradient markov chain monte carlo sgmcmc developed flexible family scalable bayesian samp...\"],[\"edge partition model epm fundamental bayesian nonparametric model extracting overlapping structure b...\"],[\"propose analyze two new mcmc sampling algorithms vaidya walk john walk generating samples uniform di...\"],[\"many matching tracking sorting ranking problems require probabilistic reasoning possible permutation...\"],[\"paper presents novel twostep approach fundamental problem learning optimal map one distribution anot...\"],[\"wellestablished methods solution stochastic partial differential equations spdes typically struggle ...\"],[\"technical report explores estimation methodologies hyperparameters markov random field gaussian hidd...\"],[\"paper investigates asymptotic behaviors gradient descent algorithms particularly accelerated gradien...\"],[\"stochastic gradient markov chain monte carlo sgmcmc increasingly popular bayesian learning due abili...\"],[\"propose new sampling method thermostatassisted continuouslytempered hamiltonian monte carlo bayesian...\"],[\"performing inference simulators generally intractable runtime means cannot compute marginal likeliho...\"],[\"need reason uncertainty large complex multimodal datasets become increasingly common across modern s...\"],[\"propose novel approach parameter estimation simulatorbased statistical models intractable likelihood...\"],[\"optimal transport theory informally described using words french mathematician gaspard monge worker ...\"],[\"optimal transport based distances powerful tools machine learning compare probability measures manip...\"]],\"hovertemplate\":\"label=sgmcmc\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"sgmcmc\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"sgmcmc\",\"showlegend\":true,\"x\":[-4.5465264,-2.8608527,-4.502121,-4.2957177,-4.6419353,-4.4453034,-3.7675135,-4.5567994,-3.1347754,-2.6649208,-2.6593974,-3.9002166,-4.294873,-4.398933,-4.646098,-3.8682973,-4.6337924,-4.5242386,-4.276453,-1.7439045,-3.7156088,-3.9101741,-4.579681,-2.9786656,-4.2834444,-4.640523,-4.7454576,-4.541907,-2.6911166,-4.6246724,-4.4995413,-1.9197454,-4.204885,-4.5282636,-4.007346,-4.5000815,-4.564432,-4.724087,-4.6916203,-4.5973725,-4.710059,-4.6937633,-3.271831,-4.1331825,-4.3685894,-4.1381335,-4.7741776,-4.361565,-4.9204235,-4.4816985,-4.296583,-4.7519946,-4.633618,-1.9626892,-4.7162595,-3.8357491,-3.7967784,-4.244778,-3.9593318,-2.6277616,-4.3768396,-1.6784405,-4.9042687,-4.606694,-4.741699,-4.7099814,-4.714113,-4.123317,-4.4653974,-2.7441535,-4.72095,-4.3694944,-4.305723,-2.3424811,-4.871745,-4.7915893,-4.0225687,-4.658052,-3.9367256,-2.7023606,-2.340494],\"xaxis\":\"x\",\"y\":[5.217755,5.4868336,4.979817,4.541516,4.852147,4.746954,4.5139556,4.836305,4.941223,5.533588,5.5735416,4.6962147,4.775813,4.869188,5.1045117,4.9085402,4.6503606,4.758365,4.84167,5.9408765,2.769301,4.912106,5.202838,5.5711803,4.8224573,4.6280823,4.715852,4.8445177,5.595399,5.291449,4.794649,4.6794553,4.9852133,4.858388,4.8817883,4.556103,4.6225247,5.284386,4.6715946,5.2722044,4.6183414,4.569614,4.567742,4.6862698,4.74278,4.724029,4.7232757,4.8413033,5.2684774,4.7873034,4.8874598,4.688014,4.523331,3.564814,4.5586605,4.7022843,4.7618637,5.0227685,4.868128,4.326355,4.3044677,6.011359,4.4913855,4.6939325,4.0959845,4.736192,4.516407,5.1528325,4.8406305,5.5098987,4.227444,4.121164,4.851988,5.159774,4.5355597,4.6557755,4.909702,4.370377,4.8683257,5.526844,5.2008452],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"recent years kernel density estimation exploited computer scientists model machine learning problems...\"],[\"monograph deals adaptive supervised classification using tools borrowed statistical mechanics inform...\"],[\"propose investigate test statistics testing homogeneity reproducing kernel hilbert spaces asymptotic...\"],[\"problem supervised classification discrimination functional data considered special interest popular...\"],[\"direct way express arbitrary dependencies datasets estimate joint distribution apply afterwards argm...\"],[\"autoencoder neural network implemented estimate missing data genetic algorithm implemented network o...\"],[\"give improved constants data dependent variance sensitive confidence bounds called empirical bernste...\"],[\"discuss meanfield theory cellular automata model metalearning metalearning process combining outcome...\"],[\"present work new family kernels compare positive measures arbitrary spaces xcal endowed positive ker...\"],[\"last years due growing ubiquity unlabeled data much effort spent machine learning community develop ...\"],[\"paper reviews functional aspects statistical learning theory main point consideration nature hypothe...\"],[\"survey introduction positive definite kernels set methods inspired machine learning literature namel...\"],[\"nonparametric kernelbased method realizing bayes rule proposed based representations probabilities r...\"],[\"nonparametric classification regression problems regularized kernel methods particular support vecto...\"],[\"despite recent progress towards efficient multiple kernel learning mkl structured output case remain...\"],[\"density ratio defined ratio two probability densities study inference problem density ratios apply s...\"],[\"derive upper bound local rademacher complexity ellpnorm multiple kernel learning yields tighter exce...\"],[\"present probabilistic viewpoint multiple kernel learning unifying wellknown regularised risk approac...\"],[\"paper give new sharp generalization bound lpmkl generalized framework multiple kernel learning mkl i...\"],[\"response problem vidyasagar state criterion pac learnability concept class mathscr family nonatomic ...\"],[\"given reproducing kernel hilbert space realvalued functions suitable measure source space subset dec...\"],[\"propose novel algebraic framework treating probability distributions represented cumulants mean cova...\"],[\"modelling real world complexity music challenge machine learning address task modeling melodic seque...\"],[\"vapnikchervonenkis dimension fundamental measure generalization capacity learning algorithms however...\"],[\"paper give new generalization error bound multiple kernel learning mkl general class regularizations...\"],[\"regularized kernel methods support vector machines leastsquares support vector regression constitute...\"],[\"paper describes novel method approximate polynomial coefficients regression functions particular int...\"],[\"consider problem learning set random samples show relevant geometric topological properties set stud...\"],[\"paper study statistical properties semisupervised learning considered important problem community ma...\"],[\"prove density function gradient sufficiently smooth function omega subset mathbbrd rightarrow mathbb...\"],[\"study highdimensional asymptotic performance limits binary supervised classification problems class ...\"],[\"develop approach feature elimination statistical learning kernel machines based recursive eliminatio...\"],[\"important aspect classifier error rate quantifies predictive capacity thus accuracy error estimation...\"],[\"kernel methods widely applied machine learning questions approximating unknown function finite sampl...\"],[\"incorporating spatial information hyperspectral unmixing procedures shown positive effects due inher...\"],[\"main goal article address bipartite ranking issue perspective functional data analysis fda given tra...\"],[\"paper presents general vectorvalued reproducing kernel hilbert spaces rkhs framework problem learnin...\"],[\"new non parametric approach problem testing independence two random process developed test statistic...\"],[\"paper concerned obtaining distributionfree concentration inequalities mixture independent bernoulli ...\"],[\"connect shiftinvariant characteristic kernels infinitely divisible distributions mathbbrd characteri...\"],[\"first encountered pacbayesian concentration inequalities seemed rather disconnected good oldfashione...\"],[\"additive models play important role semiparametric statistics paper gives learning rates regularized...\"],[\"paper deals problem nonparametric independence testing fundamental decisiontheoretic problem asks tw...\"],[\"analyze generalization robustness batched weighted average algorithm vgeometrically ergodic markov d...\"],[\"wild bootstrap method nonparametric hypothesis tests based kernel distribution embeddings proposed b...\"],[\"study prediction estimation problems using empirical risk minimization relative general convex loss ...\"],[\"give comprehensive theoretical characterization nonparametric estimator divergence two continuous di...\"],[\"propose likelihood ratio based inferential framework high dimensional semiparametric generalized lin...\"],[\"consider problem uncertainty assessment low dimensional components high dimensional models specifica...\"],[\"consider setting linear regression high dimension focus problem constructing adaptive honest confide...\"],[\"statistical test independence may constructed using hilbertschmidt independence criterion hsic test ...\"],[\"concerned obtaining novel concentration inequalities missing mass total probability mass outcomes ob...\"],[\"novel concentration inequalities obtained missing mass total probability mass outcomes observed samp...\"],[\"paper propose family tractable kernels dense family bounded positive semidefinite functions approxim...\"],[\"propose class nonparametric twosample tests cost linear sample size two tests given based ensemble d...\"],[\"kernel methods ubiquitous tools machine learning however often little reason common practice selecti...\"],[\"kernel bayes rule proposed nonparametric kernelbased method realize bayesian inference reproducing k...\"],[\"work studies class algorithms learning sideinformation emerge extending generative models embedded c...\"],[\"study paper consequences using mean absolute percentage error mape measure quality regression models...\"],[\"propose nonparametric statistical test goodnessoffit given set samples test determines likely genera...\"],[\"derive new discrepancy statistic measuring differences two probability distributions based combining...\"],[\"many machine learning problems characterized mutual contamination models problems one observes sever...\"],[\"apply wild bootstrap method lancaster threevariable interaction measure order detect factorisation j...\"],[\"kernel methods one mainstays machine learning problem kernel learning remains challenging heuristics...\"],[\"consider statistical inverse learning problem observe image function linear operator iid random desi...\"],[\"study paper consequences using mean absolute percentage error mape measure quality regression models...\"],[\"kernelbased quadrature rules becoming important machine learning statistics achieve supersqrtn conve...\"],[\"vast majority neural network literature focuses predicting point values given set response variables...\"],[\"introduce mondrian kernel fast random feature approximation laplace kernel suitable batch online lea...\"],[\"study twolevel multiview learning two views pacbayesian framework approach sometimes referred late f...\"],[\"propose vectorvalued regression problem whose solution equivalent reproducing kernel hilbert space r...\"],[\"study density estimation problem observations generated certain dynamical systems admit unique under...\"],[\"concentration inequalities indispensable tools studying generalization capacity learning models hoef...\"],[\"highdimensional estimation prediction methods propose minimize cost function empirical risk written ...\"],[\"nonlinear similarity measures defined kernel space correntropy extract higherorder statistics data o...\"],[\"many applications particular information systems pattern recognition machine learning cheminformatic...\"],[\"propose nonparametric sequential test aims address two practical problems pertinent online randomize...\"],[\"provide theoretical foundation nonparametric estimation functions random variables using kernel mean...\"],[\"kernel dependence measures yield accurate estimates nonlinear relations random variables also endors...\"],[\"consider learning algorithms general source condition polynomial decay eigenvalues integral operator...\"],[\"investigate kernel regularization methods achieve minimax convergence rates source condition regular...\"],[\"random sinusoidal features popular approach speeding kernelbased inference large datasets prior infe...\"],[\"hypothesis tests models whose dimension far exceeds sample size formulated much like classical stude...\"],[\"learning rates leastsquares regression typically expressed terms lnorms paper extend rates norms str...\"],[\"study fundamental class regression models called second order linear model slm slm extends linear mo...\"],[\"massive amount available data potentially used discover patters machine learning challenge kernel ba...\"],[\"kernel embeddings distributions maximum mean discrepancy mmd resulting distance distributions useful...\"],[\"integrating visual linguistic information single multimodal representation unsolved problem widereac...\"],[\"study strictly proper scoring rules reproducing kernel hilbert space propose general kernel scoring ...\"],[\"paper aims formulating issue ranking multivariate unlabeled observations depending degree abnormalit...\"],[\"paper introduces kernel mixture network new method nonparametric estimation conditional probability ...\"],[\"minimizing empirical risk popular training strategy learning tasks data may noisy heavytailed one ma...\"],[\"twosample hypothesis testing problem studied challenging scenario high dimensional data sets small s...\"],[\"paper introduces youtubem video understanding challenge hosted kaggle competition also describes app...\"],[\"propose ksparse exhaustive search esk method ksparse approximate exhaustive search method aesk selec...\"],[\"study learning problems involving arbitrary classes functions distributions targets proper learning ...\"],[\"twosample feature selection problem finding features describe difference two probability distributio...\"],[\"smallball method introduced way obtaining high probability isomorphic lower bound quadratic empirica...\"],[\"many machine learning problems characterized mutual contamination models problems one observes sever...\"],[\"work constructs hypothesis test detecting whether datagenerating function rightarrow belongs specifi...\"],[\"lay theoretical foundations new database release mechanisms allow thirdparties construct consistent ...\"],[\"nonparametric family conditional distributions introduced generalizes conditional exponential famili...\"],[\"problem domain generalization labeled training data sets several related prediction problems goal ma...\"],[\"formulate supervised learning problem referred continuous ranking continuous realvalued label assign...\"],[\"simple framework probabilistic multiview graph embedding pmvge proposed multiview feature learning m...\"],[\"modeling complex conditional distributions critical variety settings despite long tradition research...\"],[\"paper presents approximate confidence intervals function parameters banach space based bootstrap alg...\"],[\"selection validation basis full dataset often required industrial use supervised machine learning al...\"],[\"paper introduce conformal prediction method construct prediction sets oneshot federated learning set...\"],[\"semisupervised learning powerful technique leveraging unlabeled data improve machine learning models...\"]],\"hovertemplate\":\"label=kernels\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"kernels\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"kernels\",\"showlegend\":true,\"x\":[-2.27789,-1.913516,-2.1438022,-1.8598846,-2.50474,-1.843473,-2.0082052,-1.817901,-2.1887825,-1.4982964,-1.8163476,-2.044988,-2.8302996,-1.8937366,-1.7278247,-2.446809,-1.7774644,-2.0746005,-1.7200811,-2.1083004,-2.0410166,-2.602396,-4.001474,-1.9560499,-1.7387886,-1.8904458,-1.6657912,-1.8617164,-1.7720388,-2.6843863,-1.8197592,-1.9361099,-2.157212,-1.9863021,-2.3007662,-1.6620679,-1.7361822,-2.137068,-2.2214851,-2.4678485,-2.0539882,-2.0840707,-2.0310206,-2.0663652,-2.132468,-1.8346161,-2.3994331,-1.5999827,-1.5137957,-1.4547814,-2.1179347,-2.1532838,-2.1526327,-2.4127812,-2.1847382,-1.9890003,-2.2652447,-3.3642802,-1.5373726,-2.3866386,-2.2725585,-1.8532178,-2.1750445,-2.3220563,-1.6489946,-1.8224169,-2.11041,-3.0444202,-0.92891794,-1.8601272,-2.9464304,-2.7385705,-2.135756,-1.53526,-2.2118979,-2.2763648,-2.0873513,-2.2828372,-1.9922713,-1.6256753,-1.7587502,-2.079005,-1.7216336,-1.850578,-1.5638981,-1.6607352,-2.1090617,-1.9667016,-2.1009674,-2.2706645,-3.2676175,-2.1855404,-1.5749214,-1.7806497,-2.846414,-2.114535,-0.7948734,-2.1759093,-1.8284057,-2.0563297,-1.9260222,-3.0160725,-1.7011948,-1.7160354,-2.2324507,-3.025468,-2.172566,-1.7595611,-2.0178792,-1.7858448],\"xaxis\":\"x\",\"y\":[4.325052,3.439399,4.2170734,3.882961,4.4812827,3.2708445,3.5833774,3.2249935,4.4046416,3.522115,3.9894133,4.15033,4.365752,4.2709684,4.3128276,4.5485487,4.511357,4.445591,4.5397053,3.5848186,4.3386455,4.106552,4.599362,3.595426,4.5693603,4.270981,4.565982,4.014289,3.365892,4.709834,3.4527407,4.0750575,3.9098437,4.1772223,4.7675743,3.4696813,3.9558997,4.169747,3.615982,4.2566614,3.5701668,4.285291,4.228007,3.392935,4.2081532,3.7466736,4.5667305,4.26863,4.350291,4.482223,4.258492,3.566767,3.560738,4.431068,4.211515,4.1536474,4.2622237,4.6463833,4.2444053,4.4334917,4.274749,3.5715952,4.2029796,4.290458,4.9851556,4.1749644,4.293578,4.785936,3.0547693,3.496264,4.3019643,4.8151174,3.5563133,4.0832534,4.487332,4.0998373,4.1012225,4.3720303,4.2595615,4.7583685,4.594625,4.558415,4.3178153,4.404557,4.230255,3.8762403,4.259006,3.935629,4.268266,3.763592,4.6134353,3.1566596,3.556836,3.0619705,5.234199,3.5698788,4.003666,3.561715,3.5032935,4.0916505,3.7189388,4.8100224,3.3554306,3.245161,3.8011713,4.8327336,4.0793324,3.4144676,3.5780838,3.2023325],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"simulated annealing popular method approaching solution global optimization problem existing results...\"],[\"propose extension concept expected improvement criterion commonly used kriging based optimization ex...\"],[\"provide yet another proof existence calibrated forecasters two merits first valid arbitrary finite n...\"],[\"study problem allocating stocks dark pools propose analyze optimal approach allocations continuousva...\"],[\"address online linear optimization problem actions forecaster represented binary vectors goal unders...\"],[\"paper studies deviations regret stochastic multiarmed bandit problem total number plays known before...\"],[\"methods decisiontheoretic online learning based hedge algorithm takes parameter called learning rate...\"],[\"paper devoted regret lower bounds classical model stochastic multiarmed bandit wellknown result lai ...\"],[\"thompson sampling demonstrated many complex bandit models however theoretical guarantees available p...\"],[\"introduce means automating machine learning big data tasks performing scalable stochastic bayesian o...\"],[\"practical bayesian optimization must often search structures differing numbers parameters instance m...\"],[\"classical approach inverse problems based optimization misfit function despite computational appeal ...\"],[\"paper focus developing efficient sensitivity analysis methods computationally expensive objective fu...\"],[\"bayesian optimization powerful tool finetuning hyperparameters wide variety machine learning models ...\"],[\"unknown constraints arise many types expensive blackbox optimization problems several methods propos...\"],[\"bayesian optimization effective methodology global optimization functions expensive evaluations reli...\"],[\"address problem synthetic gene design using bayesian optimization main issue designing gene design s...\"],[\"popularity bayesian optimization methods efficient exploration parameter spaces lead series papers a...\"],[\"renewed interest formulating integration inference problem motivated obtaining full distribution num...\"],[\"approximate bayesian computation abc using sequential monte carlo method provides comprehensive plat...\"],[\"bayesian optimization recently emerged popular efficient tool global optimization hyperparameter tun...\"],[\"study restless bandit associated extremely simple scalar kalman filter model discrete time certain a...\"],[\"consider mnkclassical problem controller activating sampling sequentially finite number geq populati...\"],[\"paper consider problem stochastic optimization bandit feedback model generalize gpucb algorithm srin...\"],[\"present glasses global optimisation lookahead stochastic simulation expectedloss search majority glo...\"],[\"inventory control unknown demand distribution considered emphasis placed case involving discrete non...\"],[\"present pesmo bayesian method identifying pareto set multiobjective optimization problems functions ...\"],[\"present informationtheoretic framework solving global blackbox optimization problems also blackbox c...\"],[\"paper index policies minimizing frequentist regret stochastic multiarmed bandit model inspired bayes...\"],[\"consider bayesian optimization expensivetoevaluate blackbox objective function also access cheaper a...\"],[\"many retailers today employ inventory management systems based reorder point policies rely assumptio...\"],[\"consider firm sells products periods without knowing demand function firm sequentially sets prices e...\"],[\"important problem sequential decisionmaking uncertainty use limited data compute safe policy policy ...\"],[\"work presents pesmoc predictive entropy search multiobjective bayesian optimization constraints info...\"],[\"game theory finds nowadays broad range applications engineering machine learning however derivativef...\"],[\"consider problem estimating expected value information knowledge gradient bayesian learning problems...\"],[\"typical applications bayesian optimization minimal assumptions made objective function optimized tru...\"],[\"propose framework modeling estimating state controlled dynamical systems agent affect system actions...\"],[\"present mlrmbo flexible comprehensive toolbox modelbased optimization mbo also known bayesian optimi...\"],[\"propose novel bayesian optimization approach blackbox functions environmental variable whose value d...\"],[\"bandit methods blackbox optimisation bayesian optimisation used variety applications including hyper...\"],[\"existing strategies finitearmed stochastic bandits mostly depend parameter scale must known advance ...\"],[\"consider explorationexploitation tradeoff linear quadratic control problems state dynamics linear co...\"],[\"tradeoff cost acquiring processing data uncertainty due lack data fundamental machine learning basic...\"],[\"exciting branch machine learning research focuses methods learning optimizing integrating unknown fu...\"],[\"bayesian optimization emerged last years effective approach optimizing blackbox functions direct que...\"],[\"order achieve stateoftheart performance modern machine learning techniques require careful data prep...\"],[\"actorcritic methods solve reinforcement learning problems updating parameterized policy known actor ...\"],[\"consider parametric exponential families dimension real line study variant textitboundary crossing p...\"],[\"chemical space large brute force searches new interesting molecules infeasible highthroughput virtua...\"],[\"bayesian optimization methods useful optimizing functions expensive evaluate lack analytical express...\"],[\"propose novel theoreticallygrounded acquisition function batch bayesian optimization informed insigh...\"],[\"choosing bestperforming optimizers portfolio optimization algorithms usually difficult complex task ...\"],[\"consider problem sequentially making decisions rewarded successes failures predicted unknown relatio...\"],[\"introduce methodology efficiently computing lower bound empowerment allowing used unsupervised cost ...\"],[\"propose first fullyadaptive algorithm pure exploration linear banditsthe task find arm largest expec...\"],[\"consider novel stochastic multiarmed bandit problem called good arm identification gai good arm defi...\"],[\"informationtheoretic bayesian optimisation techniques demonstrated stateoftheart performance tacklin...\"],[\"novel python framework bayesian optimization known gpflowopt introduced package based popular gpflow...\"],[\"bayesian optimization modelbased approach gradientfree blackbox function optimization typically powe...\"],[\"stochastic bandit problem goal maximize unknown function via sequence noisy evaluations typically ob...\"],[\"paper presents novel approach direct covariance function learning bayesian optimisation particular e...\"],[\"scaling bayesian optimization high dimensions challenging task global optimization highdimensional a...\"],[\"distributional approaches valuebased reinforcement learning model entire distribution returns rather...\"],[\"modern supervised machine learning algorithms involve hyperparameters set running options setting hy...\"],[\"propose practical extensions bayesian optimization solving dynamic problems model dynamic objective ...\"],[\"paper investigate capability universal kriging model singleobjective global optimization applied wit...\"]],\"hovertemplate\":\"label=optimisation\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"optimisation\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"optimisation\",\"showlegend\":true,\"x\":[-3.6229222,-3.4549992,-2.7639647,-3.5088158,-3.175972,-3.177786,-3.0425851,-3.1788564,-3.194493,-2.2632847,-3.6825202,-3.7609582,-3.6247678,-3.3984187,-3.669648,-3.9289315,-3.7211456,-3.7803254,-3.828849,-3.8745966,-3.6457312,-3.184772,-3.151324,-3.216796,-3.425945,-3.0503445,-3.6088223,-3.6450546,-3.1946678,-3.5016794,-3.0060627,-2.944211,-3.2748961,-3.6371171,-3.548184,-3.2576284,-3.7108037,-3.583705,-3.6079297,-3.6482322,-3.3474586,-3.1830826,-3.2182384,-3.1875005,-3.7122982,-3.55908,-2.2068844,-3.1960812,-3.1816165,-3.8267372,-3.7324526,-3.6726437,-3.524572,-3.0529892,-3.5688334,-3.1811442,-3.1906273,-3.6127274,-3.7779949,-3.751347,-3.191859,-3.6786451,-3.683445,-3.122148,-1.9915563,-3.688236,-3.209887],\"xaxis\":\"x\",\"y\":[2.5990431,2.6061604,2.7626135,2.4484615,2.0650368,2.0627925,2.2720315,2.067172,2.0695796,2.9660752,2.5565333,2.6327226,2.7343647,2.6487606,2.513852,2.9075787,2.6526978,2.731337,3.1497827,3.179023,2.5951147,2.0600877,2.1062095,2.0786533,2.3497934,2.4189634,2.4523659,2.492537,2.0718343,2.3937132,2.4736197,2.624175,2.2945156,2.4648843,2.3936763,2.3889408,2.5056005,3.3672142,2.4665992,2.4832864,2.2146885,2.0667531,2.0905237,2.102937,2.7409701,2.363881,2.914224,2.193999,2.0556276,3.038609,2.5773625,2.5458395,2.4667706,2.245017,3.166681,2.0647857,2.072162,2.4305933,2.7421021,2.6225264,2.0522835,2.5590012,2.52929,2.3383873,3.012719,2.566211,2.680951],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"present nested chinese restaurant process ncrp stochastic process assigns probability distributions ...\"],[\"among easiest ways find meaningful structure discrete data latent dirichlet allocation lda related c...\"],[\"describe new method visualizing topics distributions terms automatically extracted large text corpor...\"],[\"propose dirichlet process mixtures generalized linear models dpglm new method nonparametric regressi...\"],[\"introduce supervised latent dirichlet allocation slda statistical model labelled documents model acc...\"],[\"present discrete infinite logistic normal distribution diln bayesian nonparametric prior mixed membe...\"],[\"describe simple efficient procedure approximating levy measure textgammaalpha random variable use ap...\"],[\"single stationary topic model latent dirichlet allocation inappropriate modeling corpora span long t...\"],[\"nonparametric mixture models based dirichlet process elegant alternative finite models number underl...\"],[\"new geometricallymotivated algorithm nonnegative matrix factorization developed applied discovery la...\"],[\"develop nested hierarchical dirichlet process nhdp hierarchical topic modeling nhdp generalization n...\"],[\"question determine number independent latent factors topics mixture models latent dirichlet allocati...\"],[\"bayesian mixture models widely applied unsupervised learning exploratory data analysis markov chain ...\"],[\"present novel scalable bayesian approach modelling occurrence pairs symbols drawn large vocabulary o...\"],[\"dirichlet process mixture dpm ubiquitous flexible bayesian nonparametric statistical model however f...\"],[\"theory bayesian nonparametric bnp models well suited streaming data scenarios due ability adapt mode...\"],[\"using nonparametric methods increasingly explored bayesian hierarchical modeling way increase model ...\"],[\"introduce novel approach estimating latent dirichlet allocation lda parameters collapsed gibbs sampl...\"],[\"many practical modeling problems involve discrete data best represented draws multinomial categorica...\"],[\"many modern data analysis problems involve inferences streaming data however streaming data easily a...\"],[\"tree structures ubiquitous data across many domains many datasets naturally modelled unobserved tree...\"],[\"robust bayesian models appealing alternatives standard models providing protection data contains out...\"],[\"present sparse treebased listbased density estimation methods binarycategorical data density estimat...\"],[\"document going derive equations needed implement variational bayes estimation parameters simplified ...\"],[\"document going derive equations needed implement variational bayes ivector extractor used extract lo...\"],[\"investigate class feature allocation models generalize indian buffet process parameterized gibbstype...\"],[\"topic models popular modeling discrete data texts images videos links provide efficient way discover...\"],[\"one core problems statistical models estimation posterior distribution topic models problem posterio...\"],[\"histogram method powerful nonparametric approach estimating probability density function continuous ...\"],[\"generating user interpretable multiclass predictions data rich environments many classes explanatory...\"],[\"dynamic topic models dtms effective discovering topics capturing evolution trends time series data p...\"],[\"document travel may expect short snippets document also travel introduce general framework incorpora...\"],[\"timevarying mixture densities occur many scenarios example distributions keywords appear publication...\"],[\"novel dynamic bayesian nonparametric topic model anomaly detection video proposed paper batch online...\"],[\"paper proposes novel dynamic hierarchical dirichlet process topic model considers dependence success...\"],[\"nonnegative matrix factorization nmf popular tool data exploration bayesian nmf promises also charac...\"],[\"propose geometric algorithm topic learning inference built convex geometry topics arising latent dir...\"],[\"semisupervised unsupervised systems provide operators invaluable support tremendously reduce operato...\"],[\"present wrightfisher indian buffet process wfibp probabilistic model timedependent data assumed gene...\"],[\"supervised topic models help clinical researchers find interpretable cooccurence patterns count data...\"],[\"paper presents algorithm unsupervised learning latent variable models unlabeled sets data base techn...\"],[\"topic models one popular methods learning representations text major challenge change topic model re...\"],[\"indian buffet process based models elegant way discovering underlying features within data set infer...\"],[\"emergence thriving development social networks huge number short texts accumulated need processed in...\"],[\"inference latent feature models bayesian nonparametric setting generally difficult especially high d...\"],[\"rising topic computational journalism enhance diversity news served subscribers foster exploration b...\"],[\"bayesian models mix multiple dirichlet prior parameters called multidirichlet priors paper gaining p...\"],[\"propose new algorithms topic modeling number topics unknown approach relies analysis concentration m...\"],[\"nonnegative matrix factorization nmf technique finding latent representations data method applied co...\"],[\"introduce new approach topic modeling supervised survival analysis specifically build recent work un...\"],[\"large scale online inference problems update strategy critical performance derive adaptive scan gibb...\"],[\"modern vehicles equipped increasingly complex sensors sensors generate large volumes data provide op...\"],[\"topic models bayesian models frequently used capture latent structure certain corpora documents imag...\"],[\"full length article draft version problem number topics topic modeling discussed proposed idea renyi...\"],[\"bayesian nonnegative matrix factorization nmf promising approach understanding uncertainty structure...\"]],\"hovertemplate\":\"label=topics\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"topics\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"topics\",\"showlegend\":true,\"x\":[-4.468881,-4.5472064,-4.8509445,-4.3690596,-4.8305655,-4.6498404,-4.5181046,-4.851157,-4.404427,-4.619837,-4.836074,-4.7930584,-4.4184704,-4.5138535,-4.3516545,-4.4655213,-4.584308,-4.7807913,-4.5905724,-4.466633,-4.126127,-4.7095866,-4.0375605,-4.671588,-4.678787,-4.5202723,-4.8274455,-4.822992,-4.291004,-4.8301845,-4.8500285,-4.2528634,-4.3738403,-4.615687,-4.717145,-4.6557198,-4.861895,-4.7489977,-4.5591736,-4.868392,-4.8342857,-4.8805566,-4.4844265,-4.8784056,-4.368417,-4.7792172,-4.4506907,-4.8404717,-4.793885,-4.8819175,-4.707337,-4.7193756,-4.8479166,-4.857499,-4.5914226],\"xaxis\":\"x\",\"y\":[6.1627216,6.4310994,6.365045,5.9239144,6.348945,6.1358395,5.881026,6.3298945,5.9714613,6.54437,6.380164,6.3425517,5.6050897,6.0364513,5.8777695,5.979855,6.0764093,6.294092,6.1270337,5.986854,6.285338,6.092036,6.2152395,5.9739246,5.900509,5.9802675,6.34788,6.3209844,6.149956,6.313799,6.328179,5.5381026,6.000498,6.1768517,6.2233005,5.510573,6.3358984,6.259016,5.9359674,6.395584,6.3477287,6.3593645,5.797834,6.3527145,5.6130867,6.285536,5.8416734,6.339903,6.3926573,6.3705034,6.2084174,6.2252526,6.3222303,6.3553534,5.47718],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"provide selfcontained proof theorem relating probabilistic coherence forecasts nondomination rival f...\"],[\"changepoints abrupt variations generative parameters data sequence online detection changepoints use...\"],[\"article introduces new algorithm reconstructing epsilonmachines data well decisional states defined ...\"],[\"propose work new family kernels variablelength time series work builds upon vector autoregressive va...\"],[\"unsupervised discovery latent representations addition useful density modeling visualisation explora...\"],[\"properties data frequently seen vary depending sampled situations usually changes along time evoluti...\"],[\"vector autoregressive var model powerful tool modeling complex time series exploited many fields how...\"],[\"deep learning methods attempt learn generic features unsupervised fashion large unlabelled data set ...\"],[\"abtesting popular technique web companies since makes possible accurately predict impact modificatio...\"],[\"challenging task modeling multivariate time series propose new class models use dependent matern pro...\"],[\"present novel algorithm westfallyoung light detecting patterns itemsets subgraphs statistically sign...\"],[\"present new approach estimating interdependence industries economy applying data science solutions e...\"],[\"present vectorspace markov random fields vsmrfs novel class undirected graphical models variable bel...\"],[\"introduce new class nonstationary kernels derive covariance functions novel family stochastic proces...\"],[\"using proper model characterize time series crucial making accurate predictions work use timevarying...\"],[\"paper introduce novel online time series forecasting model refer pmgp filter show model equivalent g...\"],[\"address problem detecting changes multivariate datastreams investigate intrinsic difficulty changede...\"],[\"present scalable gaussian process model identifying characterizing smooth multidimensional changepoi...\"],[\"paper study predictive pattern mining problems goal construct predictive model based subset predicti...\"],[\"discovering statistically significant patterns databases important challenging problem main obstacle...\"],[\"linear autoregressive models serve basic representations discrete time stochastic processes differen...\"],[\"gaussian graphical models ggms probabilistic tools choice analyzing conditional dependencies variabl...\"],[\"separating short jobs long known technique improve scheduling performance paper describe method deve...\"],[\"study problem estimating parameters regression model set observations consisting response predictor ...\"],[\"paper discusses package implements pattern sequence based forecasting psf algorithm developed univar...\"],[\"exploration hydrocarbon resources highly complicated expensive process various geological geochemica...\"],[\"scale data growing every day reducing dimensionality aka sketching highdimensional data emerged task...\"],[\"paper develop new sequential regression modeling approach data streams data streams commonly found a...\"],[\"paper investigate link state space models gaussian processes time series modeling forecasting partic...\"],[\"widespread need techniques discover structure time series data recently introduced techniques automa...\"],[\"gaussian graphical model graphical representation dependence structure gaussian random vector recogn...\"],[\"paper presents novel datadriven technique based spatiotemporal pattern network stpn energypower pred...\"],[\"paper proposes hierarchical feature extractor nonstationary streaming time series based concept swit...\"],[\"statistical downscaling global climate models gcms allows researchers study local climate change eff...\"],[\"yield curve forecasting important problem finance work explore use gaussian processes conjunction dy...\"],[\"propose paper differentiable learning loss time series building upon celebrated dynamic time warping...\"],[\"gaussian process model vectorvalued function shown useful multioutput prediction existing method mod...\"],[\"present method conditional time series forecasting based adaptation recent deep convolutional wavene...\"],[\"analyzing multivariate time series data important predict future events changes complex systems fina...\"],[\"waggle dance honeybees perform astonishing way communicating location food source years discovery re...\"],[\"work develop fast saliency detection method applied differentiable image classifier train masking mo...\"],[\"study problem detecting change points cps characterized subset dimensions multidimensional sequence ...\"],[\"time series analysis used understand predict dynamic processes including evolving demands business w...\"],[\"latent feature modeling allows capturing latent structure responsible generating observed properties...\"],[\"paper present method determine global horizontal irradiance ghi power measurements one systems locat...\"],[\"power supply renewable resources global rise forecasted renewable generation surpass types generatio...\"],[\"nowadays unprecedented penetration renewable distributed energy resources ders necessity efficient e...\"],[\"soil moisture active passive smap mission delivered valuable sensing surface soil moisture since how...\"],[\"renewable distributed energy resources ders penetrate power grid accelerating speed essential operat...\"],[\"paper presents technique reducedorder markov modeling compact representation timeseries data work sy...\"],[\"present new method forecasting systems multiple interrelated time series method learns forecast mode...\"],[\"deep convolutional neural networks cnns based approaches stateoftheart various computer vision tasks...\"],[\"uncertainty analysis form probabilistic forecasting significantly improve decision making processes ...\"],[\"identifying changes generative process sequential data known changepoint detection become increasing...\"],[\"time series forecasting widely used multitude domains paper present four models predict stock price ...\"],[\"use covariance kernels ubiquitous field spatial statistics kernels allow data mapped highdimensional...\"],[\"modern aircraft may require order thousands custom shims fill gaps structural components airframe ar...\"],[\"propose framework general probabilistic multistep time series regression specifically exploit expres...\"],[\"interpretation deep learning models challenge due size complexity often opaque internal state additi...\"],[\"point forecasting univariate time series challenging problem extensive work conducted however nonpar...\"],[\"paper deals inference prediction multiple correlated time series one also choice using candidate poo...\"]],\"hovertemplate\":\"label=forecasting\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"forecasting\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"forecasting\",\"showlegend\":true,\"x\":[-2.7423446,-2.8424616,-2.7652347,-3.003081,-2.9020936,-2.9687026,-3.0573063,-4.5835123,-2.0004585,-3.190295,-0.9636413,-3.7512057,-2.5235362,-3.2938578,-2.9743612,-3.2773888,-2.8360066,-3.272894,-0.8256898,-0.8478418,-3.106751,-3.025966,-1.4887646,-2.9078217,-2.9636884,-2.8046005,-2.6140983,-3.0471957,-3.3010466,-2.9650028,-2.7519407,-3.0909417,-2.9100738,-3.5427575,-2.9705417,-3.0596843,-3.2788465,-3.2002327,-3.0904162,-2.6277633,-4.4795775,-2.6967328,-3.115681,-3.5065224,-3.0803335,-3.0883605,-3.0997708,-3.5374982,-3.021354,-3.1560037,-3.0588982,-4.528135,-3.1107965,-2.9490402,-3.0731494,-3.0376627,-2.4471807,-3.1664045,-4.5602117,-3.1015136,-3.038682],\"xaxis\":\"x\",\"y\":[3.3287795,3.7099059,3.392615,3.5562313,3.834871,3.8170125,3.5051918,3.3358426,3.757174,3.6110363,3.5005348,5.868148,3.8060925,3.9523282,3.468213,3.6779175,3.7501502,4.092563,3.4274948,3.4499843,3.6909094,3.8013654,3.3228083,3.3601646,3.400456,3.5344794,3.7057145,3.4216993,3.7819488,3.5861819,3.8468032,3.226051,3.559533,3.6275063,3.4326072,3.4290228,3.7598038,3.2680051,3.4991052,3.5976508,3.3537621,3.6782486,3.292446,4.571811,3.2273197,3.2182748,3.2310927,3.5679297,3.1903903,3.5284083,3.344427,3.3355842,3.2244122,3.7424214,3.2986355,3.845756,3.4430695,3.289125,3.3680177,3.2407131,3.356036],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"characterize study variable importance vimp pairwise variable associations binary regression trees k...\"],[\"present new online boosting algorithm adapting weights boosted classifier yields closer approximatio...\"],[\"paper examines experimental perspective random forests increasingly used statistical method classifi...\"],[\"present first treebased regressor whose convergence rate depends intrinsic dimension data namely ass...\"],[\"present new boosting algorithm motivated large margins theory boosting give experimental evidence ne...\"],[\"variable selection highdimensional linear models received lot attention lately mostly context lregul...\"],[\"accurate predictions typically obtained learning machines complex feature spaces induced kernels unf...\"],[\"many data mining applications collection sufficiently large datasets time consuming expensive hand i...\"],[\"consider problem learning forest nonlinear decision rules general loss functions standard methods em...\"],[\"article proposed several approaches post processing large ensemble prediction models rules results s...\"],[\"offer novel view adaboost statistical setting propose bayesian model binary classification label noi...\"],[\"testament success theory random forests long outpaced application practice paper take step towards n...\"],[\"concept hierarchies formal concept analysis theoretically well grounded largely experimented methods...\"],[\"propose original model inferring team strengths using markov random field used generate historical e...\"],[\"big data bring new opportunities modern society challenges data scientists one hand big data hold gr...\"],[\"article proposes multinomial probit bayesian additive regression trees mpbart multinomial probit ext...\"],[\"motivation work improve performance standard stacking approaches ensembles composed simple heterogen...\"],[\"crowdsourcing popular paradigm effectively collecting labels low cost dawidskene estimator widely us...\"],[\"data analysis machine learning become integrative part modern scientific methodology offering automa...\"],[\"big data comes various ways types shapes forms sizes indeed almost areas science technology medicine...\"],[\"item response theory irt models categorical response data widely used analysis educational data comp...\"],[\"paper introduces develops novel variable importance score function context ensemble learning demonst...\"],[\"modern scientific research massive datasets huge numbers observations frequently encountered facilit...\"],[\"paper examines use residual bootstrap bias correction machine learning regression methods accounting...\"],[\"becoming increasingly important machine learning methods make predictions interpretable well accurat...\"],[\"methods transfer learning try combine knowledge several related tasks domains improve performance te...\"],[\"consider sequential decision making problems binary classification scenario learner takes active rol...\"],[\"estimating strength dependency two variables fundamental exploratory analysis many applications data...\"],[\"currently machine learning plays important role lives individual activities numerous people accordin...\"],[\"recursive partitioning approaches producing treelike models long standing staple predictive modeling...\"],[\"sampling replacement occurs many settings machine learning notably bagging ensemble technique valida...\"],[\"one objectively measure performance individual offensive lineman nfl existing literature proposes va...\"],[\"multiplayer online battle arena moba games among played digital games world games teams players figh...\"],[\"population migration valuable information leads proper decision urbanplanning strategy massive inves...\"],[\"rapid growth crowdsourcing platforms become easy relatively inexpensive collect dataset labeled mult...\"],[\"regression trees becoming increasingly popular omnibus predicting tools basis numerous modern statis...\"],[\"tree ensembles random forest boosted trees renowned high prediction performance whereas interpretabi...\"],[\"tree ensembles random forests boosted trees renowned high prediction performance however interpretab...\"],[\"ensemble regression trees become popular statistical tools estimation conditional mean given set pre...\"],[\"boosting combines weak biased learners obtain effective learning algorithms classification predictio...\"],[\"years ensemble methods become staple machine learning similarly generalized linear models glms becom...\"],[\"aim create framework transfer learning using latent factor models learn dependence structure larger ...\"],[\"bagging device intended reducing prediction error learning algorithms simplest form bagging draws bo...\"],[\"processes governing lives use part automatic decision step based feature vector derived applicant al...\"],[\"random forest missing data algorithms attractive approach dealing missing data desirable properties ...\"],[\"data collections become larger exploratory regression analysis becomes important challenging observa...\"],[\"work addresses various open questions theory active learning nonparametric classification contributi...\"],[\"inferring correct answers binary tasks based multiple noisy answers unsupervised manner emerged cano...\"],[\"objectives discussions fairness criminal justice risk assessments typically lack conceptual precisio...\"],[\"paper describes structuring data constructing plots explore forest classification models interactive...\"],[\"current work characterizes users vod streaming space userpersonas based tenure timeline temporal beh...\"],[\"sales forecasting plays prominent role business planning business strategy value importance advance ...\"],[\"possible perform linear regression datasets whose labels shuffled respect inputs explore question pr...\"],[\"infinitesimal jackknife recently applied random forest estimate prediction variance theorems verifie...\"],[\"machinelearned models often described black boxes many realworld applications however models may sac...\"],[\"transfer learning aims improve learning target domain borrowing knowledge related different source d...\"],[\"knearestneighbor knn ensembles exist despite efficacy approach regression classification outlier det...\"],[\"understanding player behavior fundamental game data science video games evolve players interact game...\"],[\"emergence mobile games caused paradigm shift videogame industry game developers disposal plethora in...\"],[\"reducing user attrition churn broad challenge faced several industries mobile social games decreasin...\"],[\"new social economic activities massively exploit big data machine learning algorithms inference peop...\"],[\"paper present technique using bootstrap estimate operating characteristics variability certain types...\"],[\"work presents new classifier specifically designed fully interpretable technique determines probabil...\"],[\"establish consistency algorithm mondrian forests randomized classification algorithm implemented onl...\"],[\"recently crowdsourcing emerged effective paradigm humanpowered large scale problem solving various d...\"],[\"missing data expected issue large amounts data collected several imputation techniques proposed tack...\"],[\"work analyze problem adoption mobile money pakistan using call detail records major telecom company ...\"],[\"decision trees algorithms use gain function select best split trees induction function crucial obtai...\"],[\"marketing analytics diverse field academic researchers practitioners coming range backgrounds includ...\"],[\"study logistic modelbased active learning procedure binary classification problems adopt batch subje...\"],[\"predictive models generalize well distributional shift often desirable sometimes crucial building ro...\"],[\"paper propose tackle problem reducing discrepancies multiple domains referred multisource domain ada...\"],[\"uplift modeling aimed estimating incremental impact action individuals behavior useful various appli...\"],[\"arrival digital platforms revolutionized occupational health giving possibility occupational health ...\"],[\"causal random forests provide efficient estimates heterogeneous treatment effects however forest alg...\"],[\"mapping forest resources carbon important improving forest management meeting objectives storing car...\"]],\"hovertemplate\":\"label=ensembles\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"ensembles\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"ensembles\",\"showlegend\":true,\"x\":[-0.39746752,-1.2575328,-0.42922145,-0.6097802,-1.320524,-0.41019621,-0.42705962,-1.5829837,-0.5642318,-0.45336333,-1.4398987,-0.40520757,-0.7119797,-1.6117102,-0.951918,-0.70024765,-0.46923593,-1.5120819,-0.47259548,-0.8671184,-0.3803746,-0.4379876,-1.0965077,-0.52013993,-0.4228859,-1.7646391,-1.5434785,-0.44722998,-1.1163738,-0.4350513,-0.9160311,-1.3934047,-1.0368915,-0.8337203,-1.510724,-0.42300588,-0.4344552,-0.41284174,-0.4907344,-1.4330442,-0.3536011,-1.6809103,-0.5614945,-0.6324212,-0.37688565,-0.36938608,-1.5990658,-2.8570032,-0.85509163,-0.39897552,-0.99555135,-0.81566644,-1.6316937,-0.43070737,-0.5517716,-1.7594116,-0.42386895,-1.0539194,-0.9639398,-0.83492744,-0.9074158,-0.4956829,-0.5285686,-0.57883364,-1.5224653,-0.35190392,-0.84445226,-0.43048522,-0.82370657,-1.517918,-1.7931596,-1.801227,-0.32564712,-0.6728833,-0.28631628,-0.56967556],\"xaxis\":\"x\",\"y\":[2.9050186,3.1562514,2.820288,2.9883997,3.160439,2.860929,2.8550944,3.016317,2.8620162,2.8150368,3.2191663,2.7811654,2.8712122,3.07843,3.3074574,3.012419,2.8672805,3.0112152,2.8667932,3.192162,3.1006143,2.8153918,3.353778,2.754764,2.881463,2.809608,2.9924417,2.833422,3.1567276,2.882745,2.8371584,3.0473628,2.6652405,2.9660764,2.9703655,2.8453596,2.7739213,2.843299,2.8027816,3.335847,2.8601835,2.8401597,2.7547696,2.9295275,2.8623393,2.952021,3.043896,7.760511,3.270665,2.832534,2.647052,2.93609,2.8298435,2.755262,2.8818974,2.8486757,2.793356,2.636509,2.627927,2.7254786,3.2937875,2.7279396,2.8777962,2.9350562,2.9525719,2.8636937,2.973114,2.8190408,2.9339795,2.9904654,2.8450694,2.8775518,2.879592,2.945316,2.782317,2.8986082],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"using knearest neighbors method one often ignores uncertainty choice account uncertainty holmes adam...\"],[\"runtime kernel partial least squares kpls compute fit quadratic number examples however necessity ob...\"],[\"recently increasing interest methods deal multiple outputs motivated partly frameworks like multitas...\"],[\"interest multioutput kernel methods increasing whether guise multitask learning multisensor networks...\"],[\"improve recently published results resources restricted boltzmann machines rbm deep belief networks ...\"],[\"since learning typically slow boltzmann machines need restrict connections within hidden layers howe...\"],[\"propose active set selection framework gaussian process classification cases dataset large enough re...\"],[\"gaussian process models often used mathematical approximations computationally expensive experiments...\"],[\"many realworld problems encountered several disciplines deal modeling timeseries containing differen...\"],[\"consider gaussian process formulation multiple kernel learning problem goal select convex combinatio...\"],[\"gaussian process models also called kriging models often used mathematical approximations expensive ...\"],[\"gaussian probability densities omnipresent applied mathematics gaussian cumulative probabilities har...\"],[\"exact inference linear regression model spike slab priors often intractable expectation propagation ...\"],[\"consider probabilistic multinomial probit classification using gaussian process priors challenges mu...\"],[\"exact gaussian process regression runtime data size making intractable large many algorithms improvi...\"],[\"gaussian process promising novel technology applied regression problem classification problem regres...\"],[\"propose novel approach nonlinear regression using twolayer neural network model structure sparsityfa...\"],[\"spatiotemporal point process models play central role analysis spatially distributed systems several...\"],[\"introduce general constructive setting density ratio estimation problem solution multidimensional in...\"],[\"adaptive filtering algorithms operating reproducing kernel hilbert spaces demonstrated superiority l...\"],[\"gaussian process latent variable model gplvm nonlinear probabilistic method embedding high dimension...\"],[\"informally call stochastic process learnable admits generalization error approaching zero probabilit...\"],[\"significant success reported recently using deep neural networks classification large networks compu...\"],[\"latent force models lfm principled approaches incorporating solutions differential equations within ...\"],[\"kernel leastmeansquare klms algorithm appealing tool online identification nonlinear systems due sim...\"],[\"propose family multivariate gaussian process models correlated outputs based assuming likelihood fun...\"],[\"paper propose outlierrobust regularized kernelbased method linear system identification unknown impu...\"],[\"paper proposes novel scheme reducedrank gaussian process regression method based approximate series ...\"],[\"tutorial explain inference procedures developed sparse gaussian process regression gaussian process ...\"],[\"paper presents novel approach approximate integration uncertainty noise signal variances gaussian pr...\"],[\"nonparametric regression massive numbers samples features increasingly important problem big setting...\"],[\"paper presents bayesian generative model dependent cox point processes alongside efficient inference...\"],[\"large amount observational data accumulated various fields recent times growing need estimate genera...\"],[\"paper introduces kernelbased information criterion kic model selection regression analysis novel ker...\"],[\"paper introduces linear statespace model timevarying dynamics time dependency obtained forming state...\"],[\"paper propose first nonparametric bayesian model using gaussian processes make inference poisson poi...\"],[\"present first fully variational bayesian inference scheme continuous gaussianprocessmodulated poisso...\"],[\"use statistical learning methods construct adaptive state estimator nonlinear stochastic systems opt...\"],[\"gaussian process classification popular method number appealing properties show scale model within v...\"],[\"multioutput gaussian processes mogp probability distributions vectorvalued functions previously used...\"],[\"scale gaussian processes gps large data sets introduce robust bayesian committee machine rbcm practi...\"],[\"large multilayer neural networks trained backpropagation recently achieved stateoftheart results wid...\"],[\"multivariate categorical data occur many applications machine learning one main difficulties vectors...\"],[\"standard sparse pseudoinput approximations gaussian process cannot handle complex functions well spa...\"],[\"multioutput gaussian processes received increasing attention last years natural mechanism extend pow...\"],[\"variational framework learning inducing variables titsias large impact gaussian process literature f...\"],[\"show neural network arbitrary depth nonlinearities dropout applied every weight layer mathematically...\"],[\"corrupting input hidden layers deep neural networks dnns multiplicative noise often drawn bernoulli ...\"],[\"gaussian process models form core part probabilistic machine learning considerable research effort m...\"],[\"study gaussian process regression model context training data noise input output presence two source...\"],[\"new bayesian approach linear system identification proposed series recent papers main idea frame lin...\"],[\"paper compares classical parametric methods recently developed bayesian methods system identificatio...\"],[\"paper proposes novel gaussian process approach fault removal timeseries data fault removal delete fa...\"],[\"variational methods recently considered scaling training process gaussian process classifiers large ...\"],[\"paper introduce novel framework making exact nonparametric bayesian inference latent functions parti...\"],[\"study statistical calibration adjusting features computational model observable controllable associa...\"],[\"study largescale spatial systems contain exogenous variables environmental factors significant predi...\"],[\"dropout recently emerged powerful simple method training neural networks preventing coadaptation sto...\"],[\"present novel approach fully nonstationary gaussian process regression gpr three key parameters nois...\"],[\"interested solving multiple measurement vector mmv problem instances underlying sparsity pattern exh...\"],[\"online passiveaggressive learning class online marginbased algorithms suitable wide range realtime p...\"],[\"paper develop statistical theory implementation deep learning models show elegant variable splitting...\"],[\"present blitzkriging new approach fast inference gaussian processes applicable regression optimisati...\"],[\"method large scale gaussian process classification recently proposed based expectation propagation m...\"],[\"deep gaussian processes dgps multilayer hierarchical generalisations gaussian processes gps formally...\"],[\"latent variable timeseries models among heavily used tools machine learning applied statistics model...\"],[\"recurrent neural networks rnns stand forefront many recent developments deep learning yet major diff...\"],[\"consider inverse problem reconstructing posterior measure trajec tories diffusion process discrete t...\"],[\"effective training deep neural networks suffers two main issues first parameter spaces models exhibi...\"],[\"circular variables arise multitude datamodelling contexts ranging robotics social sciences largely o...\"],[\"hyperparameters gaussian process regression gpr model specified kernel often estimated data via maxi...\"],[\"paper considers quantification prediction performance gaussian process regression standard approach ...\"],[\"paper develop method learning nonlinear systems multiple outputs inputs begin modelling errors nomin...\"],[\"good sparse approximations essential practical inference gaussian processes computational cost exact...\"],[\"work falls within context predicting value real function input locations given limited number observ...\"],[\"kalman filter used variety applications computing posterior distribution latent states state space m...\"],[\"unscented transformation efficient method solve state estimation problem nonlinear dynamic system ut...\"],[\"incrementalonline state dynamic learning method proposed identification nonlinear gaussian state spa...\"],[\"develop automated variational method inference models gaussian process priors general likelihoods me...\"],[\"gaussian processes powerful yet analytically tractable models supervised learning gaussian process c...\"],[\"quantitative modeling posttranscriptional regulation process challenging problem systems biology mec...\"],[\"investigate capabilities limitations gaussian process models jointly exploring three complementary d...\"],[\"gradient matching gaussian processes promising tool learning parameters ordinary differential equati...\"],[\"propose parallelizable sparse inverse formulation gaussian process spingp temporal models uses spars...\"],[\"despite fundamental nature inhomogeneous poisson process theory application stochastic processes att...\"],[\"gpflow gaussian process library uses tensorflow core computations python front end distinguishing fe...\"],[\"exploiting fact arrival processes exhibit cyclic behaviour propose simple procedure estimating inten...\"],[\"work brings together two powerful concepts gaussian processes variational approach sparse approximat...\"],[\"noise injection efficient technique mitigate overfitting neural networks nns bernoulli procedure imp...\"],[\"dynamic mode decomposition dmd emerged powerful tool analyzing dynamics nonlinear systems experiment...\"],[\"gaussian processes gps proven powerful tools various areas machine learning however applications gps...\"],[\"many modern data sets sampled error complex highdimensional surfaces methods tensor product splines ...\"],[\"background statistical mechanics results dauphin choromanska suggest local minima high error exponen...\"],[\"training gaussian processbased models typically involves computational bottleneck due inverting cova...\"],[\"consider modification covariance function gaussian processes correctly account known linear constrai...\"],[\"present first treatment arc length gaussian process single output dimension gps commonly used tasks ...\"],[\"gaussian processes gps powerful nonparametric function estimators however applications largely limit...\"],[\"paper new bayesian model sparse linear regression spatiotemporal structure proposed incorporates str...\"],[\"consider class misspecified dynamical models governing term approximately known assumption observati...\"],[\"gradient matching promising tool learning parameters state dynamics ordinary differential equations ...\"],[\"sparse pseudopoint approximations gaussian process models provide suite methods support deployment g...\"],[\"dropoutbased regularization methods regarded injecting random noise predefined magnitude different p...\"],[\"dropout used practical tool obtain uncertainty estimates large vision models reinforcement learning ...\"],[\"gaussian processes gps good choice function approximation flexible robust overfitting provide wellca...\"],[\"exponential family distributions highly useful machine learning since calculation performed efficien...\"],[\"deep learning applies hierarchical layers hidden variables construct nonlinear high dimensional pred...\"],[\"bayesian neural networks bnns recently received increasing attention ability provide wellcalibrated ...\"],[\"gaussian processes gps distributions arbitrary functions continuous domain generalized multioutput c...\"],[\"gaussian process state space model gpssm nonlinear dynamical system unknown transition andor measure...\"],[\"present efficient blockdiagonal proximation gaussnewton matrix feedforward neural networks result in...\"],[\"paper describes expectation propagation method multiclass classification gaussian processes scales w...\"],[\"bayesian neural networks bnns latent variables probabilistic models automatically identify complex s...\"],[\"neural network based generative models discriminative components powerful approach semisupervised le...\"],[\"deep neural networks dnns excellent representative power state art classifiers many tasks however of...\"],[\"propose simple method combines neural networks gaussian processes proposed method estimate uncertain...\"],[\"gaussian process regression generally scale beyond thousands data points without applying sort kerne...\"],[\"reliable uncertainty estimation time series prediction critical many fields including physics biolog...\"],[\"parametric point process model developed modeling based assumption sequential observations often sha...\"],[\"study introduce new technique symbolic regression guarantees global optimality achieved formulating ...\"],[\"reduced modeling computationally demanding dynamical system aims approximating trajectories optimizi...\"],[\"modeling sequential data become important practice applications autonomous driving virtual sensors w...\"],[\"gaussian multiplicative noise commonly used stochastic regularisation technique training determinist...\"],[\"model criticism usually carried assessing replicated data generated fitted model looks similar obser...\"],[\"compression neural networks become highly studied topic recent years main reason demand industrial s...\"],[\"derive novel sensitivity analysis input variables predictive epistemic aleatoric uncertainty use bay...\"],[\"basis adaptation homogeneous chaos spaces rely suitable rotation underlying gaussian germ several ro...\"],[\"deep gaussian processes dgp hierarchical generalizations gaussian processes proven work effectively ...\"],[\"gaussian process priors commonly used aerospace design performing bayesian optimization nonetheless ...\"],[\"statespace models ssms highly expressive model class learning patterns time series data system ident...\"],[\"gaussian process models provide powerful tool prediction computationally prohibitive using large dat...\"],[\"learning using privileged information attractive problem setting helps many learning scenarios real ...\"],[\"theoretically discuss deep neural networks dnns performs better models cases investigating statistic...\"],[\"provide comprehensive overview tooling modeling nongaussian likelihoods using state space methods st...\"],[\"show training deep network using batch normalization equivalent approximate inference bayesian model...\"],[\"paper considers generation prediction intervals pis neural networks quantifying uncertainty regressi...\"],[\"multioutput regression models must exploit dependencies outputs maximise predictive performance appl...\"],[\"present causal gaussian process convolution model cgpcm doubly nonparametric model causal spectrally...\"],[\"multiresolution gaussian process gained increasing attention viable approach towards improving quali...\"],[\"paper presents novel formulation solution orbit determination finite time horizons learning problem ...\"],[\"ordinary stochastic neural networks mostly rely expected values weights make predictions whereas ind...\"],[\"present first framework gaussianprocessmodulated poisson processes temporal data appear form panel c...\"],[\"conventional ode modelling coefficients equation driving system state forward time estimated however...\"],[\"zeroinflated datasets excess zero outputs commonly encountered problems climate rare event modelling...\"],[\"framework supervised learning real function defined space called kriging method stands real gaussian...\"],[\"paper presents simulation free framework solving reliability analysis problems method proposed roote...\"],[\"connection bayesian neural networks gaussian processes gained lot attention last years flagship resu...\"],[\"mapping nearfield pollutant concentration essential track accidental toxic plume dispersion urban ar...\"],[\"present application conformal prediction form uncertainty quantification guarantees detection railwa...\"],[\"sequential neural architectures become deeper complex uncertainty estimation challenging efforts qua...\"],[\"inverse problems arise anywhere indirect measurement general illposed obtain satisfactory solutions ...\"]],\"hovertemplate\":\"label=gaussian\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"gaussian\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"gaussian\",\"showlegend\":true,\"x\":[-4.5524273,-1.4336843,-3.2396302,-3.2620735,-4.739567,-4.6823997,-2.191423,-3.5028315,-4.0227017,-3.313183,-3.4053137,-3.2699533,-3.6956663,-3.7605255,-3.7461026,-3.4900453,-3.6165698,-3.8836586,-2.8280694,-2.7029212,-3.5388947,-3.8524413,-4.640256,-3.6721563,-2.642482,-3.5048263,-3.2931056,-2.877763,-3.722623,-3.7112954,-3.249869,-3.9791985,-4.192479,-3.1630266,-3.6625395,-4.0543017,-4.172413,-3.8631852,-3.8297675,-3.3574438,-3.9468231,-4.5715895,-3.575995,-3.4987214,-3.4736736,-3.882476,-4.686613,-4.679464,-3.6733825,-3.6629972,-3.059298,-3.769988,-3.3069983,-4.03639,-3.8661478,-3.485902,-3.2733192,-4.714961,-3.6759117,-3.7017086,-3.813869,-4.613095,-3.8067355,-3.9418962,-4.47589,-3.9898815,-4.7670846,-4.5873065,-4.6772814,-3.5512638,-3.4084833,-3.8263054,-2.7278943,-3.6253996,-3.524066,-3.7539425,-2.9655778,-3.8251529,-3.7270253,-3.3852377,-3.726896,-3.3516836,-3.6808827,-3.7228491,-3.271979,-3.6512756,-3.615201,-3.4176633,-1.0422505,-3.6999073,-3.817254,-3.437201,-4.645184,-3.7030876,-3.4154058,-3.5178137,-4.065156,-3.6529043,-3.6331186,-3.8523436,-3.8027012,-4.730967,-4.583617,-4.0813923,-3.4067104,-4.645189,-4.6009727,-3.7427104,-3.8648055,-4.1863594,-3.804571,-4.461657,-4.45473,-4.4893627,-4.0615425,-3.3805265,-4.328811,-3.748896,-3.3955708,-3.6783843,-4.0466447,-4.6609707,-3.6741838,-4.700896,-4.434399,-2.8064044,-4.082393,-3.5229626,-3.94517,-3.5885727,-3.4815774,-4.64602,-4.1157947,-4.6972213,-4.424373,-3.8067741,-3.4841104,-3.8744173,-3.4426427,-4.6301126,-3.9991615,-3.7013402,-3.431801,-3.1488564,-4.5849366,-4.540215,-3.0457075,-2.5029538,-4.4313397,-4.5649753],\"xaxis\":\"x\",\"y\":[3.784682,5.5383945,4.089605,4.1752768,3.5630627,3.4890065,3.2492135,3.9519541,4.126493,4.1679087,4.062762,4.3071327,4.7088127,4.4906464,3.964588,3.9987307,4.4475594,4.4038415,4.802451,4.4309893,4.427241,4.410197,3.529377,3.581389,4.453214,4.2097607,4.284029,4.4676766,4.3032956,4.0674224,4.5909805,4.5896435,4.9936132,4.2044315,3.6358013,4.7774973,5.037023,3.9660864,4.2214446,4.3759108,4.1821017,3.661884,4.4448237,4.34826,4.1707397,4.545317,3.6445205,3.472128,4.2697525,4.1482735,4.426658,4.8843455,4.2253385,4.312264,4.239305,3.7829669,4.5440903,3.6158457,3.9939098,4.9534655,3.9563746,3.4321918,4.0175776,4.196496,3.8240175,4.218537,3.6100829,4.3457093,3.7675495,4.391738,3.992021,3.9405587,4.4520683,4.363108,3.9855185,3.7671432,4.362655,3.9646559,4.439118,4.073939,4.1134453,4.127769,3.9427555,4.233063,4.2933006,4.07082,5.319327,4.288104,4.6749897,3.4785244,3.991962,4.3274083,3.434052,4.274553,4.005173,4.011689,4.082427,4.8371468,3.5990462,3.9490306,4.002578,3.5770051,3.7694254,3.931278,4.3702106,3.4722662,3.7481458,3.9290316,3.8682518,2.959718,4.3864293,3.8476453,3.9369094,3.641463,3.8853443,4.2310295,3.8135595,4.056319,3.5500813,3.6349146,3.8560476,3.7745903,4.672353,3.5220637,3.8597095,4.5635448,3.8944242,3.7824485,3.8656113,3.9108496,4.052293,3.4013915,4.510737,3.6970024,3.7983096,3.900202,4.1678395,3.9655495,3.6919432,3.7138605,5.222696,3.6596768,4.1937733,4.0853148,3.5949278,3.789591,3.8813653,3.3051336,3.811891,4.2345424],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"flow cytometry often used characterize malignant cells leukemia lymphoma patients traced level indiv...\"],[\"present procedure effective estimation entropy mutual information smallsample data apply problem inf...\"],[\"extend multiway multivariate anovatype analysis cases one covariate view features view coming differ...\"],[\"translating potential disease biomarkers multispecies omics experiments new direction biomedical res...\"],[\"paper treats problem screening variables high correlations high dimensional data many fewer samples ...\"],[\"inverse inference brain reading recent paradigm analyzing functional magnetic resonance imaging fmri...\"],[\"response variables nominal populations crossclassified respect multiple polytomies questions often a...\"],[\"introduce factor analysis model summarizes dependencies observed variable groups instead dependencie...\"],[\"case control comparisons classical approach study neurological diseases however patients fall cleanl...\"],[\"typical cohorts brain imaging studies large enough systematic testing information contained images b...\"],[\"extend kernelized matrix factorization fully bayesian treatment ability work multiple side informati...\"],[\"problems machine learning involve noisy input data classification methods reached limiting accuracie...\"],[\"systems biomedicine experimenter encounters different potential sources variation data individual sa...\"],[\"canonical correlation analysis cca one popular methods frequency recognition steadystate visual evok...\"],[\"regularized variants principal components analysis especially sparse pca functional pca among useful...\"],[\"substantial evidence indicates major psychiatric disorders associated distributed neural dysconnecti...\"],[\"combining information different sources common way improve classification accuracy braincomputer int...\"],[\"manuscript consider problem jointly estimating multiple graphical models high dimensions assume data...\"],[\"symmetric binary matrices representing relations among entities commonly collected many areas focus ...\"],[\"despite fact consider temporal nature data classic dimensionality reduction techniques pca widely ap...\"],[\"use machinelearning neuroimaging offers new perspectives early diagnosis prognosis brain diseases al...\"],[\"diffusionweighted magnetic resonance imaging dwi fiber tractography methods measure structure white ...\"],[\"highdimensional data structured noise caused observed unobserved factors affecting multiple target v...\"],[\"advances modern sensing sequencing technologies generate deluge high dimensional spacetemporal physi...\"],[\"diffusionweighted imaging dwi method currently measure connections different parts human brain vivo ...\"],[\"factor analysis provides linear factors describe relationships individual variables data set extend ...\"],[\"measuring dependence two random variables important critical many applied areas variable selection b...\"],[\"explosion interest functional magnetic resonance imaging mri past two decades naturally accompanied ...\"],[\"brain decoding involves determination subjects cognitive state associated stimulus functional neuroi...\"],[\"central goal neuroscience understand activity nervous system related features external world feature...\"],[\"propose macau powerful flexible bayesian factorization method heterogeneous data model factorize set...\"],[\"clinical neuroscientific studies systematic differences two populations brain networks investigated ...\"],[\"understanding type inhibitory interaction plays important role drug design therefore researchers int...\"],[\"neuroimaging data analysis gaussian graphical models often used model statistical dependencies acros...\"],[\"propose novel classification model weak signal data building upon recent model bayesian multiview le...\"],[\"functional brain networks well described estimated data gaussian graphical models ggms using sparse ...\"],[\"introduce general framework estimation inverse covariance precision matrices heterogeneous populatio...\"],[\"best knowledge general wellfounded robust methods statistical unsupervised learning unsupervised met...\"],[\"volume collection contributions workshop machine learning interpretation neuroimaging mlini neural i...\"],[\"predictive models used highdimensional brain images diagnosis clinical condition spatial regularizat...\"],[\"imaging genetic research essentially focused discovering unique coassociation effects typically igno...\"],[\"genomewide interaction studies detect genegene interactions methods divided two folds single nucleot...\"],[\"decoding prediction brain images signals calls empirical evaluation predictive power evaluation achi...\"],[\"improving interpretability brain decoding approaches primary interest many neuroimaging studies desp...\"],[\"present general framework classifying partially observed dynamical systems based idea learning model...\"],[\"genomewide association study gwas correlates marker variation trait variation sample individuals stu...\"],[\"machine learning methods used discover complex nonlinear relationships biological medical data howev...\"],[\"principal component analysis pca exploratory tool widely used data analysis uncover dominant pattern...\"],[\"understanding relationships different properties data whether connectome genome information disease ...\"],[\"extracting information functional magnetic resonance fmri images major area research two decades goa...\"],[\"sources variability experimentally derived data include measurement error addition physical phenomen...\"],[\"many natural systems neurons firing brain basketball teams traversing court give rise time series da...\"],[\"analysis nonstationary time series great importance many scientific fields physics neuroscience rece...\"],[\"proposed complex populations arise genomics studies may exhibit dependencies among observations well...\"],[\"construction synthetic complexvalued signals realvalued observations important step many time series...\"],[\"personalized treatment patients based tissuespecific cancer subtypes strongly increased efficacy cho...\"],[\"workshop explores interface cognitive neuroscience recent advances fields aim reproduce human perfor...\"],[\"flow cytometry highthroughput technology used quantify multiple surface intracellular markers level ...\"],[\"estimating state dynamical system series noisecorrupted observations fundamental many areas science ...\"],[\"introduce novel kernel models inputdependent couplings across multiple latent processes pairwise joi...\"],[\"present study proposes deep learning model named deepsleepnet automatic sleep stage scoring based ra...\"],[\"development computed tomography image reconstruction methods significantly reduce patient radiation ...\"],[\"objective work perform margin assessment human breast tissue optical coherence tomography oct images...\"],[\"integrative analysis disparate data blocks measured common set experimental subjects major challenge...\"],[\"computing accurate estimates fourier transform analog signals discrete data points important many fi...\"],[\"matrix factorisation methods decompose multivariate observations linear combinations latent feature ...\"],[\"many unsupervised kernel methods rely estimation kernel covariance operator kernel kernel crosscovar...\"],[\"increasing size complexity scientific data could dramatically enhance discovery prediction basic sci...\"],[\"paper analyzes use convolutional neural networks brain tumor segmentation images address problem usi...\"],[\"present new model predictive state recurrent neural networks psrnns filtering prediction dynamical s...\"],[\"multiple instance dictionary learning approach dictionary learning using functions multiple instance...\"],[\"adopt data structure form cover trees iteratively apply approximate nearest neighbour ann searches f...\"],[\"learning children animals occurs effortlessly largely without obvious supervision successes automati...\"],[\"study tested interaction effect multimodal datasets using novel method called kernel method detectin...\"],[\"new technologies recording activity large neural populations complex behavior provide exciting oppor...\"],[\"malaria serious infectious disease responsible half million deaths yearly worldwide major cause mort...\"],[\"solve key biomedical problems experimentalists routinely measure millions billions features dimensio...\"],[\"discovering correlation one variable another variable fundamental scientific practical interest exis...\"],[\"high throughput screening compounds chemicals essential part drug discovery involving thousands mill...\"],[\"paper propose framework automatic classification patients multimodal genetic brain imaging data opti...\"],[\"paper considers problem brain disease classification based connectome data connectome network repres...\"],[\"profiling cellular phenotypes microscopic imaging provide meaningful biological information resultin...\"],[\"powerful approach understanding neural population dynamics extract lowdimensional trajectories popul...\"],[\"mechanistic models singleneuron dynamics extensively studied computational neuroscience however iden...\"],[\"many problem settings parameter vectors merely sparse dependent way nonzero coefficients tend cluste...\"],[\"identifying altered pathways associated specific cancer types potentially bring significant impact c...\"],[\"gene expression data represents unique challenge predictive model building small number samples comp...\"],[\"melanoma deadliest form skin cancer computer systems assist melanoma detection widespread clinical p...\"],[\"paper taskrelated fmri problem treated matrix factorization formulation focused dictionary learning ...\"],[\"correlated component analysis proposed dmochowski tool investigating brain process similarity respon...\"],[\"neuroscientists enjoyed much success understanding brain functions constructing brain connectivity n...\"],[\"brain segmentation neonatal mri images challenging task due large changes shape cerebral structures ...\"]],\"hovertemplate\":\"label=fmri\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"fmri\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"fmri\",\"showlegend\":true,\"x\":[0.51646495,0.44379416,0.4195511,0.44477352,-0.044589765,0.96161115,-0.011837899,0.38944635,1.0164591,0.9541085,0.17093666,-0.010749481,0.48071918,0.6892371,0.657897,0.95858765,0.8845603,0.8505658,-2.314319,0.57735586,0.86901945,0.7629003,0.81319046,0.3821629,0.8725567,0.54271954,0.4856127,0.9817193,0.97610754,1.1262472,0.3616613,1.0317276,0.2912725,1.0160652,0.883023,0.96237284,0.46406657,-2.1150045,1.05131,0.8302833,0.77444345,0.4920255,0.9472132,0.95140326,-3.6435237,0.36623213,0.06023435,0.85270256,0.5817907,0.88361067,0.46247393,-3.6222842,0.65479314,0.45316124,0.5502301,0.4557609,1.0821487,0.51604676,-3.6685143,0.7866099,1.0194045,0.7168645,0.8120334,0.5243063,0.44551104,0.61671793,-2.1029012,0.21278863,0.87967175,-3.6655712,0.3330466,0.41353357,1.0692271,0.9196687,-3.5896938,0.71795255,0.6103441,0.06139763,0.16311787,0.88420564,1.0059426,0.60234225,1.1071497,1.2017761,0.77251583,0.4428626,0.1487275,0.70162004,0.9129256,0.87350863,1.0522017,0.96324176],\"xaxis\":\"x\",\"y\":[4.2514253,4.227561,4.484156,4.24445,4.422631,5.2787538,4.2799187,4.921901,5.230591,5.141682,4.348939,4.176454,4.1824617,5.3904786,5.551144,5.1747165,5.153241,5.36673,6.8502126,5.5812316,5.1968446,5.3177395,5.36031,4.1507683,5.185877,5.3402815,4.996674,5.201199,5.2777505,5.303769,4.5579596,5.1884885,4.4194427,5.1984773,5.461263,5.2497063,4.2234993,4.541197,5.182675,5.25417,4.8223267,4.443537,5.224941,5.2315993,3.4783642,4.16319,3.8100069,5.273362,4.4764347,5.362281,4.1991887,3.4481025,5.455452,4.2634106,5.4459925,4.289446,5.2270966,4.230723,3.6823099,5.520625,5.269698,5.301665,5.1262403,4.2618046,5.500294,5.5286193,4.4971046,3.951492,5.171138,3.5308044,5.102835,5.673233,5.2807474,4.9918604,3.3566787,4.863553,4.4604945,4.468882,4.3060594,5.1811323,5.2022076,4.367011,5.2173276,5.1740227,5.281054,4.2210393,3.8277256,4.5714827,5.393988,5.452523,5.236548,5.1521707],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"simple computationally efficient scheme treestructured vector quantization presented unlike previous...\"],[\"many problems lowlevel computer vision image processing denoising deconvolution tomographic reconstr...\"],[\"consider problem jointly estimating parameters well structure binary valued markov random fields con...\"],[\"obtain index complexity random sequence allowing role measure classical probability theory played fu...\"],[\"section incorrect removed submissions rewritten version posted future...\"],[\"article addresses modeling reverberant recording environments context underdetermined convolutive bl...\"],[\"paper consider problem hypersparse aggregation namely given dictionary functions look optimal aggreg...\"],[\"assume data independently sampled mixture distribution unit ball ddimensional euclidean space compon...\"],[\"johnsonlindenstrauss lemma allows projection points pdimensional euclidean space onto kdimensional e...\"],[\"sparse coding consists representing signals sparse linear combinations atoms selected dictionary con...\"],[\"finding sparse solutions underdetermined systems linear equations fundamental problem signal process...\"],[\"modeling data linear combinations elements learned dictionary focus much recent research machine lea...\"],[\"developed efficient algorithm maximum likelihood joint tracking association problem strong clutter g...\"],[\"present revival interest bistatic radar systems research area gained momentum given strategic advant...\"],[\"applications related airborne radars simulation always played important role mainly two fold reason ...\"],[\"collaborative convex framework factoring data matrix nonnegative product sparse coefficient matrix p...\"],[\"define discuss first sparse coding algorithm based closedform updates continuous latent variables un...\"],[\"due space limitations submission source separation clustering phaselocked subspaces accepted publica...\"],[\"nonnegative matrix factorization nmf common tool audio source separation learning nmf large audio da...\"],[\"many areas machine learning becomes necessary find eigenvector decompositions large matrices discuss...\"],[\"present probabilistic model natural images based gaussian scale mixtures simple multiscale represent...\"],[\"propose method called ideal regression approximating arbitrary system polynomial equations system pa...\"],[\"paper addresses problem segmenting timeseries respect changes mean value variance first case time da...\"],[\"highdimensional tensors multiway data becoming prevalent areas biomedical imaging chemometrics netwo...\"],[\"present alternating augmented lagrangian method convex optimization problems cost function sum two t...\"],[\"paper propose novel framework construction sparsityinducing priors particular define priors mixture ...\"],[\"study problem prediction evolving graph data formulate problem minimization convex objective encoura...\"],[\"paper considers problem robust subspace recovery given set points mathbbrd many lie ddimensional sub...\"],[\"recently sparsitybased algorithms proposed superresolution spectrum estimation however achieve adequ...\"],[\"consider problems detection localization contiguous block weak activation large matrix small number ...\"],[\"many applications data analysis rely decomposition data matrix lowrank sparse component existing met...\"],[\"study inference learning based sparse coding model spikeandslab prior standard sparse coding model u...\"],[\"paper prove probabilistic continuous complexity conjecture continuous complexity theory states compl...\"],[\"suppose two large multidimensional data sets noisy measurements underlying random process principle ...\"],[\"dictionary learning proven powerful tool many image processing tasks atoms typically defined small i...\"],[\"measurements made satellite remote sensing moderate resolution imaging spectroradiometer modis globa...\"],[\"study low rank matrix tensor completion propose novel algorithms employ adaptive sampling schemes ob...\"],[\"consider following signal recovery problem given measurement matrix phiin mathbbrntimes noisy observ...\"],[\"strategy early stopping regularization technique based choosing stopping time iterative algorithm fo...\"],[\"problem lowrank matrix estimation recently received lot attention due challenging applications lot w...\"],[\"provide theoretical analysis statistical computational properties penalized mestimators formulated s...\"],[\"propose general framework reducedrank modeling matrixvalued data applying generalized nuclear norm p...\"],[\"describe ways define calculate lnorm signal subspaces less sensitive outlying data lcalculated subsp...\"],[\"paper considers problem subspace clustering noise specifically study behavior sparse subspace cluste...\"],[\"present numerical algorithm nonnegative matrix factorization nmf problems noisy separability nmf pro...\"],[\"mixed linear regression involves recovery two unknown vectors unlabeled linear measurements sample c...\"],[\"propose new stochastic dual coordinate ascent technique applied wide range regularized learning prob...\"],[\"work compute lower lipschitz bounds ellp pooling operators infty well ellp pooling operators precede...\"],[\"robust tensor recovery plays instrumental role robustifying tensor decompositions multilinear data a...\"],[\"nonorthogonal joint diagonalization njd free prewhitening widely studied context blind source separa...\"],[\"present exploration rich theoretical connections several classes regularized models network flows re...\"],[\"concerned approximation problem symmetric positive semidefinite matrix due motivation class nonlinea...\"],[\"archetypal analysis represents set observations convex combinations pure patterns archetypes origina...\"],[\"consider forwardbackward greedy algorithms solving sparse feature selection problems general convex ...\"],[\"main goal paper propose novel method perform matrix completion online motivated wide variety applica...\"],[\"new shrinkagebased construction developed compressible vector boldsymbolxinmathbbrn cases components...\"],[\"recently number mostly ellnorm regularized least squares type deterministic algorithms proposed addr...\"],[\"survey present compare different approaches estimate mutual information data analyse general depende...\"],[\"practical model building processes often timeconsuming many different models must trained validated ...\"],[\"consider problem dictionary learning assumption observed signals represented sparse linear combinati...\"],[\"ksupport norm regularizer successfully applied sparse vector prediction problems show belongs genera...\"],[\"synthesis model signals represented sparse combinations atoms dictionary dictionary learning describ...\"],[\"nonnegative matrix factorization nmf shown identifiable separability assumption columnsor rows input...\"],[\"paper consider low rank matrix estimation using either matrixversion dantzig selector hatalambdad ma...\"],[\"given iid observations unknown absolute continuous distribution defined domain omega propose nonpara...\"],[\"study problem lowrank tensor factorization presence missing data ask following question many sampled...\"],[\"paper presents novel algorithms exploit intrinsic algebraic combinatorial structure matrix completio...\"],[\"subsampling methods recently proposed speed least squares estimation large scale settings however al...\"],[\"present technique significantly speeding alternating least squares als gradient descent two widely u...\"],[\"consider statistical well algorithmic aspects solving largescale leastsquares problems using randomi...\"],[\"consider class optimization problems arising computationally intensive lregularized mestimators func...\"],[\"matrix factorization become common approach collaborative filtering due ease implementation scalabil...\"],[\"large number algorithms machine learning principal component analysis pca nonlinear kernel extension...\"],[\"focusing bound constrained global optimization problems whose objective functions computationally ex...\"],[\"motivated largescale collaborativefiltering applications present noncommuting latent factor nclf ten...\"],[\"classical stochastic gradient methods well suited minimizing expectedvalue objective functions howev...\"],[\"robust parameter estimation well studied parametric density estimation little investigation robust d...\"],[\"article derive new stepsize adaptation normalized least mean square algorithm nlms describing task l...\"],[\"introduce bayesian multitensor factorization model first bayesian formulation joint factorization mu...\"],[\"highdimensional data often lie lowdimensional subspaces corresponding different classes belong findi...\"],[\"provide general theory expectationmaximization algorithm inferring high dimensional latent variable ...\"],[\"certain situations shall undoubtedly common big data era datasets available massive computing statis...\"],[\"using bayesian approach consider problem recovering sparse signals additive sparse dense noise typic...\"],[\"motivated problems arise number applications online marketing explosives detection observations usua...\"],[\"sparse representation classifier src utilized various classification problems makes use minimization...\"],[\"propose novel sparse tensor decomposition method namely tensor truncated power ttp method incorporat...\"],[\"careful tuning regularization parameter indispensable many machine learning tasks significant impact...\"],[\"consider problem noisy bit matrix completion exact rank constraint true underlying matrix instead ob...\"],[\"paper examine problem approximating general linear dimensionality reduction ldr operator represented...\"],[\"paper considers problem matrix completion observed entries noisy contain outliers begins introducing...\"],[\"many high dimensional sparse learning problems formulated nonconvex optimization popular approach so...\"],[\"estimation response functions important task dynamic medical imaging task arises example dynamic ren...\"],[\"lowrank representationlrr significant method segmenting data generated union subspaces however known...\"],[\"consider problem discriminative factor analysis data general nongaussian bayesian model based ranks ...\"],[\"study theoretical properties learning dictionary signals mathbf xiin mathbb via lminimization assume...\"],[\"present unified framework lowrank matrix estimation nonconvex penalties first prove proposed estimat...\"],[\"consider statistical algorithmic aspects solving largescale leastsquares problems using randomized s...\"],[\"subdifferential convex functions singular spectrum real matrices widely studied matrix analysis opti...\"],[\"past years robust pca established standard tool reliable lowrank approximation matrices presence out...\"],[\"paper propose online algorithm compute matrix factorizations proposed algorithm updates dictionary m...\"],[\"nonparametric extension tensor regression proposed nonlinearity highdimensional tensor space broken ...\"],[\"dictionaryaided sparse regression approach recently emerged promising alternative hyperspectral unmi...\"],[\"nonnegative matrix factorization nmf aims factorize matrix two optimized nonnegative matrices approp...\"],[\"consider factoring lowrank tensors presence outlying slabs problem important practice data collected...\"],[\"density matrices positively semidefinite hermitian matrices unit trace describe state quantum system...\"],[\"typical goal supervised dimension reduction find lowdimensional subspace input space projected input...\"],[\"consider problem minimizing sum functions convex parameter set mathcalc subset mathbbrp ngg pgg regi...\"],[\"text investigates relations two wellknown family algorithms matrix factorisations recursive linear f...\"],[\"large sample size brings computation bottleneck modern data analysis subsampling one efficient strat...\"],[\"given iid samples unknown continuous density hyperrectangle attempt learn piecewise constant functio...\"],[\"briefly review recent progress techniques modeling analyzing hyperspectral images movies particular ...\"],[\"consider problem extracting lowdimensional linear latent variable structure highdimensional random v...\"],[\"early stopping well known approach reduce time complexity performing training model selection large ...\"],[\"dimensionality reduction methods common field high dimensional data analysis typically algorithms di...\"],[\"speaker recognition scenarios find conversations recorded simultaneously multiple channels case inte...\"],[\"consider convexconcave saddle point problems separable structure nonstrongly convex functions propos...\"],[\"stateoftheart speaker recognition relays models need large amount training data models successful ta...\"],[\"paper aims achieving good estimator gradient function highdimensional space often functions sensitiv...\"],[\"study fundamental tradeoffs computational tractability statistical accuracy general family hypothesi...\"],[\"nongaussian component analysis ngca aimed identifying linear subspace projected data follows nongaus...\"],[\"signal processing problems involve challenging task multidimensional probability density function pd...\"],[\"hearing aid algorithms need tuned fitted match impairment specific patient lack fundamental fitting ...\"],[\"extracting underlying lowdimensional space highdimensional signals often reside long center numerous...\"],[\"mixture experts moe model popular neural network architecture nonlinear regression classification cl...\"],[\"paper investigates phase retrieval problem aims recover signal magnitudes linear measurements develo...\"],[\"consider problem maximizing unknown function compact convex set using observations possible observe ...\"],[\"paper present unified analysis matrix completion general lowdimensional structural constraints induc...\"],[\"study sparse nonnegative least squares snnls problem snnls occurs naturally wide variety application...\"],[\"recent years structured matrix recovery problems gained considerable attention real world applicatio...\"],[\"due challenging applications collaborative filtering matrix completion problem widely studied past y...\"],[\"introduce sparse random projection important dimensionreduction tool machine learning estimation dis...\"],[\"simple interpretation matrix completion problem introduced based statistical models combined wellkno...\"],[\"many applications desirable extract relevant aspects data principled way information bottleneck meth...\"],[\"extend traditional worstcase minimax analysis stochastic convex optimization introducing localized f...\"],[\"dictionary learning cuttingedge area imaging processing recently led stateoftheart results many sign...\"],[\"mixture models gamma inversegamma distributed mixture components useful medical image tissue segment...\"],[\"study problem demixing pair sparse signals noisy nonlinear observations superposition mathematically...\"],[\"recently general method analyzing statistical accuracy algorithm developed applied simple latent var...\"],[\"paper considers emphvolume minimization volminbased structured matrix factorization smf volmin facto...\"],[\"sparse coding core building block many data analysis machine learning pipelines typically solved rel...\"],[\"sparse subspace clustering ssc elegant approach unsupervised segmentation data points cluster locate...\"],[\"motivated electricity consumption metering extend existing nonnegative matrix factorization nmf algo...\"],[\"constrained adaptive filtering algorithms inculding constrained least mean square clms constrained a...\"],[\"density matrices positively semidefinite hermitian matrices unit trace describe states quantum syste...\"],[\"propose unified framework estimating lowrank matrices nonconvex optimization based gradient descent ...\"],[\"effective accurate model selection important problem modern data analysis one major challenges compu...\"],[\"present sparse estimation dictionary learning framework compressed fiber sensing based probabilistic...\"],[\"paper proposes subspace decomposition method based overcomplete dictionary sparse representation cal...\"],[\"tensor decomposition important technique capturing highorder interactions among multiway data multil...\"],[\"joint blind source separation jbss emerging datadriven technique multiset datafusion paper jbss addr...\"],[\"study problem estimating lowrank matrices linear measurements aka matrix sensing nonconvex optimizat...\"],[\"propose generic framework based new stochastic variancereduced gradient descent algorithm accelerati...\"],[\"consider demixing problem two highdimensional vectors nonlinear observations number observations far...\"],[\"prototypal analysis introduced overcome two shortcomings archetypal analysis sensitivity outliers no...\"],[\"paper introduces method efficiently inferring highdimensional distributed quantity observations quan...\"],[\"saga fast incremental gradient method finite sum problem effectiveness tested vast applications pape...\"],[\"many statistical learning problems posed minimization sum two convex functions one typically composi...\"],[\"note answer question lecue showing column normalization random matrix iid entries need lead good spa...\"],[\"propose unified framework solve general lowrank plus sparse matrix recovery problems based matrix fa...\"],[\"pseudolikelihood method one popular algorithms learning sparse binary pairwise markov networks paper...\"],[\"paper studies new bayesian algorithm joint reconstruction classification reflectance confocal micros...\"],[\"study two procedures reversemode forwardmode computing gradient validation error respect hyperparame...\"],[\"goal paper design sequential strategies lead efficient optimization unknown function assumption fini...\"],[\"density ratio estimation vital tool machine learning statistical community however due unbounded nat...\"],[\"maximum correntropy criterion mcc recently successfully applied robust regression classification ada...\"],[\"consider generalization lowrank matrix completion case data belongs algebraic variety data point sol...\"],[\"ensure interpretability extracted sources tensor decomposition introduce paper dictionarybased tenso...\"],[\"recently shown many existing quasinewton algorithms formulated learning algorithms capable learning ...\"],[\"superresolution classical problem image processing numerous applications remote sensing image enhanc...\"],[\"paper study problem noisy tensor completion tensors admit canonical polyadic candecompparafac decomp...\"],[\"frankwolfe algorithm widely used solving nuclear norm constrained problems since require projections...\"],[\"give convergence guarantees estimating coefficients symmetric mixture two linear regressions expecta...\"],[\"propose set convex low rank inducing norms coupled matrices tensors hereafter coupled tensors shares...\"],[\"consider problem efficient randomized dimensionality reduction normpreservation guarantees specifica...\"],[\"recently blanchet kang murhy blanchet kang showed several machine learning algorithms squareroot las...\"],[\"datadriven distributionally robust optimization dddro via optimal transport shown encompass wide ran...\"],[\"consider problem estimation lowrank matrix limited number noisy rankone projections particular propo...\"],[\"sparse coding core building block many data analysis machine learning pipelines typically solved rel...\"],[\"piecewise linearquadratic plq penalties widely used develop models statistical inference signal proc...\"],[\"paper propose new volumepreserving flow show performs similarly linear general normalizing flow idea...\"],[\"present distributionally robust optimization dro approach estimate robustified regression plane line...\"],[\"restricted isometry property rip universal tool data recovery explore implication rip framework gene...\"],[\"interpreting gradient methods fixedpoint iterations provide detailed analysis methods minimizing con...\"],[\"given full partial information collection points lie close union several subspaces subspace clusteri...\"],[\"orthogonal matching pursuit omp orthogonal least squares ols widely used sparse signal reconstructio...\"],[\"tensor train decomposition provides spaceefficient representation higherorder tensors despite advant...\"],[\"consider structured matrix factorization model one factor restricted columns lying unit simplex simp...\"],[\"consider demixing problem two structured highdimensional vectors limited number nonlinear observatio...\"],[\"comment fact gradient ascent logistic regression connection perceptron learning algorithm logistic l...\"],[\"consider problem reconstructing signals images periodic nonlinearities problems design measurement s...\"],[\"consider problem solving largescale quadratically constrained quadratic program problems occur natur...\"],[\"modified cholesky decomposition commonly used precision matrix estimation given specified order rand...\"],[\"tensor decomposition methods popular tools learning latent variables given lowerorder moments data h...\"],[\"paper problem onebit compressed sensing obcs formulated problem probably approximately correct pac l...\"],[\"consider unknown smooth function rightarrow mathbbr say given noisymod samples fxi etaimod etai deno...\"],[\"goal extract meaningful transformations raw images varying thickness lines handwriting lighting port...\"],[\"present efficient alternating direction method multipliers admm algorithm segmenting multivariate no...\"],[\"paper study effects different prior likelihood choices bayesian matrix factorisation focusing small ...\"],[\"problems outliers detection robust regression highdimensional setting fundamental statistics numerou...\"],[\"paper study general problem optimizing convex function set times matrices subject rank constraints h...\"],[\"study problem recovering structured signal mathbfx highdimensional data mathbfyifmathbfaitmathbfx no...\"],[\"gradient descent optimization requires choose learning rate deeper deeper models tuning learning rat...\"],[\"vanishing ideal set polynomials takes zero value given data points originally proposed computer alge...\"],[\"fundamental task general density estimation keen interest machine learning work attempt systematical...\"],[\"modeling variability tensor decomposition methods one challenges source separation one possible solu...\"],[\"study problem recovery matrices simultaneously low rank row andor column sparse matrices appear rece...\"],[\"paper first work propose network predict structured uncertainty distribution synthesized image previ...\"],[\"determinantal point processes dpps enable modeling repulsion provide diverse sets points repulsion e...\"],[\"latent variable models hidden binary units appear various applications learning models particular pr...\"],[\"consider unknown smooth function rightarrow mathbbr say given noisy mod samples fxi etaimod etai den...\"],[\"spectral features empirical moment matrix constitute resourceful tool unveiling properties cloud poi...\"],[\"tensor data multidimension arrays lowrank decompositionbased regression methods tensor predictors ex...\"],[\"paper introduces general framework iterative optimization algorithms establishes general assumptions...\"],[\"context large samples small number individuals might spoil basic statistical indicators like mean di...\"],[\"consider problem minimizing convex function closed convex set projected gradient descent pgd propose...\"]],\"hovertemplate\":\"label=algorithms\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"algorithms\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"algorithms\",\"showlegend\":true,\"x\":[-1.4145582,-3.7018342,-1.4815822,-2.2459722,-1.356004,-3.25035,-0.6051509,-1.3066537,-1.4731208,-0.3281795,-0.49780264,-0.14869796,-0.8109651,-2.6170413,-2.6321115,-0.39892435,-0.23348095,-1.3464279,-0.46015668,-1.5282207,-3.1485567,-1.29657,-1.4466333,0.058173947,-1.3842257,-3.6431751,-0.7869349,-1.1451545,-0.37596092,-0.5245872,-0.84975564,-0.10440927,-2.3825455,-1.6240088,-0.09526228,-2.756955,-0.37708962,-0.7346298,-1.5569993,-0.91643924,-1.4971234,-1.0796269,-1.0116357,-1.3159606,-0.6716992,-1.0190572,-1.4375767,-1.5352124,-0.2510453,-0.19829527,-1.4432315,-1.6433982,-2.8954833,-1.405359,-0.7006311,-3.8601468,-3.6161788,-2.8649435,-1.5427498,-0.8250334,-0.8585605,-0.7046419,-0.562969,-0.8080239,-2.9510741,-0.2665633,-0.7773596,-1.5743735,-0.40284237,-1.9877077,-1.6231241,-0.5481883,-1.5191137,-3.5406055,-0.36846727,-2.1107564,-2.5264833,-3.3362093,0.033625968,-1.3377221,-1.5894507,-1.9125749,-0.31820595,-0.61653084,-1.1556414,-0.01014379,-1.4204086,-0.768926,-0.694299,-0.7781052,-1.1771368,0.6937842,-1.0020245,-4.2007027,-0.85277563,-0.8512804,-1.906853,-0.6876769,-0.9135548,-0.59453255,-0.13733886,-0.34864464,-0.5950173,-0.37386477,-0.84304285,-1.787254,-1.4930848,-0.64861846,-1.6101717,-2.9096384,-2.7785008,-1.5293071,-1.4864695,-1.4589479,-3.306656,-1.3178514,-3.480131,-0.7314971,-2.0178604,-0.09665397,-2.8167117,-3.3522937,-1.1939298,-2.754627,-0.57434505,-3.3378017,-0.7389546,-0.49732104,-0.76380086,-0.71893454,-1.4030954,-0.6723621,-0.6583971,-1.9862812,-0.34595865,0.71447736,-0.5787897,-2.8197186,-0.5146677,-0.81439424,-1.3700243,-0.60394615,-2.5537362,-0.8677555,-0.8041384,-1.2956127,-0.36629036,-0.37053776,-0.0051930463,-0.0803722,-0.7782323,-1.1831481,-0.7379032,-1.8654969,-3.0410047,-1.1796168,-1.4918232,-0.8829937,-0.5916563,-1.5212265,-3.6971104,-3.5244243,-3.3885155,-2.3223498,-2.0820107,-1.3089843,-0.27840257,-3.6716263,-0.3340033,-0.22319944,-0.79711974,-2.7550058,-0.3098737,-1.3032014,-1.5429935,-2.5436172,-0.8893264,-0.79246795,-1.3043703,-1.690995,-1.6875831,-0.44201994,-1.7240181,-1.3862618,-0.3678241,-0.23650356,-0.46894935,-0.6054285,-1.6345835,-0.442083,-1.8985509,-0.74981034,-0.2629284,-0.5599073,-1.8946006,-2.8785183,-1.346842,-4.00138,-1.7680329,-0.83811533,-0.56739706,-1.8993558,-1.2738378,-2.9235654,0.021885956,-0.738098,-2.8471324,-1.6042998,-1.4270141,-1.8769423,-1.4563504,-0.12010407,-1.7530745,-1.6820567,-1.948268],\"xaxis\":\"x\",\"y\":[6.0724535,5.2445326,5.9721017,5.533698,6.425888,5.1083045,5.521571,6.259471,6.09414,5.7935734,5.927297,5.6336217,6.186572,4.4967737,4.4696646,6.305879,5.9234905,6.459284,6.5884643,6.386157,4.930992,6.1533628,5.1502957,6.493053,5.2759786,5.4396033,6.314533,6.3875823,5.8994646,5.920588,6.3729234,5.6807923,3.6839125,6.168184,5.6539264,3.838157,6.688616,6.022026,4.58421,6.5035753,5.0957594,6.6538415,6.2190704,6.4146876,6.6465745,6.2837257,5.2310953,5.0076675,6.678695,6.20966,5.213279,6.3857236,5.8320436,5.1740417,6.5786195,5.1435165,5.167285,3.8307881,4.386388,5.888487,6.2127914,5.8639517,6.517606,6.3960733,5.0139465,6.7157536,6.5430636,4.8592553,6.487677,5.592791,4.833707,6.686473,6.3996525,2.5687122,6.6760483,5.243776,4.8587294,5.0234566,6.534995,6.284492,6.5429916,3.6728888,5.7879,5.9375725,6.29075,6.6021175,4.9737387,6.494289,6.1446276,6.4951615,5.7753587,5.3317385,6.46672,5.782832,5.9159274,6.4679685,5.69428,6.24095,6.495231,6.588663,6.6869955,6.0523453,6.8500605,6.6380143,6.4442105,4.8399568,5.198207,6.491489,4.879467,4.948025,4.0049844,6.655086,4.716925,6.1199827,5.1319904,5.442344,5.1299553,5.743823,5.6070876,6.0878043,4.5776534,5.0596123,6.328905,4.8938913,5.947574,2.7647717,6.476491,6.269998,6.4612517,6.609017,5.839199,6.5854697,5.726893,5.223511,5.8082547,5.32461,5.978618,5.162198,6.481735,5.686478,6.44993,6.5591316,4.582776,6.408001,6.4110055,4.523313,5.7414317,5.8613663,6.609665,6.154131,6.3966866,5.799649,6.0651746,4.7714963,5.0944076,5.2599187,5.3228664,6.322106,6.2158537,5.272562,5.214902,3.0348618,2.7205026,4.806095,4.6507545,6.2722616,6.338294,2.746746,6.0770054,6.7000794,6.509166,5.074386,6.7009115,6.1232862,4.800584,4.4998612,6.4959574,5.716067,5.044048,5.6683817,4.7520175,5.7037973,5.2595196,6.4096494,5.728373,6.706338,6.498905,5.969714,5.179659,5.9460793,5.649309,6.111477,6.68411,5.834972,5.1609707,4.782193,5.3145356,5.5945306,4.7625394,6.4808707,5.951145,5.1775513,6.209831,4.852584,6.4780765,6.3426075,4.830397,6.1218333,6.7058544,5.0435147,6.6279907,6.5955544,5.23333,4.4710426,5.255082],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"analyze performance class manifoldlearning algorithms find output minimizing quadratic form normaliz...\"],[\"present procrustes measure novel measure based procrustes rotation enables quantitative comparison o...\"],[\"local linear embedding algorithm lle nonlinear dimensionreducing technique widely used due computati...\"],[\"slow feature analysis sfa method extracting slowly varying driving forces quickly varying nonstation...\"],[\"independent component analysis ica aims decomposing observed random vector statistically independent...\"],[\"present contribution suggests use multidimensional scaling mds algorithm visualization tool manifold...\"],[\"density modeling notoriously difficult high dimensional data one approach problem search lower dimen...\"],[\"goal crossdomain object matching cdom find correspondence two sets objects different domains unsuper...\"],[\"purpose sufficient dimension reduction sdr find lowdimensional subspace input features sufficient pr...\"],[\"paper autoassociative models proposed candidates generalization principal component analysis show mo...\"],[\"study problem estimating data sparse approximation inverse covariance matrix estimating sparsity con...\"],[\"propose novel method introducing structure existing machine learning techniques developing structure...\"],[\"highdimensional data common genomics proteomics chemometrics often contains complicated correlation ...\"],[\"maximum variance unfolding one main methods nonlinear dimensionality reduction study large sample li...\"],[\"technical note considers problems blind sparse learning inference electrogram egm signals atrial fib...\"],[\"multiple multivariate data sets derive conditions generalized canonical correlation analysis gcca im...\"],[\"correlation matrices play key role many multivariate methods graphical model estimation factor analy...\"],[\"recent years manifold learning become increasingly popular tool performing nonlinear dimensionality ...\"],[\"consider robust covariance estimation group symmetry constraints nongaussian covariance estimation t...\"],[\"study sparse principal component analysis high dimensional vector autoregressive time series doubly ...\"],[\"reshef reshef recently published paper present method called maximal information coefficient mic det...\"],[\"present robust alternative principal component analysis pca called elliptical component analysis eca...\"],[\"address structured covariance estimation elliptical distributions assuming covariance priori known b...\"],[\"linear dimensionality reduction methods cornerstone analyzing high dimensional data due simple geome...\"],[\"canonical correlation analysis cca widely used statistical tool well established theory favorable pe...\"],[\"contribution deals generalized symmetric fastica algorithm domain independent component analysis ica...\"],[\"recently focus penalized loglikelihood covariance estimation sparse inverse covariance precision mat...\"],[\"fastica algorithm one popular iterative algorithms domain linear independent component analysis desp...\"],[\"principal components analysis widely used technique dimension reduction characterization variability...\"],[\"introduce locally linear latent variable model lllvm probabilistic model nonlinear manifold discover...\"],[\"matching datasets multiple modalities become important task data analysis existing methods often rel...\"],[\"given iid observations random vector highdimensional vector lowdimensional index variable study prob...\"],[\"learning low dimensional structure multidimensional data canonical problem machine learning one comm...\"],[\"statistical dependencies independent component analysis ica cannot remove often provide rich informa...\"],[\"aim paper provide new method learning relationships data obtained independently unlike existing meth...\"],[\"paper presents new framework manifold learning based sequence principal polynomials capture possibly...\"],[\"nongaussian component analysis ngca unsupervised linear dimension reduction method extracts lowdimen...\"],[\"manifold learning dimensionality reduction techniques ubiquitous science engineering computationally...\"],[\"research manifold learning within density ridge estimation framework shown great potential recent wo...\"],[\"sparse generalized eigenvalue problem gep plays pivotal role large family highdimensional statistica...\"],[\"multivariate analysis mva comprises family wellknown methods feature extraction exploit correlations...\"],[\"present new method estimating multivariate secondorder stationary gaussian random field grf models b...\"],[\"sparse versions principal component analysis pca imposed simple yet powerful ways selecting relevant...\"],[\"generalized canonical correlation analysis gcca aims finding latent lowdimensional common structure ...\"],[\"independent component analysis ica powerful method blind source separation based assumption sources ...\"],[\"independent component analysis ica popular method blind source separation bss diverse set applicatio...\"],[\"regularised canonical correlation analysis recently extended two sets variables multiblock method re...\"],[\"many machine learning algorithms require precise estimates covariance matrices sample covariance mat...\"],[\"spectral dimensionality reduction frequently used identify lowdimensional structure highdimensional ...\"],[\"provide way infer existence topological circularity highdimensional data sets mathbbrd projection ma...\"],[\"canonical correlation analysis cca multivariate statistical technique finding linear relationship tw...\"],[\"microwavebased breast cancer detection proposed complementary approach compensate drawbacks existing...\"],[\"semiparametric nonlinear regression model presence latent variables introduced latent variables corr...\"],[\"various problems data analysis statistical genetics call recovery columnsparse lowrank matrix noisy ...\"],[\"consider stationary autoregressive processes coefficients restricted ellipsoid includes autoregressi...\"],[\"statistical dimensionality reduction common rely assumption high dimensional data tend concentrate n...\"],[\"ability many powerful machine learning algorithms deal large data sets without compromise often hamp...\"],[\"principal component analysis pca popular perform dimension reduction selection number significant co...\"],[\"stochastic principal component analysis spca become popular dimensionality reduction strategy large ...\"],[\"reliable measures statistical dependence could useful tools learning independent features performing...\"],[\"inverse covariance matrix provides considerable insight understanding statistical models multivariat...\"],[\"introduce approach based givens representation posterior inference statistical models orthogonal mat...\"],[\"independent component analysis ica technique unsupervised exploration multichannel data widely used ...\"],[\"independent component analysis ica widely used bss method uniquely achieve source recovery subject s...\"],[\"multitude methods perform multiset correlated component analysis mcca including require iterative so...\"],[\"paper propose new algorithm streaming principal component analysis limited memory small devices cann...\"],[\"independent component analysis ica one basic tools data analysis aims find coordinate system compone...\"],[\"develop theory nonlinear dimensionality reduction nldr number nldr methods developed limited underst...\"],[\"past decade techniques topological data analysis tda grown prominence describe shape data recent yea...\"]],\"hovertemplate\":\"label=pca\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"pca\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"pca\",\"showlegend\":true,\"x\":[-2.2452781,-2.231826,-2.253539,0.21168412,0.25382748,-2.3253303,-2.4834998,-2.0921075,0.09651743,-2.2193966,-1.0719587,-2.2853794,0.34214768,-2.1763678,-0.050655544,0.015409523,-0.037997976,-2.2943087,-1.4634997,-0.15760136,-0.2406963,-0.36768365,-1.2274286,-1.2004771,-0.8133126,0.20160834,-1.1066357,0.22549365,0.16091803,-2.3877952,-2.0721514,-1.1435536,-2.3514128,0.24328884,-1.9442061,-2.2390888,0.08949758,-2.24289,-2.3423383,-0.6434444,0.055605147,-2.5037324,0.017798211,-0.38826767,0.25508463,0.249886,0.23676997,-1.0061382,-2.2805703,-2.3014317,-0.67001593,0.20018137,-1.5397576,0.1549529,-1.2995661,-2.2421637,-1.0742224,-3.7343614,-1.2113941,0.26005822,-1.1201352,-2.674831,0.22132848,0.27942744,0.1275869,0.24470413,0.2367932,-2.2525144,-0.6537912],\"xaxis\":\"x\",\"y\":[6.155852,6.099156,6.1263313,5.922965,5.7947345,6.10382,6.0073376,5.418204,6.00777,6.0855184,5.819627,6.12905,4.617593,6.1610274,5.4410315,5.1699533,4.5846205,6.126215,5.805386,6.1403403,4.3650274,6.191772,5.8991957,5.656471,5.888586,6.0032535,5.699516,5.979162,5.7037024,6.0655055,5.5372314,5.5808277,6.0848703,5.879866,5.505701,6.0648355,6.055879,6.1404853,6.0829864,6.0140333,5.989297,5.2036023,5.942461,6.1885138,5.9405036,5.9584713,5.1073475,5.8361106,6.1263003,6.135902,5.7292666,5.413771,6.5353546,5.9819283,6.035962,6.1109567,5.9567657,6.039782,5.9610653,5.931636,5.7291107,5.9887576,5.9815116,5.909501,6.0054626,6.0051136,6.0079417,6.1835117,4.3516383],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"consider principal component analysis pca decomposable gaussian graphical models exploit prior infor...\"],[\"mincut clustering based minimizing one two heuristic costfunctions proposed shi malik spawned tremen...\"],[\"explosion interest statistical models analyzing network data considerable interest class exponential...\"],[\"prove criterion markov equivalence provided zhao may involve set features graph exponential number v...\"],[\"define class euclidean distances weighted graphs enabling perform thermodynamic soft graph clusterin...\"],[\"existing approaches analyzing asymptotics graph laplacians typically assume wellbehaved kernel funct...\"],[\"present method estimate block membership nodes random graph generated stochastic blockmodel use embe...\"],[\"unbalanced data arises many learning tasks clustering multiclass data hierarchical divisive clusteri...\"],[\"propose new yet natural algorithm learning graph structure general discrete graphical models aka mar...\"],[\"consider structure learning problem graphical models call loosely connected markov random fields num...\"],[\"given two graphs graph matching problem align two vertex sets minimize number adjacency disagreement...\"],[\"paper consider problem link prediction timeevolving graphs assume certain graph features node degree...\"],[\"graph clustering involves task dividing nodes clusters edge density higher within clusters opposed a...\"],[\"given time series graphs fixed vertex set represents actors edge vertex vertex time represents exist...\"],[\"statistical inference graphs burgeoning field applied theoretical statistics communities well throug...\"],[\"lately several suggestions parametrized distances graph generalize shortest path distance commute ti...\"],[\"partitioning graph groups vertices within group densely connected vertices assigned different groups...\"],[\"propose spectral clustering method based local principal components analysis pca performing local pc...\"],[\"present two graphbased algorithms multiclass segmentation highdimensional data algorithms use diffus...\"],[\"spectral clustering graphbased semisupervised learning ssl algorithms sensitive graphs constructed d...\"],[\"work develops generic framework called bagofpaths bop link network data analysis central idea assign...\"],[\"consider problem vertex classification graphs constructed latent position model shown previously app...\"],[\"interaction transitivity sparsity two common features empirical networks implies local regions large...\"],[\"community detection graphs subject many algorithms recent methods want optimize modularity function ...\"],[\"spectral clustering sensitive graphs constructed data particularly proximal imbalanced clusters pres...\"],[\"vertex clustering stochastic blockmodel graph wide applicability subject extensive research thispape...\"],[\"stochastic blockmodels among prominent statistical models cluster analysis complex networks clusters...\"],[\"random graphs distributed according stochastic blockmodels special case latent position graphs adjac...\"],[\"performance spectral clustering considerably improved via regularization demonstrated empirically am...\"],[\"paper aims justifying lwf amp chain graphs showing represent arbitrary independence models specifica...\"],[\"detection anomalous activity graphs statistical problem arises many applications network surveillanc...\"],[\"modeling structure complex networks using bayesian nonparametrics makes possible specify flexible mo...\"],[\"visual rendering graphs key task mapping complex network data although graph drawing algorithms emph...\"],[\"consider problem grouping multiple graphs several clusters using singular value thesholding nonnegat...\"],[\"natural approach analyze interaction data form whatconnectstowhatwhen create timeseries rather seque...\"],[\"many graph clustering quality functions suffer resolution limit inability find small clusters large ...\"],[\"network metrics form fundamental part network analysis toolbox used quantitatively measure different...\"],[\"several problems network intrusion community detection disease outbreak described observations attri...\"],[\"community detection fundamental problem network analysis made challenging overlaps communities often...\"],[\"consider problem unveiling implicit network structure node interactions user interactions social net...\"],[\"laplacian mixture models identify overlapping regions influence unlabeled graph network data scalabl...\"],[\"labeled stochastic block model random graph model representing networks community structure interact...\"],[\"consider problem embedding unweighted directed knearest neighbor graphs lowdimensional euclidean spa...\"],[\"consider problem estimating undirected trianglefree graphs high dimensional distributions trianglefr...\"],[\"hypergraph partitioning lies heart number problems machine learning network sciences many algorithms...\"],[\"paper develops exact linear relationship leading eigenvector unnormalized modularity matrix eigenvec...\"],[\"recent years increased interest statistical analysis data multiple types relations among set entitie...\"],[\"latent block model lbm flexible probabilistic tool describe interactions node sets bipartite network...\"],[\"paper presents novel spectral algorithm additive clustering designed identify overlapping communitie...\"],[\"many statistical methods network data parameterize edgeprobability attributing latent traits vertice...\"],[\"paper focus stochastic block model sbma probabilistic tool describing interactions nodes network usi...\"],[\"estimation probabilities network edges observed adjacency matrix important applications predicting m...\"],[\"latent space model family random graphs assigns realvalued vectors nodes graph edge probabilities de...\"],[\"paper exact linear relation leading eigenvectors modularity matrix singular vectors uncentered data ...\"],[\"laplacian eigenvectors graph constructed data set used many spectral manifold learning algorithms di...\"],[\"stochastic block model sbm widely used random graph model networks communities despite recent burst ...\"],[\"paper present framework fitting multivariate hawkes processes largescale problems number events obse...\"],[\"many real world graphs graphs molecules exhibit structure multiple different scales existing kernels...\"],[\"nonparametric detection existence anomalous structure network investigated nodes corresponding anoma...\"],[\"many complex ecosystems formed multiple microbial taxa involve intricate interactions amongst variou...\"],[\"present method based orthogonal symmetric nonnegative matrix trifactorization normalized laplacian m...\"],[\"problem finding overlapping communities networks gained much attention recently optimizationbased ap...\"],[\"given graph vertices deemed interesting priori vertex nomination task order remaining vertices nomin...\"],[\"paper propose bayesian nonparametric approach modelling sparse timevarying networks positive paramet...\"],[\"spectral embedding uses eigenfunctions discrete laplacian weighted graph obtain coordinates embeddin...\"],[\"prove central limit theorem components eigenvectors corresponding largest eigenvalues normalized lap...\"],[\"article study spectral methods community detection based alphaparametrized normalized modularity mat...\"],[\"spectral analysis neighborhood graphs one widely used techniques exploratory data analysis applicati...\"],[\"many popular network models rely assumption vertex exchangeability distribution graph invariant rela...\"],[\"present model random simple graphs degree distribution obeys power law heavytailed attain behavior e...\"],[\"graphbased semisupervised learning one popular methods machine learning theoretical properties bound...\"],[\"spectral clustering popular versatile clustering method based relaxation normalised graph cut object...\"],[\"consider problem accelerating distributed optimization multiagent networks sequentially adding edges...\"],[\"consider problem estimating consensus community structure combining information multiple layers mult...\"],[\"consider two networks overlapping nonidentical vertex sets given vertices interest first network see...\"],[\"present work deals active sampling graph nodes representing training data binary classification grap...\"],[\"traditionally community detection graphs solved using spectral methods posterior inference probabili...\"],[\"volume data generated internet social networks increasing every day clear need efficient ways extrac...\"],[\"recent papers formulated problem learning graphs data inverse covariance estimation graph laplacian ...\"],[\"present probabilistic framework overlapping community discovery link prediction relational data give...\"],[\"develop model interactions nodes dynamic network counted non homogeneous poisson processes block mod...\"],[\"superposition temporal point processes studied many years although usefulness models practical appli...\"],[\"network clustering reveals organization network corresponding complex system elements represented ve...\"],[\"given vertex interest network vertex nomination problem seeks find corresponding vertex interest exi...\"],[\"variety machine learning taskseg matrix factorization topic modelling feature allocationcan viewed l...\"],[\"consider problem clustering longestleg path distance llpd metric informative elongated irregularly s...\"],[\"paper design nonparametric online algorithm estimating triggering functions multivariate hawkes proc...\"],[\"consider problem model selection gaussian markov fields sample deficient scenario benchmark informat...\"],[\"consider learning multiagent hawkes processes model containing multiple hawkes processes shared endo...\"],[\"suppose one particular block stochastic block model interest block labels observed vertices network ...\"],[\"many popular dimensionality reduction procedures outofsample extensions allow practitioner apply lea...\"],[\"propose simulation method multidimensional hawkes processes based superposition theory point process...\"],[\"propose novel class network models temporal dyadic interaction data goal capture number important fe...\"]],\"hovertemplate\":\"label=graphs\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"graphs\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"graphs\",\"showlegend\":true,\"x\":[-2.9342616,-3.1454298,-2.4850407,-2.574306,-3.1462982,-2.9692078,-2.9489932,-3.1740615,-2.5277135,-2.4390683,-2.686362,-2.578905,-3.0794926,-2.5614207,-2.5799448,-2.5449202,-3.1601317,-3.144834,-3.0598352,-3.1797934,-2.4355795,-2.7306492,-2.8029714,-3.0636506,-3.1796865,-2.970507,-2.7938783,-2.791284,-3.1346276,-2.711194,-2.5986233,-2.3709083,-2.8209746,-3.2418153,-3.227597,-3.1307065,-2.4367008,-2.721368,-2.9933803,-2.479923,-3.1811314,-2.7631838,-2.8104792,-2.4192102,-3.0800002,-3.1346407,-2.5823605,-2.678738,-3.047069,-2.5536323,-2.6822069,-2.6378188,-2.967152,-3.0551538,-3.1690185,-2.7908049,-4.1478724,-2.952407,-2.5269022,-2.287089,-3.0959458,-3.0101588,-2.6438723,-2.598003,-3.092816,-2.846999,-3.138336,-3.1739388,-2.6372535,-2.4723082,-3.0610003,-3.2164502,-2.5362294,-3.0999336,-2.7244964,-1.2065701,-2.707142,-2.1927888,-2.6744874,-2.750895,-2.6371005,-4.0842876,-2.8032575,-2.588869,-2.8292,-3.1861978,-4.125158,-2.3777692,-4.1372905,-3.007885,-2.7846935,-4.1558957,-2.515318],\"xaxis\":\"x\",\"y\":[7.9765706,7.9602985,8.077706,8.001013,7.8851957,7.956482,8.111632,7.930139,7.95993,7.9363103,8.000705,8.244081,8.040609,7.933619,7.982851,7.9616756,7.970493,7.793057,7.7018347,7.958869,8.098812,7.9791684,8.259179,8.156455,8.017194,8.093487,8.264925,8.03436,8.010961,8.055647,8.140909,8.196713,7.9511843,7.898294,7.867128,8.025952,8.159083,8.199703,8.239039,8.348546,7.935474,8.203469,7.9355555,7.8874936,8.044166,7.9025364,8.161142,8.328424,8.18736,8.09675,8.320209,8.253193,8.029053,7.462695,7.8377566,8.21306,3.3683612,7.997521,8.060886,8.170484,8.029419,8.298372,8.008256,8.27684,7.7709165,8.020139,8.080501,7.9699078,8.152744,7.972736,7.904558,7.988838,8.190965,8.122873,8.2222,2.9245458,8.194084,8.141195,8.018842,8.36074,8.392423,3.3337865,8.278723,8.046795,7.8406954,7.8201275,3.3463852,7.879265,3.3075492,8.0552845,7.98219,3.3513763,8.356715],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"report derive nonnegative series expansion jensenshannon divergence jsd two probability distribution...\"],[\"many inference problems involving questions optimality ask maximum minimum finite set unknown quanti...\"],[\"deep belief networks powerful way model complex probability distributions however learning structure...\"],[\"propose novel algorithm solve expectation propagation relaxation bayesian inference continuousvariab...\"],[\"compute expected value kullbackleibler divergence various fundamental statistical models respect can...\"],[\"meanfield variational methods widely used approximate posterior inference many probabilistic models ...\"],[\"study mixtures factorizing probability distributions represented visible marginal distributions stoc...\"],[\"consider problem joint modelling metabolic signals gene expression systems biology applications prop...\"],[\"paper generalizes beta divergence beyond classical form associated power variance functions tweedie ...\"],[\"article describe model derivation implementation variational bayesian inference linear logistic regr...\"],[\"laplace approximation calls computation second derivatives likelihood maximum maximum found emalgori...\"],[\"variational inference powerful concept underlies many iterative approximation algorithms expectation...\"],[\"variational inference algorithms proven successful bayesian analysis large data settings recent adva...\"],[\"deep gaussian processes provide flexible approach probabilistic modelling data using either supervis...\"],[\"two recently introduced criteria estimation generative models based reduction binary classification ...\"],[\"present derivation kullback leibler kldivergence also known relative entropy von mises fisher vmf di...\"],[\"introduce local expectation gradients general purpose stochastic variational inference algorithm con...\"],[\"empirically evaluate stochastic annealing strategy bayesian posterior optimization variational infer...\"],[\"recurring problem building probabilistic latent variable models regularization model selection insta...\"],[\"variational inference scalable technique approximate bayesian inference deriving variational inferen...\"],[\"mean field variational bayes mfvb popular posterior approximation method due fast runtime largescale...\"],[\"highlight pitfall applying stochastic variational inference general bayesian networks global random ...\"],[\"introduce incremental variational inference apply latent dirichlet allocation lda incremental variat...\"],[\"stochastic variational inference relatively well known scaling inference bayesian probabilistic mode...\"],[\"propose secondorder hessian hessianfree based optimization method variational inference inspired gau...\"],[\"blackbox alpha bbalpha new approximate inference method based minimization alphadivergences bbalpha ...\"],[\"stochastic variational inference collapsed models recently successfully applied large scale topic mo...\"],[\"stochastic variational inference collapsed models recently successfully applied large scale topic mo...\"],[\"learning deep models using bayesian methods generated significant attention recently largely feasibi...\"],[\"datasets growing size complexity creating demand rich models quantification uncertainty bayesian met...\"],[\"introduce new approach amortizing inference directed graphical models learning heuristic approximati...\"],[\"introduce overdispersed blackbox variational inference method reduce variance monte carlo estimator ...\"],[\"partition functions probability distributions important quantities model evaluation comparisons pres...\"],[\"note compares two recently published machine learning methods constructing flexible tractable famili...\"],[\"propose general modeling inference framework composes probabilistic graphical models deep learning m...\"],[\"quantitatively assessing relationships latent variables observed variables important understanding d...\"],[\"matrix generalized inverse gaussian mathcalmgig distribution arises naturally settings distribution ...\"],[\"extend stochastic gradient variational bayes perform posterior inference weights stickbreaking proce...\"],[\"stochastic variational inference svi stateoftheart algorithm scaling variational inference largedata...\"],[\"approximate bayesian computation abc framework performing likelihoodfree posterior inference simulat...\"],[\"variational inference lies core many stateoftheart algorithms improve approximation posterior beyond...\"],[\"reparameterization gradient become widely used method obtain monte carlo gradients optimize variatio...\"],[\"generative adversarial networks gans successful deep generative models gans based twoplayer minimax ...\"],[\"derive novel variational expectation maximization approach based truncated posterior distributions t...\"],[\"goal twosample tests assess whether two samples sim sim drawn distribution perhaps intriguingly one ...\"],[\"variational inference provides powerful tool approximate probabilistic ference complex structured mo...\"],[\"stochastic variational inference svi paradigm combines variational inference natural gradients stoch...\"],[\"present approach deep estimation discrete conditional probability distributions models several appli...\"],[\"computing partition function important statistical inference task arising applications graphical mod...\"],[\"deep generative models wildly successful learning coherent latent representations continuous data vi...\"],[\"standard interpretation importanceweighted autoencoders maximize tighter lower bound marginal likeli...\"],[\"variational inference approximates posterior distribution probabilistic model parameterized density ...\"],[\"stein variational gradient descent svgd deterministic sampling algorithm iteratively transports set ...\"],[\"probability distributions produced crossentropy loss ordinal classification problems possess undesir...\"],[\"study unsupervised generative modeling terms optimal transport problem true unknown data distributio...\"],[\"deep generative models provide powerful tools distributions complicated manifolds natural images man...\"],[\"generative adversarial networks gans become widely popular framework generative modelling highdimens...\"],[\"build autoencoding sequential monte carlo aesmc method model proposal learning based maximizing lowe...\"],[\"short article revisits ideas introduced arxiv arxiv simple setup sheds lights connexions variational...\"],[\"present two deep generative models based variational autoencoders improve accuracy drug response pre...\"],[\"propose simple algorithm train stochastic neural networks draw samples given target distributions pr...\"],[\"introduce new algorithm approximate inference combines reparametrization markov chain monte carlo va...\"],[\"two fundamental problems unsupervised learning efficient inference latentvariable models robust dens...\"],[\"propose new framework hamiltonian monte carlo hmc truncated probability distributions smooth underly...\"],[\"automatic chemical design framework generating novel molecules optimized properties original scheme ...\"],[\"robustness outliers central issue realworld machine learning applications replacing model heavytaile...\"],[\"paper raises implicit manifold learning perspective generative adversarial networks gans studying su...\"],[\"deep generative models provide systematic way learn nonlinear data distributions set latent variable...\"],[\"stein variational gradient descent svgd recently proposed particlebased bayesian inference method at...\"],[\"develop riemannian stein variational gradient descent rsvgd bayesian inference method generalizes st...\"],[\"investigate optimization two probabilistic generative models binary latent variables using novel var...\"],[\"computing partition function discrete graphical model fundamental inference challenge since computat...\"],[\"motivations using variational inference neural networks differ significantly latent variable models ...\"],[\"introduce novel generative formulation deep probabilistic models implementing soft constraints funct...\"],[\"measuring divergence two distributions essential machine learning statistics various applications in...\"],[\"present novel model architecture leverages deep learning tools perform exact bayesian inference sets...\"],[\"provide sharp empirical estimates expectation variance normal approximation class statistics whose v...\"],[\"probabilistic graphical models key tool machine learning applications computing partition function n...\"],[\"recent efforts combining deep models probabilistic graphical models promising providing flexible mod...\"],[\"vector quantizedvariational autoencoders vqvae generative models based discrete latent representatio...\"],[\"discrete latent space models recently achieved performance par continuous counterparts deep variatio...\"]],\"hovertemplate\":\"label=variational\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"variational\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"variational\",\"showlegend\":true,\"x\":[-4.2679462,-4.892758,-4.848768,-4.965491,-4.3921227,-5.0092645,-4.92403,-4.9170284,-4.3068385,-5.0281954,-5.0171113,-5.0091095,-5.004359,-5.1362348,-5.38496,-4.20611,-5.0630236,-5.120061,-5.267962,-5.098538,-4.863253,-5.0267186,-5.0123277,-5.018129,-4.998894,-5.082569,-4.9019365,-4.9128723,-4.844451,-4.7267756,-4.8467965,-5.132272,-4.9176435,-5.194834,-5.1844077,-5.2417393,-4.462402,-5.2687573,-5.0332046,-5.117319,-5.032144,-5.2047067,-5.4118147,-5.0517244,-1.8720716,-5.1668477,-5.0775137,-5.0169225,-5.2291155,-5.2928123,-5.237878,-5.141709,-4.9321012,-4.9169025,-5.382616,-5.4267516,-5.4484415,-5.0453506,-5.420282,-5.3679056,-5.1884227,-5.1619763,-5.40014,-4.797403,-5.313054,-4.8577523,-5.440878,-5.3801765,-5.037217,-5.0217285,-5.1944847,-5.2588787,-5.217553,-5.0914297,-5.397319,-4.8649874,-5.0337977,-5.283809,-5.231437,-5.327433,-5.3094444],\"xaxis\":\"x\",\"y\":[5.352194,5.0937266,3.91236,5.1367292,5.3718033,5.1444836,3.887568,5.1367526,5.3749313,5.115073,4.7956586,5.055434,5.0981917,4.6057043,4.1771636,5.368363,4.959614,4.89987,4.3981433,4.937002,5.077091,4.90821,5.375879,5.0260153,4.752085,4.9225645,5.7156196,5.67519,4.0587516,4.889849,4.4266696,4.916905,4.579901,4.7093077,4.430528,4.4343944,5.207685,4.480314,5.1818476,4.923545,4.961038,4.912532,4.1718745,4.9738617,2.6609693,4.8352985,5.0699854,4.067253,5.0107255,4.3657594,4.3968506,4.876548,4.541489,3.904998,4.227072,4.166451,4.147316,4.314138,4.1990714,4.3054056,4.421923,4.789668,4.1736884,4.738056,4.3667574,4.4419484,4.1663103,4.2662683,4.6249804,4.5922494,4.821308,4.9697533,4.813399,4.750477,4.1419773,3.9725983,5.0224304,4.988485,4.6088834,4.3695555,4.384941],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"recent methods estimating sparse undirected graphs realvalued data high dimensional problems rely he...\"],[\"network models popular modeling representing complex relationships dependencies observed variables d...\"],[\"bnlearn package includes several algorithms learning structure bayesian networks either discrete con...\"],[\"study graph estimation density estimation high dimensions using family density estimators based fore...\"],[\"paper consider sparse identifiable linear latent variable factor linear bayesian network models pars...\"],[\"challenging problem estimating highdimensional graphical models choose regularization parameter data...\"],[\"study problem estimating temporally varying coefficient varying structure vcvs graphical model under...\"],[\"paper propose semiparametric approach named nonparanormal skeptic efficiently robustly estimating hi...\"],[\"many real world network problems often concern multivariate nodal attributes image textual multiview...\"],[\"propose new method detecting changes markov network structure two sets samples instead naively fitti...\"],[\"introduce randomized dependence coefficient rdc measure nonlinear dependence random variables arbitr...\"],[\"study adaptive estimation copula correlation matrix sigma semiparametric elliptical copula model con...\"],[\"estimation dependencies multiple variables central problem analysis financial time series common app...\"],[\"theory graphical models matured three decades provide backbone several classes models used myriad ap...\"],[\"proposed new statistical dependency measure called copula dependency coefficientcdc two sets variabl...\"],[\"consider discrete graphical models markov respect graph propose two distributed marginal methods est...\"],[\"analyzing understanding structure complex relational data important many applications including anal...\"],[\"propose method inferring conditional indepen dence graph cig highdimensional discretetime gaus sian ...\"],[\"investigate generic problem learning pairwise exponential family graphical models pairwise sufficien...\"],[\"real world systems typically feature variety different dependency types topologies complicate model ...\"],[\"tree structured graphical models powerful expressing long range hierarchical dependency among many v...\"],[\"undirected graphical models known markov networks popular wide variety applications ranging statisti...\"],[\"inductive probabilistic classification rule must generally obey principles bayesian predictive infer...\"],[\"propose new high dimensional semiparametric principal component analysis pca method named copula com...\"],[\"bayesian networks convenient graphical expressions high dimensional probability distributions repres...\"],[\"propose method inferring conditional independence graph cig highdimensional gaussian vector time ser...\"],[\"graphical models commonly used tools modeling multivariate random variables exist many convenient mu...\"],[\"paper presents foundational theoretical results distributed parameter estimation undirected probabil...\"],[\"study problem learning sparse structure changes two markov networks rather fitting two markov networ...\"],[\"loglinear models popular workhorses analyzing contingency tables loglinear parameterization interact...\"],[\"propose novel graphical model selection gms scheme highdimensional stationary time series discrete t...\"],[\"propose new class semiparametric exponential family graphical models analysis high dimensional mixed...\"],[\"graphical models provide powerful tools uncover complicated patterns multivariate data commonly used...\"],[\"inference learning graphical models wellstudied problems statistics machine learning found many appl...\"],[\"paper proposes unified framework quantify local global inferential uncertainty high dimensional nonp...\"],[\"high dimensions propose analyze aggregation estimator precision matrix gaussian graphical models est...\"],[\"learn structure markov network two groups random variables joint observations since modelling learni...\"],[\"consider inference structure undirected graphical model exact bayesian framework specifically aim ac...\"],[\"presence weak overall correlation may useful investigate correlation significantly substantially pro...\"],[\"consider two connected aspects maximum likelihood estimation parameter highdimensional discrete grap...\"],[\"main contribution article new prior distribution directed acyclic graphs gives larger weight sparse ...\"],[\"networks capture intuition relationships world describe friendships facebook users interactions fina...\"],[\"paper addresses problem scalable optimization lregularized conditional gaussian graphical models con...\"],[\"gaussian graphical models ggms popular tools studying network structures however many modern applica...\"],[\"propose novel class timevarying nonparanormal graphical models allows model high dimensional heavyta...\"],[\"develop square root graphical models sqr novel class parametric graphical models provides multivaria...\"],[\"consider problem changepoint detection multivariate timeseries multivariate distribution observation...\"],[\"structural equation models sems widely adopted inference causal interactions complex networks recent...\"],[\"consider structure discovery undirected graphical models observational data inferring likely structu...\"],[\"present novel kway highdimensional graphical model called generalized root model grm explicitly mode...\"],[\"introduce truncated gaussian graphical model tggm novel framework designing statistical models nonli...\"],[\"introduce network maximal correlation nmc multivariate measure nonlinear association among random va...\"],[\"contagions spread popular news stories infectious diseases propagate cascades dynamic networks unobs...\"],[\"gaussian graphical models widely used represent conditional dependence among random variables paper ...\"],[\"bayesian networks bns graphical models useful representing highdimensional probability distributions...\"],[\"directed networks pervasive nature engineered systems often underlying complex behavior observed bio...\"],[\"propose methodology explore measure pairwise correlations exist variables dataset methodology levera...\"],[\"introduce novel multivariate random process producing bernoulli outputs per dimension possibly forma...\"],[\"propose communicationefficient distributed estimation inference methods transelliptical graphical mo...\"],[\"recent years seen increasing popularity learning sparse emphchanges markov networks changes structur...\"],[\"present semiparametric spectral modeling complete larval drosophila mushroom body connectome motivat...\"],[\"numerous social medical engineering biological challenges framed graphbased learning tasks propose n...\"],[\"study problem learning latent variables gaussian graphical models existing methods problem assume pr...\"],[\"tick statistical learning library python particular emphasis timedependent models point processes to...\"],[\"study present multiclass graphical bayesian predictive classifier incorporates uncertainty model sel...\"],[\"present novel approach estimating conditional probability tables based joint rather independent esti...\"],[\"model high dimensional data gaussian methods widely used since remain tractable yield parsimonious m...\"],[\"consider graphical model multivariate normal vector associated node underlying graph estimate graphi...\"],[\"bayesian graphical models useful tool understanding dependence relationships among many variables pa...\"],[\"understanding developing correlation measure detect general dependencies imperative statistics machi...\"]],\"hovertemplate\":\"label=graphical\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"graphical\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"graphical\",\"showlegend\":true,\"x\":[-1.8650696,-2.0634894,-2.2802904,-1.8561294,-1.9472476,-1.9083558,-1.9336882,-1.8331395,-1.9062248,-2.221149,0.07558126,0.020673929,0.08031709,-2.172152,0.07495095,-1.9710952,-2.1345263,-1.8533813,-1.9955832,-2.017372,-2.045274,-2.2412658,-2.2478251,-0.18120764,-2.1900046,-1.844632,-1.7907821,-1.8967184,-2.219508,-2.0182776,-1.8452597,-1.8638873,-2.2162743,-2.1848936,-1.8945076,-1.7853962,-2.146248,-2.2927334,0.03354705,-1.8821738,-2.3909256,-2.119385,-1.8572977,-1.8923415,-1.7493237,-1.9538618,-2.3387764,-2.0069342,-1.9875699,-1.960377,-1.8759931,-1.9700859,-2.2301764,-1.8915745,-2.2632601,-2.1519141,0.09253106,-2.336252,-1.8151115,-2.2035131,-1.9747387,-2.0668597,-1.5859487,-2.4043505,-2.214139,-2.176533,0.046130214,-1.9591726,-2.2316916,0.039403915],\"xaxis\":\"x\",\"y\":[7.4309473,7.994954,7.668131,7.384709,7.0335684,7.572698,7.779878,7.4113307,7.8999233,7.7532773,4.710498,4.728,4.735651,7.3356643,4.7058067,7.4078608,8.029693,7.1319985,7.283765,7.7787747,7.164869,7.789051,7.1676335,6.089527,7.595876,7.1114745,7.2219667,7.357219,7.747213,7.2243314,7.098796,7.3946424,7.39142,7.3424797,7.3756986,7.451816,7.7465935,7.483806,4.6267567,7.3661156,7.766265,8.101868,7.2246284,7.755684,7.101766,7.2213826,7.648813,8.02222,7.878175,7.2402883,7.1605577,7.651045,8.155156,7.645953,7.552903,8.10335,4.7012486,7.866785,7.274526,7.747591,7.9797773,8.117608,6.8719316,5.1249595,7.299582,7.306607,4.760237,7.8644857,7.407275,4.6336713],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"many statistical methods proposed estimate causal models classical situations fewer variables observ...\"],[\"describe method inferring linear causal relations among multidimensional variables idea use asymmetr...\"],[\"propose method infer causal structures containing discrete continuous variables idea select causal h...\"],[\"inferring causal structure set random variables finite sample joint distribution important problem s...\"],[\"structural equation models bayesian networks widely used analyze causal relations continuous variabl...\"],[\"discovery nonlinear causal relationship additive nongaussian noise models attracted considerable att...\"],[\"linear nongaussian structural equation model called lingam identifiable model exploratory causal ana...\"],[\"paper addresses problem inferring sparse causal networks modeled multivariate autoregressive mar pro...\"],[\"consider learn causal ordering variables linear nongaussian acyclic model called lingam several exis...\"],[\"important topic systems biology developing statistical methods automatically find causal relations g...\"],[\"consider learning causal ordering variables linear nongaussian acyclic model called lingam several e...\"],[\"causal inference relies structure graph often directed acyclic graph dag different graphs may result...\"],[\"consider problem learning causal directed acyclic graphs observational joint distribution one use gr...\"],[\"consider learning possible causal direction two observed variables presence latent confounding varia...\"],[\"given data sampled number variables one often interested underlying causal relationships form direct...\"],[\"notion causality used many situations dealing uncertainty consider problem whether causality identif...\"],[\"information geometric causal inference igci new approach distinguish cause effect two variables base...\"],[\"paper considers problem estimating structure multiple related directed acyclic graph dag models buil...\"],[\"one fundamental problems causal inference estimation causal effect variables confounded difficult ob...\"],[\"interested learning causal relationships pairs random variables purely observational data effectivel...\"],[\"provide theoretical empirical evidence type asymmetry causes effects present related via linear mode...\"],[\"article contains detailed proofs additional examples related uai submission learning sparse causal m...\"],[\"widely applied approach causal inference nonexperimental time series often referred linear granger c...\"],[\"consider problem structure learning bowfree acyclic path diagrams baps baps viewed generalization li...\"],[\"learning causal effect observational data straightforward possible without assumptions hidden common...\"],[\"causal inference deals identifying random variables cause control random variables recent advances t...\"],[\"controlled interventions provide direct source information learning causal effects particular dosere...\"],[\"present new methods estimate causal effects retrospectively micro data assistance machine learning e...\"],[\"machine learning science discovering statistical dependencies data use dependencies perform predicti...\"],[\"propose new method discovering causal relationships temporal data based notion causal compression en...\"],[\"paper frames causal structure estimation machine learning task idea treat indicators causal relation...\"],[\"study model one target variable correlated vector xxxd predictor variables potential causes describe...\"],[\"paper consider problem fair statistical inference involving outcome variables examples include class...\"],[\"interpretability prediction mechanisms respect underlying prediction problem often unclear several s...\"],[\"introduce new approach functional causal modeling observational data called causal generative neural...\"],[\"consider fundamental problem inferring causal direction two univariate numeric random variables obse...\"],[\"many decisions healthcare business policy domains made without support rigorous evidence due cost co...\"],[\"classical approaches granger causality detection repose upon linear time series assumptions many int...\"],[\"estimation optimal treatment regimes considerable interest precision medicine work propose causal kn...\"],[\"present causal generative neural networks cgnns learn functional causal models observational data cg...\"],[\"causal inference using observational data challenging especially bivariate case minimum description ...\"],[\"study optimal covariate balance causal inferences observational data rich covariates complex relatio...\"],[\"classical approaches granger causality detection assume linear dynamics many interactions realworld ...\"],[\"consider problem learning fair decision systems complex scenarios sensitive attribute might affect d...\"],[\"new causal discovery method structural agnostic modeling sam presented paper leveraging conditional ...\"]],\"hovertemplate\":\"label=causal\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"causal\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"causal\",\"showlegend\":true,\"x\":[1.6730087,1.7077923,1.6974531,1.6901164,1.7422812,1.6938928,1.6832055,1.7590528,1.7117504,1.6519492,1.727449,1.702107,1.7443285,1.7118565,1.7397912,1.7029724,1.6376083,-1.9048666,1.6625074,1.6505511,1.7033259,1.8103795,1.6563958,1.7540543,1.671506,1.6594498,0.9762178,1.5286796,1.3132728,1.7129822,1.7303814,1.7202585,1.4571632,1.4962682,1.6026502,1.6701336,1.3277476,1.6502935,0.86685246,1.7136192,1.6691602,1.5487859,1.4757179,1.3070706,1.7482445],\"xaxis\":\"x\",\"y\":[2.011017,1.9791759,1.9998411,1.9952638,1.9692141,2.005433,1.9665153,2.0514722,1.9764657,2.098296,2.0299168,1.9984746,2.054021,1.9991562,2.015114,1.9813529,2.0186944,7.8463387,1.9805181,1.9989659,1.9579687,2.0848515,2.0148787,2.0516837,1.9827604,1.9917909,2.6243067,2.1034458,2.1480558,2.0205479,1.9718028,2.0140145,2.0903401,1.9785588,2.037253,2.0131156,2.2774992,2.012498,2.7288992,1.9835913,1.9711177,2.0970645,2.1791384,2.1137726,1.9987242],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"introduce new bayesian model hierarchical clustering based prior trees called kingmans coalescent de...\"],[\"introduce pitman yor diffusion tree pydt hierarchical clustering generalization dirichlet diffusion ...\"],[\"present new sequential monte carlo sampler coalescent based bayesian hierarchical clustering model a...\"],[\"hierarchical parametric models consisting observable latent variables widely used unsupervised learn...\"],[\"classical mixture gaussians model related kmeans via smallvariance asymptotics covariances gaussians...\"],[\"hierarchical probabilistic models gaussian mixture models widely used unsupervised learning tasks mo...\"],[\"work consider problem detecting anomalous spatiotemporal behavior videos approach learn normative mu...\"],[\"paper variable selection clustering estimation unsupervised highdimensional setting approach based f...\"],[\"define beta diffusion tree random tree structure set leaves defines collection overlapping subsets o...\"],[\"recent years rank aggregation received significant attention machine learning community goal problem...\"],[\"motivated generating personalized recommendations using ordinal preference data study question learn...\"],[\"paper considers statistical estimation problems probability distribution observed random variable in...\"],[\"gaussian mixture models gmm found many applications density estimation data clustering however model...\"],[\"reciprocating interactions represent central feature human exchanges target various recent experimen...\"],[\"given set pairwise comparisons classical ranking problem computes single ranking best represents pre...\"],[\"hierarchical learning models mixture models bayesian networks widely employed unsupervised learning ...\"],[\"present convex approach probabilistic segmentation modeling time series data approach builds upon re...\"],[\"unsupervised image segmentation aims clustering set pixels image spatially homogeneous regions intro...\"],[\"multivariate normal density monotonic function distance mean ellipsoidal shape due underlying euclid...\"],[\"propose probabilistic modeling framework learning dynamic patterns collective behaviors social agent...\"],[\"observations organized groups commonalties exist amongst dependent random measures ideal choice mode...\"],[\"hierarchical probabilistic models mixture models used cluster analysis models two types variables ob...\"],[\"heavytailed distributions widely used robust mixture modelling due possessing thick tails computatio...\"],[\"propose probabilistic model aggregate answers respondents answering multiplechoice questions model a...\"],[\"show kmeans lloyds algorithm obtained special case truncated variational approximations applied gaus...\"],[\"analyzing underlying structure multiple timesequences provides insights understanding social network...\"],[\"propose bayesian nonparametric mixture model prediction information extraction tasks efficient infer...\"],[\"neymanscott classic example estimation problem partiallyconsistent posterior standard estimation met...\"],[\"bayesian nonparametrics class probabilistic models model size inferred data recently developed metho...\"],[\"paper generalized multivariate studentt mixture model developed classification clustering low probab...\"],[\"compare two statistical models three binary random variables one mixture model product mixtures mode...\"],[\"paper scale mixture normal distributions model developed classification clustering data outliers mis...\"]],\"hovertemplate\":\"label=bayesian\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bayesian\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bayesian\",\"showlegend\":true,\"x\":[-4.047861,-4.003938,-3.792328,-3.766566,-3.7785678,-3.71552,-2.4063523,-3.6670156,-4.072594,-0.9739248,-0.95747423,-3.745818,-3.485234,-0.34237152,-0.960306,-3.7622483,-3.585753,-3.5949242,-3.450907,-3.9869268,-4.0227275,-3.7787902,-3.7357066,-4.1578226,-3.5313804,-4.025748,-3.97484,-3.850656,-3.6690066,-3.7399569,-3.9043903,-3.6886656],\"xaxis\":\"x\",\"y\":[6.3551874,6.4736857,6.321623,5.7791076,6.221025,5.768991,4.759347,6.3712816,6.4561954,6.819542,6.8707485,5.9719286,6.407974,3.673526,6.8753214,6.0771966,6.228922,6.3856883,5.9595327,6.133279,5.949182,6.064249,6.1211343,6.0425525,6.4312463,6.1484194,6.1090565,5.3249373,6.3805647,6.238581,5.915909,6.167026],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"kernel induced random survival forests kirsf statistical learning algorithm aims improve prediction ...\"],[\"predicting individuals risk experiencing future clinical outcome statistical task important conseque...\"],[\"portable wearable wireless electrocardiogram ecg systems potential used pointofcare cardiovascular d...\"],[\"realizations stochastic process often observed temporal data functional data growing interests class...\"],[\"random survival forests rsf powerful method risk prediction rightcensored outcomes biomedical resear...\"],[\"many complex diseases wide variety ways individual manifest disease challenge personalized medicine ...\"],[\"patent lawsuits costly timeconsuming ability forecast patent litigation time litigation allows compa...\"],[\"biological systems often modelled different levels abstraction depending particular aimsresources st...\"],[\"given two possible treatments may exist subgroups benefit greater one treatment problem relevant fie...\"],[\"communitybased question answering cqa sites play important role addressing health information needs ...\"],[\"preterm births occur alarming rate preemies higher risk infant mortality developmental retardation l...\"],[\"technique formal concept analysis applied dataset describing traits rodents goal identifying zoonoti...\"],[\"date instability prognostic predictors sparse high dimensional model hinders clinical adoption recei...\"],[\"datasets containing large samples timetoevent data arising several small heterogeneous groups common...\"],[\"introduce mixture model censored durations cmix develop maximum likelihood inference joint estimatio...\"],[\"consider problem constructing diffusion operators high dimensional data address counterfactual funct...\"],[\"introduce semiparametric bayesian model survival analysis model centred parametric baseline hazard u...\"],[\"vision precision medicine use individual patient characteristics inform personalized treatment plan ...\"],[\"decision makers doctors judges make crucial decisions recommending treatments patients granting bail...\"],[\"proceedings nips workshop interpretable machine learning complex systems held barcelona spain decemb...\"],[\"estimation individual treatment effect observational data complicated due challenges confounding sel...\"],[\"given functional data survival process timedependent covariates derive smooth convex representation ...\"],[\"growing interest applying machine learning methods electronic medical records emr across different i...\"],[\"scenario realtime monitoring hospital patients highquality inference patients health status using in...\"],[\"accurate reliable predictions infectious disease dynamics valuable public health organizations plan ...\"],[\"daytime hypoglycemia accurately predicted achieve normoglycemia avoid disastrous situations hypoglyc...\"],[\"healthcare applications temporal variables encode movement health status longitudinal patient evolut...\"],[\"devising course treatment patient doctors often little quantitative evidence base decisions beyond m...\"],[\"mobile technologies offer opportunities higher resolution monitoring health conditions opportunity s...\"],[\"paper presents systematic review stateoftheart approaches identify patient cohorts using electronic ...\"],[\"disease classification crucial element biomedical research recent studies demonstrated machine learn...\"],[\"quick accurate medical diagnosis crucial successful treatment disease using machine learning algorit...\"],[\"recurrent major mood episodes subsyndromal mood instability cause substantial disability patients bi...\"],[\"prediction disease onset patient survey lifestyle data quickly becoming important tool diagnosing di...\"],[\"assessing heterogeneous treatment effects become growing interest advancing precision medicine indiv...\"],[\"inpatient care large share total health care spending making analysis inpatient utilization patterns...\"],[\"calls arms build interpretable models express wellfounded discomfort machine learning software agent...\"],[\"research interpretability machine learning systems focuses development rigorous notion interpretabil...\"],[\"proceedings nips workshop machine learning developing world held long beach california usa december...\"],[\"proceedings nips symposium interpretable machine learning held long beach california usa december...\"],[\"tremendous interest precision medicine means improve patient outcomes tailoring treatment individual...\"],[\"develop model using deep learning techniques natural language processing unstructured text medical r...\"],[\"lactate threshold considered essential parameter assessing performance elite recreational runners pr...\"],[\"predicting individual risk clinical event using complete patient history still major challenge perso...\"]],\"hovertemplate\":\"label=clinical\\u003cbr\\u003ex1=%{x}\\u003cbr\\u003ex2=%{y}\\u003cbr\\u003edocs_short=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"clinical\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"clinical\",\"showlegend\":true,\"x\":[-0.3699215,0.26312694,0.1820924,0.21044135,-0.023332499,0.26519006,0.037344296,-0.22086641,0.38111955,0.10179719,0.14870234,0.16509093,0.28967595,-4.2946653,0.10223172,0.5435022,-0.11612301,0.48397812,0.43067774,-0.3712067,0.39765292,-0.13095436,0.31531003,0.3481981,0.028137106,0.19871223,0.3624843,0.50010145,0.4619303,0.3182152,0.14319775,0.26389366,0.54446393,0.17650479,0.41954467,0.33685857,-0.35992277,-0.33312905,-0.41631237,-0.37600458,0.5056951,0.34426236,0.23280354,0.29380298],\"xaxis\":\"x\",\"y\":[3.349528,3.4422886,3.606548,3.5779498,3.2009957,3.506697,2.9187784,3.1670146,2.9734707,3.3309753,3.3818142,3.5816035,3.5705304,5.881867,3.6122847,3.0303588,3.674097,3.1227758,3.091684,2.4343612,2.9215744,3.6797812,3.4506235,3.4442894,3.2328107,3.5333884,3.4681365,3.076089,3.6033418,3.4332485,3.5191762,3.5148826,3.7760017,3.4796212,2.948562,3.7040725,2.4513726,2.4211905,2.4302554,2.4419918,3.086663,3.4320536,3.5458605,3.376126],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x1\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x2\"}},\"legend\":{\"title\":{\"text\":\"label\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Category: stat.ML\",\"font\":{\"size\":20}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('91eb554c-fe05-4da3-97a1-672f94035a06');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 124
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimize Hyperparameters (UMAP & HDBSCAN)\n"
      ],
      "metadata": {
        "id": "LcWSXEQCcYLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Generate parameter vectors\n",
        "def grid(param_grid):\n",
        "\n",
        "  param_combinations = list(itertools.product(*param_grid.values()))\n",
        "  param_list = [dict(zip(param_grid.keys(), values)) for values in param_combinations]\n",
        "\n",
        "  return param_list\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:50.000931Z",
          "iopub.status.idle": "2025-05-13T21:01:50.001287Z",
          "shell.execute_reply.started": "2025-05-13T21:01:50.001121Z",
          "shell.execute_reply": "2025-05-13T21:01:50.001137Z"
        },
        "id": "xuF6e74ccYLy"
      },
      "outputs": [],
      "execution_count": 106
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    \"st\": [\"all-MiniLM-L6-v2\", \"BAAI/bge-base-en-v1.5\"], #embedding model (SentenceTransformer)\n",
        "    \"nn\": [2, 15, 20],             # umap_n_neighbors\n",
        "    \"cs\": [15, 20, 25],            # hdbscan_min_cluster_size\n",
        "    \"ts\": [10, 25, 50],            # min_topic_size\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:50.002012Z",
          "iopub.status.idle": "2025-05-13T21:01:50.002359Z",
          "shell.execute_reply.started": "2025-05-13T21:01:50.002205Z",
          "shell.execute_reply": "2025-05-13T21:01:50.002216Z"
        },
        "id": "1HIe2d8McYLz"
      },
      "outputs": [],
      "execution_count": 107
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: If you would like to proceed with Hyperparameter tuning, uncomment the last line of code"
      ],
      "metadata": {
        "id": "A2Bp25b8hHPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "def GridSearch(param_grid):\n",
        "\n",
        "    param_combinations = grid(param_grid)\n",
        "    print(\"Total combinations:\", len(param_combinations))\n",
        "\n",
        "    best_score = -1\n",
        "    best_params = {}\n",
        "\n",
        "    for params in param_combinations:\n",
        "\n",
        "        embedding_model = SentenceTransformer(params[\"st\"])\n",
        "        print(\"emb done\")\n",
        "        topic_model = BERTopic(verbose=False,\n",
        "           embedding_model=embedding_model,\n",
        "           # umap_model=UMAP(n_components=2, n_neighbors=params[\"nn\"], min_dist=0.0, metric='cosine'),\n",
        "           hdbscan_model=HDBSCAN(min_cluster_size=params[\"cs\"], metric='euclidean', cluster_selection_method='eom', prediction_data=True),\n",
        "           representation_model={\"KeyBERT\": KeyBERTInspired()},\n",
        "           min_topic_size=params[\"ts\"],\n",
        "           top_n_words=10\n",
        "        )\n",
        "        topics, probs = topic_model.fit_transform(abstracts)\n",
        "\n",
        "        # Get embeddings and labels\n",
        "        embeddings = embedding_model.encode(abstracts, convert_to_tensor=False)\n",
        "        labels = topic_model.get_document_info(abstracts)['Topic'].values\n",
        "\n",
        "        # Compute silhouette score\n",
        "        score = silhouette_score(embeddings, labels, metric='euclidean')\n",
        "        output = \"Silhouette Score:\" + str(score) + \"| Params:\" + str(params)\n",
        "        print(output)\n",
        "        print(topic_model.get_topic_info()[[\"Topic\", \"Count\", \"Name\"]])\n",
        "\n",
        "        #========================= Visualization ====================================\n",
        "        keybert_labels = [label[0][0].split(\"\\n\")[0] for label in topic_model.get_topics(full=True)[\"KeyBERT\"].values()]\n",
        "\n",
        "        # Get document info\n",
        "        document_info = topic_model.get_document_info(abstracts)\n",
        "        document_info[\"KeyBERT\"] = document_info[\"KeyBERT\"].apply(lambda x: x[0])\n",
        "        all_labels = document_info[\"KeyBERT\"]\n",
        "\n",
        "        df_plot = pd.DataFrame({\n",
        "            \"x1\": [point[0] for point in reduced_embeddings],\n",
        "            \"x2\": [point[1] for point in reduced_embeddings],\n",
        "            \"docs\": abstracts,\n",
        "            \"label\": all_labels\n",
        "        })\n",
        "        df_plot[\"docs_short\"] = df_plot[\"docs\"].str[:100] + \"...\"\n",
        "\n",
        "        fig = px.scatter(df_plot, x=\"x1\", y=\"x2\", color=\"label\", hover_data=[\"docs_short\"])\n",
        "        fig.show()\n",
        "        #==========================================================================\n",
        "\n",
        "        # Update best score and parameters if current score is higher\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = {\n",
        "                'n_neighbors': params[\"nn\"],\n",
        "                # 'n_components': params[\"nc\"],\n",
        "                'min_cluster_size': params[\"cs\"]\n",
        "            }\n",
        "\n",
        "        print(\"=======================================================================================================\\n\")\n",
        "    print(\"best_score\", best_score)\n",
        "    print(\"best_params\", best_params)\n",
        "# GridSearch(param_grid)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-13T21:01:50.005917Z",
          "iopub.status.idle": "2025-05-13T21:01:50.00626Z",
          "shell.execute_reply.started": "2025-05-13T21:01:50.006058Z",
          "shell.execute_reply": "2025-05-13T21:01:50.006073Z"
        },
        "id": "9EeMkWWJcYLz"
      },
      "outputs": [],
      "execution_count": 108
    }
  ]
}